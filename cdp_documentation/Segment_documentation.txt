=== Content from https://segment.com/docs/connections/spec/native-mobile-spec/ ===

What is the native mobile spec?
        

The 
Native Mobile Spec
 is a common blueprint for the mobile user lifecycle. The Spec outlines the most important events for mobile apps to be tracking and automatically collects many of these events with the Segment Android and iOS SDKs.

This article outlines common questions about our Native Mobile Spec. To learn what the benefits are of the feature, check out our 
blog
. For technical set up and documentation, see our 
spec docs
.

How does the Native Mobile Spec help me?

Smaller SDK: Move more destinations to the server-side, including the Facebook App Events destination.

Less engineering time: Automatically collect key user events instead of coding them in yourself.

Faster time to value: Set up your destinations with key metrics like Daily Active Users, sessions, and in-app purchases right away!

Measure ROI of campaigns: Analyze campaign performance with spec’d events like “campaign hit”, “install attributed” and “push notification opened” in your favorite analytics or BI tool.

Which destinations currently take advantage of the mobile spec?

Our 
Facebook App Events
 cloud-mode destination currently takes advantage of the “Application Installed” event to power new features like custom audience creation, dynamic ads and conversion tracking, without needing to sit on the device. Soon, more destinations like Google Adwords and Salesforce Marketing Cloud and attribution providers will offer similar functionality.

How does all of this work?

When an engineer installs the SDK, the SDK will automatically register on iOS and Android operation system handlers.

When the application is foregrounded on the phone, our SDK will be called and emit the Application Opened event. Similarly, when the user updates the app, on the next app open, the iOS and Android operation system will tell us and we’ll emit a track event called Application Updated event. In-app purchases will trigger Order Completed, etc.

How do I opt-in to the new feature?

This feature is opted out by default. You have to opt in to collect these events as mentioned in our Quick Start guides (
iOS
, 
Android
). You’ll be doing this in code by altering the configuration you pass into the SDK initialization methods (telling the SDK to collect these events automatically).

What happens if I’m already tracking these events? Will they be double counted?

Yes, they will be double counted, but that’s only if you opt into this feature. You can either remove your own tracking code for these events or not opt into auto collection at all.

Do I still benefit from this new SDK if I opt out of automatic tracking?

Yes. If you follow the Spec when you write your own custom events, you will be able to take advantage of certain features in downstream destinations on the server-side, like with our 
Facebook App Events destination
.

Will I need to change the names of the events I am currently tracking?

We recommend migrating to these event names if you’re tracking similar events so that you can take advantage of available features in our integrations which will depend on the spec as they become available.

Can I send custom properties inside of automatic events?

Not currently.

Is there a way to link the old event name with the new event name?

Not currently. 
Contact us
 for alternative options.

Can I do this later?

You can, but the sooner you switch to the spec’d events, the further back you’ll be able to look in your reporting with the same event name!

How will I be able to take advantage of new campaign events?

In the coming months, we’ll be updating our mobile marketing destinations to automatically capture campaign events around attribution, deep linking, and push notifications. These events will go to 
destinations
, including 
warehouses
.

Why don’t Push Notification events reach Segment when my Android App is backgrounded?

Android applications can’t receive Push Notifications when the process is not running, and when apps are put into background they are eligible to have their Process killed when there is memory pressure. For more more on Android processes, view Android’s 
Processes and app lifecycle documentation
.

Segment tracks messages delivered to the application. So if the process has been killed for any reason, messages won’t be delivered.

Why do Push Notifications work with Firebase Cloud Messaging when the app is backgrounded?

Firebase Cloud Messaging (FCM) has its own servers. When an FCM message is created and sent to a device, that message travels through FCM servers and is delivered to the local FCM client device that is part of Android OS. This FCM client is almost always running, even when the app is backgrounded. From there, the FCM message is intended to be delivered to a local application. If the app process is running, the message gets delivered. Otherwise, the user must tap a notification that starts the app and delivers the FCM message.

For more context on how this process works, view the 
Firebase FCM architecture documentation
.

This page was last modified: 25 Sep 2023

Need support?

Questions? Problems? Need more info? Contact Segment Support for assistance!

Help improve these docs!

Was this page helpful?

Thanks for your feedback!

Can we improve this doc? 
Send us feedback!

Get started with Segment

Was this page helpful?

Thanks for your feedback!

Can we improve
 this doc? 
Send us feedback!

Product

For Developers

Company

Support

© 2025 Segment.io, Inc.

=== Content from https://segment.com/docs/connections/storage/warehouses/redshift-useful-sql/ ===

Useful SQL Queries for Redshift
        

On this page

Below you’ll find a library of some of the most useful SQL queries customers use in their Redshift warehouses. You can run these in your Redshift instance with little to no modification.

If you’re looking to improve the speed of your queries, check out Segment’s 
Speeding Up Redshift Queries
 page.

You can use SQL queries for the following tasks:

If you’re looking for SQL queries for warehouses other than Redshift, check out some of Segment’s 
Analyzing with SQL guides
.

Tracking events

The Track call allows you to record any actions your users perform. A Track call takes three parameters: the userId, the event, and any optional properties.

Here’s a basic Track call:

analytics
.
track
(
'
Completed Order
'
,

  
item
:
 
'
pants
'
,

  
color
:
 
'
blue
'

  
size
:
 
'
32x32
'

  
payment
:
 
'
credit card
'


});



A completed order Track call might look like this:

analytics
.
track
(
'
Completed Order
'
,
 
{

 
item
:
 
'
shirt
'
,

 
color
:
 
'
green
'

 
size
:
 
'
Large
'

 
payment
:
 
'
paypal
'


});



Each Track call is stored as a distinct row in a single Redshift table called 
tracks
. To get a table of your completed orders, you can run the following query:

select
 
*


from
 
initech
.
tracks


where
 
event
 
=
 
'completed_order'



That SQL query returns a table that looks like this:



But why are there columns in the table that weren’t a part of the Track call, like 
event_id
? 
This is because the Track method (for client-side libraries) includes additional properties of the event, like 
event_id
, 
sent_at
, and 
user_id
!

Grouping events by day

If you want to know how many orders were completed over a span of time, you can use the 
date()
 and 
count
 function with the 
sent_at
 timestamp:

select
 
date
(
sent_at
)
 
as
 
date
,
 
count
(
event
)


from
 
initech
.
tracks


where
 
event
 
=
 
'completed_order'


group
 
by
 
date



That query returns a table like this:

To see the number of pants and shirts that were sold on each of those dates, you can query that using case statements:

select
 
date
(
sent_at
)
 
as
 
date
,


sum
(
case
 
when
 
item
 
=
 
'shirt'
 
then
 
1
 
else
 
0
 
end
)
 
as
 
shirts
,


sum
(
case
 
when
 
item
 
=
 
'pants'
 
then
 
1
 
else
 
0
 
end
)
 
as
 
pants


from
 
initech
.
tracks


where
 
event
 
=
 
'completed_order'


group
 
by
 
date



That query returns a table like this:

Define sessions

Segment’s API does not impose any restrictions on your data with regard to user sessions.

Sessions aren’t fundamental facts about the user experience. They’re stories Segment builds around the data to understand how customers actually use the product in their day-to-day lives. And since Segment’s API is about collecting raw, factual data, there’s no API for collecting sessions. Segment leaves session interpretation to SQL partners, which let you design how you measure sessions based on how customers use your product.

For more on why Segment doesn’t collect session data at the API level, 
check out a blog post here
.

How to define user sessions using SQL

Each of Segment’s SQL partners allow you to define sessions based on your specific business needs. With 
Looker
, for example, you can take advantage of their persistent derived tables and LookML modeling language to layer sessionization on top of your Segment SQL data. Segment recommends 
checking out Looker’s approach here
.

To define sessions with raw SQL, a great query and explanation comes from 
Mode Analytics
.

Here’s the query to make it happen, but read Mode Analytics’ 
blog post
 for more information. Mode walks you through the reasoning behind the query, what each portion accomplishes, how you can tweak it to suit your needs, and the kinds of further analysis you can add on top of it.

-- Finding the start of every session


SELECT
 
*

  
FROM
 
(

       
SELECT
 
*

              
LAG
(
sent_at
,
1
)
 
OVER
 
(
PARTITION
 
BY
 
user_id
 
ORDER
 
BY
 
sent_at
)
 
AS
 
last_event

        
FROM
 
"your_source"
.
tracks

      
)
 
last


WHERE
 
EXTRACT
(
'EPOCH'
 
FROM
 
sent_at
)
 
-
 
EXTRACT
(
'EPOCH'
 
FROM
 
last_event
)
 
>=
 
(
60
 
*
 
10
)

   
OR
 
last_event
 
IS
 
NULL



-- Mapping every event to its session


SELECT
 
*
,

       
SUM
(
is_new_session
)
 
OVER
 
(
ORDER
 
BY
 
user_id
,
 
sent_at
 
ROWS
 
BETWEEN
 
UNBOUNDED
 
PRECEDING
 
AND
 
CURRENT
 
ROW
)
 
AS
 
global_session_id
,

       
SUM
(
is_new_session
)
 
OVER
 
(
PARTITION
 
BY
 
user_id
 
ORDER
 
BY
 
sent_at
 
ROWS
 
BETWEEN
 
UNBOUNDED
 
PRECEDING
 
AND
 
CURRENT
 
ROW
)
 
AS
 
user_session_id

  
FROM
 
(

       
SELECT
 
*
,

              
CASE
 
WHEN
 
EXTRACT
(
'EPOCH'
 
FROM
 
sent_at
)
 
-
 
EXTRACT
(
'EPOCH'
 
FROM
 
last_event
)
 
>=
 
(
60
 
*
 
10
)
 
                     
OR
 
last_event
 
IS
 
NULL
 
                   
THEN
 
1
 
ELSE
 
0
 
END
 
AS
 
is_new_session

         
FROM
 
(

              
SELECT
 
*
,

                     
LAG
(
sent_at
,
1
)
 
OVER
 
(
PARTITION
 
BY
 
user_id
 
ORDER
 
BY
 
sent_at
)
 
AS
 
last_event

                
FROM
 
"your_source"
.
tracks

              
)
 
last

       
)
 
final



Identify users

Historical traits

The Identify method ties user attributes to a 
userId
.

analytics
.
identify
(
'
bob123
'
,{

  
email
:
 
'
bob@initech.com
'
,

  
plan
:
 
'
Free
'


});



As these user traits change over time, you can continue calling the Identify method to update their changes. With this query, you can update Bob’s account plan to “Premium”.

analytics
.
identify
(
'
bob123
'
,
 
{

  
email
:
 
'
bob@initech.com
'
,

  
plan
:
 
'
Premium
'


});



Each Identify call is stored in a single Redshift table called 
identifies
. To see how a user’s plan changes over time, you can run the following query:

select
 
email
,
 
plan
,
 
sent_at


from
 
initech
.
identifies


where
 
email
 
=
 
'bob@initech.com'



This SQL query returns a table of Bob’s account information, with each entry representing the state of his account at different time periods:

If you want to see what your users looked like at a previous point in time, you can find that data in the 
identifies
 table. To get this table for your users, replace ‘initech’ in the SQL query with your source slug.

If you only want the current state of the users, convert the 
identifies
 table into a 
distinct users table
 by returning the most recent Identify call for each account.

Convert the identifies table into a users table

The following query returns the 
identifies
 table:

select
 
*


from
 
initech
.
identifies



That query returns a table like this:

If all you want is a table of distinct user with their current traits and without duplicates, you can do so with the following query:

with
 
identifies
 
as
 
(

  
select
 
user_id
,

         
email
,

         
plan
,

         
sent_at
,

         
row_number
()
 
over
 
(
partition
 
by
 
user_id
 
order
 
by
 
sent_at
 
desc
)
 
as
 
rn

  
from
 
initech
.
identifies


),


users
 
as
 
(

  
select
 
user_id
,
 
         
email
,

         
plan

  
from
 
identifies

  
where
 
rn
 
=
 
1


)



select
 
*


from
 
users



Counts of user traits

Let’s say you have an 
identifies
 table that looks like this:

If you want to query the traits of these users, you first need to 
convert the identifies table into a users table
. From there, run a query like this to get a count of users with each type of plan:

with
 
identifies
 
as
 
(

  
select
 
user_id
,

         
email
,

         
plan
,

         
sent_at
,

         
row_number
()
 
over
 
(
partition
 
by
 
user_id
 
order
 
by
 
sent_at
 
desc
)
 
as
 
rn

  
from
 
initech
.
identifies


),


users
 
as
 
(

  
select
 
plan

  
from
 
identifies

  
where
 
rn
 
=
 
1


)



select
 
sum
(
case
 
when
 
plan
 
=
 
'Premium'
 
then
 
1
 
else
 
0
 
end
)
 
as
 
premium
,

       
sum
(
case
 
when
 
plan
 
=
 
'Free'
 
then
 
1
 
else
 
0
 
end
)
 
as
 
free


from
 
users



And there you go: a count of users with each type of plan!

Groups to accounts

Historical Traits

The 
group
 method ties a user to a group. It also lets you record custom traits about the group, like the industry or number of employees.

Here’s what a basic 
group
 call looks like:

analytics
.
group
(
'
0e8c78ea9d97a7b8185e8632
'
,
 
{

  
name
:
 
'
Initech
'
,

  
industry
:
 
'
Technology
'
,

  
employees
:
 
329
,

  
plan
:
 
'
Premium
'


});



As these group traits change over time, you can continue calling the group method to update their changes.

analytics
.
group
(
'
0e8c78ea9d97a7b8185e8632
'
,
 
{

  
name
:
 
'
Initech
'
,

  
industry
:
 
'
Technology
'
,

  
employees
:
 
600
,

  
plan
:
 
'
Enterprise
'


});



Each group call is stored as a distinct row in a single Redshift table called 
groups
. To see how a group changes over time, you can run the following query:

select
 
name
,
 
industry
,
 
plan
,
 
employees
,
 
sent_at


from
 
initech
.
groups


where
 
name
 
=
 
'Initech'



The previous query will return a table of Initech’s group information, with each entry representing the state of the account at different times.

If you want to see a group’s traits at a previous point in time, this query is useful (To get this table for your groups, replace ‘initech’ with your source slug).

If you only want to see the most recent state of the group, you can convert the groups table into a distinct groups table by viewing the most recent groups call for each account.

Converting the Groups Table into an Organizations Table

The following query will return your groups table:

select
 
*


from
 
initech
.
groups



The previous query returns the following table:

However, if all you want is a table of distinct groups and current traits, you can do so with the following query:

with
 
groups
 
as
 
(

  
select
 
name
,

         
industry
,

         
employees
,

         
plan
,

         
sent_at
,

         
row_number
()
 
over
 
(
partition
 
by
 
name
 
order
 
by
 
sent_at
 
desc
)
 
as
 
rn

  
from
 
initech
.
groups


),


organizations
 
as
 
(

  
select
 
name
,

         
industry
,

         
employees
,

         
plan

  
from
 
groups

  
where
 
rn
 
=
 
1


)



select
 
*


from
 
organizations



This query will return a table with your distinct groups, without duplicates.

This page was last modified: 07 Feb 2025

Need support?

Questions? Problems? Need more info? Contact Segment Support for assistance!

Help improve these docs!

Was this page helpful?

Thanks for your feedback!

Can we improve this doc? 
Send us feedback!

Get started with Segment

On this page

Was this page helpful?

Thanks for your feedback!

Can we improve
 this doc? 
Send us feedback!

Product

For Developers

Company

Support

© 2025 Segment.io, Inc.

=== Content from https://segment.com/docs/segment-app/extensions/dbt/ ===

dbt Extension
        

On this page

Segment’s dbt extension lets you use 
Reverse ETL
 with your existing dbt labs models and syncs to help centralize model management and versioning, reduce redundancies, and run CI checks to prevent breaking changes.

With Segment’s dbt extension, you can:

This page explains how to set up a dbt Model and then use the model with Reverse ETL.

Before you begin

Keep the following in mind as you set up the dbt extension:

Set up Git dbt Models and dbt Cloud

To set up the dbt extension, you’ll need:

Git repository and dbt Models setup

Follow these steps to connect the Git repository that stores your dbt Models:

After you’ve saved your setup, you can configure your Git repository’s settings to your needs by changing the repository, branch, dbt version, default schema, and project path.

dbt Cloud setup

You can also use dbt Cloud to schedule Reverse ETL syncs after a dbt Cloud job successfully runs.

To set up dbt Cloud:

Adding a custom subdomain

By default, dbt sets the subdomain to cloud. To identify your custom subdomain, open your URL and copy the portion before 
.getdbt.com
. For example, if your domain was 
https://subdomain.getdbt.com/
, your subdomain would be 
subdomain
.

dbt Cloud Webhooks

The dbt Cloud integration allows you to schedule Reverse ETL syncs based on a dbt Cloud job.  When a dbt Cloud job is selected under the Reverse ETL scheduling section, Segment creates a webhook in the dbt Cloud account that will initiate to run the Reverse ETL sync when the job is scheduled.

In order to create the webhook, ensure that you have webhook permissions associated with the dbt Cloud token in the previous step.

Model syncs

After you set up dbt, Segment runs an initial sync to load models from your connected Git repository. This initial sync lets you use the most recent models when you set up Reverse ETL. In addition to Segment’s initial dbt sync, you can also trigger manual dbt model syncs.

Use a model with Reverse ETL

After you’ve successfully set up dbt with a warehouse and connected to your Git repository, you can select dbt models for use with Reverse ETL by following these steps:

To change a connected model, ensure that you’ve removed it from all active Reverse ETL syncs.

Git Connections

Git Connections enable Segment to sync data with your preferred Git repository through supported like SSH and token-based authentication.

Git Sync and the dbt integration operate independently. You don’t need to set up Git Sync to use dbt, and dbt Cloud can trigger its own syncs without relying on Git Sync.

Supported connection types

Segment supports the following credential types for setting up a Git Connection:

Reusing Git Connections

Segment lets you set up multiple Git Connections, allowing you to reuse credentials across both dbt and Git Sync. You can either use the same credential for multiple configurations or create separate Git Connections for each product and environment as needed.

If you plan to reuse a Git token across both dbt and Git Sync, ensure it has the necessary read and write permissions for both integrations.

Setting Up CI checks

CI check availability

CI checks are available only with the GitHub App connection.

CI checks in Segment help prevent breaking changes to active dbt models. Avoid changing dbt models currently in use with an active Reverse ETL sync, since changes could disrupt existing mappings and active syncs.

When CI checks are enabled, Segment monitors model changes in your Git repository. If a model already linked to an active Reverse ETL sync gets modified, Segment automatically rejects the change to maintain data integrity.

To enable CI Checks, authorize a GitHub App credential for your Git connection. Once connected, you can enable CI Checks in the dbt model sync configuration section.

Troubleshooting dbt Extensions

The following table lists common dbt Extension errors, as well as their solutions:

This page was last modified: 11 Dec 2024

Need support?

Questions? Problems? Need more info? Contact Segment Support for assistance!

Help improve these docs!

Was this page helpful?

Thanks for your feedback!

Can we improve this doc? 
Send us feedback!

Get started with Segment

On this page

Was this page helpful?

Thanks for your feedback!

Can we improve
 this doc? 
Send us feedback!

Product

For Developers

Company

Support

© 2025 Segment.io, Inc.

=== Content from https://segment.com/docs/config-api/ ===

Redirecting…

=== Content from https://segment.com/docs/connections/reverse-etl/setup/ ===

Set up Reverse ETL
        

On this page

There are four components to Reverse ETL: Sources, Models, Destinations, and Mappings.



Follow these 4 steps to set up Reverse ETL:

Step 1: Add a source

In Reverse ETL, a source is your data warehouse.

You need to be a user that has both read and write access to the warehouse.

To add your warehouse as a source:

Step 2: Add a model

Models define sets of data you want to sync to your Reverse ETL destinations. A source can have multiple models. Segment supports 
SQL models
 and 
dbt models
.

SQL editor

dbt model

Use Segment’s dbt extension to centralize model management and versioning. Users who set up a BigQuery, Databricks, Postgres, Redshift, or Snowflake source can use Segment’s 
dbt extension
 to centralize model management and versioning, reduce redundancies, and run CI checks to prevent breaking changes.

If you use dbt Cloud with Reverse ETL, you can 
create up to 5 mappings
 that use the sync strategy 
dbt Cloud
, which extracts data from your warehouse and syncs it with your destination after a job in dbt Cloud is complete.

Step 3: Add a destination

In Reverse ETL, destinations are the business tools or apps you use that Segment syncs the data from your warehouse to. A model can have multiple destinations.

Reverse ETL supports the destinations in the 
Reverse ETL catalog
. If the destination you want to send data to is not listed in the Reverse ETL catalog, use the 
Segment Connections Destination
 to send data from your Reverse ETL warehouse to your destination.

Engage users can use the 
Segment Profiles Destination
 to create and update 
Profiles
 that can then be accessed through 
Profile API
 and activated within 
Twilio Engage
.

Separate endpoints and credentials required to set up third party destinations

Before you begin setting up your destinations, note that each destination has different authentication requirements. See the documentation for your intended destination for more details.

To add your first destination:

Step 4: Create mappings

Mappings enable you to map the data you extract from your warehouse to the fields in your destination. A destination can have multiple mappings.

To create a mapping:

Use Segment’s 
Duplicate mappings
 feature to create an exact copy of an existing mapping. The copied mapping has the same configurations and enrichments as your original mapping.

Supported object and arrays

When you set up destination actions in Reverse ETL, depending on the destination, some 
mapping fields
 may require data as an 
object
 or 
array
.

You can send data to a mapping field that requires object data. An example of object mapping is an 
Order completed
 model with a 
Products
 column that’s in object format.

Example:

    
{

        
"product"
:
 
{

            
"id"
:
 
0001
,

            
"color"
:
 
"pink"
,

            
"name"
:
 
"tshirt"
,

            
"revenue"
:
 
20
,

            
"inventory"
:
 
500

        
}

    
}



To send data to a mapping field that requires object data, you can choose between these two options:

Certain object mapping fields have a fixed list of properties they can accept. If the names of the nested properties in your object don’t match with the destination properties, the data won’t send. Segment recommends you to use 
Customize Object
 to ensure your mapping is successful.

To send data to a mapping field that requires array data, the model must provide data in the format of an array of objects. An example is an 
Order completed
 model with a 
Product purchased
 column that’s in an array format.

Example:

    
[

    
{

        
"currency"
:
 
"USD"
,

        
"price"
:
 
40
,

        
"productName"
:
 
"jacket"
,

        
"purchaseTime"
:
 
"2021-12-17 23:43:47.102"
,

        
"quantity"
:
 
1

    
},

    
{

        
"currency"
:
 
"USD"
,

        
"price"
:
 
5
,

        
"productName"
:
 
"socks"
,

        
"quantity"
:
 
2

    
}

    
]



To send data to a mapping field that requires array data, you can choose between these two options:

Certain array mapping fields have a fixed list of properties they can accept. If the names of the nested properties in your array don’t match the destination properties, the data won’t send. Segment recommends you to use the 
Customize array
 option to ensure your mapping is successful.

Objects in an array don’t need to have the same properties. If a user selects a missing property in the input object for a mapping field, the output object will miss the property.

Null value management

You can choose to exclude null values from optional mapping fields in your syncs to some destinations. Excluding null values helps you maintain data integrity in your downstream destinations, as syncing a null value for an optional field may overwrite an existing value in your downstream tool.

For example, if you opt to sync null values with your destination and an end user fills out a form but chooses to leave an optional telephone number field blank, the existing telephone number you have on file in your destination could be overwritten with the null value. By opting out of null values for your downstream destination, you would preserve the existing telephone number in your destination.

By default, Segment syncs null values from mapped fields to your downstream destinations. Some destinations do not allow the syncing of null values, and will reject requests that contain them. Segment disables the option to opt out of syncing null values for these destinations.

To opt out of including null values in your downstream syncs:

Initial sync for a given mapping

After you’ve set up your source, model, destination, and mappings for Reverse ETL, your data will extract and sync to your destination(s) right away if you chose an interval schedule. If you set your data to extract at a specific day and time, the extraction will take place then.

Edit Reverse ETL syncs

Edit your model

To edit your model:

Suggested mappings

Suggested mappings is fully available for RETL mappings.

Segment offers suggested mappings that automatically propose relevant destination fields for model columns and payload elements. For example, if your model includes a column or payload field named 
transaction_amount
, the feature might suggest mapping it to a destination field like 
Amount
 or 
TransactionValue
. This automation, powered by intelligent autocompletion, matches and identifies near-matching field names to streamline the mappings setup process. For more information, see 
Segment’s suggested mappings blog post
 and the 
Suggested Mappings Nutrition Facts Label
.

Review the suggested mappings for accuracy before finalizing them, as Segment can’t guarantee all of the suggested mappings are accurate.

Edit your mapping

To edit your mapping:

This page was last modified: 18 Dec 2024

Need support?

Questions? Problems? Need more info? Contact Segment Support for assistance!

Help improve these docs!

Was this page helpful?

Thanks for your feedback!

Can we improve this doc? 
Send us feedback!

Get started with Segment

On this page

Was this page helpful?

Thanks for your feedback!

Can we improve
 this doc? 
Send us feedback!

Product

For Developers

Company

Support

© 2025 Segment.io, Inc.

=== Content from https://segment.com/docs/unify/traits/sql-traits/ ===

SQL Traits
        

Unify Plus requires a business tier account and is included with Engage


See the 
available plans
, or 
contact Support
.

On this page

SQL Traits End of Sale

SQL Traits entered End of Sale as of March 31, 2024. Existing Segment customers will continue to have access to SQL Traits, but Segment will no longer offer SQL Traits to new customers. Segment recommends using 
Reverse ETL
 to sync your data into Segment.

Use SQL Traits to import user or account traits from your data warehouse back into Unify or Engage to build audiences or to enhance data that you send to other Destinations.

SQL Traits are only limited by the data in your warehouse. Because anything you can write a query for can become a SQL Trait, you can add detail to your user and account profiles, resulting in more nuanced personalization.

This unlocks some interesting possibilities to help you meet your business goals.

Check out Segment’s 
SQL Traits blog post
 for more customer case studies.

To view SQL Traits in a user profile, you must have 
PII access
. Without this access, Segment redacts all SQL traits in a profile.

Note that after you bring in data with SQL Traits, changing data types for fields may not be compatible with all destinations.

Example: cloud sources sync

SQL Traits allow you to import data from 
object cloud sources
 like Salesforce, Stripe, Zendesk, Hubspot, Marketo, Intercom, and more. For example, bring in Salesforce Leads or Accounts, Zendesk ticket behavior, or Stripe LTV calculations.

The two examples below show SQL queries you can use to retrieve cloud-source information from your warehouse.

Salesforce lead import

If you want to import data from the Salesforce leads and contacts table, you can use SQL similar to the following query:

    
select
 
external_id_c
 
as
 
user_id
,

    
lead_score_c
,

    
lead_age_c
,

    
lead_status

    
-- …more properties

    
from
 
salesforce
.
leads



Has Open Ticket in Zendesk

This query computes whether a user has an open ticket:

    
select
 
distinct
 
u
.
external_id
 
as
 
user_id
,
 
true
 
as
 
has_open_ticket

    
from
 
zendesk
.
tickets
 
t

    
join
 
zendesk
.
users
 
u

    
on
 
u
.
id
 
=
 
t
.
requester_id

    
where
 
t
.
status
 
in
 
(
'pending'
,
'open'
,
'hold'
,
'new'
)



Comparing trait types

View the table below to better understand how Segment collects custom, computed, and SQL traits.

You can use the Profile explorer (
Unify > Profile explorer
) to view traits attached to a profile.

Configure SQL Traits

To use SQL Traits, you need the following:

Step 1. Set up a warehouse source

Segment supports Redshift, Postgres, Snowflake, Azure SQL, and BigQuery as data warehouse sources for SQL Traits. Note that the BigQuery setup process 
requires
 a service user.

Safeguard your data

For any warehouse, Segment recommends that you create a separate read-only user for building SQL Traits.

If you don’t already have a data warehouse, use one of the following guides to get started:

To connect BigQuery to Segment SQL Traits, follow these instructions to create a service account for Segment to use:

Navigate to the Google Developers Console.

Click the drop down to the left of the search bar and select the project that you want to connect.

Note
: If you don’t see the project you want in the menu, click the account switcher in the upper right corner, and verify that you’re logged in to the right Google account for the project.

Click the menu in the upper left and select 
IAM & Admin
, then 
Service accounts
.

Click 
Create Service Account
.

Give the service account a name like 
segment-sqltraits
.

Under 
Project Role
, add 
only
 the 
BigQuery Data Viewer
 and 
BigQuery Job User
 roles.

IMPORTANT: Do not add any other roles to the service account. Adding other roles can prevent Segment from connecting to the account.

Click 
Create Key
.

Select 
JSON
 and click 
Create
.

A file with the key is saved to your computer. Save this; you’ll need it to set up the warehouse source in the next step.

You’re now ready to create a new BigQuery warehouse source, upload the JSON key you just downloaded, and complete the BigQuery setup.

Step 2. Add the warehouse as a Source

Once your warehouse is up and running, follow these steps:

Navigate to the Engage settings (Engage > Engage Settings > Warehouse Sources), and click 
Add Warehouse Source
.

Select the type of warehouse you’re connecting.

In the next screen, provide the connection credentials, and click 
Save
.

If you’re connecting a BigQuery warehouse, use the JSON key file that you downloaded as the last step.

Create a SQL Trait

Before you create a SQL Trait, you must first preview it to validate your query. If you’re new to SQL, try out one of the templates Segment offers.

Preview the SQL Trait

From the Audiences viewer, go to the Computed Traits tab, and click 
New Computed Trait
. Next, choose SQL, and click 
Configure
. Select the data warehouse that contains the data you want to query.

If you’re sending data from 
object cloud sources
 to your warehouse, the SQL Traits UI has some pre-made templates you can try out.

When you’re building your query, keep the following requirements in mind for the data your query returns.

A successful preview returns a sample of users and their traits.
If Segment recognizes a user already in Engage, it displays a green checkmark on their profile. Clicking the checkmark displays the user’s profile. If a user has a question mark, Segment hasn’t detected this 
user_id
 in Engage before.

Configure SQL Trait options

Once you’re ready to import the SQL Trait, select the Destinations to which you want to send the data.  If you prefer to build Engage audiences directly from the data instead of sending it to a Destination, click 
Skip
.

Give your SQL Trait a descriptive name. If you’re importing multiple Traits, use a name like “Zendesk Traits”. The Trait names you use in audience-building or in your downstream tools correspond to the column names from the query.

If you’re building Engage audiences from this data, select “Compute without enabled destinations”.

Click 
Create Computed Trait
 to save the Trait.

Check 
Compute without destinations
 if you only want to send to Engage.

When you create a SQL Trait, Segment runs the query on the warehouse twice a day by default. You can customize the time at which Segment queries the data warehouse and  the frequency, up to once per hour, from the SQL Trait’s settings.

For each row (user or account) in the query result, Engage sends an identify or group call with all the columns that were returned as Traits. For example, if you write a query that returns 
user_id, has_open_ticket, num_tickets_90_days, avg_zendesk_rating_90days
 Segment sends an identify call with the following payload:

    
{

      
type
:
 
'identify'
,

      
userId
:
 
'u123'
,

      
traits
:
 
{

        
has_open_ticket
:
 
true
,

        
num_tickets_90_days
:
 
3
,

        
avg_zendesk_rating_90_days
:
 
8

      
}

    
}



FAQs

Is there a limit to the result set that can be queried and imported?

Yes. The result set is capped at 25 million rows.

How often does Segment query the customer’s data warehouse?

For each SQL Trait you create, you can set a compute schedule to query the data warehouse up to once per hour. Your query may run at any given time during the hour you select.

What identifiers can I use to query a list?

You can query based on 
email
, 
user_id
, or 
anonymous_id
. If Segment doesn’t locate a match based on the chosen identifier, it creates a new profile. See more below.

Can I use SQL Traits to create users in Segment? Or do SQL Traits only append Traits to existing users?

Yes. The Engage engine sends an identify call if there is no match between the identifier you chose and an existing record. When this happens, Segment creates a new user profile. This identify call takes place in the back-end and doesn’t show up in your Debugger.

Does Engage send identify/track/group calls on every run?

No. Engage only sends an identify/track/group call if the values in a row have changed from previous runs.

I have a large (1M+) query of users to import, should I be worried?

If you’re importing a large list of users and traits, you’ll need to consider your API call usage as well as volume among the partners receiving your data. These vary depending on Segment’s partners, 
contact support
 for more information.

Is there a limit on the size of a SQL Trait’s payload?

Yes, Segment limits request sizes to a maximum of 16KB. Records larger than this are discarded.

Do SQL Traits support arrays?

No, SQL Traits supports string and numeric data types. You can cast arrays as a comma-separated string. In this case, if you used this trait to build an audience, you could check if the array contains a certain value with the “contains” operator, but the value is sent to any connected destinations as a string.

Can I change the Warehouse Source after a SQL trait has been created?

After a SQL trait has been created, you can’t change its Warehouse Source. You’ll need to create a new trait if you want to change the Warehouse source.

What happens if a user is no longer returned by the SQL trait?

If a user was present in one computation, but it is no longer present in the following one, the SQL trait will detect this difference and nullify all trait values for the user. 
Contact Segment
 if you have a use case which calls for an exemption from this default behavior.

Troubleshooting

I’m getting a permissions error.

You might encounter a 
permission denied for schema
 error, like the following:

Segment usually displays this error because you’re querying a schema and table that the current user cannot access. To check the table privileges for a specific grantee (user), view the credentials of the stored warehouse user.

To grant access to a table, an admin usually needs to grant access to both a schema and table through the following similar commands:

    
GRANT
 
USAGE
 
ON
 
SCHEMA
 
ecommerce
 
TO
 
segment_user
;

    
GRANT
 
SELECT
 
ON
 
TABLE
 
ecommerce
.
users
 
TO
 
segment_user
;



Learn more about granting permissions using the following links:

I’m seeing a maximum columns error.

Segment supports returning only 25 columns. 
Contact Segment
 with a description of your use case if you need access to more than 25 columns.

I’m seeing a duplicate 
user_id
 error.

Each query row must correspond to a unique user. Segment displays this error if it detects multiple rows with the same 
user_id
. Use a 
distinct
 or 
group by
 statement to ensure that each row has a unique user_id.

I’m seeing some users/accounts in my preview with question marks. What does that mean?

Question marks in previews indicate one of two things:

1. Segment doesn’t recognize this 
user_id
/
group_id
 in Engage.

In this case, for sources connected to Engage, Segment hasn’t received any event (for example, identify, track, or page) with this 
user_id
. This could still be a legitimate 
user_id
 for a number of reasons, but before syncing, make sure you rule out option two (below), as sending a different identifier as the 
user_id
 can corrupt your identity graph.

2. You have the wrong 
user_id
 column.

You might be returning a value for 
user_id
 that’s inconsistent with how you track 
user_id
 elsewhere. Some customers want to return 
email
 as the 
user_id
, or a partner’s tool ID as the 
user_id
. These conflict with Segment best practices and corrupt the identity graph if you then track 
user_id
 differently elsewhere in your apps.

If you see only question marks in the preview, and have already tracked data historically with Segment, then you likely have the wrong column. If your cloud source doesn’t have the database 
user_id
, Segment recommends using a 
JOIN
 clause with an internal users table before sending the results back to Segment.

Why do some SQL Trait settings not have the “Compute schedule” option?

Segment added the compute schedule feature on Feb 8, 2021, so traits created prior to this date will not have this option. If your trait lacks this feature, recreating it will make it available.

Why doesn’t the value of a SQL trait show in a user profile after a successful sync?

Check that you’ve configured the identifier that uniquely identifies users in a SQL query (
user_id
, 
anonymous_id
, 
email
, or 
group_id
 for account traits) in Identity Resolution settings as an identifier. This ensures the trait is added to the user’s profile with the correct identifier. If you don’t configure the identifier in Identity Resolution settings, the trait’s value is not added to the user profile.

Why doesn’t the identifier updated by a SQL trait show the correct value found in the column?

Ensure that the name given to the SQL trait is not the same name as the identifier or column name from the query. To use SQL traits to update an identifier, the identifier will need to be a column in the query of your SQL trait. The column name in the query of the SQL trait should be the one that Identity Resolution uses to generate the identifier.

Are there any errors in the browser’s Network or Console tab?

If you experience issues saving the SQL Trait query or previewing the results of the SQL Trait query, open the browser’s Console and Network tabs to see if any errors occurred upon clicking the Save/Preview buttons. If you find any errors, please expand the error and take a screenshot of it. You can then share these details when creating a support ticket.

Why can’t I see error messages in SQL traits while other users can?

To see error messages in SQL traits, you will need to have PII Access.

If I edit the SQL Trait query, when will that edit apply those changes?

The SQL Trait edit will apply to its next scheduled computational run. If the edit was made too closely to its next scheduled run, then its changes will be applied to the subsequent scheduled run, at which point you’ll see those updates reflected on its user’s profiles.

If I request a resync for my SQL Trait, when will that resync run?

The SQL Trait resync will apply to its next scheduled computational run. If the resync was made too closely to its next scheduled run, then its changes will be applied to the subsequent scheduled run, at which point you’ll see those updates reflected on its user’s profiles.

This page was last modified: 08 Aug 2024

Need support?

Questions? Problems? Need more info? Contact Segment Support for assistance!

Help improve these docs!

Was this page helpful?

Thanks for your feedback!

Can we improve this doc? 
Send us feedback!

Get started with Segment

On this page

Was this page helpful?

Thanks for your feedback!

Can we improve
 this doc? 
Send us feedback!

Product

For Developers

Company

Support

© 2025 Segment.io, Inc.

=== Content from https://segment.com/docs/privacy/consent-management/ ===

Consent Management Overview
        

Consent Management is available to customers on Business tier plans.


See the 
available plans
, or 
contact Support
.

When an end user visits your web or mobile app, they set 
consent preferences
, or make decisions about the types of data they want you to collect, use, and share. These consent preferences are typically presented as a set list of categories that describe how your company intends to use that data. Some common categories include personalization, advertising, and site performance.

Segment integrates with your commercial third-party or bespoke consent management platform (CMP) that captures an end user’s consent preferences and enforces those preferences by only routing events to the categories consented to by an end user.



After a user sets their consent preferences on your web or mobile app, Segment requires you to add the 
consent object
 to all events. If you are using OneTrust, Segment provides a wrapper for your web and mobile libraries that can add the consent object to your events. If you are using another CMP, you are required to add the consent object to your events by either creating your own wrapper or using another mechanism. For more information, see the 
Configure Consent Management documentation
.

The events, stamped with the consent object, are then sent downstream to any destinations in categories that an end user consented to share data with.

Segment collects consent for both registered users and anonymous users.

For more information about consent in Segment Connections, see the 
Consent in Segment Connections
 documentation.

If you are a Unify user, you can also see the 
Consent in Unify
 for more information about the Segment Consent Preference Updated event, which Segment uses to add consent preference to the Profile.

This page was last modified: 30 May 2024

Need support?

Questions? Problems? Need more info? Contact Segment Support for assistance!

Help improve these docs!

Was this page helpful?

Thanks for your feedback!

Can we improve this doc? 
Send us feedback!

Get started with Segment

Was this page helpful?

Thanks for your feedback!

Can we improve
 this doc? 
Send us feedback!

Product

For Developers

Company

Support

© 2025 Segment.io, Inc.

=== Content from https://segment.com/docs/guides/ignore-bots/ ===

Internet Bots
        

On this page

What’s a bot?

If you stumbled onto this page by accident and don’t know what a bot is or are just curious to learn more, the following Wikipedia article provides an awesome summary: 
https://en.wikipedia.org/wiki/Internet_bot
.

Surprisingly, more than half of all web traffic is made up of bots. While a fraction of them are good bots with a regulated pattern, and therefore beneficial to all online businesses, the majority of them have malicious intents and are mostly unregulated.

Is it possible to ignore bad bots?

Segment doesn’t offer an out-of-the-box solution to filter or ignore bot traffic.

As such, you generally have two options:

Handle the filtering at a destination-level:
 Some of Segment’s destination partners, 
like Mixpanel
, filter bots automatically. Whereas others 
such as Hubspot
 allow you to set up bot filtering manually. The advantage of filtering bots at a destination level is that it allows you to implement a robust, easy-to-maintain solution. However, as it pertains to Segment, the downside is that bot traffic will 
still
 make it to Segment, 
affecting your MTU count.

Write custom logic that suppresses bot activity from being sent to Segment:
 if you want to prevent bot traffic from making it to Segment in the first place, another option is to write your own custom code. The logic, in pseudo-code, would look something like this if you know a particular characteristic of the bot traffic to filter out, such as the userAgent:

var
 
robots
 
=
 
[
useragent1
,
 
useragent2
]


if
 
!
 
window
.
navigator
.
userAgent
 
in
 
robots

  
// send analytics calls

 
 
analytics
.
track



The benefit here is that you would be able to limit the impact that bots have on your MTU count. On the flip side, it’s much harder to implement and maintain a custom filter.

If I see a massive MTU spike because of bots, can I apply for a refund?

As a matter of policy, Segment doesn’t provide refunds for bot-related MTU spikes, as bot traffic is out of Segment’s control. However for extenuating circumstances, 
you can petition for a refund
, assuming you’re able to provide proof of the bot’s effect.

I’m seeing a lot of browser traffic from Boardman; is that from Segment or a bot?

Segment uses Amazon’s hosting services, which are based in Boardman, Oregon. However 
many bots also originate from AWS in Boardman as well
.

One way you can confirm whether or not traffic is coming from Segment vs. a bot is to check the userAgent of the inbound call. Segment’s is:

'
Mozilla/5.0 (
'
 
+
 
deviceModel
.
slice
(
0
,
 
-
3
)
 
+
 
'
; CPU 
'
 
+
 
osName
 
+
 
'
 
'
 
+


osVersion
.
replace
(
/
\.
/g
,
 
'
_
'
)
 
+
 
'
 like Mac OS X) AppleWebKit/600.1.4 (KHTML,
like Gecko) Version/
'
 
+
 
osVersion
.
charAt
(
0
)
 
+
 
'
.0 Mobile/10B329 Safari/8536.25
'



This page was last modified: 28 Oct 2022

Need support?

Questions? Problems? Need more info? Contact Segment Support for assistance!

Help improve these docs!

Was this page helpful?

Thanks for your feedback!

Can we improve this doc? 
Send us feedback!

Get started with Segment

On this page

Was this page helpful?

Thanks for your feedback!

Can we improve
 this doc? 
Send us feedback!

Product

For Developers

Company

Support

© 2025 Segment.io, Inc.

=== Content from https://segment.com/docs/connections/reverse-etl/manage-retl/ ===

Manage Reverse ETL Syncs
        

On this page

View your sync history, reset your syncs, or subscribe to alerts.

Sync overview

The Reverse ETL sync overview tab, located under 
Connections > Destinations
, gives you an overview of your latest Reverse ETL syncs.



You can view the following information about each sync:

Sync history

Check the status of your data extractions and see details of your syncs. Click into failed records to view additional details on the error, sample payloads to help you debug the issue, and recommended actions.

To check the status of your extractions:

Automatic retry handling

Automatic retry handling might not yet be available in your workspace

To ensure overall system stability and performance, Segment is releasing automatic retry handling to all workspaces in a phased rollout program. Segment expects this feature to be available to all customers by January 31, 2025.

Segment automatically retries events that were extracted from your data warehouse but failed to load for up to 14 days or 5 syncs following a partially successful sync or a sync failure.

Segment checks for the latest changes in your data before loading the failed records on a subsequent (automatically scheduled or manually triggered) sync to ensure the data loaded into Segment isn’t stale and only the latest version of the data is loaded to destination. If the error causing the load failure is coming from an upstream tool, you can fix the error in the upstream tool to resolve the load error on a subsequent sync.

Syncs with intervals less than or equal to two hours may not see failed events on the sync immediately following failed record

Syncs with intervals less than or equal to two hours may not see failed events right away, as Segment’s internal systems take up to two hours to retry events that initially failed.

Reset syncs

Reverse ETL uses the Unique Identifier column to detect data changes, like new, updated, and deleted records. If you encounter an error, you can reset Segment’s tracking of this column and force Segment to manually add all records from your dataset.

To reset a sync:

Cancel syncs

You can cancel syncs when your sync is currently running during the extraction and load phase.

To cancel a sync:

Your canceled syncs with have a status as 
Canceled,
 and any syncs that are in the process of being canceled will have a status of 
Canceling
.

Once you cancel a sync, the record count under 
Extraction Results
 reflects the records already processed. These records won’t be included in future syncs. To reprocess these records, you can reset or replay the sync.

Replays

You can choose to replay syncs. To replay a specific sync, contact 
friends@segment.com
. Keep in mind that triggering a replay resyncs all records for a given sync.

Alerting

You can opt in to receive email, Slack, and in-app alerts about Reverse ETL sync failures and fluctuations in the volume of events successfully delivered to your mapping.

The notification channels that you select for one alert will apply to all alerts in your workspace.

Failed or partially successful syncs

To subscribe to alerts for a failed or partially successful sync:

If you opted to receive notifications by email, you can click 
View active email addresses
 to see the email addresses that are currently signed up to receive notifications.

Mapping-level successful delivery rate fluctuations

You can create an alert that notifies you when the volume of events successfully received by your mapping in the last 24 hours falls below a percentage you set. For example, if you set a percentage of 99%, Segment notifies you if your destination had a successful delivery rate of 98% or below.

To receive a successful delivery rate fluctuation alert in a Slack channel, you must first create a Slack webhook. For more information about Slack webhooks, see Slack’s 
Sending messages using incoming webhooks
 documentation.



To subscribe to alerts for successful delivery fluctuations at the mapping level:

To edit or disable your alert, navigate to your mapping’s Alerts tab and select the Actions menu for the alert you’d like to edit.

This page was last modified: 12 Dec 2024

Need support?

Questions? Problems? Need more info? Contact Segment Support for assistance!

Help improve these docs!

Was this page helpful?

Thanks for your feedback!

Can we improve this doc? 
Send us feedback!

Get started with Segment

On this page

Was this page helpful?

Thanks for your feedback!

Can we improve
 this doc? 
Send us feedback!

Product

For Developers

Company

Support

© 2025 Segment.io, Inc.

=== Content from https://segment.com/docs/unify/traits/recommended-items/ ===

Recommended Items
        

Unify Plus requires a business tier account and is included with Engage


See the 
available plans
, or 
contact Support
.

On this page

With Recommended Items, you can add personalized item recommendations as a 
computed trait
 to each user profile.

Based on a user’s past interactions, this trait generates a list of up to 5 items, like products, articles, or songs, that each user is most likely to engage with.

Segment designed Recommended Items for cases where you want to personalize experiences, like email content, in-app recommendations, or website suggestions, to fit each user’s unique preferences.

On this page, you’ll learn how Recommended Items works, how to create a Recommended Item trait, and best practices to get the most out of your recommendations.

.

How Recommended Items works

Recommended Items uses your interaction events (like 
order_completed
, 
product_added
, and 
product_searched
) along with event metadata to generate personalized recommendations for each user. Here’s an overview of the process:

Once Segment attaches the recommendation array to a profile, you can use it to:

Exclusion rules

Exclusion rules let you filter out specific items from recommendations, helping keep suggestions relevant and valuable. For example, you could use them to remove items a user has already purchased or exclude products above a certain price.

There are two types of exclusion rules:

Create a Recommended Items trait

Before you begin

Before you create Recommended Item traits, you’ll first need to set up a Recommendation Catalog. The catalog setup process involves mapping your interaction events and providing product metadata to support recommendations. If you haven’t yet set up your Recommendation Catalog, follow the steps in the 
Product Based Audiences documentation
.

To create a Recommended Item trait:

Segment begins creating your new trait. This process could take up to 48 hours.

Example use case: personalized album recommendations

Suppose you’re managing a music streaming app and want to give each user personalized music recommendations based on their listening habits.

Here’s how you could configure this trait:

By setting up a trait like this, each user profile now includes personalized recommendations that reflect individual tastes. You can use these recommendations across a range of touchpoints, like in-app sections, personalized email content, or targeted messaging, to create a more engaging and customized user experience.

Best practices

Keep the following in mind as you work with Recommended Items:

This page was last modified: 13 Feb 2025

Need support?

Questions? Problems? Need more info? Contact Segment Support for assistance!

Help improve these docs!

Was this page helpful?

Thanks for your feedback!

Can we improve this doc? 
Send us feedback!

Get started with Segment

On this page

Was this page helpful?

Thanks for your feedback!

Can we improve
 this doc? 
Send us feedback!

Product

For Developers

Company

Support

© 2025 Segment.io, Inc.

=== Content from https://segment.com/docs/connections/oauth/ ===

OAuth 2.0
        

OAuth 2.0 is available to customers on Business tier plans.


See the 
available plans
, or 
contact Support
.

On this page

OAuth 2.0 is an online authorization standard that uses tokens to grant access to API resources like Segment’s tracking API. You can use OAuth 2.0 as a security requirement for connections to third-party tools.

Permissions

Depending on your workspace permissions, your access to OAuth apps is limited.

Create an OAuth app

You must have already created a workspace in Segment to use OAuth.

To create a new OAuth application:

Enter the configuration settings:

Once you create your OAuth app, you can now connect a source to your OAuth app.

Connect a source to OAuth

OAuth only supports server-side sources. See the list of 
supported sources
.

To connect a source to OAuth:

To disconnect your source from OAuth, click 
Disconnect
.

Enable a source to OAuth

Once you’ve connected your source to OAuth, you can enable it. To enable your source:

To disable your source from OAuth, turn the toggle off for 
Enable OAuth
.

Obtain the access token

You can obtain an access token once you create an OAuth application and enable a source to OAuth.

Access tokens are only valid within a region. The supported regional authorization servers are:

To obtain the access token:

Create a JWT token with the header and payload as below:

Header

 {
     "alg":"RS256", 
     "typ":"JWT", 
     "kid":"<<KID>>"
 }


Payload

 {
     "iss":"<<ISS>>",
     "sub":"<<SUB>>",
     "aud":"<<AUD>>", 
     "iat":"<<IAT>>",
     "exp":"<<EXP>>",
     "jti":"<<JTI>>"
 }


Send a form-url-encoded 
POST
 request to the regional authorization server’s 
\token
 route with the following parameters:

 grant_type=client_credentials
 client_assertion_type=urn:ietf:params:oauth:client-assertion-type:jwt-bearer
 client_assertion=<<JWT>>
 scope=<<SCOPE>>


To use the access token, see an example of how to use the access token in the 
HTTP API source
.

Edit an OAuth application

To edit an existing OAuth application:

Delete an OAuth app

To delete an OAuth app, you must remove all connected sources from the app.

To delete an OAuth app:

Revoke a token

When security incidents expose access tokens, you can revoke your access token. To revoke a token:

Supported sources

OAuth 2.0 currently supports these sources:

Supported scopes

OAuth 2.0 currently supports these scopes:

Tracking API scopes

Source Functions scopes

Public API scopes

This page was last modified: 03 Sep 2024

Need support?

Questions? Problems? Need more info? Contact Segment Support for assistance!

Help improve these docs!

Was this page helpful?

Thanks for your feedback!

Can we improve this doc? 
Send us feedback!

Get started with Segment

On this page

Was this page helpful?

Thanks for your feedback!

Can we improve
 this doc? 
Send us feedback!

Product

For Developers

Company

Support

© 2025 Segment.io, Inc.

=== Content from https://segment.com/docs/engage/user-subscriptions/subscription-sql/ ===

Subscriptions with SQL Traits
        

Engage Premier requires a Business tier account and includes Engage Foundations and Unify.


See the 
available plans
, or 
contact Support
.

On this page

Engage Premier entered an End of Sale (EOS) period effective  June 10, 2024. Existing Segment customers will continue to have access and support to Engage Premier until an end-of-life (EOL) date is announced. We recommend exploring the following pages in preparation of a migration or future MCM needs:

Twilio Marketing Campaigns

Preferred ISV Partners:

Airship Blog
 


Bloomreach Blog
 


Braze Blog
 


Insider Blog
 


Klaviyo Blog
 


Twilio Engage Foundations Documentation
 

Use Subscriptions with SQL Traits to connect to your data warehouse and query user subscription data to Engage on a scheduled basis. Use your data warehouse as a single source of truth for subscription statuses and query from warehouses such as BigQuery, Redshift, or Snowflake.

On this page, you’ll learn how to use SQL to sync subscription data from your warehouse to Engage.

Updating subscription statuses with SQL creates new profiles and updates existing profiles. These profile updates may lead to users entering existing audiences or message campaigns.

Getting started

To use Subscriptions with SQL Traits, you need the following:

Segment supports Redshift, Postgres, Snowflake, Azure SQL, and BigQuery as data warehouse sources for SQL Traits. Visit 
Segment’s warehouse docs
 for more on getting started with data warehouses.

Before you begin with Subscriptions with SQL Traits, you may also want to visit the 
subscription docs
 to learn more about user subscriptions in Engage.

Sync subscription data with SQL

You can sync with SQL from two locations in the Segment app. Navigate to 
Unify > Profile explorer
 or 
Engage > Audiences > Profile explorer
, then:

Configure your SQL query

To configure Subscriptions with SQL Traits, you can write your own query or click 
Use Template
 to use one of the templates Engage provides. For any new users that your query returns, Engage adds a new profile.

Click 
Reset Template
 to reset your SQL query.

Queries must return at least one pair of the columns below with a value of 
subscribed
, 
unsubscribed
, or 
did_not_subscribe
:

For more subscription SQL best practices, view the 
query requirements
 below.

Select a warehouse and preview your query

Once you write your SQL query, click 
Select warehouse
 from the Configure screen to select the data warehouse you’d like to query.

Before you schedule your sync intervals, click 
Preview
 to preview a subset of data and validate your results. To see subscription statuses for a particular profile, select a user row, then select the Identities tab.

Schedule sync intervals

You can schedule sync intervals to import subscription data from your warehouse to Engage:

SQL queries are executed directly to your data warehouse, which could generate additional costs for pay-per-query warehouses.

View SQL job history

Use the Update History page to view all SQL jobs.

From the Update History page, you can view details for each SQL job including the creation date and time, compute status, and the number of users updated across all runs for a job. Click the job link to visit a particular SQL job Overview page.

Query requirements

When you build your SQL query, keep the following requirements in mind for the data your query returns.

Your query must:

Your query must not:

This page was last modified: 15 Jul 2024

Need support?

Questions? Problems? Need more info? Contact Segment Support for assistance!

Help improve these docs!

Was this page helpful?

Thanks for your feedback!

Can we improve this doc? 
Send us feedback!

Get started with Segment

On this page

Was this page helpful?

Thanks for your feedback!

Can we improve
 this doc? 
Send us feedback!

Product

For Developers

Company

Support

© 2025 Segment.io, Inc.

=== Content from https://segment.com/docs/api/config-api/api-design/ ===

API Design
        

On this page

The Segment Public API is available

Segment’s 
Public API
 is available for Team and Business tier customers to use. You can use the Public API and Config APIs in parallel, but moving forward any API updates will come to the Public API exclusively. 

Please contact your account team or 
friends@segment.com
 with any questions.

API Evolution: Versioning and Compatibility

Segment strives to maintain a strict contract around the stability of our APIs once they reach maturity.

The Config API is versioned statically by path prefix. The current version, 
/v1beta
, is stable and suitable for production use, but not 100% mature. We will not ship any backwards incompatible breaking changes after its public launch on December 12, 2018. However, we may release new versions of the API as further 
/v1betaX
 releases or as major version releases (such as 
/v1
).

The reason the path is currently 
/v1beta
 is to signify both that Segment’s product model is evolving and that we are looking for feedback from developers on how best to expose that model for public consumption, extension, and automation.

Therefore, 
/v1beta
 is subject to deprecation with 3 months of written notice after the release of 
/v1
, should we decide to land any breaking changes in the final 
/v1
 release.

We’re always actively seeking feedback on the ergonomics and breadth of our APIs. If you have ideas of how we can improve them that you’d like to see as extensions to 
/v1beta
 or considered for changes in 
/v1
, don’t hesitate to 
get in touch
.

Common Methods and Fields

The Config API is a set of REST APIs for managing Segment resources. The primary Segment resources are:

You can manage each resource using standard methods:

Errors

Pagination

The List method is paginated. Requests take an optional 
page_size
 parameter which generally defaults to 10 or 100 items. If there are more than 
page_size
 items in a collection, the response includes a 
next_page_token
 field. You can then supply this as 
page_token
 parameter to the next List request. If there are no more items in the collection, 
next_page_token
 will be empty.

Update Mask

Update methods follow PATCH semantics. In addition to the data to update, the method requires an explicit set of field names to update. This removes all ambiguity around empty values.

$ 
curl 
-X
 PATCH 
\

  
-h
 
"Authorization: Bearer 
$ACCESS_TOKEN
"
 
\

  
-d
 
'{
	"destination": {
		"enabled": true,
    "update_mask": {
		  "paths": [
  			"destination.enabled"
	  	]
    }
  }'

  
'https://platform.segmentapis.com/v1beta/workspaces/myworkspace/sources/js/destinations/google-analytics'



Note that specifying an update path with no value will change the field to an empty value (e.g. “”, 0, or false). For example this will result in 
enabled
 false:

$ 
curl 
-X
 PATCH 
\

  
-h
 
"Authorization: Bearer 
$ACCESS_TOKEN
"
 
\

  
-d
 
'{
	"destination": {
    "update_mask": {
		  "paths": [
  			"destination.enabled"
	  	]
    }
  }'

  
'https://platform.segmentapis.com/v1beta/workspaces/myworkspace/sources/js/destinations/google-analytics'



And note that specifying data but no paths will not change the field value. For example this has no effect:

$ 
curl 
-X
 PATCH 
\

  
-h
 
"Authorization: Bearer 
$ACCESS_TOKEN
"
 
\

  
-d
 
'{
	"destination": {
    "enabled": true
  }'

  
'https://platform.segmentapis.com/v1beta/workspaces/myworkspace/sources/js/destinations/google-analytics'



Resource Names

Resources are named entities exposed by our services and APIs, and resource names are their identifiers. Each resource has its own unique resource name made up of the ID or slug of the resource itself and the IDs or slugs of any parent resources.

So a given Source resource might look like:

workspaces/<customer-workspace-slug>/sources/<customer-source-slug>

Segment APIs use scheme-less URIs for resource names. This allows us to generally follow REST URL conventions and provides a machine and human-legible standard for our identifiers.

Segment favors “slugs” over opaque IDs when the resource is customer- or Segment-named and unlikely to change, such as sources and destinations. For resources whose name is subject to change frequently, such as Tracking Plans, we will autogenerate prefixed IDs for you upon creation and use that in the fully qualified resource name.

While full resource names resemble normal URLs, they are not the same thing. A single resource may be exposed by different API versions, API protocols, or API network endpoints as Segment’s APIs evolve.

This page was last modified: 07 Sep 2022

Need support?

Questions? Problems? Need more info? Contact Segment Support for assistance!

Help improve these docs!

Was this page helpful?

Thanks for your feedback!

Can we improve this doc? 
Send us feedback!

Get started with Segment

On this page

Was this page helpful?

Thanks for your feedback!

Can we improve
 this doc? 
Send us feedback!

Product

For Developers

Company

Support

© 2025 Segment.io, Inc.

=== Content from https://segment.com/docs/connections/aws-privatelink/ ===

Amazon Web Services PrivateLink
        

On this page

Amazon Web Services’ PrivateLink
 is an AWS service that provides private connectivity between VPCs without exposing traffic to the public Internet. Keeping traffic in the Amazon network reduces the data security risk associated with exposing your Warehouse traffic to the Internet.

Segment’s PrivateLink integration is currently in private beta and is governed by Segment’s 
First Access and Beta Preview Terms
. You might incur additional networking costs while using AWS PrivateLink.

You can configure AWS PrivateLink for 
Databricks
, 
RDS Postgres
, 
Redshift
, and 
Snowflake
. Only warehouses located in regions 
us-east-1
, 
us-east-2
, 
us-west-2
, or 
eu-west-1
 are eligible.

Usage limits for each customer during the AWS PrivateLink Private Beta include the following:

Databricks

The following Databricks integrations support PrivateLink:

Segment recommends reviewing the Databricks documentation before attempting AWS PrivateLink setup

The setup required to configure the Databricks PrivateLink integration requires front-end and back-end PrivateLink configuration. Review the 
Databricks documentation on AWS PrivateLink
 to ensure you have everything required to set up this configuration before continuing.

Prerequisites

Before you can implement AWS PrivateLink for Databricks, complete the following prerequisites in your Databricks workspace:

Implement PrivateLink for Databricks

To implement Segment’s PrivateLink integration for Databricks:

RDS Postgres

The following RDS Postgres integrations support PrivateLink:

Prerequisites

Before you can implement AWS PrivateLink for RDS Postgres, complete the following prerequisites:

Implement PrivateLink for RDS Postgres

To implement Segment’s PrivateLink integration for RDS Postgres:

Redshift

The following Redshift integrations support PrivateLink:

Prerequisites

Before you can implement AWS PrivateLink for Redshift, complete the following prerequisites:

Implement PrivateLink for Redshift

To implement Segment’s PrivateLink integration for Redshift:

Snowflake

The following Snowflake integrations support PrivateLink:

Prerequisites

Before you can implement AWS PrivateLink for Snowflake, complete the following prerequisites:

Implement PrivateLink for Snowflake

To implement Segment’s PrivateLink integration for Snowflake:

This page was last modified: 09 Jan 2025

Need support?

Questions? Problems? Need more info? Contact Segment Support for assistance!

Help improve these docs!

Was this page helpful?

Thanks for your feedback!

Can we improve this doc? 
Send us feedback!

Get started with Segment

On this page

Was this page helpful?

Thanks for your feedback!

Can we improve
 this doc? 
Send us feedback!

Product

For Developers

Company

Support

© 2025 Segment.io, Inc.

=== Content from https://segment.com/docs/getting-started/use-cases/setup// ===

Use Cases Setup
        

On this page

Use Cases help you onboard quickly and efficiently to Segment by guiding you through specific steps tailored to your business needs.

This page walks you through the steps to set up a use case in your Segment instance.

Permissions

To implement a use case, you’ll need to be a Workspace Owner for your Segment account. See the 
Roles
 documentation for more information.

You can onboard to Segment with a Use Case if you’re a new Business Tier customer or haven’t yet connected a source and destination.

Use case setup overview

From a high level, setting Segment up with a use case takes place in four stages:

Example setup: Personalize winback

This section provides a detailed, step-by-step guide to setting up the 
Personalize Winback
 use case from the 
Personalize communications and product experiences
 business goal in your Segment account. All use cases follow this same setup flow.

Step 1: Navigate to Use Cases

Step 2: Pick your business goal and select a use case

Choosing a use case

Segment lets you implement one use case. If you’re not sure which use case to choose, view 
Choosing a Use Case
.

For most cases, you’ll want to start with development or staging sources to test and debug your Segment implementation. This approach lets you verify that everything is working correctly before sending live data downstream. To facilitate this, Segment automatically creates development (dev) and production (prod) spaces for you and labels your sources accordingly to simplify tracking.

Segment strongly recommends beginning your setup in the dev environment. This allows for thorough testing and debugging of your configuration. Once you’re confident in your dev setup, Segment will guide you on how to apply these configurations to your live production sources.

Step 3: Review suggested events

Changing your use case

Once you’ve reviewed the suggested events for a use case, you won’t be able to change the use case. If you want to see a full breakdown of each use case before commiting to one, click 
Change use case
 to begin the use case flow again. You can also view the 
Use Cases Reference guide
 to see what Segment recommends for each use case.

On the 
Setup checklist
 page, you’ll see the full checklist for the use case you’ve chosen. This checklist applies to all use cases, though the suggested events, sources, and destinations differ between use cases.

This table shows Segment’s recommended events and properties for the Personalize winback use case:

Make sure that you’re tracking these events to get the most of the Personalize winback campaign. For more information on event tracking, see 
Data Collection Best Practices
.

Step 4: Connect dev sources

You’re now ready to connect sources to your dev environment.

Adding a warehouse as a souce

If you connect a warehouse as a source, Segment automatically creates a Profiles destination that shows up in the 
Connect your data
 tab. Do not delete this destination, as Segment requires this destination to create profiles from your warehouse.

Cloud object sources

If you connect a cloud object source, you’ll need to create a warehouse to sync profiles into Segment. For more information, see 
Cloud Sources
.

Step 5: Connect dev destinations

With sources connected, you can now connect destinations to your dev environment.

Step 6: Publish your setup to a prod environment

Until this point, you’ve set up event tracking and connected sources and destinations to a development environment.

After you’ve confirmed that data is flowing from your sources into your destinations as expected, you’re ready to publish your setup to a production environment.

Your data is now in production, and you’ve successfully configured Segment.

Activate your data with Unify and Engage

Now that you’ve successfully set up Connections and Destinations, you can build upon your Segment implementation with Unify and Engage.

Accessing Unify and Engage

Unify and Engage may not yet be enabled for your account. To add Engage to your Segment workspace, click 
Request a demo
 in the Unify and Engage tabs on the Guided Setup page.

Step 1: Set up identifiers with Unify

Your identifiers are now set up in your dev space, though it could take a few minutes for Segment to create profiles from your selected identifiers.

For more information, see the 
Unify documentation
.

Step 2: Create audiences with Engage

Segment then begins sending your new audience(s) to the destinations in your dev environment. Verify in those destinations that the audiences are coming through as intended, then click 
Mark complete
.

For more information on Audiences, see the 
Engage documentation
.

Step 3: Republish to a prod environment

At this point, you’ll have already published your initial setup to a prod environment. Next, you’ll publish your Unify and Engage setup to the same prod environment.

Segment then begins sending your new audience(s) to the destinations in your dev environment. Verify in those destinations that your audiences are coming through as intended, then click 
Mark complete
.

Next steps

Use Cases pulls together a number of core Segment features, like 
Sources
, 
Destinations
, 
data collection
, and 
Reverse ETL
. View the documentation for each to learn how you can continue to expand and build on what you’ve alreay achieved.

This page was last modified: 08 Oct 2024

Need support?

Questions? Problems? Need more info? Contact Segment Support for assistance!

Help improve these docs!

Was this page helpful?

Thanks for your feedback!

Can we improve this doc? 
Send us feedback!

Get started with Segment

On this page

Was this page helpful?

Thanks for your feedback!

Can we improve
 this doc? 
Send us feedback!

Product

For Developers

Company

Support

© 2025 Segment.io, Inc.

=== Content from https://segment.com/docs/engage/onboarding/ ===

Twilio Engage Premier Onboarding Guide
        

Engage Premier requires a Business tier account and includes Engage Foundations and Unify.


See the 
available plans
, or 
contact Support
.

On this page

Engage Premier entered an End of Sale (EOS) period effective  June 10, 2024. Existing Segment customers will continue to have access and support to Engage Premier until an end-of-life (EOL) date is announced. We recommend exploring the following pages in preparation of a migration or future MCM needs:

Twilio Marketing Campaigns

Preferred ISV Partners:

Airship Blog
 


Bloomreach Blog
 


Braze Blog
 


Insider Blog
 


Klaviyo Blog
 


Twilio Engage Foundations Documentation
 

Twilio Engage brings Segment, Twilio, SendGrid, and WhatsApp together to help you create and send email, SMS, and WhatsApp campaigns to your customers.

Before sending your first Engage campaign, though, you’ll need to configure and connect accounts with all four platforms.

This guide lists all required onboarding steps and walks you through Engage setup. By the end of the onboarding process, you’ll be ready to send your first campaign.

The steps in this guide are only required if you plan to send email, SMS, and WhatsApp messages with Engage. Visit the 
Engage Foundations Onboarding Guide
 for general onboarding steps to set up your Engage space, connect sources, create audiences, and more.

Regional Segment

You can use Engage Premier on 
Segment’s regional infrastructure in the EU
. Twilio Engage ensures data residency in the EU, but the channels you connect to, may not guarantee the same level of data residency. Check directly with the providers of the channels you use for information about data residency in their applications. Native channels like email and SMS, which use Twilio, are not data resident.

Twilio is GDPR compliant, and has 
Binding Corporate Rules
 to ensure that data is protected when it’s transferred between countries.

Before you begin: overview and task checklist

You’ll set up Twilio Engage in four stages:

The following table shows a high-level checklist of tasks you’ll need to complete in each platform:

Several onboarding steps require copying and pasting information between Segment and SendGrid or Twilio. To streamline setup, open your Segment workspace in one browser tab and open two others for tasks you’ll carry out in SendGrid and Twilio.

Continue reading for a detailed, step-by-step breakdown of each onboarding stage.

Stage 1: Configure Engage Identifiers in Unify

Through 
identity resolution
, Segment uses the 
phone
 and 
email
 traits to identify users who can receive your Engage campaigns. To begin using Engage, you’ll need to verify that these identifiers exist in your workspace and add them if they don’t.

Follow these steps to configure the traits:

Your Segment workspace is now configured for Engage.  Next, you’ll create a SendGrid account and connect it to Segment.

Stage 2: Create and configure a SendGrid account

SendGrid powers delivery of your Engage email campaigns. During this stage of onboarding, you’ll create and set up a SendGrid Pro account. You’ll then configure SendGrid and Engage to enable both subscription tracking and an event webhook.

Create your SendGrid Pro account

Start by creating a SendGrid account and then upgrading to the SendGrid Pro Plan:

Upgrading to SendGrid Pro

Upgrading to a SendGrid Pro account may require additional action on your part. Follow the instructions in 
SendGrid’s account upgrade guide
 to complete your upgrade.

Create a subuser and check the dedicated IP address

Next, you’ll 
create a SendGrid subuser
 and ensure that a dedicated IP has been assigned:

In the 
Create New Subuser
 window, create a username for the subuser, then add an email address and password. Your SendGrid subuser username must begin with the prefix 
twilio_engage_app_
. Add a unique identifier to the end of the prefix, for example, 
twilio_engage_app_someusername
.



Authenticate your domain

SendGrid parent account

In this section, you’ll authenticate your domain and set up reverse DNS with your SendGrid parent account.

Now, you’ll authenticate your domain with SendGrid and your DNS provider and 
enable link branding
. Domain authentication protects your sending reputation by showing email providers that you’ve given SendGrid permission to send email campaigns for you.

To authenticate your domain, you’ll copy CNAME records given to you by SendGrid and paste them into your DNS provider. 
Before you begin, verify that you have the necessary permissions to add CNAME records to your DNS.
 If you’re not sure if you have the right permissions, reach out to your organization’s IT department.

You’ll authenticate your domain using the SendGrid platform and your DNS provider:

Complete authentication by setting up reverse DNS:

Enable subscription tracking

You’ll also need to enable 
subscription tracking
, which keeps records of users who unsubscribe from your email campaigns:

Enable event webhook

Subuser Step

This step takes place in the 
subuser
 space.

You’ll now need to enable event webhooks, which trigger webhook notifications for campaign-related events like clicks and opens:



Generate an API key

Copying SendGrid Credentials

This step creates an API key and API Key ID that you’ll immediately add to Segment. Make sure you’re ready to copy and save the API key before proceeding; SendGrid only displays the API key once. You must follow these steps from within the SendGrid subuser account 
you created for use with Twilio Engage
.

Re-using API Keys

It is not possible to re-use API Keys in different Engage spaces. For each space, a new API Key is required.

Now, you’ll generate an API key and API Key ID within SendGrid. 
With your SendGrid account open in one tab, open your 
Segment workspace
 open in another. You’ll need both open to copy and paste the API credentials into your Engage settings.

SendGrid Subuser Step

Carry out the following steps in your SendGrid 
subuser
 space.

To finish linking the API credentials to your Segment account, follow these steps:



Warm up your IP

If you already send emails regularly on your chosen IP, proceed to 
Stage 3
.

To finish configuring your SendGrid account for usage with Twilio Engage, you’ll 
warm up your IP
. IP warmup protects your sender reputation and ensures that you avoid email deliverability issues.

As a best practice, 
only warm up your IP when you’re ready to begin sending campaigns.

You can enable automated IP warmup by following these steps:

Once you’ve enabled IP warmup, you’re ready to send as many campaigns as you’d like. SendGrid will begin sending your campaigns through a shared pool of IP addresses, including your own. Over the next 30 days, SendGrid will gradually increase the number of campaigns sent through your chosen IP. After 30 days, SendGrid will send all your emails from your dedicated IP.

IP warmup best practices

Keep the following in mind once you’ve enabled automated IP warmup:

Once you’ve completed IP warmup, your SendGrid account will be fully configured and ready to use with Engage.  You’re ready to move to Stage 3 and set up Twilio SMS.

Stage 3: Create and configure Twilio SMS services

To add the ability to send SMS campaigns in Engage, you’ll now create a Twilio account, set up a phone number and messaging service, and generate an API key.

Set up a Twilio Messaging Service

Phone Number Registration

You’ll need to purchase a phone number to set up 
Twilio Messaging
. Depending on the phone number type you purchase, you may have to register the number. Before completing this section, read Twilio’s documentation on 
short code
, 
long code
, and 
toll free numbers
.

Once you’ve identified the type of phone number you’ll use with Twilio Engage, follow these steps to create a Twilio Messaging Service:

Generate an API key, and select your messaging service(s)

Copying Twilio Credentials

This step generates an Account SID, API key SID, and API key secret that you’ll later add to Segment. Make sure you’re ready to copy and save both before proceeding.

Start by creating your Twilio account and getting an API key for Engage:

Copy and save both the 
SID
 and 
Secret
 field contents.



Return to the API keys & tokens page. In the 
Live credentials
 section, copy the Account SID credentials.



Under 
Enter your Twilio API Key information
 (shown below), paste the Account SID, API Key SID, and API Key Secret you copied above into their corresponding fields.



Click 
Verify
, then select the messaging services you want to use in your space.



If you’re unable to verify your Account SID, SID, or API Key secret, you may have copied an extra space at the end of one or the other. Verify that you’ve not added any extra characters or spaces, then try to verify again.

Stage 4: Create and configure Twilio WhatsApp services

To send WhatsApp messages in Twilio Engage, you’ll register a Twilio number with WhatsApp, connect your Facebook account, and create a WhatsApp messaging service.

Register a Twilio number with WhatsApp

Connect your Facebook account

In the Facebook popup from the previous section, carry out these steps:

Create the WhatsApp messaging service

You’ll now create a messaging service to connect your number to Engage:

Your WhatsApp messaging service is now created.

Regional Segment

You can use Engage Premier on 
Segment’s regional infrastructure in the EU
. Twilio Engage ensures data residency in the EU, but the channels you connect to, may not guarantee the same level of data residency. Check directly with the providers of the channels you use for information about data residency in their applications. Native channels like email and SMS, which use Twilio, are not data resident.

Twilio is GDPR compliant, and has 
Binding Corporate Rules
 to ensure that data is protected when it’s transferred between countries.

Next steps

With configured accounts and services for all platforms, you’ve completed Engage onboarding and are ready to create and send campaigns to your users.

Not sure where to start? Read the Engage documentation on 
sending email campaigns
, 
SMS campaigns
, and 
WhatsApp campaigns
. To save time when generating Engage campaigns, check out the Engage guides on creating 
SMS templates
, 
email templates
, and 
WhatsApp templates
.

If you’re planning to import contacts to Engage, learn how to 
update your audiences with a CSV file
.

This page was last modified: 15 Jul 2024

Need support?

Questions? Problems? Need more info? Contact Segment Support for assistance!

Help improve these docs!

Was this page helpful?

Thanks for your feedback!

Can we improve this doc? 
Send us feedback!

Get started with Segment

On this page

Was this page helpful?

Thanks for your feedback!

Can we improve
 this doc? 
Send us feedback!

Product

For Developers

Company

Support

© 2025 Segment.io, Inc.

=== Content from https://segment.com/docs/unify/profiles-sync/profiles-sync-setup/ ===

Set up Profiles Sync
        

Unify requires a Business tier account and is included with Engage.


See the 
available plans
, or 
contact Support
.

On this page

On this page, you’ll learn how to set up Profiles Sync, enable historical backfill, and adjust settings for warehouses that you’ve connected to Profiles Sync.

Initially Setting up Profiles Sync

Identity Resolution setup

To use Profiles Sync, you must first set up 
Identity Resolution
.

To set up Profiles Sync, first create a warehouse, then connect the warehouse within the Segment app.

Before you begin, prepare for setup with these tips:

Step 1: Select a warehouse

You’ll first choose the destination warehouse to which Segment will sync profiles. Profiles Sync supports the Snowflake, Redshift, BigQuery, Azure, Postgres, and Databricks warehouse Destinations. Your initial setup will depend on the warehouse you choose.

The following table shows the supported Profiles Sync warehouse destinations and the corresponding required steps for each. Select a warehouse, view its Segment documentation, then carry out the warehouse’s required steps before moving to step 2 of Profiles Sync setup:

After you’ve finished the required steps for your chosen warehouse, you’re ready to connect your warehouse to Segment. Because you’ll next enter credentials from the warehouse you just created, 
leave the warehouse tab open to streamline setup.

To allow Segment to write to the warehouse you’re using for Profiles Sync, you’ll need to set up specific permissions.

For example, if you’re using BigQuery, you must 
create a service account
 for Segment and assign the following roles:

Review the required steps for each warehouse in the table above to see which permissions you’ll need.

The following Segment access 
roles
 apply to Profiles Sync:

Unify and Engage read-only
: Read-only access to Profiles Sync, including the sync history and configuration settings. With these roles assigned, you can’t download PII or edit Profiles Sync settings.

Unify read-only and Engage user
: Read-only access to Profiles Sync, including the sync history and configuration settings. With these roles assigned, you can’t download PII or edit Profiles Sync settings.

Unify and Engage Admin access
: Full edit access to Profiles Sync, including the sync history and configuration settings.

Step 2: Connect the warehouse and enable Profiles Sync

After selecting your warehouse, you can connect it to Segment.

During this step, you’ll copy credentials from the warehouse you just set up and enter them into the Segment app. The specific credentials you’ll enter depend on the warehouse you chose during step 1.

Segment may also display IP addresses you’ll need to allowlist in your warehouse. Make sure to copy the IP addresses and enter them into your warehouse account.

To connect your warehouse:

Step 3: Set up Selective Sync

Set up Selective Sync to control the exact tables and columns that Segment will sync to your connected data warehouse.

Data will be backfilled to your warehouse based on the last two months of history.

You can sync the following tables:

Use Selective Sync to manage the data you send to your warehouses by choosing which tables and columns (also known as properties) to sync. Syncing fewer tables and properties will lead to faster and more frequent syncs, faster queries, and using less disk space.

You can access Selective Sync in two ways:

You’ll see a list of event type tables, event tables, and 
tables Segment materializes
 available to sync. Select the tables and properties that you’d like to sync, and be sure the ones you’d like to prevent from syncing aren’t selected.

Regardless of schema size, only the first 5,000 collections and 5,000 properties per collection can be managed using your Segment space. To edit Selective Sync settings for any collection which exceeds this limit, 
contact Segment support
.

You must be a workspace owner to change Selective Sync settings.

Use Selective Sync when you want to prevent specific tables and properties from syncing to your warehouse. Segment stops syncing from disabled tables or properties, but will not delete any historical data from your warehouse.

If you choose to re-enable a table or property to sync again, only new data generated will sync to your warehouse. Segment doesn’t backfill data that was omitted with Selective Sync.

Profiles Sync sends profiles to your warehouse hourly once setup completes. Setup is complete after an initial automated backfill syncs all profile data. To initiate the backfill, the Profiles Sync requires live data flowing into your workspace. If live data isn’t available, you can send test data to trigger the backfill sooner. Backfill can also sync historical profiles to your warehouse.

You can only use historical backfill for tables that you enable with 
Selective Sync
 during setup. Segment does not backfill tables that you disable with Selective Sync.

When Segment runs historical backfills:

Segment lands the data on an internal staging location, then removes the backfill banner. Segment then syncs the backfill data to your warehouse.

Reach out to 
Segment support
 if your use case exceeds the scope of the initial setup backfill.

While historical backfill is running, you can start building 
materialized views
 and running 
sample queries
.

Step 4 (Optional): Materialize key views using a SQL automation tool

During setup, you have the option of setting up materialized key views in one of two ways:

You can choose to materialize views on your own by using 
profiles raw tables
. 
You may want to materialize your own tables if, for example, you want to transform additional data or join Segment profile data with external data before materialization.

You can choose to use Segment’s open source dbt models by using 
profiles materialized
 tables.

You can alternatively use 
tables that Segment materializes
 and syncs to your data warehouse.

To start seeing unified profiles in your warehouse and build attribution models, you’ll need to materialize the tables that Profiles Sync lands into three key views:

See 
Tables you materialize
 for more on how to materialize these views either on your own, or with 
Segment’s open source dbt models
.

Note that dbt models are in beta and need modifications to run efficiently on BigQuery, Synapse, and Postgres warehouses. Segment is actively working on this feature.

Profiles Sync limits

As you use Profiles Sync, keep the following limits in mind:

Working with synced warehouses

Monitor Profiles Sync

You can view warehouse sync information in the overview section of the Profiles Sync page. Segment displays the dates and times of the last and next syncs, as well as your sync frequency.

In the Syncs table, you’ll find reports on individual syncs. Segment lists your most recent syncs first. The following table shows the information Segment tracks for each sync:

Selecting a row from the Syncs table opens a pane that contains granular sync information. In this view, you’ll see the sync’s status, duration, and start time. Segment also displays a nuanced breakdown of the total rows synced, sorting them into identity graph tables, event type tables, and event tables.

If the sync failed, Segment shows any available error messages in the sync report.

Settings and maintenance

The 
Settings
 tab of the Profiles Sync page contains tools that can help you monitor and maintain your synced warehouse.

In the 
Basic settings
 tab, you can disable warehouse syncs or delete your connected warehouse altogether.

To disable syncs, toggle 
Sync status
 to off. Segment retains your warehouse credentials but stops further syncs. Toggle Sync status back on at any point to continue syncs.

To delete your warehouse, toggle 
Sync status
 to off, then select 
Delete warehouse
. Segment doesn’t retain credentials for deleted warehouses; to reconnect a deleted warehouse, you must set it up as a new warehouse.

In the 
Connection settings
 tab, you can verify your synced warehouse’s credentials and view IP addresses you’ll need to allowlist so that Segment can successfully sync profiles.

If you have write access, you can verify that your warehouse is successfully connected to Segment by entering your password and then selecting 
Test Connection
.

Changing your synced warehouse

If you’d like to change the warehouse connected to Profiles Sync, 
reach out to Segment support
.

Segment supports hourly syncs.

This page was last modified: 07 Nov 2024

Need support?

Questions? Problems? Need more info? Contact Segment Support for assistance!

Help improve these docs!

Was this page helpful?

Thanks for your feedback!

Can we improve this doc? 
Send us feedback!

Get started with Segment

On this page

Was this page helpful?

Thanks for your feedback!

Can we improve
 this doc? 
Send us feedback!

Product

For Developers

Company

Support

© 2025 Segment.io, Inc.

=== Content from https://segment.com/docs/engage/audiences/organization/ ===

Organizing Your Audiences
        

Engage Foundations requires a Business tier account and includes Unify.


See the 
available plans
, or 
contact Support
.

On this page

To add structure to your 
Spaces
, you can organize Audiences into folders and clone Audiences within, and between, Spaces.

Working with folders

Folders allow you to group Audiences together. You can create, edit, and search through folders directly within the Engage Audiences page.

Creating a folder

To create a Folder, follow the steps below:

Editing and disbanding folders

To edit the name or description of a Folder you’ve created, click the 
More Options
 icon and select 
Edit
. Once you’ve made your desired changes, click 
Save
.

To disband a Folder you’ve made, click the 
More Options
 icon and select 
Disband
. Audiences from the disbanded Folder return to your main Audience list.

Disbanding folders does not delete audiences.

Moving Audiences into folders

To move an Audience to a Folder you’ve already created, follow the steps below:

Clone Audiences

Audience cloning creates a copy of your Audience. You can clone an Audience within the same space, or clone an Audience to a different space.

Clone an Audience inside a Space

To clone an Audience within the same Space, follow the steps below:

Cloning an Audience between Spaces

You may wish to clone an Audience between spaces for a number of use cases, including the following:

Note

When you clone an Audience to a different space, first verify that the target Space includes the same events and traits for the cloned Audience.

To clone an Audience between Spaces, follow the steps below:

If your target Space doesn’t include the cloned Audience’s events and traits, Engage prompts you to resolve the Space incompatibilities during Step 5.  As a best practice, verify that the target Space includes the Audience’s traits and events before cloning.

Delete an Audience

To delete an Audience, follow the steps below:

This page was last modified: 28 Jun 2023

Need support?

Questions? Problems? Need more info? Contact Segment Support for assistance!

Help improve these docs!

Was this page helpful?

Thanks for your feedback!

Can we improve this doc? 
Send us feedback!

Get started with Segment

On this page

Was this page helpful?

Thanks for your feedback!

Can we improve
 this doc? 
Send us feedback!

Product

For Developers

Company

Support

© 2025 Segment.io, Inc.

=== Content from https://segment.com/docs/guides/how-to-guides/dynamic-coupon-program/ ===

Setting Up a Dynamic Coupon Program to Reward Loyal Customers
        

On this page

One component of building a successful and engaging e-commerce brand is rewarding your most loyal customers. With Segment Warehouses and SQL, you can retrieve a table of your most valuable customers, then reward them.

This guide will walk you through setting up a dynamic and automated coupon program based on conditions that define your most valuable customers, as well as how to measure the program’s performance.

Talk to a product specialist today
 about using data to tailor your brand experience.

Tools used

Emails with 
Customer.io
: Customer.io is a flexible email provider that allows you to create cohorts based on customer actions. You can build complex onboarding emails, nurture email campaigns, as well as marketing automation workflows.

Retention Analytics with 
Amplitude
: Amplitude is an analytics tool that focuses on understanding retention and funnel analysis.

It’s important to register for these tools and enable them on your Segment source project. When Segment collects tracking data, it routes it to all of your enabled tools, meaning that they get a single consistent data set. Most importantly, the data generated by users interacting with emails is sent through Segment so you can analyze email performance, and how it impacts conversion with Amplitude.

Not using Customer.io or Amplitude?
 Check out the other Segment Supported 
Email Marketing
 and 
Analytics tools
.

The Loyalty Program

Say, as the marketing manager of our fictitious, on-demand artisanal toast company, Toastmates, you want to experiment with a coupon program to retain your best customers.

Through a combination of SQL and statistical analysis on a set of historical data, you’ve identified the conditions for our most valuable customers as:

Learn how to define these conditions in 
How to Forecast LTV for e-commerce with Excel and SQL
.

Will rewarding a $5 coupon to this cohort after they make the second purchase a month lead to higher engagement and LTV? Set up this program using Customer.io as the email provider and measure it’s performance on engagement and LTV with Amplitude.

Conduct a split test (half of the cohort will represent the control group and will not receive any emails; the other half will receive an email with the $5 coupon) for one month. After which, use Amplitude to see if there were any correlations between the coupon email and conversions.

Set it up

First, register for an account with Customer.io and Amplitude. Then, enable Customer.io and enable Amplitude on your Segment project. Finally, go into your Customer.io account and enable “sending data to Segment”:



You can find those destination settings in Customer.io here
.

When everything is enabled, customer event data such as 
Order Completed
 and 
Product Added
, as well as their properties, will all be sent to your configured destinations, including Customer.io and Amplitude. Then you can define cohorts based on these events in Customer.io to add to email campaigns or conduct funnel analytics in Amplitude.

Talk to a product specialist to learn what else you can accomplish with these tools
.

Define the cohort in Customer.io

Now define the specific cohort in Customer.io as per our conditions listed earlier: someone who spends over $20 per order and shops over twice a month. In Customer.io, go to “Segments” and “Create Segment”:



After this cohort is created, then when a customer makes the third purchase in a month and it’s over $20, they will be added to this segment.

Next, create a “segment trigger campaign”, where Customer.io will send a message the first time someone enters a segment. The segment in this case will be the one you just created: Coupon Loyalty Experiment.



Save the changes and enable the campaign. Then, make sure that your e-commerce backend is set up properly to handle the coupons. If it’s available in your system, create a coupon that only works for a specific set of customers.

Measure performance

After a month has passed for the split test, you can measure the performance of the email coupon program to see whether it’s making a material impact on conversions.

In Amplitude, create a funnel that compares the two cohorts—one who received this coupon email vs. the control group who did not—and see its impact on conversions and revenue generated.

First, define a behavioral cohort with the conditions of being loyal customers so you can use it when analyzing the conversion funnel:



You’ll also have to create a second identical cohort, except with the only difference that these customers did not receive the coupon email. You need this cohort to create the conversion funnel with the control group.



After you’ve created these two cohorts, create two funnel charts. The first funnel will look at the control group. The second funnel will look at the group that received the coupon email.



Resulting in:



The control group that did not receive the email for the coupon resulted in 233 people visiting the store, with 66 conversions.

The funnel for the group who did receive the emails can be created with these parameters:



Resulting in:



The email itself drove 168 customers to the store, which also saw higher conversions to 
Product Added
 and ultimately 
Order Completed
.

Note that this funnel is only looking customers who went through these events in this specific order. This analysis doesn’t consider customers who are part of the emailed cohort, yet didn’t open the email, but still visited the site and/or made a purchase.

At first glance, it appears that the group that was emailed did receive an absolute number of more conversions. However, these funnels are still inconclusive, given that you haven’t explored the impact on the top line revenue, as well as overall engagement with the brand. Fortunately, you can continue to use Amplitude to analyze impact on revenue itself.

Find new ways and channels to retain your most valuable customers

Retaining and rewarding your customers is paramount to a strong and engaging brand. This example is just one of millions that you can employ to find new ways to delight and excite your customer base.

Other ideas can be to send messages to your customers with a referral code to invite their friends. Or set up a coupon for customers who are just shy of entering your most valuable customers cohort. Or, if you’re hosting a pop up shop event, sending a special and personalized invite to your strongest users first, as a way to thank them for their business.

The possibilities are endless when you use your customer data to drive sales.

Talk to a product specialist today
 about using data to tailor your brand experience.

This page was last modified: 25 Oct 2023

Need support?

Questions? Problems? Need more info? Contact Segment Support for assistance!

Help improve these docs!

Was this page helpful?

Thanks for your feedback!

Can we improve this doc? 
Send us feedback!

Get started with Segment

On this page

Was this page helpful?

Thanks for your feedback!

Can we improve
 this doc? 
Send us feedback!

Product

For Developers

Company

Support

© 2025 Segment.io, Inc.

=== Content from https://segment.com/docs/api/public-api/fql/ ===

Destination Filter Query Language
        

On this page

The Segment Public API is available

Segment’s 
Public API
 is available for Team and Business tier customers to use. You can use the Public API and Config APIs in parallel, but moving forward any API updates will come to the Public API exclusively. 

Please contact your account team or 
friends@segment.com
 with any questions.

This reference provides a comprehensive overview of the Segment Destination Filter query language. For information on the Destination Filters API (including information on migrating from the Config API), visit the 
Destination Filters API reference
.

The 
Transformations API
 uses Filter Query Language (FQL) to filter JSON objects and conditionally apply transformations. You can use FQL statements to:

In addition to boolean and equality operators like 
and
 and 
>=
, FQL has built-in functions that make it more powerful such as 
contains( str, substr )
 and 
match( str, pattern )
.

Examples

Given the following JSON object:

{

  
"event"
:
 
"Button Clicked"
,

  
"type"
:
 
"track"
,

  
"context"
:
 
{

    
"library"
:
 
{

      
"name"
:
 
"analytics.js"
,

      
"version"
:
 
"1.0"

    
}

  
},

  
"properties"
:
 
{

    
"features"
:
 
[
"discounts"
,
 
"dark-mode"
]

  
}


}



The following FQL statements will evaluate as follows:

Field Paths

FQL statements may refer to any field in the JSON object including top-level
properties like 
userId
 or 
event
 as well as nested properties like

context.library.version
 or 
properties.title
 using dot-separated paths. For
example, the following fields can be pointed to by the associated field paths:

{

  
"type"
:
 
"..."
,
       
//
 
type

  
"event"
:
 
"..."
,
      
//
 
event

  
"context"
:
 
{
         
//
 
context

    
"library"
:
 
{
       
//
 
context.library

      
"name"
:
 
"..."
    
//
 
context.library.name

    
},

    
"page"
:
 
{
          
//
 
context.
 
page

      
"path"
:
 
"..."
,
   
//
 
context.page.path

    
}

  
}


}



Escaping Field Paths

If your field name has a character not in the set of 
{a-z A-Z 0-9 _ -}
, you must escape it using a 
\
 character. For example, the nested field below can be referred to by 
properties.product\ 1.price
:

{

  
"properties"
:
 
{

    
"product 1"
:
 
{

      
"price"
:
 
"19.99"

    
}

  
}


}



Operators

Boolean

Unary

Comparison

Subexpressions

You can use parentheses to group subexpressions for more complex “and / or” logic as long as the subexpression evaluates to true or false:

Functions

Functions handle 
null
 with sensible defaults to make writing FQL more concise.
For example, you can write 
length( userId ) > 0
 instead of 
typeof( userId ) =
'string' and length( userId ) > 0
.

match( string, pattern )

The 
match( string, pattern )
 function uses “glob” matching to return 
true
 if the given string fully matches a given pattern. Glob patterns are case sensitive. If you only need to determine if a string contains another string, you should use 
contains()
.

Error Handling

If your FQL statement is invalid (for example 
userId = oops"
), your Segment event will not be sent on to downstream Destinations. Segment defaults to not sending the event to ensure that invalid FQL doesn’t cause sensitive information like PII to be incorrectly sent to Destinations.

For this reason, Segment recommends that you use the Destination Filters “Preview” API to test your filters without impacting your production data.

This page was last modified: 04 Dec 2024

Need support?

Questions? Problems? Need more info? Contact Segment Support for assistance!

Help improve these docs!

Was this page helpful?

Thanks for your feedback!

Can we improve this doc? 
Send us feedback!

Get started with Segment

On this page

Was this page helpful?

Thanks for your feedback!

Can we improve
 this doc? 
Send us feedback!

Product

For Developers

Company

Support

© 2025 Segment.io, Inc.

=== Content from https://segment.com/docs/segment-app/extensions/ ===

Extensions
        

Extensions let you integrate third-party tools into your existing Segment workspace, helping you automate tasks, manage data flows, and maintain version control.

Segment offers the following extensions:

Segment built Extensions to help you get the most out of your Segment workspace, allowing you to keep your projects organized, efficient, and aligned with best practices for data management and version control.

This page was last modified: 16 Aug 2024

Need support?

Questions? Problems? Need more info? Contact Segment Support for assistance!

Help improve these docs!

Was this page helpful?

Thanks for your feedback!

Can we improve this doc? 
Send us feedback!

Get started with Segment

Was this page helpful?

Thanks for your feedback!

Can we improve
 this doc? 
Send us feedback!

Product

For Developers

Company

Support

© 2025 Segment.io, Inc.

=== Content from https://segment.com/docs/connections/destinations/ ===

Destinations Overview
        

On this page

Destinations are the business tools or apps that Segment forwards your data to. Adding Destinations allow you to act on your data and learn more about your customers in real time.

Destinations Catalog

If you want to explore the destinations compatible with Segment, check out the 
Destinations catalog
. Select an item from the catalog to learn more about it. The documentation for each destination explains how the Segment Tracking API methods are implemented for that destination.

Sources vs Destinations

Segment has 
Sources
 and 
Destinations
. Sources send data 
into
 Segment, while Destinations receive data 
from
 Segment.

Destination connection types

Segment has three destination connection types:

Event streams destinations

Event streams destinations are all destinations that aren’t storage or Reverse ETL destinations. Adding these destinations allow you to act on your data and learn more about your customers in real time. These include 
Destination Actions
.

Storage destinations

Storage destinations enable you to store your raw Segment data. This enables data analysts and data scientists to work with the raw data to derive deeper and more customized insights to support your organization. Learn more from the 
storage overview page
.

Reverse ETL destinations

Reverse ETL
 destinations are the business tools or apps you use that Segment syncs the data from your warehouse to.

If your destination is not listed in 
the Reverse ETL catalog
, use the 
Segment Connections destination
 to send data from your Reverse ETL warehouse to other destinations listed in the 
catalog
. The Segment Connections destination enables you to mold data extracted from your warehouse in 
Segment Spec
 API calls that are then processed by 
Segment’s HTTP Tracking API
. The Segment HTTP Tracking API lets you record analytics data. The requests hit Segment’s servers, and then Segment routes your data to any destination you want. Get started with the 
Segment Connections destination
.

Method compatibility

Not all destinations can accept data from specific method types. To know if a destination can accept data from specific method types, look for the 
Quick Info
 box at the top of the destination’s documentation page, or check out the 
Destinations Methods comparison chart
.

Source compatibility

Many destinations can accept data from all types of sources, but some are only compatible with specific source types (for example, web only, or server only). To find out which source types a specific destination can accept data from, check the documentation for that destination in the 
Quick info
 box, or in the 
Supported Sources and Connection Modes
 section.

Wondering which destinations take which data? Check out the Destination connection modes list by category.

Destination Actions

In June 2021, Segment released a new form of destinations called 
Destinations Actions
. These destinations allow users to create 
subscriptions
: sets of conditions in which data is sent to the destinations and data mappings, to format that data for the destination tool. Segment watches for data that matches the conditions you create (
triggers
) for the subscription, and when the conditions are met, uses an explicit mapping to transform the incoming data to an output format that your destination can use.

Connection modes

Segment’s web source (Analytics.js), and native client-side libraries (iOS, Android, React-native) allow you to choose how you send data to Segment from your website or app. There are two ways to send data:

Cloud-mode
: The sources send data directly to the Segment servers, which then translate it for each connected downstream destination, and send it on. Translation is done on the Segment servers, keeping your page size, method count, and load time small.

Healthcare and Life Sciences (HLS) customers can encrypt data flowing into their destinations

 HLS customers with a HIPAA eligible workspace can encrypt data in fields marked as Yellow in the Privacy Portal before they flow into an event stream, cloud-mode destination.
    
To learn more about data encryption, see the 
HIPAA Eligible Segment documentation

Device-mode
: You include additional code on your website or mobile app which allows Segment to use the data you collect on the device to make calls directly to the destination tool’s API, without sending it to the Segment servers 
first
. (You still send your data to the Segment servers, but this occurs asynchronously.) This is also called 
wrapping
 or 
bundling
, and it might be required when the source has to be loaded on the page to work, or loaded directly on the device to function correctly. When you use Analytics.js, you can change the device-mode destinations that a specific source sends from within the Segment web app, without touching any code.



If you use Server source libraries, they only send data directly to Segment in Cloud-mode. Server library implementations operate in the server backend, and can't load additional destination SDKs.

Choosing a connection mode

Cloud-mode destinations send data through Segment. Device-mode destinations send data in parallel to Segment. There are tradeoffs between using cloud-mode and device-mode destinations. In general, Cloud-mode is preferred because you then benefit from Segment’s system features, like retries, Replay, Warehouses, Privacy blocking, filtering, and more.

You should consider using device-mode if you use destinations which record information directly on the user’s device. These types of tools might lose functionality if they aren’t loaded directly on the device.

Take a look at the pros and cons chart of device-mode and cloud-mode destinations to determine which connection mode is best for you:

Segment’s website sources use device-mode by default, because so many website-based destinations require that they be loaded on the page, and because size and page performance are less of a concern than on mobile. If your website source only collects information that you can instrument yourself, then you can use cloud-mode.

For example, a web-chat destination must be loaded to connect to the service and collect metrics efficiently - you don’t expect it to route chat messages through Segment! This 
does
 mean that Segment might not receive a small amount of the destination-specific information from your users. In the chat example, if the destination is calculating idle time between messages, that data would appear in the destination’s tooling, but not necessarily in the Segment data.

By default, destinations configured on a mobile source send their data directly to the Segment servers, then translate it and use Cloud-mode to forward it to destinations. 
Cloud-mode
 means that Segment sends the data directly from the Segment servers, to their servers. This means you don’t need to package third-party SDKs for destinations that can accept cloud-mode data. Some primarily web-based destinations also allow cloud-mode, which can help reduce app size, and improve load time and performance. You can read more about the 
effects of mobile app size on downloads in Segment’s blog
.

Before you turn on or opt-in for cloud-mode for a mobile source, consider if your destinations have features that require interactions on the device or require device-specific data (see the examples above). For example, if you use cloud-mode for Mixpanel, you’ll get your data on reporting and people, but won’t be able to use their features for in-app surveys or auto-tracking. These can be really valuable, but might not be a priority for your team.

How Segment determines Device-mode and Cloud-mode destinations

There are two main things Segment considers when deciding to use Device-mode or Cloud-mode, or both, for a destination partner:

The anonymous identifiers used on mobile devices are usually static, which means Segment doesn’t need to do additional resolution, and can build Cloud-mode destinations by default. Because Segment uses native advertising identifiers on mobile devices, you don’t need a full SDK on the device to reconcile or identify a user. For example, you might track users who viewed an advertisement in one app and installed another app as a result.

However, some mobile attribution tools do more advanced reconciliation based on more than the native identifier, which requires the SDK on the device to work properly. For those destinations, Segment offers device-mode, which packages the tool’s SDK with the client-side library so that you can get the entire range of tool functionality.

Cross-domain identity resolution for websites requires that the attribution tool use a third-party cookie so it can track a user anonymously across domains. This is a critical component of attribution modeling. As a matter of principle, Segment only uses first-party cookies and doesn’t share cookies with partners, so Analytics.js and the data it collects aren’t enough to generate view-through attribution in ad networks.

Customers can load their libraries and pixels in the context of the browser and trigger requests to attribution providers from their device in response to Segment API calls to take advantage of advertising and attribution tools.

Many of Segment’s destinations offer client-side features beyond data collection in their SDKs and libraries, for both mobile and web. In these cases, Segment offers Device-mode SDKs so that you can collect information on the device using Segment, but still get the destination’s complete native functionality.

Some features that usually require Device-mode include: automatic A/B testing, displaying user surveys, live chat or in-app notifications, touch and hover heatmapping, and accessing rich device data such as CPU usage, network data, or raised exceptions.

How can I tell which connection modes and platforms are supported for a destination?

The first place to look is the individual destination documentation. Each one includes a matrix of supported Sources and Connection Modes. Segment provides a list of 
all destinations and their connection modes
.

In order to override the default, check the destination settings pane in the Segment web App either for a 
Connection Mode
 toggle or instructions on bundling any additional mobile components required.

Sync modes

Sync modes allow users to define how changes in the source should send downstream to your destination. Depending on which destinations you set up in Segment, you may need to choose a sync mode for your data. This configuration determines how Segment updates your destination based on the source data.

The available sync modes can vary based on the destination, integration type, and actions within the destination. For example, if you sync customer data, you might have the option to Insert, Update, or Upsert records.

Available sync modes include:

Add a destination

To add a Destination:

Learn more
 about what adding a destination entails.

Disabled destinations do not receive data

If you haven’t enabled your destination for the first time after you created it or if you actively disable a destination, Segment prevents any data from reaching the destination. Business Tier customers can request 
a Replay
, which resends data from the time the destination was disabled to the time it was re-enabled. Replays can also send data to currently disabled destinations.

Some destinations are not compatible with Replays after a certain period of time, for example, 14 days. Check with Segment’s support team 
friends@segment.com
 to confirm that your intended destination allows historical timestamps.

Data deliverability

Segment increases deliverability to destinations using 
retries
 and 
replays
. Retries happen automatically for all customers, while replays are available on request for 
Business Tier
 customers.

Segment’s data flow is primarily unidirectional, from Segment to integrated destinations. Segment does not inherently support a bidirectional flow where events, once delivered and processed by a destination, are sent back to Segment.

Segment also uses 
batching
 to increase deliverability to your destinations. Some destinations have batching enabled by default, and some, like Segment’s 
Webhook (Actions) Destination
, let you opt in to batching.

Some cases of event batching might lead to observability loss

While batching does increase event deliverability, you might experience error amplification, as if the entire batch fails, all events will be marked with the same status. For example, if a batch fails due to one 
429
 (Rate Limit) error, it might appear in the UI that there was one 429s request failure for each item in the batch.

Retries

Segment’s client libraries ensure delivery of your data to the API reliably in the face of spotty connections, device failure, or network partitions in your data centers.

When you use Segment’s mobile SDK, Segment dispatches each event to a background thread where the event is then written to a queue. Later, Segment’s SDK batches together many requests in to one compressed request and sends it to Segment’s servers. Segment’s SDKs minimize battery use and bandwidth use by powering up the radio less frequently and for shorter time periods.

If the delivery of the payload is not successfully sent due to connection issues, all of your SDKs will automatically retry the request until successful receipt of the payload according to the following policies. Note that retry policies are subject to change / tuning in the future.

All mobile libraries handle retries by periodically attempting to flush their internal queue of events to Segment. If the flush is unsuccessful, the library waits until the next regularly-scheduled flush time to try again. The background queue of requests to Segment is bounded in size so if events are being queued faster than we can successfully flush them to Segment, some events may be dropped.

The destination endpoint APIs have fluctuations in availability due to a number of issues ranging from network failures to bugs to overload. Segment’s internal systems retry failed destination API calls for four hours with a randomized exponential backoff after each attempt. This substantially improves delivery rates.

Here’s an example destination that was only successfully accepting 93.36% of all API requests but was achieving a 99.28% final deliverability rate due to Segment’s retry functionality.



You can see the current destination endpoint API success rates and final delivery rates for Segment’s server-side destinations on Segment’s 
status page
.

Replays

Replay is available to 
Business tier
 customers. 
Contact Segment
 to learn more.

Replays
 allow customers to load historical data from Segment’s S3 logs into downstream destinations which accept cloud-mode data. So, for example, if you wanted to try out a new email or analytics tool, Segment can replay your historical data into that tool. This gives you a great testing environment and prevents data lock-in when vendors try to hold data hostage.

If you submitted 
suppress_only
 requests
, Segment still retains historical events for those users, which can be replayed. If you do not want historical events replayed for suppressed users, submit 
suppress_and_delete
 requests instead.

Batching

Segment uses 
stream batching
 for all destinations that require near-realtime data and 
bulk batching
 for some data flows in our pipeline.

For all destinations, except for non-realtime Engage syncs and Reverse ETL syncs, Segment processes events from your source as they arrive and then flows the data downstream to your destinations in small batches, in a process called 
stream batching
. These batches might contain different events between retry attempts, as events in previous batches may have succeeded, failed with a permanent error, or expired. This variability reduces the workload the system processes during partial successes, allows for better per-event handling, and reduces the chance of load-related failures by using variable batch formations.

Some data flows may be able to use a process called 
bulk batching
, which supports batching for destinations that produce between several thousand and a million events at a time. Real-time workloads or using a Destination Insert Function may prevent bulk batches from being formed. Batches contain the same events between retries.

The following destinations support bulk batching:

You must manually configure bulk batches for Actions destinations

To support bulk batching for the Actions Webhook destination, you must set 
enable-batching: true
 and 
batch_size: >= 1000
.

IP Allowlisting

IP Allowlisting uses a NAT gateway to route traffic from Segment’s servers to your destination through a limited range of IP addresses, which can prevent malicious actors from establishing TCP and UDP connections with your integrations.

IP Allowlisting is available for customers on Business Tier plans.

Supported destinations

Segment supports IP Allowlisting in 
all destinations
 except for the following:

Destinations that are not supported receive traffic from randomly assigned IP addresses.

Configure IP Allowlisting

To enable IP Allowlisting for your workspace:

IP restriction might not be supported in all destinations.

This page was last modified: 17 Jan 2025

Need support?

Questions? Problems? Need more info? Contact Segment Support for assistance!

Help improve these docs!

Was this page helpful?

Thanks for your feedback!

Can we improve this doc? 
Send us feedback!

Get started with Segment

On this page

Was this page helpful?

Thanks for your feedback!

Can we improve
 this doc? 
Send us feedback!

Product

For Developers

Company

Support

© 2025 Segment.io, Inc.

=== Content from https://segment.com/docs/unify/data-graph/linked-events-limits// ===

Linked Events Limits
        

Unify requires a Business tier account and is included with Engage.


See the 
available plans
, or 
contact Support
.

On this page

To provide consistent performance and reliability at scale, Segment enforces default use limits for Linked Events.

Usage limits

Linked Events provides you with the flexibility to enrich unlimited events in downstream destinations. This means you won’t encounter any limitations or pauses in service related to the number of Linked Events enrichments.

Segment measures Linked Events limits based on entities and entity rows.

To see how many entities and entity rows you’re using with Linked Events, navigate to 
Settings > Usage & billing
 and select the 
Linked Events
 tab.

Special cases

There is a hard limit of 100 million entity rows that causes syncs to pause.

This page was last modified: 15 Oct 2024

Need support?

Questions? Problems? Need more info? Contact Segment Support for assistance!

Help improve these docs!

Was this page helpful?

Thanks for your feedback!

Can we improve this doc? 
Send us feedback!

Get started with Segment

On this page

Was this page helpful?

Thanks for your feedback!

Can we improve
 this doc? 
Send us feedback!

Product

For Developers

Company

Support

© 2025 Segment.io, Inc.

=== Content from https://segment.com/docs/segment-app/ ===

The Segment Web App
        

On this page

When you first log in, you go to your workspace. (If you’re a member of several workspaces, you get to choose which one to go to.) Workspaces organize sets of sources and destinations into a central location.

Want a video tour of the Segment workspace? Head over to Segment University! (Must be logged in to access.)

What’s a Workspace?

A workspace is a group of sources that can be administered and billed together. Workspaces help companies manage access for multiple users and data sources. Workspaces let you collaborate with team members, add permissions, and share sources across your whole team using a shared billing account.

When you first log in to your Segment account, you can create a new workspace, or choose to log into an existing workspace if your account is part of an existing organization.

Workspace Overview page

This is a Segment workspace.



The first thing you see is a graph of the sources and destinations you have connected to Segment. Sources send data to your workspace: these are your mobile apps, server sources, and website-based sources. Destinations are tools which get the data, and can also include warehouses, which just store large amounts of data for later reuse and analysis.

The graph on this overview page includes lines which can show you which sources send data to which destinations. If this is the first time you’re looking at your workspace and you haven’t set it up yet, it won’t look quite like this.

In the left navigation bar, you see the main parts of the Segment application: 
Sources
, 
Destinations
, 
Privacy
, 
Engage
, and 
Protocols
, if your subscription includes them.

You can also find the Catalog in the left navigation, which lists the sources you can collect data from, and the destinations you can send data to.

You can always click the Segment logo in the top left corner to get back to the Overview page.

Sources



The Sources tab lists everything that is sending data to your Segment workspace. Sources are organized by type: website, mobile, sever, or by cloud-app type, like CRM or payments.

Each source has a status and a list of destinations. A source’s status tells you whether or not the source is sending data to Segment, and how long it’s been since Segment last saw data from the source. The source’s destinations list shows you which destinations are receiving data from that source. You can expand them for more detail.

Destinations



The Destination tab lists all of the destinations connected to your workspace. These are sorted into categories like analytics, email marketing, and other tool types. The list also shows whether or not Segment is sending data to that tool, or if the tool is enabled or disabled.

The Segment Integration Catalog



Next up is the Catalog. The catalog includes a list of all 
sources
 and 
destinations
 available in Segment. You can search either by category or name. When you click on a catalog tile, the tile shows instructions on how to connect the tool to your Segment workspace.

The Catalog is always growing, so check out the “New and Noteworthy” section from time to time to see what’s new.

Engage and Protocols

If you have Engage or Protocols enabled in your workspace, you’ll see sections for those too. Engage helps you use your Segment data to build audiences and better understand your users, and Protocols helps you structure and maintain the format of the data you send through Segment.

These features are fairly advanced, but you can learn more about them by 
requesting a demo
, or reading more in the 
Engage documentation
, and the 
Protocols documentation
.

Segment Settings

The 
Workspace Settings
 tab shows more information about your workspace, including your team settings, GDPR requests, and so on. You might not have access to edit these settings.

The 
User Preferences
 tab shows your individual account settings, including Notification settings.

The 
Activity Notifications
 feature in Notification settings provides alerts for specific workspace activities when enabled. These alerts keep you updated on actions taken by other workspace users, excluding activities you initiate. This ensures you’re only alerted to actions you’re not directly involved in.

The 
Usage
 tab shows how many API calls or 
Monthly Tracked Users (MTUs)
 your workspace has used this month - which can be important for keeping an eye on your Segment bill.

Health

The 
Health
 tab lists any repeated or consistent errors, which can help alert you to misconfigurations or data issues which you can correct.

Issues on the Health tab are sorted by Sources, Destinations, Warehouses, and again by type.



If errors are present, they’re sorted by type and include information about how long ago they were last seen and how many times they’ve occurred. You can click the wrench icon on an individual error line to view the Event Delivery tool and see the erroring payload and response. You can also disable or delete the erroring integration from this menu.

Privacy Portal

The Privacy Portal allows you to inspect data coming into your Segment account, check it for Personally Identifying Information (PII), classify it based on how sensitive the information is, and then determine which categories of data to send to different destinations. Read more about these tools in the 
Privacy Portal documentation
.



This page was last modified: 22 Feb 2024

Need support?

Questions? Problems? Need more info? Contact Segment Support for assistance!

Help improve these docs!

Was this page helpful?

Thanks for your feedback!

Can we improve this doc? 
Send us feedback!

Get started with Segment

On this page

Was this page helpful?

Thanks for your feedback!

Can we improve
 this doc? 
Send us feedback!

Product

For Developers

Company

Support

© 2025 Segment.io, Inc.

=== Content from https://segment.com/docs/connections/sources/schema// ===

Source Schema
        

On this page

Segment Business Tier customers can use Schema Controls to manage which events are allowed to pass through Segment and on to Destinations. These filters are a first-line defense to help you protect the integrity of your data, and the decisions made with it.

Blocking events within the source schema will exclude them from API and MTU calculations. These events are discarded before they reach the pipeline that Segment uses for MTU calculations.

Schema view

The Schema tab shows the schema of events, properties, and traits for each source that Segment receives over a specific timeframe. It also shows when the events were last seen, how many events were allowed vs. blocked, and the downstream destinations those events are connected to.

You can view events by Segment call type in the Source Schema with the 
Track
, 
Identify
, and 
Group
 tabs. 
The Schema tracks:

Click the arrow to the left of the event name to view additional event properties for Page or Track events. Since the Schema tracks Identify traits, you will need to make sure you are passing traits into your Identify call in order to view event data in your schema.

The Schema shows “Page Viewed” for all Page calls under the 
Track
 tab.

The Source Schema UI changes slightly depending on whether you have a 
Protocols Tracking Plan
 connected to the source. If you have a Tracking Plan connected to your source, the UI displays a 
Planned
 column that will indicate if the event is planned or unplanned. This allows you to quickly identify unplanned events and take action to align your schema with your Tracking Plan. If there is no Tracking Plan connected to the source, the UI will display a toggle next to each event where, if you’re a Business Tier customer, you can simply block or allow that event at the source level.

Array properties are represented with an additional nested property representing the array’s items. The nested property is the property’s name with a 
.$
 suffix.
If an array property in the connected Tracking Plan does not include the 
items
 nested property, nested properties might be marked as unplanned in the Source Schema.

Event filters

If you no longer want to track a specific event, you can either remove it from your code or, if you’re on the Business plan and don’t have a Tracking Plan connected, you can block track calls from the Segment UI. To do so, click on the Schema tab in a Source and toggle the event to enable or block an event.



For sources with a connected Tracking Plan, use Protocols to block unplanned events.

Once you block an event, Segment stops forwarding it to all of your Cloud and Device-mode Destinations, including your warehouses. You can remove the events from your code at your leisure. In addition to blocking track calls, Business plan customers can block all Page and Screen calls, as well as Identify traits and Group properties.

When an event is blocked, the name of the event or property is added to your Schema page with a counter to show how many events have been blocked. By default, data from blocked events and properties is not recoverable. You can always re-enable the event to continue sending it to downstream Destinations.

In most cases, blocking an event immediately stops that event from sending to Destinations. In rare cases, it can take 
up to six hours
 to fully block an event from delivering to all Destinations.

Blocked events appear in the debugger with a block symbol, adding visibility into events actively blocked by Segment.

Identify and Group Trait Filters

If you no longer want to capture specific traits within 
.identify()
 and 
.group()
 calls, you can either remove those traits from your code, or if you’re on the Business plan, you can block specific traits right from the Segment UI. To do so, click on the Schema tab in a Source and navigate to the Identify or Group events where you can block specific traits.



Blocked traits are not omitted from calls to device-mode Destinations.

Schema Integration Filters

All customers can filter specific events from being sent to specific Destinations (except for warehouses) by updating their tracking code. Here is an example showing how to send a single message only to Intercom and Google Analytics:

analytics
.
identify
(
'
user_123
'
,
 
{

  
email
:
 
'
jane.kim@example.com
'
,

  
name
:
 
'
Jane Kim
'


},
 
{

  
integrations
:
 
{

    
'
All
'
:
 
false
,

    
'
Intercom
'
:
 
true
,

    
'
Google Analytics
'
:
 
true

  
}


});



Destination flags are case sensitive and match the 
Destination’s name in the docs
 (for example, “AdLearn Open Platform”, “awe.sm”, “MailChimp”, and so on).

Segment Business tier customers can block track calls from delivering to specific Destinations in the Segment UI. Visit a Source Schema page and click on the 
Integrations
 column to view specific schema integration filters. Toggle the filter to block or enable an event to a Destination.



This page was last modified: 23 Jan 2025

Need support?

Questions? Problems? Need more info? Contact Segment Support for assistance!

Help improve these docs!

Was this page helpful?

Thanks for your feedback!

Can we improve this doc? 
Send us feedback!

Get started with Segment

On this page

Was this page helpful?

Thanks for your feedback!

Can we improve
 this doc? 
Send us feedback!

Product

For Developers

Company

Support

© 2025 Segment.io, Inc.

=== Content from https://segment.com/docs/glossary/ ===

Segment Glossary
        



Analytics.js

The Segment Javascript library wrapper, which makes it simple to send your data to any tool without having to learn, test or implement the new API for each tool every time.

API

Application Programming Interface. In software, this term describes a way of interacting with software in an automated fashion, without the use of a user-interface designed for humans. In Segment, we call the standard calls (
track
, 
page
 and 
screen
, 
identify
 and 
group
) our API. We also provide the 
Public API
, which you can use to configure sources and destinations, and perform other functions in your workspace, without logging in using a web browser.

App

The app is what we call the main Segment web application, the part you log in to, and where you configure sources and destinations, see events in the debugger, manage your tracking plan, and so on.

Asynchronous

Asynchronous means occurring without a specific order or sequence. In engineering, it’s most often used for things like when you have to ask a server for a piece of information. If that was a synchronous task, while you were waiting for the server to respond you wouldn’t be able to do anything else, you’d just be stuck waiting. But since it’s asynchronous, you can request information from a server, continue to go do a bunch of other tasks, and then when the server responds you can get back to what you were doing to start. Basically it lets you work on multiple things at once (in parallel) instead of needing to finish everything in the order it comes in. Think of it like one of the ways computers are able to multi-task.

Audience

Personas Audiences
 allow you to define cohorts of users or accounts based on their event behavior and traits that Segment then keeps up-to-date over time. Audiences can be built from your core tracking events, traits, or computed traits. These audiences can then be synced to hundreds of destinations or available using the Profile API.

AWS

Amazon Web Services, a large cloud service provider.

Catalog

Segment’s list of available sources, destinations, and warehouses. You can access the catalog from the 
Segment website
, and from inside the Segment app.

CDN

Content Delivery Network. CDNs are a network of servers which make downloading files faster for the user, by placing them all around the world to reduce data transit time. When you have servers in a CDN, they’re much closer on average to the end user. If someone is using Segment in Asia, they don’t have to download our files all the way from California, they can get them from a server much closer to them in the network.

CDP

Customer Data Platform: a central platform that brokers the flow of customer data through their application infrastructure so that they can build a central understanding of each customer’s interactions, comply with regulations that require them to carefully manage customer data, and ensure that their interactions with the customer are optimized.

Cookies

Cookies are small pieces of text that are stored by the browser on a website. Cookies have a 
name
 and a 
value
. They may also be set from Javascript using 
document.cookie
. Our analytics.js script sets up a cookie called 
ajs_uid
 to store the user’s id.

Every time you make a request to a website, you send along the cookies that you have stored. It’s how the website figures out who you should be logged in as. You can think of it sort of like a passport as a form of ID. When you first enter your username and password, the website generates a secure cookie, and tells your browser to store it. From then on, your browser sends along the cookie as a way of identifying yourself to the website.

Client Side

This refers to a group of libraries that can send data to Segment and which send 
Device-mode
 data. The term “client side” is usually used in contrast to 
Server Side
.

Cloud mode

Cloud mode
, (or 
Server-side
) libraries send data to Segment, where it’s then sent on to destinations. Usually used in contrast to 
Device mode
 libraries, which send the data directly to the destination’s API endpoints.

Cloud mode libraries
 run on a server and are completely invisible to end users. You can find the full list of server-side languages Segment supports 
in the Sources catalog
. These libraries can be used to track a user in realtime as they use your app, but are also able to run batches of calls to update many end users at once. Cloud-mode libraries do not maintain “state” which means every API call you make must include every detail you want to see inside of Segment.

Computed trait

Computed traits
 are per-user or per-account 
traits
 that you create or “compute” in Segment. When you build a computed trait, Segment adds it to relevant user profiles.

Custom trait

Custom traits
 are user or account traits collected from the Identify calls you send to Segment.  For example, you can add any trait that you send as a part of your Identify calls (
email
, 
first_name
, 
last_name
) as custom traits. You can then view them in the Profile explorer, and use them in your audiences, computed traits, and SQL traits.

Debugger

This is the place in the app where you can see your events flowing through Segment.

Destination

A destination is a target for Segment to forward data to, and represents a tool or storage destination.

Device mode

Device mode
, (or 
client-side
) libraries are loaded on the on the user’s client (for example their web browser, or mobile device), which means they can collect contextual data about the user. The three main libraries that are considered “device mode” are 
Analytics.js
, which is used on websites, our 
iOS SDK
, and our 
Android SDK
. Device mode libraries can maintain a cache of information about each user on the device so we know that device belongs to a consistent 
anonymousId
 or 
userId
, and has a consistent list of traits like name, email, etc.

DMP

Data management platform

DSP

Demand-side platform

ETL

Extract, Transform and Load. Referring to the process of extracting data from a production system, transforming it with enriching data into a new format, and then loading it into a data warehouse for analysis (a separate database from production).

Event

An event can refer to either an action by a user, or something a user does which triggers a 
track call
. Events have a name (for example “Product viewed”) and properties (for example the product name, price, and category), and take place in a single moment in time. (This is in contrast to “
objects
” which persist over a period of time, and which have properties that might change.)

Identify

A call that gathers information about who the user is.

JSON

JSON, or JavaScript Object Notation, is a convenient format for storing structured data.

Library

A library is a reusable piece of code which acts as a building block for higher level pieces of code. It’s the software equivalent of a ‘tool’. For instance, take Instagram filters. In the good old days, to take a picture with an interesting filter, you had to adjust the aperture length, the focal length, the exposure and film speed to get the desired effect. But now, you can just use Instagram, and press a button to get the pre-tuned filter that you want. They’ve packaged those effects in software and made them re-usable.

The most common use case for this term at Segment is in reference to the libraries people use to send data to our API. 
A full list of libraries can be found in our docs
. Libraries can collect either in “
Device mode
” or “
Cloud mode
”.

Lookback

A “lookback window” limits the period of time in which data is considered when calculating a trait or audience. For example, you might set a lookback window of 7 days on an audience or trait like 
new_users_7_days
, but you would not add a lookback window to a trait that isn’t time-bounded, such as 
lifetime_value
.

Method

A method is programming speak for “an action an object can take”. It’s a specific type of function that is attached to an object. All of our analytics tracking libraries create 
analytics
 objects, and then 
track
, 
identify
, 
page
, 
etc
 are methods you can invoke on those objects.

MTU

Monthly Tracked Users. You can find more about how they are calculated 
here
 but in short, they are calculated by adding the number unique userIds and number of unique anonymousIds that a customer tracks with Segment.

Object

An object is a type of data that persists over time and can be updated, for example a Business or a User record. Objects have “traits” which record information about that object, and which can change over time. For example a “user” object could have a trait of “email” which doesn’t change often, but could also have a 
computed trait
 like 
logged_in_last_7_days
. (Objects are in contrast to “
events
” which happen at a single moment in time, and whose properties do not change.)

OTT (Over the Top)

Over the top (OTT) refers to content providers that distribute streaming media as a standalone product directly to viewers over the Internet, bypassing telecommunications, multichannel television, and broadcast television platforms that traditionally act as a controller or distributor of such content.

Page

The Segment API call records that a user “viewed a page”. It’s recorded by calling 
analytics.page()
. You can optionally pass in a 
name
 and a 
category
.

Postgres

An open-source 
SQL
 server.

Redshift

An analytics data warehouse from Amazon Web Services. Made for loading in tons of event data, and then analyzing it with complex queries. It’s designed to be fast and cheap.

Retry

How often we attempt to redeliver a payload before marking it as failed.

Replay

The ability to re-send your user data to new destinations and tools.

Schema

A schema is a word used to refer to the design of a database, including what fields there are, and what data types each one contains. If you were to think of a database as a spreadsheet, the schema would be the top row with the field labels, plus the data types and formatting rules for each column.

In Segment, you can send data to a Warehouse which has a schema based on the types of data you collect and route through Segment, and which updates as the data you collect changes.

Server Side

Refers to a group of libraries that can send data to Segment, in 
Cloud mode
, meaning through the Segment cloud. “Server side” can could refer to people sending data directly to 
our HTTP API
 without using a “library”. Usually used in contrast to 
Client Side
.

SDK

Software Development Kit. This is a combination of libraries and used mostly in the context of building mobile or native apps.

SQL

(pronounced ‘sequel’)
Structured Query Language. The standard language for retrieving information from a database. It’s often called “relational” because all the queries are related to relations between the data. For instance, find me all users who are named “Alice” would translate to…

select * from users where first_name equals "Alice"


SQL Trait

SQL traits
 are per-user or per-account traits that you create by running queries against your data warehouse, which can include data not captured using your Segment implementation. Segment imports the results into Personas, and appends these traits to the user profile.

Source

A website, server library, mobile SDK, or cloud application which can send data into Segment.

Spec

Short for “Specification”, the Segment Spec is our set of recommendations for what data to collect and how to format it.

SSP

Supply-side Platform

Throughput

The number of API calls and Objects your account makes, per MTU. The limit on this number is defined by your account plan.

Track

Type of API call (method) which records any actions that users perform on a web or mobile app.

Tracking Plan

A tracking plan is a tool that our customers use to keep track of their analytics setup. It contains all of the events, pages, traits and properties they want to record with information about how each is laid out. It’s often in a spreadsheet or document, but we’re actually building the functionality into the core product so that people can plan out their tracking setup right from inside Segment.

Traits

Traits are individual pieces of information that we know about an 
object
. Objects persist over time, so traits can change over time.

Warehouses

Segment’s Data warehouse product. All event data can be sent to one of several Warehouses, where your team can use it to perform more complex analysis of the data.

This page was last modified: 05 Dec 2019

Need support?

Questions? Problems? Need more info? Contact Segment Support for assistance!

Help improve these docs!

Was this page helpful?

Thanks for your feedback!

Can we improve this doc? 
Send us feedback!

Get started with Segment

Was this page helpful?

Thanks for your feedback!

Can we improve
 this doc? 
Send us feedback!

Product

For Developers

Company

Support

© 2025 Segment.io, Inc.

=== Content from https://segment.com/docs/unify/identity-resolution/identity-resolution-settings/ ===

Identity Resolution Settings
        

Unify requires a Business tier account and is included with Engage.


See the 
available plans
, or 
contact Support
.

On this page

The steps in this guide pertain to spaces created before September 27th, 2020. For spaces created after September 27th, 2020, please refer to the 
Identity Resolution Onboarding
 docs.

Configure Identity Graph rules

Before you connect a source to Unify, Segment recommends that you first review the default Identity settings and configure custom rules as needed. Segment applies configuration updates to all 
new
 data flowing through the space after you save your changes. As a result, if this is your first time setting up your Identity Graph, Segment recommends that you get started with a 
Dev
 space in the 
Space Setup
 docs.

Workspace owners and users with the Identity Admin role can edit the Identity Resolution table.

Changing Identity Resolution rules

Making a space’s Identity Resolution rules less restrictive by changing the 
limit
 shouldn’t cause any issues to existing or future profiles. 
 However, making a space’s rules more restrictive might have an impact existing profiles that don’t adhere to the new rules (for example, decreasing an identifier’s limit or changing the 
priority
 of identifiers).

Segment recommends to get started with a Dev space in the 
Space Setup
 docs, test the rules with the expected data, and then create an identical Production space with those rules. Document any changes to a space’s Identity Resolution rules, and don’t update rules to be more restrictive after profiles already exist outside the bounds of those new rules.

ExternalIDs

Segment creates and merges user profiles based on externalIDs used as identifiers. You can view these externalIDs in the Identities tab of a User Profile in the Profile explorer.



By default, Segment promotes the following traits and IDs in track and identify calls to externalIDs:

You’ll notice that these identifiers have the 
Default
 label next to it under 
Identifier Type
.

To create your own custom externalID, click 
Add Identifier
, and add the following:

These custom identifiers must be sent in the custom 
externalIds
 field in the 
context
 object of any call to the Segment API. The four fields below are all required:

The following example payload adds a custom 
phone
 externalID type:

analytics
.
track
(
'
Subscription Upgraded
'
,
 
{

   
plan
:
 
'
Pro
'
,

   
mrr
:
 
99.99


},
 
{

  
externalIds
:
 
[

    
{

      
id
:
 
'
123-456-7890
'
,

      
type
:
 
'
phone
'
,

      
collection
:
 
'
users
'
,

      
encoding
:
 
'
none
'

    
}

  
]


})



Segment recommends that you add custom externalIDs to the Identity Resolution table 
before
 events containing this identifier flow through the space. Once an event with a new type of externalID flows into the space, the externalID is automatically added to the table if it wasn’t manually added. When the externalID is automatically added, it defaults to the preset priority and limit, as explained below.

Flat matching logic

When a new event flows into Unify, Segment looks for profiles that match any of the identifiers on the event.

Based on the existence of a match, one of three actions can occur:

1: Create a new profile

When there are no pre-existing profiles that have matching identifiers to the event, Segment creates a new user profile.

2: Add to existing profile

When there is one profile that matches all identifiers in an event, Segment attempts to map the traits, identifiers, and events on the call to that existing profile. If there’s an excess of any identifier on the final profile, Segment defers to the Identity Resolution rules outlined below.

3: Merge existing profiles

When there are multiple profiles that match the identifiers in an event, Segment checks the Identity Resolution rules outlined below, and attempts to merge profiles.

One common example of a use-case that can cause inaccurate merges is the Shared iPad setup. For example, many companies now have iPads available in-store for customers to register for an account or submit order information. If different users submit information on the same device, there will now be multiple events sent with the same deviceID. Without Identity Resolution rules in place, Segment might see all these different users merged into the same user profile based on this common identifier.

Segment’s three Identity Resolution rules allow Identity Admins to block incorrect values from causing incorrect merges, to set the maximum number of values allowed per externalID, and to customize the priority of these externalIDs.

Identity Resolution rules

The following rules exist to increase the likelihood that identities are resolved correctly.

Blocked values

Segment recommends that you proactively block certain values from being used as identifiers. While these values will remain in the payload on the event itself, they are not promoted to the externalID object Segment uses to determine user profiles.

This is important when developers have a hard-coded value for fields like user_id during QA or development that then erroneously makes it production. This can cause hundreds of profiles to merge incorrectly and can have costly consequences when these spaces are already feeding data into a production email marketing tool or push notification tool downstream.

In the past, certain default values cause large amounts of profiles to merge incorrectly. Segment suggests that for every externalID, customers opt into automatically blocking the following suggested values:



Before sending data through, Segment also recommends adding any default hard-coded values that your team uses during the development process, such as 
void
 or 
abc123
.

Limit

Identity Admins can specify the total number of values allowed per identifier type on a profile during a certain period. For example, in the image below, the 
anonymous_id
 field has a limit of 
5 Weekly
.


This will vary depending on how companies define a user today. In most cases, companies rely on 
user_id
 to distinguish user profiles and Segment defaults to the following configurations:

Specific cases may deviate from this default. For example, a case where a user can have more than one 
user_id
 but one email, like when 
shopify_id
 and an internal UUID define a user. In this case, an example configuration may be:

When you choose the limit on an identifier, ask the following questions about each of the identifiers you send to Segment:

Priority

Segment considers the priority of an identifier once that identifier exceeds the limit on the final profile.

For example, consider a Unify space with the following Identity Resolution configurations:

A profile already exists with 
user_id
 
abc123
 and 
email
 
jane@example1.com
. A new event comes in with new 
user_id
 
abc456
 but the same 
email
 
jane@example1.com
.

If this event maps to this profile, the resulting profile would then contain two 
user_id
 values and one 
email
. Given that 
user_id
 has a limit of 1, this exceeds the limit of that identifier. As a result, Segment checks the priority of the 
user_id
 identifier. Because 
email
 and 
user_id
 are the two identifiers on the event and 
email
 ranks lower than 
user_id
, Segment demotes 
email
 as an identifier on the incoming event and tries again.

At this point, the event searches for any profiles that match just the identifier user_id 
abc456
. Now there are no existing profiles with this identifier, so Segment creates a new profile with user_id 
abc456
.

By default, Segment explicitly orders user_id and email as rank 
1
 and 
2
, respectively. All other identifiers are in alphabetical order beginning from rank 
3
. This means that if the identifiers sent with events flowing into profiles are user_id, email, anonymous_id, and ga_client_id, the rank would be as follows:

If a new android.id identifier appeared without first giving it explicit order, the order would automatically reshuffle to:

If you require an explicit order for all identifiers, configure this in the Identity Resolution settings page before sending in events.



When choosing the priority of your identifier, ask the following questions about each of the identifiers you send to Segment:

This page was last modified: 07 Nov 2024

Need support?

Questions? Problems? Need more info? Contact Segment Support for assistance!

Help improve these docs!

Was this page helpful?

Thanks for your feedback!

Can we improve this doc? 
Send us feedback!

Get started with Segment

On this page

Was this page helpful?

Thanks for your feedback!

Can we improve
 this doc? 
Send us feedback!

Product

For Developers

Company

Support

© 2025 Segment.io, Inc.

=== Content from https://segment.com/docs/unify/ ===

Unify Overview
        

Unify requires a Business tier account and is included with Engage.


See the 
available plans
, or 
contact Support
.

On this page

Use Segment Unify, formerly known as Profiles, for a complete view of your customers.

With 
Identity Resolution
, track every interaction across the entire user journey to create unified, real-time customer identities. View user profiles in one place through the 
Profile explorer
 in the Segment app. Use the 
Profile API
 to programmatically query user profiles, traits, and events.

You can then use this interaction data with customer engagement tools, such as Engage, to deliver personalized, omnichannel experiences.

If you need to troubleshoot or learn about your profile data, use 
Profiles Insights
 for a transparent view of your Unify profiles.

Getting started

Unify is an add-on to Segment Connections Business Tier. It’s also a required add-on for Twilio Engage.
To use 
Computed Traits
 and 
Audiences
 with Unify, you must have access to Engage.

To set up and get data flowing through Unify, visit Segment’s 
Onboarding Guide
.

Identity Resolution

Set 
Identity Resolution
 rules to take event data from across devices and channels and intelligently merge it into complete user- or account-level profiles. This enables you to understand customer behavior as it evolves in real-time across multiple touchpoints.

With Identity Resolution:

Visit Segment’s 
Identity Resolution docs
 to learn more.

Profile explorer

Use the Profile explorer to view all user data, including their event history, traits, and identifiers.

With the Profile explorer, you have a complete view of your customers.

If you’re using Engage, use the Profile explorer to view audiences, traits, journey membership, and 
subscription states
 for email and phone numbers.

Enrich profiles with traits

With Unify Plus, you can add detail to user profiles with new traits and use them to power personalized marketing campaigns. Add new traits to your user or account profiles using:

Profile API

Use Segment’s Profile API to programmatically access all traits stored for a user. This includes the 
external_ids
, 
traits
, and 
events
 that make up a customer’s journey with your product.

Use the Profile API to help your organization:

Visit Segment’s 
Profile API doc
 for more information.

Profiles Insights

Use Profiles Insights to troubleshoot your event data with a transparent view of your Unify profiles.

Learn about your events and identifiers on your profiles and answer questions such as why two profiles didn’t merge, why an event wasn’t resolved to a profile, or why an external ID isn’t present.

Visit the 
Profiles Insights
 doc to learn more.

Profiles Sync

Use Profiles Sync to connect identity-resolved customer profiles to a data warehouse of your choice.

With a continual flow of synced profiles, teams can enrich and use these data sets as the basis for new audiences and models. Profiles Sync addresses a number of use cases, with applications for identity graph monitoring, attribution analysis, machine learning, and more.

Visit the 
Profiles Sync Setup
 doc to learn more.

Next steps: activate your profiles with Engage

For Engage users, after you set up your identity rules and have data flowing through Unify, you can activate profiles to deliver personalized engagement experiences. Visit the 
Engage docs
 to learn more.

This page was last modified: 26 Jun 2024

Need support?

Questions? Problems? Need more info? Contact Segment Support for assistance!

Help improve these docs!

Was this page helpful?

Thanks for your feedback!

Can we improve this doc? 
Send us feedback!

Get started with Segment

On this page

Was this page helpful?

Thanks for your feedback!

Can we improve
 this doc? 
Send us feedback!

Product

For Developers

Company

Support

© 2025 Segment.io, Inc.

=== Content from https://segment.com/docs/connections/reverse-etl/reverse-etl-source-setup-guides/databricks-setup/ ===

Databricks Reverse ETL Setup
        

On this page

Set up Databricks as your Reverse ETL source.

At a high level, when you set up Databricks for Reverse ETL, the configured service-principal needs read permissions for any resources (databases, schemas, tables) the query needs to access. Segment keeps track of changes to your query results with a managed schema (
__SEGMENT_REVERSE_ETL
), which requires the configured service-principal to allow write permissions for that schema. Segment supports only OAuth (M2M) authentication. To generate a client ID and Secret, follow the steps listed in Databricks’ 
OAuth machine-to-machine (M2M) authentication
 documentation.

Databricks Reverse ETL sources support Segment's dbt extension

If you have an existing dbt account with a Git repository, you can use 
Segment’s dbt extension
 to centralize model management and versioning, reduce redundancies, and run CI checks to prevent breaking changes.

Required permissions

Make sure the service principal you use to connect to Segment has permissions to use that warehouse. In the Databricks console go to 
SQL warehouses
 and select the warehouse you’re using. Navigate to 
Overview > Permissions
 and make sure the service principal you use to connect to Segment has 
can use
 permissions.

To grant access to read data from the tables used in the model query, run:

  GRANT USAGE ON SCHEMA <schema_name> TO `<service principal you are using to connect to Segment>`; 
  GRANT SELECT, READ_METADATA ON SCHEMA <schema_name> TO `<service principal you are using to connect to Segment>`; 


To grant Segment access to create a schema to keep track of the running syncs, run:

  GRANT CREATE on catalog <name of the catalog, usually hive_metastore or main if using unity-catalog> TO `<service principal you are using to connect to Segment>`;


If you want to create the schema yourself instead and then give Segment access to it, run:

  CREATE SCHEMA IF NOT EXISTS __segment_reverse_etl; 
  GRANT ALL PRIVILEGES ON SCHEMA __segment_reverse_etl TO `<service principal you are using to connect to Segment>`;


Set up guide

To set up Databricks as your Reverse ETL source:

Segment previously supported token-based authentication, but now uses OAuth (M2M) authentication at the recommendation of Databricks.
If you previously set up your source using token-based authentication, Segment will continue to support it. If you want to create a new source or update the connection settings of an existing source, Segment only supports 
OAuth machine-to-machine (M2M) authentication
.

After you’ve successfully added your Databricks source, 
add a model
 and follow the rest of the steps in the Reverse ETL setup guide.

This page was last modified: 22 May 2024

Need support?

Questions? Problems? Need more info? Contact Segment Support for assistance!

Help improve these docs!

Was this page helpful?

Thanks for your feedback!

Can we improve this doc? 
Send us feedback!

Get started with Segment

On this page

Was this page helpful?

Thanks for your feedback!

Can we improve
 this doc? 
Send us feedback!

Product

For Developers

Company

Support

© 2025 Segment.io, Inc.

=== Content from https://segment.com/docs/engage/campaigns/whatsapp-campaigns/ ===

WhatsApp Campaigns
        

Engage Premier requires a Business tier account and includes Engage Foundations and Unify.


See the 
available plans
, or 
contact Support
.

On this page

Engage Premier entered an End of Sale (EOS) period effective  June 10, 2024. Existing Segment customers will continue to have access and support to Engage Premier until an end-of-life (EOL) date is announced. We recommend exploring the following pages in preparation of a migration or future MCM needs:

Twilio Marketing Campaigns

Preferred ISV Partners:

Airship Blog
 


Bloomreach Blog
 


Braze Blog
 


Insider Blog
 


Klaviyo Blog
 


Twilio Engage Foundations Documentation
 

How Engage campaigns work

Twilio Engage uses Journeys to send WhatsApp, email, and SMS campaigns.  With Journeys, you add conditions and steps that trigger actions like sending a WhatsApp message.

You’ll build and send your WhatsApp campaign in three stages:

WhatsApp Templates

To send a WhatsApp campaign, you’ll first need an approved WhatsApp template. For instructions on building a template, view 
WhatsApp Templates
.

Create a Journey

Because Engage campaigns exist within Journeys, begin by creating a Journey:

Segment then opens the Journey Builder.

Add a Journey condition

With your Journey created, you’ll now set a condition to trigger your WhatsApp campaign:

With your entry condition added, you’re ready to add an approved WhatsApp template to build a campaign.

Add a WhatsApp step and publish your Journey

Your Journey and WhatsApp campaign are now live. Users who trigger the WhatsApp step’s parent Journey condition will receive your SMS campaign.

Messaging limits

WhatsApp limits the number of unique recipients that can receive your campaigns. If your Meta Business Account isn’t verified, you’ll begin with a messaging limit of 250 unique recipients every 24 hours.

Once your Meta Business Account is verified, the number of unique recipients increases, depending on your messaging limit tier. For more information, view Meta’s 
messaging limits documentation
.

This page was last modified: 15 Jul 2024

Need support?

Questions? Problems? Need more info? Contact Segment Support for assistance!

Help improve these docs!

Was this page helpful?

Thanks for your feedback!

Can we improve this doc? 
Send us feedback!

Get started with Segment

On this page

Was this page helpful?

Thanks for your feedback!

Can we improve
 this doc? 
Send us feedback!

Product

For Developers

Company

Support

© 2025 Segment.io, Inc.

=== Content from https://segment.com/docs/segment-app/iam/labels/ ===

Using Label-Based Access Control
        

Advanced Access Management is available for all Business plans.


See the 
available plans
, or 
contact Support
.

On this page

Labels let workspace owners assign permissions to users by organizing resources into groups. Groups can represent collections of 
sources
 or 
spaces
.

To create or configure labels in your Segment workspace, go to 
Settings > Admin
, then click the Label Management tab. Only Workspace Owners can manage labels for the entire workspace.

All workspaces include labels for 
Dev
 (development) and 
Prod
 (production) environments. Business Tier customers can create an unlimited number of labels.

Custom environments

By default, all workspaces include labels for 
Dev
 (development) and 
Prod
 (production) environments. Workspace Owners can configure what these labels are applied to, and can create up to 5 custom environments.

Labels must use the 
key:value
 format. Both the key and value must begin with a letter, and they can only contain letters, numbers, hyphens, or dashes.

To apply labels to sources and spaces, click the 
Assign Labels
 tab from the Manage Labels screen. In the screen that appears, select the sources and spaces to apply the label to.

Once a label is in use (either assigned to a resource or used to restrict permissions on a user), the label cannot be deleted. You must first manually remove the label from any resources and permissions before you can delete it.

While only Workspace Owners can bulk-edit labels, source and space admins can edit the labels on the sources and spaces they have access to. To do this, go to the 
Settings
 tab for each item.

Workspace Owners can also grant specific 
role access
 to specific labels. For example, you might give a Source Admin access to only sources that have the 
Prod
 label.

Permissions can then be assigned to users in Access Management by label, on the Source Admin, Source Read-Only, Engage Admin, Engage User and Engage Read-Only users.



Custom labels

All Segment workspaces can create up to 5 custom labels. Additional label types (including environment labels) are available to Segment Business Tier accounts.

To create additional custom labels, a Workspace Owner can create new key types in the Manage Labels screen. The Workspace Owner can customize any combination of labels to mirror how resources should be partitioned in their organization.

For example, some organizations may restrict access to sources and spaces by brand or product area, while others might organize resources by tech stack or engineering department.

When you create a new key, it becomes available in the Sources page as a column type that can be used to organize sources.

FAQ

You can create labels for sources and spaces from Segment workspace by going to 
Settings -> Admin
 and then clicking the 
Label Management
 tab.

You can apply labels to sources and spaces.

You can assign labels to sources and spaces using the 
Assign Labels
 tab in the 
Manage Labels
 screen. Source Admins and Space Admins can edit the labels on their individual resources in the 
Settings
 tab.

Once a label has been created and has been assigned to resources within the workspace, workspace owners can use these labels to restrict permissions on user access, restrict which sources can be connected to a space through a Connection Policy, and organize sources by viewing these labels as columns in the Sources page.

Workspace owners can only delete a label if it’s not in use. See 
Custom Environments
 for details on removing labels.

No. If you need to rename a label, first create a new label, assign it to all resources using the old label, and then delete the old label.

No, each resource can have only one value per label category. This prevents confusion about permissions. For example, if a user has access to 
brand:A
, it’s unclear whether they should also have access to sources labeled both 
brand:A
 and 
brand:B
. Limiting resources to one value per category avoids this confusion.

Labels are additive, meaning they can only further restrict a user’s permissions. For example, if a user has access to everything labeled 
environment:production
, then they’re not restricted by other label categories. This results in broader permissions compared to a user with access to both 
environment:production
 AND 
region:apac
.

For example, if the following sources had these set of labels:

Then the following users with Source Admin restricted with labels will only have access to the following sources:

To grant a user access to sources labeled 
brand:a
 or 
brand:b
, use group permissions. Create two groups: one with Source Admin access to 
brand:a
 and another with Source Admin access to 
brand:b
, then assign the user to both groups.

This page was last modified: 07 Feb 2025

Need support?

Questions? Problems? Need more info? Contact Segment Support for assistance!

Help improve these docs!

Was this page helpful?

Thanks for your feedback!

Can we improve this doc? 
Send us feedback!

Get started with Segment

On this page

Was this page helpful?

Thanks for your feedback!

Can we improve
 this doc? 
Send us feedback!

Product

For Developers

Company

Support

© 2025 Segment.io, Inc.

=== Content from https://segment.com/docs/guides/how-to-guides/forecast-with-sql/ ===

Forecasting LTV with SQL and Excel for E-Commerce
        

On this page

Customer Lifetime Value (“LTV”) is the amount of money that an individual customer will spend with a given business in the future. It’s often used to value cohorts in your customer base, determine how much to spend in acquiring or retaining new users in a given cohort, rank customers, and measure the success of marketing activities from a baseline LTV forecast.

The LTV calculation is not straightforward for e-commerce businesses, since future payments are not contractual: at any moment, a customer may never make a single purchase again. Additionally, forecasting future purchases requires statistical modeling that many current LTV formulas lack.

This guide shows how to calculate forward-looking LTV for non-contractual businesses using SQL and Excel. This analytical approach allows you to accurately rank your highest value customers, as well as predict their future purchase sizes to help focus your marketing efforts.

This guide assumes you’re using the tracking schema described in 
How to implement an e-commerce tracking plan
 and are storing data in a 
Segment Warehouse
.

Talk to a product specialist
 to learn how companies like Warby Parker and Crate & Barrel use a data warehouse to increase engagement and sales.

Calculating LTV: Buy ‘Til You Die

In a non-contractual setting, you can’t use a simple retention rate to determine when customers terminate their relationship. This is because the retention rate is a linear model that doesn’t accurately predict whether a customer has ended her relationship with the company or is merely in the midst of a long hiatus between transactions.

The most accurate non-contractual LTV model, named “Buy Til You Die” (“BTYD”), focuses on calculating the discounted estimation of future purchases based on recency of last purchase, frequency of purchases, and average purchase value. This model uses non-linear modeling to predict whether or not a user is “alive” or “dead” given historic transactions to forecast future probability and size of purchases.

Since LTV is a critical metric for e-commerce companies, it’s important that this model, instead of simpler linear formula that is based on retention rates, is used for it’s calculation.

Use SQL to build the necessary table, which will be exported as a CSV and opened in Google Sheets. Then, use Solver to estimate the predictive model parameters, which ultimately calculates the future purchases of each customer. Finally, the LTV calculation is simply the net present value of each customer’s future purchases. Rank them by LTV, then find behavioral patterns across the top 10 or 50 customers to figure out how best to target or retain this cohort.

Recency, frequency, and average size

As a growth analyst at the fictitious on-demand artisanal toast company, Toastmates, it’s important to know which customers are worth more to the business than others. Most important, you should understand what similarities these customers all have to help guide the marketing team in their efforts.

The first step in creating the BTYD model is to get historic purchasing data of at least a month. In your analysis, you can use data from the past six months. The data must include the columns 
userId
 (email is fine too), number of purchases within the specified time window, days since last purchase, and days since first purchase.

Then, use 
this Google Sheet
, which provides all of the complex calculations for estimating the model parameters, as well as forecasting the future sales of each customer. This sheet is View Only, so be sure to copy it entirely so you can use it.

To retrieve a table with the right columns for analysis, use the follow SQL query:

 
 
 
 
with


 
 
 
 
first_transaction
 
as
 
(


 
 
 
 
 
 
 
 
select
 
 
u
.
email
,


 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
datediff
(
'day'
,
 
min
(
oc
.
received_at
)::
date
,
 
current_date
)
 
as
 
first


 
 
 
 
 
 
 
 
 
 
from
 
 
toastmates
.
order_completed
 
oc


 
 
 
 
 
left
 
join
 
 
toastmates
.
users
 
u


 
 
 
 
 
 
 
 
 
 
 
 
on
 
 
oc
.
user_id
 
=
 
u
.
email


 
 
 
 
 
 
 
 
 
where
 
 
oc
.
received_at
 
>
 
dateadd
(
'month'
,
 
-
6
,
 
current_date
)


 
 
 
 
 
 
group
 
by
 
 
1


 
 
 
 
),


 
 
 
 
frequency
 
as
 
(


 
 
 
 
 
 
 
 
select
 
 
u
.
email
,


 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
count
(
distinct
 
oc
.
checkout_id
)
 
as
 
frequency


 
 
 
 
 
 
 
 
 
 
from
 
 
toastmates
.
order_completed
 
oc


 
 
 
 
 
left
 
join
 
 
toastmates
.
users
 
u


 
 
 
 
 
 
 
 
 
 
 
 
on
 
 
oc
.
user_id
 
=
 
u
.
email


 
 
 
 
 
 
 
 
 
where
 
 
oc
.
received_at
 
>
 
dateadd
(
'month'
,
 
-
6
,
 
current_date
)


 
 
 
 
 
 
group
 
by
 
 
1


 
 
 
 
),


 
 
 
 
last_transaction
 
as
 
(


 
 
 
 
 
 
 
 
select
 
 
u
.
email
,


 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
datediff
(
'day'
,
 
max
(
oc
.
received_at
)::
date
,
 
current_date
)
 
as
 
last


 
 
 
 
 
 
 
 
 
 
from
 
 
toastmates
.
order_completed
 
oc


 
 
 
 
 
left
 
join
 
 
toastmates
.
users
 
u


 
 
 
 
 
 
 
 
 
 
 
 
on
 
 
oc
.
user_id
 
=
 
u
.
email


 
 
 
 
 
 
 
 
 
where
 
 
oc
.
received_at
 
>
 
dateadd
(
'month'
,
 
-
6
,
 
current_date
)


 
 
 
 
 
 
group
 
by
 
 
1


 
 
 
 
),
 


 
 
 
 
average_transaction_size
 
as
 
(


 
 
 
 
 
 
 
 
select
 
 
u
.
email
,


 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
avg
(
oc
.
total
)
 
as
 
avg


 
 
 
 
 
 
 
 
 
 
from
 
 
toastmates
.
order_completed
 
oc


 
 
 
 
 
left
 
join
 
 
toastmates
.
users
 
u


 
 
 
 
 
 
 
 
 
 
 
 
on
 
 
oc
.
user_id
 
=
 
u
.
email


 
 
 
 
 
 
 
 
 
where
 
 
oc
.
received_at
 
>
 
dateadd
(
'month'
,
 
-
6
,
 
current_date
)


 
 
 
 
 
 
group
 
by
 
 
1


 
 
 
 
 
 
order
 
by
 
 
2
 
desc


 
 
 
 
)


 
 
 
 
 
 
 
 
select
 
 
distinct
 


 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
u
.
email
,


 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
nvl
(
f
.
frequency
,
 
0
)
 
as
 
frequency
,


 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
nvl
(
z
.
last
,
 
0
)
 
as
 
days_since_last_transaction
,


 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
nvl
(
a
.
first
,
 
0
)
 
as
 
days_since_first_transaction
,


 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
t
.
avg
 
as
 
average_transaction_size


 
 
 
 
 
 
 
 
 
 
from
 
 
toastmates
.
users
 
u


 
 
 
 
 
left
 
join
 
 
first_transaction
 
a


 
 
 
 
 
 
 
 
 
 
 
 
on
 
 
u
.
email
 
=
 
a
.
email


 
 
 
 
 
left
 
join
 
 
frequency
 
f


 
 
 
 
 
 
 
 
 
 
 
 
on
 
 
u
.
email
 
=
 
f
.
email


 
 
 
 
 
left
 
join
 
 
last_transaction
 
z


 
 
 
 
 
 
 
 
 
 
 
 
on
 
 
u
.
email
 
=
 
z
.
email


 
 
 
 
 
left
 
join
 
 
average_transaction_size
 
t


 
 
 
 
 
 
 
 
 
 
 
 
on
 
 
u
.
email
 
=
 
t
.
email


 
 
 
 
 
 
order
 
by
 
 
2
 
desc



This returns a table where each row is a unique user and the columns are email, number of purchases within the time window, number of discrete time units since last purchase, and average purchase order.



Here is a screenshot of the first twelve rows returned from the query in Mode Analytics.

Export this data to a CSV, then copy and paste it in the first sheet of the Google Sheet where the blue type is in the below screenshot:



Also be sure to add the total time in days in cell B6. This is important as the second sheet uses this time duration for calculating net present value of future payments.

How to use the Google Spreadsheet

After you paste in the CSV from the table into the first tab of the sheet, the next step is to estimate the model parameters (the variables on the top left of the sheet). In order to do this, we need to use a feature of Microsoft Excel called Solver.

You can export your Google Sheet as an Excel document. Then, use Excel Solver to minimize the log-likelihood number in cell B5, while keeping the parameters from B1:B4 greater than 0.0001.



After Solver runs, cells B1:B4 will be updated to represent the model’s estimates. Now, you can hard code those back into the sheet on Google Sheets. The next sheet relies on these model estimates to calculate the expected purchases per customer.

Model and predict future customer purchases

The model requires four pieces of information about each customer’s past purchasing history: her “recency” (how many “time units” her last transaction occurred), “frequency” (how many transactions she made over the specified time period), the length of time over which we have observed her purchasing behavior, and the average transaction size.

In the example, you have the purchasing behavior data over the course of six months with each unit of time being a single day.

You can apply a both a beta-geometric and a negative binomial distribution (“BG/NBD”) to these inputs and then use Excel to estimate the model parameters (an alternative would be the Pareto/NBD model). These probability distributions are used because they accurately reflect the underlying assumptions of the aggregation of realistic individual buying behavior. (
Learn more about these models
).

After estimating the model parameters, you can predict a particular customer’s conditional expected transactions by applying the same historic purchasing data to Bayes’ Theorem, which describes the probability of an event based on prior knowledge of conditions related to the event.

Estimating the model parameters

The top left part of the first sheet represent the parameters of the BG/NBD model that must be fitted to the historic data you paste in. These four parameters (r, alpha, a, and b) will have “starting values” of 1.0, since you’ll use Excel Solver to determine their actual values.

The values in columns F to J represent variables in the BG/NBD model. Column F, in particular, defines a single customer’s contribution to a the overarching function, on which we’ll use Solver to determine the parameters. In statistics, this function is called the likelihood function, which is a function of the parameters of a statistical model.

In this particular case, this function is the log-likelihood function, which is B5, as calculated as the sum of all cells in column F. Logarithmic functions are easier to work with, since they achieve its maximum value at the same points as the function itself. With Solver, find the maximum value of B5 given the parameters in B1:B4.

With the new parameter estimates, you can now predict a customer’s future purchases.

Predicting a customer’s future purchases

In the next sheet, you can apply Bayes’ Theorem to the historic purchasing information to forecast the quantity of transactions in the next period. Multiply the expected quantity with the average transaction size to calculate the expected revenue for that period, which you can extrapolate as an annuity, of which you can find the present discounted value (assuming discount rate is 10%).

Central to the Bayes’ Theorem formula is the Gaussian hypergeometric function, which is defined by “2F1” in column M. Evaluate the hypergeometric function as if it were a truncated series: by adding terms to the series until each term is small enough that it becomes trivial. In the spreadsheet, we sum the series to it’s 50th term.

The rest of the variables in Bayes’ Theorem is in columns I through L, which use the inputs from the customer’s historic purchasing information, as well as the model parameter estimates as determined from Solver (cells B1:B4).

The expected quantity of purchases in the next time period is calculated in column H.

Finally, multiply that with the average transaction size and you can get the expected revenue for the next time period.

Rank your customers

This exercise allows you to rank your customers from most valuable to least by ordering column F in descending order. You can take the 
userId
 s of the top several customers and look across their shopping experiences to identify any patterns that they share, to understand what behaviors are leading indicators to becoming high value customers.

Below is a simple query to get a table of a user’s actions in rows. Just replace the 
user_id
with the user in question.

 
 
 
 
with
 
anonymous_ids
 
as
 
(


 
 
 
 
 
 
 
 
select
 
 
anonymous_id
 
from
 
toastmates
.
tracks


 
 
 
 
 
 
 
 
 
where
 
 
user_id
 
=
 
'46X8VF96G6'


 
 
 
 
 
 
group
 
by
 
 
1


 
 
 
 
),



 
 
 
 
page_views
 
as
 
(


 
 
 
 
 
 
 
 
select
 
 
*


 
 
 
 
 
 
 
 
 
 
from
 
 
toastmates
.
pages
 
p


 
 
 
 
 
 
 
 
 
where
 
 
p
.
user_id
 
=
 
'46X8VF96G6'


 
 
 
 
 
 
 
 
 
 
 
 
or
 
 
anonymous_id
 
in
 
(
select
 
anonymous_id
 
from
 
anonymous_ids
)


 
 
 
 
 
 
order
 
by
 
 
p
.
received_at
 
desc


 
 
 
 
),



 
 
 
 
track_events
 
as
 
(


 
 
 
 
 
 
 
 
select
 
 
*


 
 
 
 
 
 
 
 
 
 
from
 
 
toastmates
.
tracks
 
t


 
 
 
 
 
 
 
 
 
where
 
 
t
.
user_id
 
=
 
'46X8VF96G6'


 
 
 
 
 
 
 
 
 
 
 
 
or
 
 
anonymous_id
 
in
 
(
select
 
anonymous_id
 
from
 
anonymous_ids
)


 
 
 
 
 
 
order
 
by
 
 
t
.
received_at
 
desc


 
 
 
 
)



 
 
 
 
 
 
select
 
 
url
,


 
 
 
 
 
 
 
 
 
 
 
 
 
 
received_at


 
 
 
 
 
 
 
 
from
 
 
page_views


 
 
 
 
 
 
union
 
 


 
 
 
 
 
 
select
 
 
event_text
,


 
 
 
 
 
 
 
 
 
 
 
 
 
 
received_at


 
 
 
 
 
 
 
 
from
 
 
track_events


 
 
 
 
order
 
by
 
 
received_at
 
desc



This above query for user whose 
user_id
 is 
"46X8VF96G6"
 returns the below table:



At Toastmates, most of the highest forward-looking expected LTV customers share one thing in common: averaging two orders per month with an average purchase size of $20.

With that in mind, you can define a behavioral cohort in our email tool, Customer.io, as well as create a trigger workflow so we can send an email offer to these customers.

Learn how to use email tools to target this cohort of high value customers.

Reward your best customers

This exercise is useful not only as a forward looking forecasting model for customer LTV, but also as a quality ranking system to see which customers are worth more to your business. Coupled with the ability to glance across the entire shopping experience of a given customer, you can identify broad patterns or specific actions that may be an early signal for a high value shopper. Recognizing these high value shoppers means being proactive in nurturing, rewarding, and retaining them.

And this is just the beginning. Having a rich set of raw customer data allows you to create accurate projection models for LTV so you know not only how much you can spend to acquire them, but also how to rank your customers by value. Ultimately, these insights lead to the right actions that can build an engaging shopping experience and drive sales.

Talk to a product specialist
 
to learn how companies like Warby Parker and Crate & Barrel use a data warehouse to increase engagement and sales.

This page was last modified: 15 Mar 2024

Need support?

Questions? Problems? Need more info? Contact Segment Support for assistance!

Help improve these docs!

Was this page helpful?

Thanks for your feedback!

Can we improve this doc? 
Send us feedback!

Get started with Segment

On this page

Was this page helpful?

Thanks for your feedback!

Can we improve
 this doc? 
Send us feedback!

Product

For Developers

Company

Support

© 2025 Segment.io, Inc.

=== Content from https://segment.com/docs/connections/delivery-overview/ ===

Delivery Overview
        

On this page

Delivery Overview is a visual observability tool designed to help Segment users diagnose event delivery issues for any cloud-streaming destination receiving events from cloud-streaming sources.

Delivery Overview for RETL destinations and Engage Audience Syncs currently in development

This means that Segment is actively developing Delivery Overview features for RETL destinations and Engage Audience syncs. Some functionality may change before Delivery Overview for these integrations becomes generally available.

Delivery Overview is generally available for streaming connections (cloud-streaming sources and cloud-streaming destinations) and in public beta for storage destinations. Some metrics specific to storage destinations, like selective syncs, failed row counts, and total rows seen, are not yet available. 
All users of Delivery Overview have access to the Event Delivery tab, and can configure delivery alerts for their destinations.

Key features

Delivery Overview has three core features:

You can refine these tables using the time picker and the metric toggle, located under the destination header. With the time picker, you can specify a time period (last 10 minutes, 1 hour, 24 hours, 7 days, 2 weeks, or a custom date range over the last two weeks) for which you’d like to see data. With the metric toggle, you can switch between seeing metrics represented as percentages (for example, 
85% of events
 or 
a 133% increase in events
) or as counts (
13 events
 or 
an increase of 145 events
.) Delivery Overview shows percentages by default.

Pipeline view

The pipeline view provides insights into each step your data is processed by enroute to the destination, with an emphasis on the steps where data can be discarded due to errors or your filter preferences. Each step provides details into counts, change rates, and event details (like the associated Event Type or Event Names), and the discard steps (Failed on ingest, Filtered at source, Filtered at destination, & Failed delivery) provide you with the reasons events were dropped before reaching the destination. Discard steps also include how to control or alter that outcome, when possible. The pipeline view also includes a label between the Filtered at destination and Failed delivery steps indicating how many events are currently pending retry.

Lookback window

Delivery Overview applies a 5-minute lookback period to provide stable, accurate metrics across all pipeline steps. This interval accounts for processing delays and ensures the data Segment displays reflects a reliable snapshot of recent events.

The pipeline view for classic destinations includes the following steps:

The pipeline view for Actions destination includes the following steps:



The pipeline view for storage destination includes the following steps:

The following image shows a storage destination with 23 partially successful syncs:



Breakdown table

The breakdown table provides you with greater detail about the selected events.

To open the breakdown table, select either the first step in the pipeline view, the last step in the pipeline view, or select a discard step and then click on a discard reason.

The breakdown table displays the following details:

1:
 
Segment calculates the related change percentage by subtracting the percent of events impacted in the previous time period from the percent of impacted events in the current time period. For example, if last week 15% of your events were filtered at a source, but this week, 22% of your events were filtered at a source, you would have a related change percentage of 7%.

Discard table

The discard table provides you with greater detail about the events that failed to deliver or were filtered out of your sources and destinations.

To open the discard table, click on one of the discard steps. If you click on a row in the discard table, you can see the breakdown table for the discarded events.

The discard table displays the following details:

1:
 
Segment calculates the related change percentage by subtracting the percent of events impacted in the previous time period from the percent of impacted events in the current time period. For example, if last week 15% of your events were filtered at a source, but this week, 22% of your events were filtered at a source, you would have a related change percentage of 7%.

When should I use Delivery Overview?

Delivery Overview is useful to diagnose delivery errors in the following scenarios:

Delivery Overview in Engage Destinations

Because Engage uses sources for multiple purposes, you can expect to see 
filtered at destination
 events with the integrations object in destinations linked to Engage. Engage uses the integrations object to route events to destinations you’ve added to your audiences, traits, and journey steps. As a result, some events aren’t meant to be delivered by the destination, so the integrations object filters them.

Where do I find Delivery Overview?

To view the Delivery Overview page:

How do I use Delivery Overview?

To use Delivery Overview:

How does Delivery Overview differ from other Segment monitoring and observability tools?

With Source Debugger or Event Delivery, you can only verify that events are successfully making it from your source or to your destination. If events fail, you have to troubleshoot to see where in the pipeline your events are getting stuck. With Event Tester, you can verify that your event makes it from your source to your destination, but if the results aren’t what you expected, you’re stuck troubleshooting your source, filters, tracking plans, and destinations.

With Delivery Overview, you can verify that your source receives your events, that any filters and tracking plans work as expected, and that events successfully make it to your destination. Any errors or unexpected behavior can be identified using the pipeline view, leading to quicker resolution.

How can I configure alerts?

You can use the Event Delivery alerting features (Delivery Alerts) by selecting the 
Alerts
 tab in the destination header. Once you enable alerts, if the successful delivery rate of all events is less than the threshold percentage in the last 24 hours, you’ll be notified through in-app notification and/or workspace email.

Note that this is dependent on your 
notification settings
. For example, if the threshold is set to 99%, then you’ll be notified each time less than 100% of events fail.

You can also use 
Connections Alerting
, a feature that allows Segment users to receive in-app, email, and Slack notifications related to the performance and throughput of an event-streaming connection.

Connections Alerting allows you to create two different alerts:

How “fresh” is the data in Delivery Overview?

The data in Delivery Overview has an expected latency of approximately 30 seconds after event ingestion, but this may vary, depending on the features you’ve enabled in your workspace and spikes in volume. Segment delays the data visible in the Delivery Overview UI by 5 minutes to allow for more precise metric correlation. Segment does not impose the 5 minute delay if you access data using the Public API.

Why is the Delivery Overview page only available for cloud-mode destinations?

Similar to Segment’s 
Event Delivery
 feature, the Delivery Overview page is only available for server-side integrations (also known as cloud-mode destinations). You won’t be able to use the Delivery Overview page for client side integrations (also known as device-mode destinations) because device-mode data is sent directly to the destination tool’s API. In order to report on deliverability, data must be sent to destinations using a server-side connection.

Troubleshooting

The Delivery Overview pipeline steps Failed on Ingest, Filtered at Source, Filtered at Destination, and Failed Delivery display a 
discard table
 with information about why your events failed or were discarded.

This table provides a list of all possible discard reasons available at each pipeline step.



Show all


All


Failed on Ingest


Filtered at Source


Filtered at Destination


Failed Delivery

This page was last modified: 14 Nov 2024

Need support?

Questions? Problems? Need more info? Contact Segment Support for assistance!

Help improve these docs!

Was this page helpful?

Thanks for your feedback!

Can we improve this doc? 
Send us feedback!

Get started with Segment

On this page

Was this page helpful?

Thanks for your feedback!

Can we improve
 this doc? 
Send us feedback!

Product

For Developers

Company

Support

© 2025 Segment.io, Inc.

=== Content from https://segment.com/docs/engage/audiences/product-based-audiences-nutrition-label/ ===

Product Based Audiences Nutrition Facts Label
        

Engage Foundations requires a Business tier account and includes Unify.


See the 
available plans
, or 
contact Support
.

Twilio’s 
AI Nutrition Facts
 provide an overview of the AI feature you’re using, so you can better understand how the AI is working with your data. Twilio outlines AI qualities in Product Based Audiences in the Nutrition Facts label below. For more information, including the AI Nutrition Facts label glossary, refer to the 
AI Nutrition Facts
 page.



AI Nutrition Facts






Product Based Recommendation Audiences

Description






Product Based Audiences lets customers improve marketing campaigns by segmenting users based on preferences like product, category, or brand to automate the creation and maintenance of personalized recommendations for businesses in the retail, media, and entertainment industries. 

Privacy Ladder Level 


2



Feature is Optional


Yes

Model Type 


Predictive 

Base Model 


AWS Personalize - Hierarchical recurrent neural network

Trust Ingredients

Base Model Trained with Customer Data


N/A

Customer Data is Shared with Model Vendor


No
 

Training Data Anonymized   


 No

Data Deletion


Yes

Human in the Loop 


N/A

Data Retention  


30 days

Input/Output Consistency


N/A

Other Resources

This page was last modified: 19 Sep 2024

Need support?

Questions? Problems? Need more info? Contact Segment Support for assistance!

Help improve these docs!

Was this page helpful?

Thanks for your feedback!

Can we improve this doc? 
Send us feedback!

Get started with Segment

Was this page helpful?

Thanks for your feedback!

Can we improve
 this doc? 
Send us feedback!

Product

For Developers

Company

Support

© 2025 Segment.io, Inc.

=== Content from https://segment.com/docs/engage/campaigns/email-campaigns/ ===

Email Campaigns
        

Engage Premier requires a Business tier account and includes Engage Foundations and Unify.


See the 
available plans
, or 
contact Support
.

On this page

Engage Premier entered an End of Sale (EOS) period effective June 10, 2024. Existing Segment customers will continue to have access and support to Engage Premier until an end-of-life (EOL) date is announced. Segment recommends exploring the following pages in preparation of a migration or future MCM needs:

Twilio Marketing Campaigns

Preferred ISV Partners:

Airship Blog
 


Bloomreach Blog
 


Braze Blog
 


Insider Blog
 


Klaviyo Blog
 


Twilio Engage Foundations Documentation
 

With Twilio Engage, you can send email and SMS campaigns to users who have opted in to receive your marketing materials. On this page, you’ll learn how to create and send an email campaign.

Some knowledge of the Journeys product will benefit you as you read through this guide. If you’re new to Journeys, the 
Journeys documentation
 will bring you up to speed.

How Engage campaigns work

Twilio Engage uses Journeys to send email and SMS campaigns.  With Journeys, you add conditions and steps that trigger actions like sending an email or an SMS.

You’ll build and then send your campaign in three stages:

Create a Journey

Because Engage campaigns exist within Journeys, begin by creating a Journey:

Add a Journey condition

With your Journey created, you’ll now create a 
condition
 that will trigger your email campaign:

With your entry condition added, you’re now ready to create your email.

Create, test, and publish your email campaign

Follow these steps to create an email campaign:

Some email campaign fields, like 
Sender email
 and 
Subject
, are required.  The Send Email window indicates required fields with an asterisk.  Refer to the 
email campaign fields
 table for a full description of available email fields.

Editing Templates

If you use a template for your email, Engage creates an editable copy of the original.  Editing the template within the Journey won’t alter the original template.

Send an email to all users

As you create your email campaign, you can set an email to send to all users regardless of their 
subscription state
. This may be useful, for example, when you need to send a marketing transactional email to a user who hasn’t subscribed to your marketing emails.

To send an email to all users:

When you bypass subscription states, be sure to follow local laws and comply with 
CAN-SPAM guidance
.

For more, view SendGrid’s 
email deliverability best practices
.

Test your email campaign

At this point, you can send a test email before publishing your campaign. Test emails confirm that your design, unsubscribe links, and merge tags appear as expected.

As part of the test send, you can enter custom values to populate the profile traits in your message.

Follow these steps to test your campaign:

Publish your email campaign

With your email designed and tested, you’re now ready to save the campaign and publish your Journey, with the following steps:

Your email campaign is now live. When users trigger the email’s parent Journey condition, they will receive your email campaign.

Email campaign fields

The following table contains descriptions of all available fields in the Journeys Send Email builder. Asterisks indicate required fields.

Working with IP pools

When you create an email, you have the option to select an IP pool. An IP pool is a group of IP addresses available to you in SendGrid. You can create and view your IP pools in your Engage-linked SendGrid subuser account by navigating to 
Settings > IP Addresses > IP Pools
.

Your sending reputation is based on a combination of your domain and the IP address you use to send emails. Emails that end up in your recipients’ spam folders could harm your sending reputation. As a result, you may want to keep your marketing and transactional emails on different IP addresses.

Keep the following in mind as you use IP pools:

For more information, see 
SendGrid’s IP pools documentation
.

Next steps

Using Journeys, you can create multi-channel customer engagement with both email and SMS campaigns. Having published an email, learn how 
Engage SMS campaigns
 can help you market to customers through text messages.

This page was last modified: 15 Jul 2024

Need support?

Questions? Problems? Need more info? Contact Segment Support for assistance!

Help improve these docs!

Was this page helpful?

Thanks for your feedback!

Can we improve this doc? 
Send us feedback!

Get started with Segment

On this page

Was this page helpful?

Thanks for your feedback!

Can we improve
 this doc? 
Send us feedback!

Product

For Developers

Company

Support

© 2025 Segment.io, Inc.

=== Content from https://segment.com/docs/connections/storage/warehouses/add-warehouse-users/ ===

Adding Warehouse Users
        

If you have more than one person working with your Segment Warehouse, you might want to create users for your team so that each person can have a discrete login. The three steps in this section will show you how to create a user, grant usage on a schema and then grant the privileges that the user will need to interact with that schema.

1. Creating a user with the 
CREATE USER
 command

CREATE
 
USER
 
<
name
>


[
IN
 
GROUP
 
<
group
>
]


WITH
 
PASSWORD
 
<
password
>


[
VALID
 
UNTIL
]
 
<
abstime
>



The code above in [] is optional, you don’t need to group your users or give their credentials an expiration date, the code works without it. However, if you do choose to use those parameters, 
<abstime>
 should be formatted as ‘2015-09-13’ which translates to September 13th, 2015.

For instance, you can create a user named 
flashthesloth
 as

CREATE
 
USER
 
flashthesloth


WITH
 
PASSWORD
 
'slow_is_beautiful'



This creates a user, you can run the following to get a list of users in your database.

SELECT
 
*
 
FROM
 
pg_user



Now that we’ve confirmed that the user has been created, they already have access to the public schema that contains systems-level information about the cluster but we need to give them access to the specific schemas that they’ll be working in.

2. Grant usage on the schema

Next, 
GRANT USAGE
 on the schema to the user we just created

GRANT
 
USAGE
 
ON
 
SCHEMA
 
<
schema_name
>
 
TO
 
<
user
>



The above SQL command grants the user USAGE privileges on a schema. Let’s assume you want to grant 
flashthesloth
 access to your development schema, it would look like below

GRANT
 
USAGE
 
ON
 
SCHEMA
 
development
 
TO
 
flashthesloth



Our new user now has usage rights on the 
development
 schema, now we need to grant the type of SQL commands they’ll be able to run against the cluster. For the purposes of this example, we’re going to give the user read only privileges.

3. Grant select privileges

GRANT SELECT
 privileges so the user can query the tables

GRANT
 
SELECT
 
ON
 
ALL
 
TABLES
 
IN
 
SCHEMA
 
<
schema_name
>
 
TO
 
<
user
>



The above SQL command grants the user SELECT rights on all tables in the chosen schema. For our 
flashthesloth
 user and the 
development
 schema, it would look like below.

GRANT
 
SELECT
 
ON
 
ALL
 
TABLES
 
IN
 
SCHEMA
 
development
 
TO
 
flashthesloth



Doing these three steps will result in a new user that can query all the tables in a given schema. If you want to give access to more than one schema then you can simply repeat steps 2 and 3 for each additional schema. If you have any questions or if you’re running into any issues getting this set up, 
contact us
.

This page was last modified: 14 Jul 2021

Need support?

Questions? Problems? Need more info? Contact Segment Support for assistance!

Help improve these docs!

Was this page helpful?

Thanks for your feedback!

Can we improve this doc? 
Send us feedback!

Get started with Segment

Was this page helpful?

Thanks for your feedback!

Can we improve
 this doc? 
Send us feedback!

Product

For Developers

Company

Support

© 2025 Segment.io, Inc.

=== Content from https://segment.com/docs/unify/traits/computed-traits/ ===

Computed Traits
        

Unify Plus requires a business tier account and is included with Engage


See the 
available plans
, or 
contact Support
.

On this page

Beginning August 18, 2023, new Unify Plus users can access Computed Traits in Unify.

Computed Traits allow you to quickly create user or account-level calculations that Segment keeps up-to-date over time. These can be computations like the 
total_num_orders
 a customer has completed, the 
lifetime_revenue
 of a customer, the 
most_frequent_user
 to determine which user is most active in an account, or the 
unique_visitors_count
 to assess how many visitors from a single domain. These computations are based on your events and event properties that you are sending through Segment on the 
page
 and 
track
 calls.

Comparing trait types

View the table below to better understand how Segment collects custom, computed, and SQL traits.

You can use the Profile explorer (
Unify > Profile explorer
) to view traits attached to a profile.

Types of Computed Traits

Segment currently supports the following types of computed traits:

Event Properties per Computed Trait limit

Segment limits the number of Event Properties on each Computed trait to 10,000. If your Computed Trait exceeds this limit, Segment will not persist any new Event Properties and will drop new trait keys and corresponding values.

Event Counter

An Event Counter trait stores a count of an 
event
 over a period of time. For example, you can create a trait called 
number_logins_90_days
 based on a 
User Logged In
 event. You can also use event properties to only specific types of events.

User-level examples:

Account-level examples:



Aggregation

An aggregation computes a 
sum, average, minimum, or maximum
 of a numeric 
event property
. A good example is a 
sum_cosmetics_revenue_90_days
 if you’re sending an 
Order Completed
 event with a 
revenue
 property. In the example we’re refining the revenue even further based on another event property: 
category = 'cosmetics'
. Note that you can only compute an aggregation trait for event properties that have a numeric value.

User-level examples:

Account-level use cases



Most Frequent

A most frequent user-level computed trait will return the 
most common value
 for an 
event property
.  This is  helpful to create traits like 
preferred_product_viewed
  or 
most_commonly_viewed_category
 that tell you what a user’s preferred product, or content category might be. Note that the most frequent computed trait requires the event property to have been tracked at least twice. In the case of a tie, Segment returns the first alphabetical value. For account-level computed traits, you can also return the most frequent 
user trait
. This is helpful when you want to determine which user has performed an event the most frequently. For example, you might to return the email of the user in an account most actively viewing your app.

User-level examples:

Account-level examples:



First

The first user-level trait returns the first event property value Segment has seen. This is common for creating traits like 
first_page_visited
 based on the page name. For accounts, the first computed trait could also return a trait like 
first_user_signup
, to calculate the first user to use your product.

User-level examples:

Account-level examples:



Last

The last trait returns the last event property value Segment has seen. This is common for creating traits like 
last_utm_campaign
 to help you calculate last-touch attribution for paid advertising.

User-level examples:

Account-level examples:



Unique List

Unique list computed traits will output a 
list of unique values
 in alphabetical order for an 
event property
. This is helpful to understand the different types of products or content that a customer or users in an account have interacted with or purchased. Customers are creating traits like 
unique_product_categories_viewed
 and sending them to email marketing tools and accessing them through the Profiles API for in-app personalization.

Example use cases:



Unique List Count

Unique list count computed traits will output a 
count of the unique list of values
 for an 
event property
. Customers are creating traits like 
unique_product_categories_viewed_count
 to understand the variety of products that a customer is viewing. At the account-level, customers are creating traits like 
unique_visitors_count
 to calculate the number of unique visitors by ip address.

User-level examples:

Account-level examples:



Conditions

All computed trait types support a common “Add Conditions” section. Conditions defined here restrict the messages considered when calculating the final value of the computed trait by looking at a property of the events. For example, you could limits events to only those where “price” is greater than 30.00 or where “page.url” contains “pricing”.

The following operators are available.

Connecting your Computed Trait to a Destination

Segment sends user-level computed Traits to destinations using the 
Identify call
 for user traits, or using the 
Track call
 for event properties. Segment includes the trait value and property in the identify and track calls.

For example, the name of a computed trait is added to the user profile as a trait, and the trait’s value is set to the value of the computed trait. Segment sends an identify or track call when the trait is computed, depending on the destination configuration. If a computed trait counts the number of times a user visits your pricing page, and the user visits your pricing page five times, Segment sends an identify call with the property 
pricing_page_visits: 5
.

Learn more about 
Computed trait generated events here
. The trait name corresponds to the snake cased name that you see in the trait settings, for example 
most_viewed_page_category
. See the 
list of Engage-compatible destinations

For account-level computed traits, you have the option to send either a 
group
 call and/or 
identify
 call. Group calls will send one event per account, whereas identify calls will send an identify call for each user in the account. This means that even if a user hasn’t performed an event, Segment will still set the account-level computed trait on that user. Because most marketing tools are still based at the user level, it is often important to map this account-level trait onto each user within an account. See 
Account-level Audiences
 for more information.

View compute status

After you create a computed trait, use the Overview page to view a compute progress bar, current 
status
, number of users with the trait, connected destinations, and more. For real-time traits, click 
Refresh Trait
 to update the current number of users with the trait.

Viewing compute progress

When you create a real-time computed trait, you’ll see a progress bar, computed percentage, and status updates. For existing traits that you edit, Segment displays the compute status but not the progress bar or percentage.

Editing Realtime Traits

Segment supports the editing of real-time Traits, which allows you to make nuanced changes to existing Traits in situations where cloning or building from scratch may not suit your use case.

To edit a real-time Trait, follow these steps:

Segment then processes your Trait edits. While the edit task runs, the trait remains locked and you can’t make further changes. Once Segment incorporates your changes, you’ll be able to access your updated Trait.

It is not possible to edit a trait to convert it from real-time to batch, or vice-versa. If the computation type needs to be changed, you will need to recreate the trait with the appropriate conditions.

Accessing your Computed Traits using the Profiles API

You can access your computed traits using the Profile API by querying the 
/traits
 endpoint. For example, you can query for the 
emails_opened_last_30_days
 with the following GET request:

https://profiles.segment.com/v1/spaces/<workspace_id>/collections/users/profiles/email:john.doe@segment.com/traits?include=emails_opened_last_30_days


returns:

    
{

        
"traits"
:
 
{

            
"emails_opened_last_30_days"
:
 
255

        
},

        
"cursor"
:
 
{

            
"url"
:
 
""
,

            
"has_more"
:
 
false
,

            
"next"
:
 
""
,

            
"limit"
:
 
100

        
}

    
}



Traits

You can query a user’s traits (such as 
first_name
, 
last_name
, and more):

https://profiles.segment.com/v1/spaces/<space_id>/collections/users/profiles/<external_id>/traits

By default, the response includes 20 traits. You can return up to 200 traits by appending 
?limit=200
 to the querystring. If you wish to return a specific trait, append 
?include={trait}
 to the querystring (for example, 
?include=age
). You can also use the 
?class=audience​
 or 
?class=computed_trait​
 URL parameters to retrieve audiences or computed traits specifically.

You can read the 
full Profile API docs
 to learn more.

Deleting Computed Traits

When computed traits are deleted, any user that had a value for that trait will now have a custom trait on the Unify profile.

Downloading your Computed Trait as a CSV file

You can download a copy of your trait by visiting the the computed trait overview page.


Computed Trait CSVs are generated on demand. Before you can download the CSV, you will need to generate it. There are three different options for formatting:

You can’t add account traits and identifiers using the CSV downloader with account level audiences. This is because every row listed in the CSV file is a user, and since account traits and identifiers only exist on accounts, they wouldn’t exist as a user’s custom trait and appear on the CSV.

This page was last modified: 13 Feb 2025

Need support?

Questions? Problems? Need more info? Contact Segment Support for assistance!

Help improve these docs!

Was this page helpful?

Thanks for your feedback!

Can we improve this doc? 
Send us feedback!

Get started with Segment

On this page

Was this page helpful?

Thanks for your feedback!

Can we improve
 this doc? 
Send us feedback!

Product

For Developers

Company

Support

© 2025 Segment.io, Inc.

=== Content from https://segment.com/docs/unify/identity-resolution/externalids/ ===

Identity Resolution ExternalIDs
        

Unify requires a Business tier account and is included with Engage.


See the 
available plans
, or 
contact Support
.

On this page

The steps in this guide pertain to spaces created before September 27th, 2020

For spaces created after September 27th, 2020, please refer to the 
Identity onboarding guide
.

Default externalIDs

The Identity Graph creates or merges profiles based on externalIDs. ExternalIDs will become the identities attached to a user profile in the Profile explorer.

Navigate to 
Unify > Profile explorer
 to view identities attached to a profile, along with custom traits, event history, and more.



Segment automatically promotes the following traits and IDs in track and identify calls to externalIDs:

The Google clientID (ga_clientid) is a unique value created for each browser-device pair and will exist for 2 years if the cookie is not cleared. The analytics.reset() call should be triggered from Segment end when the user logs off. This call will clear the cookies and local Storage created by Segment. It doesn’t clear data from other integrated tools. So on the next login, the user will be assigned with a new unique anonymous_id, but the same ga_clientid will remain if this cookie is not cleared. Hence, the profiles with different anonymous_id but with same ga_clientid will get merged.

Custom externalIDs

Unify resolves identity for any other externalIDs that you bind to users - such as a phone number or any custom identifier that you support.

As long as you’ve configured custom externalIDs, such as 
phone
, in your Space’s Identity Resolution rules, you can include it with the 
context.externalIds
 array, the 
properties
 object, or the 
context.traits
 object.

As seen in the example below, you can send custom 
externalIds
 in the 
context
 object of any call to Segment’s API.

The four fields below (id, type, collection, encoding) are all required:

As an example:

analytics
.
track
(
'
Subscription Upgraded
'
,
 
{

   
plan
:
 
'
Pro
'
,

   
mrr
:
 
99.99


},
 
{

  
externalIds
:
 
[

    
{

      
id
:
 
'
123-456-7890
'
,

      
type
:
 
'
phone
'
,

      
collection
:
 
'
users
'
,

      
encoding
:
 
'
none
'

    
}

  
]


})



Additionally, adding 
phone
 with the 
properties
 object gets picked up by Unify and applied as an externalID:

analytics
.
track
(
'
Subscription Upgraded
'
,
 
{
 
plan
:
 
'
Pro
'
,
 
mrr
:
 
99.99
,
 
phone
:
 
'
123-456-7890
'
})



You can also include 
phone
 using the 
context.traits
 object and Unify adds it as an externalID to the profile.

analytics
.
track
(
'
Subscription Upgraded
'
,
 
{
 
plan
:
 
'
Pro
'
,
 
mrr
:
 
99.99
},
 
{
traits
 
:
 
{
phone_number
:
 
'
123-456-7890
'
}})



Unify creates a user (user_id: 
use_123
)  with the custom externalID (phone: 
123-456-7890
). Query the user’s phone record by using the externalID (phone: 
123-456-7890
), or update the profile with that externalID going forward. (Note: externalIDs must be lower-case.)

Viewing promoted externalIDs

Users can view which externalIDs are promoted on each event by viewing the raw payload on Events in the User Profile in the “external_ids” object.

For example, the following user had anonymous_id and user_id promoted as identifiers from the Course Clicked track call:



Example

For example, a new anonymous user visits your Pricing page:

analytics
.
page
(
'
Pricing
'
,
 
{

  
anonymousId
:
 
'
anon_123
'

  
title
:
 
'
Acme Pricing
'
,

  
url
:
 
'
https://acme.com/pricing
'
,

  
referrer
:
 
'
https://google.com/
'


});



At this point, the Identity Graph will create a new user with external id (anonymous_id: 
anon_123
) and a persistent and globally unique segment_id, in this case: 
use_4paotyretuj4Ta2bEYQ0vKOq1e7
.



Any new events received with the same external id (anonymous_id: 
anon_123
) are appended to same user 
use_4paotyretuj4Ta2bEYQ0vKOq1e7
.

Next, the user goes to a sign up form and signs up:

analytics
.
track
(
'
User Signup
'
,
 
{

  
userId
:
 
'
use_123
'
,

  
anonymousId
:
 
'
anon_123
'


});



At this point, the Identity Graph associates external ID (user_id: 
use_123
) with the same user 
use_4paotyretuj4Ta2bEYQ0vKOq1e7
.


This page was last modified: 07 Feb 2025

Need support?

Questions? Problems? Need more info? Contact Segment Support for assistance!

Help improve these docs!

Was this page helpful?

Thanks for your feedback!

Can we improve this doc? 
Send us feedback!

Get started with Segment

On this page

Was this page helpful?

Thanks for your feedback!

Can we improve
 this doc? 
Send us feedback!

Product

For Developers

Company

Support

© 2025 Segment.io, Inc.

=== Content from https://segment.com/docs/connections/spec/mobile/ ===

Native Mobile Spec
        

On this page

One of the core components of the Segment 
Spec
 is the 
Track
 method. It records any arbitrary event that the user has triggered. For Mobile tracking, in addition to Screen calls, you’ll want to send 
specific event names
 that Segment recognizes semantically. That way, Segment can transform them correctly before sending them off to downstream destinations.

By standardizing the events that comprise the core 
mobile application lifecycle
 and associated 
mobile campaign and referral events
, Segment and its partners can, wherever possible, forward these events on your behalf and build downstream destinations that take full advantage of the semantic meaning associated with these events and their properties.

If you’re already collecting similar events, Segment recommends migrating to these event names so that you can take advantage of available features in Segment destinations that depend on the spec as they become available.

These events pair nicely with Segment’s 
ecommerce spec
 for mobile marketplaces to take full advantage of features like dynamic ads in Facebook and the ability to take full advantage of server-side destinations with Mobile Attribution Platforms like 
Tune
 and 
Kochava
.

Per the 
Privacy Policy
 and applicable terms, don’t send Segment sensitive personal information about your users. Certain features from Segment and its partners allow you to opt-in to automatically track data (for example: Application Installed or Deep Link Clicked). When working with these features and Segment in general, be cognizant of the data that is being tracked to ensure its matching both your obligations under your agreement with Segment and the privacy expectations of your users.

Overview of events

The Segment Native Mobile Spec includes the following semantic events:

Application Lifecycle Events

Campaign Events

Segment recommends using the above event names if you’re going to be integrating the events yourself. This will ensure that they can be mapped effectively in downstream tools.

Lifecycle events

Mobile applications live within a fairly bounded lifecycle. To understand and communicate effectively with your users, it’s crucial to instrument the core flows associated with installing and opening your app. The following events allow you to get a picture of top-line metrics such as DAUs, MAUs, and Screen Views per session. Automatic lifecycle event tracking is optional - you can learn how to enable and disable them in Segment’s docs for each library below:

The following events will be tracked automatically when lifecycle events are enabled in all mobile libraries:

In Kotlin, Swift, and React Native, the following additional events are tracked:

In Swift, the following event is also tracked:

Application Installed

This event fires when a user 
first
 opens your mobile application. Note, if the user never opens your app after installing, Segment will not collect this event. This event doesn’t wait for attribution or campaign information to be received, and is collected automatically by Segment’s SDKs. Advertising providers like Facebook and Google require discrete install events to correctly attribute installs to ads served through their platform.

{

  
"userId"
:
 
"019mr8mf4r"
,

  
"type"
:
 
"track"
,

  
"event"
:
 
"Application Installed"
,

  
"properties"
:
 
{

    
"version"
:
 
"1.2.3"
,
 
"build"
:
 
"1234"

  
}


}



Application Opened

This event fires when a user launches or foregrounds your mobile application after the first open. It will fire after the 
Application Installed
 event and again after the app is re-opened after being closed. This event does not wait for attribution information to be received but may include information about referring applications or deep link URLs if available to the application upon open.

{

  
"userId"
:
 
"019mr8mf4r"
,

  
"type"
:
 
"track"
,

  
"event"
:
 
"Application Opened"
,

  
"properties"
:
 
{

    
"from_background"
:
 
false
,

    
"referring_application"
:
 
"GMail"
,

    
"url"
:
 
"url://location"

  
}


}



Application Backgrounded

This event should be sent when a user backgrounds the application upon 
applicationDidEnterBackground
.

{

  
"userId"
:
 
"019mr8mf4r"
,

  
"type"
:
 
"track"
,

  
"event"
:
 
"Application Backgrounded"
,

  
"properties"
:
 
{}


}



Application Foregrounded

This event is fired when a user opens the app or brings it back into the foreground of their device. This is only collected by the Swift library.

{

  
"userId"
:
 
"019mr8mf4r"
,

  
"type"
:
 
"track"
,

  
"event"
:
 
"Application Foregrounded"
,

  
"properties"
:
 
{}


}



Application Updated

This event fires when a user updates the application. Segment’s SDK will automatically collect this event instead of an “Application Opened” event when we determine that the Open is first since an update.

{

  
"userId"
:
 
"019mr8mf4r"
,

  
"type"
:
 
"track"
,

  
"event"
:
 
"Application Updated"
,

  
"properties"
:
 
{

    
"previous_version"
:
 
"1.1.2"
,

    
"previous_build"
:
 
"1234"
,

    
"version"
:
 
"1.2.0"
,

    
"build"
:
 
"1456"

  
}


}



Application Uninstalled

Fire this event when a user uninstalls the application. Some destination partners will detect this for you using Silent Push Notifications through their SDK. You might be able to send these events to Segment using a callback. Visit the partner docs to see if this is available.

{

  
"userId"
:
 
"019mr8mf4r"
,

  
"type"
:
 
"track"
,

  
"event"
:
 
"Application Uninstalled"
,

  
"properties"
:
 
{}


}



Application Crashed

You can send this event when you receive a crash notification from your app, but it is not meant to supplant traditional crash reporting tools. By tracking crashes as an analytics event with device and user information, you can analyze the which types of users are impacted by crashes and how those crashes, in turn, affect their engagement. You may also want to target those customers with tailored communications in other channels if they’ve encountered several crashes. Segment does not collect this event. To capture the event, use a destination that collects this data and route that event back to Segment through a webhook or some other callback.

{

  
"userId"
:
 
"019mr8mf4r"
,

  
"type"
:
 
"track"
,

  
"event"
:
 
"Application Crashed"
,

  
"properties"
:
 
{}


}



Campaign events

As the walls between apps become increasingly lowered, capturing information about the content and campaigns that drive users to engage with your app is critical to building more targeted, relevant, personalized experiences for your users.

Segment does not collect any campaign events automatically unless configured to do so.

Install Attributed

When Segment or an integrated partner can discern the source of an install, we’ll collect an 
Install Attributed
 event. This event may be sent to Segment using server-to-server connection from your attribution provider, or directly on the device using packaged destinations. In either case, this will happen 
after
 install, and does not apply to all installs, which is why it is a discrete event.

{

  
"userId"
:
 
"019mr8mf4r"
,

  
"type"
:
 
"track"
,

  
"event"
:
 
"Install Attributed"
,

  
"properties"
:
 
{

    
"provider"
:
 
"Tune/Kochava/Branch/AppsFlyer"
,

    
"campaign"
:
 
{

      
"source"
:
 
"Network/FB/AdWords/MoPub/Source"
,

      
"name"
:
 
"Campaign Name"
,

      
"content"
:
 
"Organic Content Title"
,

      
"ad_creative"
:
 
"Red Hello World Ad"
,

      
"ad_group"
:
 
"Red Ones"

    
}

  
}


}



Push Notification Received

This event can be sent when a push notification is received in the app. It can be automatically enabled on 
iOS
.

{

  
"userId"
:
 
"019mr8mf4r"
,

  
"type"
:
 
"track"
,

  
"event"
:
 
"Push Notification Received"
,

  
"properties"
:
 
{

    
"campaign"
:
 
{

      
"medium"
:
 
"Push"
,

      
"source"
:
 
"Vendor Name"
,

      
"name"
:
 
"Referral Flow"
,

      
"content"
:
 
"Your friend invited you to play a match."

    
}

  
}


}



Push Notification Tapped

This event can be sent when a user taps on a push notification associated with your app. It can be automatically enabled on 
iOS
.

{

  
"userId"
:
 
"019mr8mf4r"
,

  
"type"
:
 
"track"
,

  
"event"
:
 
"Push Notification Tapped"
,

  
"properties"
:
 
{

    
"action"
:
 
"Accept"
,

    
"campaign"
:
 
{

      
"medium"
:
 
"Push"
,

      
"source"
:
 
"Vendor Name"
,

      
"name"
:
 
"Referral Flow"
,

      
"content"
:
 
"Your friend invited you to play a match."

    
}

  
}


}



Push Notification Bounced

This event fires when a push notification from a provider bounces. If your push notification provider forwards push lifecycle events to Segment, they should include this event in their suite.

{

  
"userId"
:
 
"019mr8mf4r"
,

  
"type"
:
 
"track"
,

  
"event"
:
"Push Notification Bounced"
,

  
"properties"
:
 
{

    
"action"
:
 
"Accept"
,

    
"campaign"
:
 
{

      
"medium"
:
 
"Push"
,

      
"source"
:
 
"Vendor Name"
,

      
"name"
:
 
"Referral Flow"
,

      
"content"
:
 
"Your friend invited you to play a match."

    
}

  
}


}



Deep Link Opened

When your application is opened using a referring link, Segment or your packaged deep link partner can fire this event on your behalf. If the deep link has additional data associated with it, either passed through the third party service or as 
annotations
 in 
launchOption
, you may want to include those values as properties here as well.

This event is fired 
in addition
 to the associated 
Application Opened
 event.

{

  
"userId"
:
 
"019mr8mf4r"
,

  
"type"
:
 
"track"
,

  
"event"
:
 
"Deep Link Opened"
,

  
"properties"
:
 
{

    
"provider"
:
 
"Branch Metrics"
,

    
"url"
:
 
"app://landing"

  
}


}



Deep Link Clicked

This event may be provided by deep link providers postback mechanisms or an internal redirect service if you use one in order to provide a waypoint funnel step between your content or advertisement and the resulting app open.

{

  
"userId"
:
 
"019mr8mf4r"
,

  
"type"
:
 
"track"
,

  
"event"
:
 
"Deep Link Clicked"
,

  
"properties"
:
 
{

    
"provider"
:
 
"Branch Metrics"
,

    
"url"
:
 
"brnch.io/1234"

  
}


}



This page was last modified: 31 May 2024

Need support?

Questions? Problems? Need more info? Contact Segment Support for assistance!

Help improve these docs!

Was this page helpful?

Thanks for your feedback!

Can we improve this doc? 
Send us feedback!

Get started with Segment

On this page

Was this page helpful?

Thanks for your feedback!

Can we improve
 this doc? 
Send us feedback!

Product

For Developers

Company

Support

© 2025 Segment.io, Inc.

=== Content from https://segment.com/docs/unify/data-graph/setup-guides/redshift-setup// ===

Redshift Data Graph Setup
        

Unify requires a Business tier account and is included with Engage.


See the 
available plans
, or 
contact Support
.

On this page

Redshift for Data Graph is in beta and Segment is actively working on this feature. Some functionality may change before it becomes generally available. This feature is governed by Twilio Segment’s 
First Access and Beta Preview Terms
.

Set up your Redshift data warehouse to Segment for the 
Data Graph
.

Prerequisite

To use Linked Audiences with Redshift, the Data Graph only supports 
materialized views
.

If you’re setting up Profiles Sync for the first time in the Unify space, go through the setup flow for Selective sync. If Profiles Sync is already set up for your Unify space, follow these steps to configure Profiles Sync for your Unify space:

Getting started

You need to be an AWS Redshift account admin to set up the Segment Redshift connector as well as write permissions for the 
__segment_reverse_etl
 dataset.

To get started with Redshift:

Step 1: Roles and permissions

Segment recommends you to create a new Redshift user and role with only the required permissions.

Create a new role and user for the Segment Data Graph. This new role will only have access to the datasets you provide access to for the Data Graph. Run the SQL commands in your Redshift cluster:

  
-- Create a user with role for the Data Graph

  
CREATE
 
ROLE
 
SEGMENT_LINKED_ROLE
;

  
CREATE
 
USER
 
SEGMENT_LINKED_USER
 
PASSWORD
 
"your_password"
;

  
GRANT
 
ROLE
 
SEGMENT_LINKED_ROLE
 
TO
 
SEGMENT_LINKED_USER
;



Step 2: Create a database for Segment to store checkpoint tables

Segment recommends you to create a new database for the Data Graph. If you choose to use an existing database that has also been used for 
Segment Reverse ETL
, you must follow the 
additional instructions
 to update user access for the Segment Reverse ETL schema.

Provide write access to the database as Segment requires this in order to create a schema for internal bookkeeping and to store checkpoint tables for the queries that are executed. Segment recommends you to create a new database for this purpose. This is also the database you’ll be required to specify for the 
Database Name
 when connecting Redshift with the Segment app.

Run the following SQL commands in your Redshift cluster:

-- Create and Grant access to a Segment internal DB used for bookkeeping 



CREATE
 
DATABASE
 
SEGMENT_LINKED_PROFILES_DB
;


GRANT
 
CREATE
 
ON
 
DATABASE
 
SEGMENT_LINKED_PROFILES_DB
 
TO
 
ROLE
 
SEGMENT_LINKED_ROLE
;



Step 3: Grant read-only access for the Data Graph

Grant the Segment role read-only access to additional schemas you want to use for the Data Graph including the Profiles Sync database.

To locate the Profile Sync database, navigate to 
Unify > Profiles Sync > Settings > Connection Settings
. You will see the database and schema name.

Schemas

Grant schema permissions based on customer need. See Amazon’s docs to view 
schema permissions
 and 
example commands
 that you can use to grant permissions. Repeat the following SQL query for each schema you want to use for the Data Graph.

-- ********** REPEAT THE SQL QUERY BELOW FOR EACH SCHEMA YOU WANT TO USE FOR THE DATA GRAPH **********



GRANT
 
USAGE
 
ON
 
SCHEMA
 
"the_schema_name"
 
TO
 
ROLE
 
SEGMENT_LINKED_ROLE
;



Table

Grant table permissions based on your needs. Learn more about 
Amazon’s table permissions
.

Table permissions can either be handled in bulk:

-- query data from all tables in a schema


GRANT
 
SELECT
 
ON
 
ALL
 
TABLES
 
IN
 
SCHEMA
 
"the_schema_name"
 
TO
 
ROLE
 
SEGMENT_LINKED_ROLE
;



Or in a more granular fashion if needed:

-- query data from a specific table in a schema


GRANT
 
SELECT
 
ON
 
TABLE
 
<
schema
-
name
>
.
<
table
-
name
>
 
TO
 
ROLE
 
segment_linked_role
;



Step 4: Validate permissions

To verify you have set up the right permissions for a specific table, log in with the username and password you created for 
SEGMENT_LINKED_USER
 and run the following command to verify the role you created has the correct permissions. If this command succeeds, you should be able to view the respective table.

SHOW
 
SCHEMAS
 
FROM
 
DATABASE
 
"THE_READ_ONLY_DB"
;


SELECT
 
*
 
FROM
 
"THE_READ_ONLY_DB.A_SCHEMA.SOME_TABLE"
 
LIMIT
 
10
;



Step 5: Connect your warehouse to Segment

To connect your warehouse to Segment:

Update user access for Segment Reverse ETL dataset

If Segment Reverse ETL ran in the project you are configuring as the Segment connection project, a Segment-managed dataset is already created, and you need to provide the new Segment user access to the existing dataset. Run the following SQL if you run into an error on the Segment app indicating that the user doesn’t have sufficient privileges on an existing 
__segment_reverse_etl
:

-- If you want to use an existing database that already has Segment Reverse ETL schemas, you’ll need to run some additional steps below to grant the role access to the existing schemas.



GRANT
 
USAGE
,
 
CREATE
,
 
DROP
 
ON
 
SCHEMA
 
segment_connection_db
.
__segment_reverse_etl
 
TO
 
ROLE
 
SEGMENT_LINKED_ROLE
;


GRANT
 
SELECT
,
INSERT
,
UPDATE
,
DELETE
,
DROP
 
ON
 
ALL
 
TABLES
 
IN
 
SCHEMA
 
segment_connection_db
.
__segment_reverse_etl
 
TO
 
ROLE
 
SEGMENT_LINKED_ROLE
;



This page was last modified: 10 Dec 2024

Need support?

Questions? Problems? Need more info? Contact Segment Support for assistance!

Help improve these docs!

Was this page helpful?

Thanks for your feedback!

Can we improve this doc? 
Send us feedback!

Get started with Segment

On this page

Was this page helpful?

Thanks for your feedback!

Can we improve
 this doc? 
Send us feedback!

Product

For Developers

Company

Support

© 2025 Segment.io, Inc.

=== Content from https://segment.com/docs/connections/spec/alias/ ===

Spec: Alias
        

On this page

The Alias method is an advanced method used to merge 2 unassociated user identities, effectively connecting 2 sets of user data in one profile.

Alias and Unify

Alias calls can’t be used to merge profiles in 
Unify
. For more information on how Unify merges user profiles, view the 
Identity Resolution documentation
.

Alias is an advanced method

The Alias method allows you to explicitly change the ID of a tracked user. This should only be done when it’s required for downstream destination compatibility. See the 
Best Practices for Identifying Users
 docs for more information.

Syntax

The Alias call has the following fields:

The Alias method follows the format below:

analytics
.
alias
(
userId
,
 
[
previousId
],
 
[
options
],
 
[
callback
]);



Here’s the payload of a basic Alias call that will associate this user’s existing 
id
 (email address) with a new one (a database ID), with most 
common fields
 removed:

{

  
"
type
"
:
 
"
alias
"
,

  
"
previousId
"
:
 
"
jen@email.com
"
,

  
"
userId
"
:
 
"
507f191e81
"


}



Here’s the corresponding JavaScript event that would generate the above payload. If you’re using Segment’s JavaScript library, Segment automatically passes in the user’s 
anonymousId
 as 
previousId
 for you:

analytics
.
alias
(
"
507f191e81
"
);



If you’re instrumenting a website, the Anonymous ID is generated in the browser so you must call Alias from the client-side. If you’re using a server-side session ID as the Anonymous ID, then you must call Alias from the server-side.

Based on the library you use, the syntax in the examples might be different. You can find library-specific documentation on the 
Sources Overview
 page.

Examples

Here’s a complete example of an Alias call:

{

  
"
anonymousId
"
:
 
"
507f191e810c19729de860ea
"
,

  
"
channel
"
:
 
"
browser
"
,

  
"
context
"
:
 
{

    
"
ip
"
:
 
"
8.8.8.8
"
,

    
"
userAgent
"
:
 
"
Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_5) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/40.0.2214.115 Safari/537.36
"

  
},

  
"
integrations
"
:
 
{

    
"
All
"
:
 
true
,

    
"
Mixpanel
"
:
 
false
,

    
"
Salesforce
"
:
 
false

  
},

  
"
messageId
"
:
 
"
022bb90c-bbac-11e4-8dfc-aa07a5b093db
"
,

  
"
previousId
"
:
 
"
39239-239239-239239-23923
"
,

  
"
receivedAt
"
:
 
"
2015-02-23T22:28:55.387Z
"
,

  
"
sentAt
"
:
 
"
2015-02-23T22:28:55.111Z
"
,

  
"
timestamp
"
:
 
"
2015-02-23T22:28:55.111Z
"
,

  
"
type
"
:
 
"
alias
"
,

  
"
userId
"
:
 
"
507f191e81
"
,

  
"
version
"
:
 
"
1.1
"


}



This page was last modified: 24 Jan 2024

Need support?

Questions? Problems? Need more info? Contact Segment Support for assistance!

Help improve these docs!

Was this page helpful?

Thanks for your feedback!

Can we improve this doc? 
Send us feedback!

Get started with Segment

On this page

Was this page helpful?

Thanks for your feedback!

Can we improve
 this doc? 
Send us feedback!

Product

For Developers

Company

Support

© 2025 Segment.io, Inc.

=== Content from https://segment.com/docs/unify/profile-api/ ===

Profile API
        

Unify requires a Business tier account and is included with Engage.


See the 
available plans
, or 
contact Support
.

On this page

The Segment Profile API provides a single API to read user-level and account-level customer data. Segment now allows you to query the entire user or account object programmatically, including the 
external_ids
 , 
traits
 , and 
events
 that make up a user’s journey through your product.

You can use this API to:

This document has four parts:

Product highlights

Quickstart

Important
: The Profile API is intended to be used server-side. You should not implement directly in client applications. See the 
Best Practices
 section for more details.

Configure access

Your access token enables you to call the Profile API and access customer data.

European Union requirements

To implement the Profile API in the European Union, you must complete the following steps within an EU workspace. View the 
regional Segment documentation
 for more information.

Navigate to the API access settings page 
Unify > Unify settings > API access
.

Create your 
Access Token
 with a name that describes your use case, for example 
testing/development
. Take note of the 
space ID
 value, you’ll pass this into the Profile API request URL in a later step.

Click 
Generate token
. Copy the resulting 
Access Token
 and store it in a file on your computer. You’ll pass in the 
Access Token
 into the Profile API for authorization as an HTTP Basic Auth username in a later step.

Find a user’s external id

To query phone numbers that contain a plus sign (
+
), insert the escape characters 
%2B
 in place of the plus sign.
For example, if a 
phone_number
 identifier has the value 
+5555550123
, enter 
phone_number:%2B5555550123
 in your query.

If the field you’re using within the Profile API endpoint contains a value with a non-alphanumeric character, then the Profile API may respond with 
500
 error. In this case, see 
W3’s ASCII Encoding Refernece
, which lists the escape characters you can use to replace the non-alphanumeric character in the Profile API endpoint so that the Profile API will respond with a 
200 Success
.

Query the user’s event traits

Explore the user’s traits in the response

The response is returned as a JSON object which contains the queried user’s assigned traits.

{

    
"traits"
:
 
{

        
"3_product_views_in_last_60_days"
:
 
false
,

        
"Campaign Name"
:
 
"Organic"
,

        
"Campaign Source"
:
 
"Organic"
,

        
"Experiment Group"
:
 
"Group A"
,

        
"Invited User?"
:
 
"Invited User?"
,

        
"Referrering Domain"
:
 
"http://duckduckgo.com"
,

        
"all_users_order_completed"
:
 
true
,

        
"big_spender"
:
 
false

    
},

    
"cursor"
:
 
{

        
"url"
:
 
"https://profiles.segment.com/v1/spaces/kNU0gh7EVl/collections/users/profiles/user_id:1413639574/traits?%3Acollection=users&%3Aid=user_id%3A1413639574&%3Anamespace=kNU0gh7EVl&next=browser"
,

        
"has_more"
:
 
true
,

        
"next"
:
 
"browser"
,

        
"limit"
:
 
10

    
}


}



Explore more of the API

Search by an External ID
: You can query directly by a user’s user_id or other external_id.

https://profiles.segment.com/v1/spaces/<space_id>/collections/users/profiles/<user_identifier>/events

External IDs
: You can query all of a user’s external IDs such as 
anonymous_id
 or 
user_id
.

https://profiles.segment.com/v1/spaces/<space_id>/collections/users/profiles/<user_identifier>/external_ids

Traits

You can query a user’s traits (such as 
first_name
, 
last_name
, and more):

https://profiles.segment.com/v1/spaces/<space_id>/collections/users/profiles/<external_id>/traits

By default, the response includes 10 traits. You can return up to 200 traits by appending 
?limit=200
 to the querystring. If you wish to return a specific trait, append 
?include={trait}
 to the querystring (for example 
?include=age
). You can also use the 
?class=audience​
 or 
?class=computed_trait​
 URL parameters to retrieve audiences or computed traits specifically.

Metadata

You can query all of a user’s metadata (such as 
created_at
, 
updated_at
, and more):

https://profiles.segment.com/v1/spaces/<space_id>/collections/users/profiles/<external_id>/metadata

Search an account profile

If you’re sending group calls to Segment, you can now access your account profiles as well. Retrieve your account traits, computed traits, and audience traits by querying the 
group_id
 you are interested in:

https://profiles.segment.com/v1/spaces/<space_id>/collections/accounts/profiles/group_id:12345/traits

Search for linked users or accounts

If you’re looking to find all the users linked to an account, you can search for an account’s linked users, or a user’s linked accounts.

https://profiles.segment.com/v1/spaces/<space_id>/collections/accounts/profiles/group_id:12345/links

The return limit for the 
/links
 endpoint is 20 records, which you can request by appending 
?limit=20
 to the query string.

cURL

You can also request using cURL:

export 
SEGMENT_ACCESS_SECRET
=
"YOUR_API_ACCESS_TOKEN_SECRET_HERE"


curl https://profiles.segment.com/v1/spaces/<space_id>/collections/users/profiles/<external_id>/traits 
-u
 
$SEGMENT_ACCESS_SECRET
:


API reference

The Segment API is organized around 
REST
. The API has predictable, resource-oriented URLs, and uses HTTP response codes to indicate API errors. Segment uses standard HTTP features, like HTTP authentication and HTTP verbs, which are understood by off-the-shelf HTTP clients.  
JSON
 is returned by all API responses, including errors.

Endpoint

https://profiles.segment.com


European Union endpoint

https://profiles.euw1.segment.com


Authentication

The Profile API uses basic authentication for authorization — with the 
Access Token
 as the authorization key. Your 
Access Token
 carries access to all of your customer data, so be sure to keep them secret. Don’t share your Access Token in publicly accessible areas such as GitHub or client-side code.

You can create your Access Secret in your Unify settings page. Segment recommends that you name your tokens with the name of your app and its environment, such as 
marketing_site/production
. Access tokens are shown once — you won’t be able to see it again. In the event of a security incident, you can revoke and cycle the access token.

When you make requests to the Profile API, use the Access Token as the basic authentication username and keep the password blank. Base64 is a requirement for authentication. If you use a tool like Postman, or if you use the 
-u
 flag in a cURL request, this encoding occurs automatically. Otherwise, you’ll need to use Base64 to manually encode your Access Token.

curl https://profiles.segment.com/v1/spaces/<space_id>/collections/users/profiles
  
-u
 
$SEGMENT_ACCESS_TOKEN
:


If you’re using a Segment Function or Node.js you can format your header object to include authentication, like so:

headers: {
          'Content-Type': 'application/json',
          Authorization:
            `Basic ${btoa('<access_token>' + ':')}`,
        }


Errors

Segment uses conventional HTTP response codes to indicate the success or failure of an API request. In general, codes in the 
2xx
 range indicate success, codes in the 
4xx
 range indicate an error that failed given the information provided (for example, a required parameter was omitted), and codes in the 
5xx
 range indicate an error with Segment’s servers.

HTTP Status

Error Body

{

  
"
error
"
:
 
{

    
"
code
"
:
 
"
validation_error
"
,

    
"
message
"
:
 
"
The parameter `collection` has invalid character(s) `!`
"

  
}


}



Rate limit

To ensure low response times, every Space has a default rate limit of 100 requests/sec. Please contact 
friends@segment.com
 if you need a higher limit with details around your use case. For more information about rate limits, see the 
Product Limits
 documentation.

Pagination

All top-level API resources have support for bulk fetches using “list” API methods. For instance you can list profiles, a profile’s events, a profile’s traits, and a profile’s external_ids. These list API methods share a common structure, taking at least two parameters: 
next
 and 
limit
.

Request arguments

Response arguments

Request IDs

Each API request has an associated request identifier. You can find this value in the response headers, under 
Request-Id
.

curl 
-i
 https://profiles.segment.com/v1/spaces/<space_id>/collections/users/profiles/<identifier>/metadata
HTTP/1.1 200 OK
Date: Mon, 01 Jul 2013 17:27:06 GMT
Status: 200 OK
Request-Id: 1111-2222-3333-4444


If you need to contact Segment regarding a specific API request, please capture and provide the 
Request-Id
.

Routes

The Profile API supports the following routes. These routes are appended the Profile API request URL:

https://profiles.segment.com/v1/spaces/<space_id>/


Retrieve a single profile’s traits within a collection using an 
external_id
. For example, two different sources can set a different 
first_name
 for a user. The traits endpoint will resolve properties from multiple sources into a canonical source using the last updated precedence order.

GET /v1/spaces/<space_id>/collections/users/profiles/<id_type:external_id>/traits


This example retrieves a profile’s traits by an external id, like an 
anonymous_id
:

GET /v1/spaces/lg8283283/collections/users/profiles/anonymous_id:a1234/traits


Or a 
user_id
:

GET /v1/spaces/lg8283283/collections/users/profiles/user_id:u1234/traits


Request

    curl https://profiles.segment.com/v1/spaces/<space_id>/collections/users/profiles/<id_type:ext_id>/traits
      
-X
 GET
      
-u
 
$SEGMENT_ACCESS_SECRET
:


404 Not Found

{

  
"
error
"
:
 
{

    
"
code
"
:
 
"
not_found
"
,

    
"
message
"
:
 
"
Profile was not found.
"

  
}


}



200 OK

{

  
"
traits
"
:
 
{

    
"
first_name
"
:
 
"
Bob
"
,

    
"
emails_opened_last_30_days
"
:
 
33
,

  
},

  
"
cursor
"
:
 
{

    
"
url
"
:
 
"
/v1/spaces/lgJ4AAjFN4/collections/users/profiles/use_RkjG0kW53igMijEISMH0vKBF4sL/traits
"
,

    
"
has_more
"
:
 
false
,

    
"
next
"
:
 
""

  
}


}




With 
?verbose=true
 enabled:

{

  
"
traits
"
:
 
{

    
"
first_name
"
:
 
{

      
"
value
"
:
 
"
Bob
"
,

      
"
source_id
"
:
 
"
..
"
,

      
"
updated_at
"
:
 
"
..
"

    
}

    
"
emails_opened_last_30_days
"
:
 
{

      
"
value
"
:
 
33
,

      
"
source_id
"
:
 
"
..
"
,

      
"
updated_at
"
:
 
"
..
"

    
}

  
},

  
"
cursor
"
:
 
{

    
"
url
"
:
 
"
/v1/spaces/lgJ4AAjFN4/collections/users/profiles/use_RkjG0kW53igMijEISMH0vKBF4sL/traits
"
,

    
"
has_more
"
:
 
false
,

    
"
next
"
:
 
""

  
}


}



Get a single profile’s external ids within a collection using an 
external_id
.

GET /v1/spaces/<space_id>/collections/users/profiles/<id_type:ext_id>/external_ids


Request

curl https://profiles.segment.com/v1/spaces/<space_id>/collections/users/profiles/<id_type:ext_id>/external_ids
  
-X
 GET
  
-u
 
$SEGMENT_ACCESS_TOKEN
:


404 Not Found

{

  
"
error
"
:
 
{

    
"
code
"
:
 
"
not_found
"
,

    
"
message
"
:
 
"
Profile was not found.
"

  
}


}



200 OK

{

  
"
data
"
:
 
[

      
{

        
"
source_id
"
:
 
"
GFu4AJc2bE
"

        
"
collection
"
:
 
"
users
"
,

        
"
id
"
:
 
"
1d1cd931-bc7d-4e39-a1a7-61563296fb15
"
,

        
"
type
"
:
 
"
cross_domain_id
"
,

        
"
created_at
"
:
 
"
2017-11-30T06:05:01.40468Z
"
,

        
"
encoding
"
:
 
"
none
"
,

        
"
first_message_id
"
:
 
"
ajs-0af8675aa114c759210a76b2baea0a03-clean
"
,

      
}

    
],

    
"
cursor
"
:
 
{

      
"
url
"
:
 
"
/v1/spaces/lgJ4AAjFN4/collections/users/profiles/use_RkjG0kW53igMijEISMH0vKBF4sL/external_ids
"
,

      
"
has_more
"
:
 
true
,

      
"
next
"
:
 
"
map_0vKouKs2XyirgwMO4SmnDGaps7j
"

    
}


}



Get up to 14 days of a profile’s historical events within a collection using an 
external_id
.

    GET /v1/spaces/<space_id>/collections/users/profiles/<external_id>/events


Request

    
curl
 
https
:
//profiles.segment.com/v1/spaces/<space_id>/collections/users/profiles/<external_id>/events

      
-
X
 
GET

      
-
u
 
$SEGMENT_ACCESS_SECRET
:



404 Not Found

{

  
"
error
"
:
 
{

    
"
code
"
:
 
"
not_found
"
,

    
"
message
"
:
 
"
Profile was not found.
"

  
}


}



200 OK

{

  
"
data
"
:
 
[

    
{

      
"
external_ids
"
:
 
[

        
{

          
"
collection
"
:
 
"
users
"
,

          
"
type
"
:
 
"
user_id
"
,

          
"
id
"
:
 
"
c0HN02fNe1
"
,

          
"
encoding
"
:
 
"
none
"

        
},

        
{

          
"
collection
"
:
 
"
users
"
,

          
"
type
"
:
 
"
cross_domain_id
"
,

          
"
id
"
:
 
"
1d1cd931-bc7d-4e39-a1a7-61563296fb15
"
,

          
"
encoding
"
:
 
"
none
"

        
}

      
],

      
"
context
"
:
 
{

        
"
ip
"
:
 
"
73.92.233.78
"
,

        
"
library
"
:
 
{

            
"
name
"
:
 
"
analytics.js
"
,

            
"
version
"
:
 
"
3.2.5
"

        
},

        
"
page
"
:
 
{

            
"
path
"
:
 
"
/docs/connections/spec/ecommerce/v2/
"
,

            
"
referrer
"
:
 
"
https://www.google.com/
"
,

            
"
search
"
:
 
""
,

            
"
title
"
:
 
"
Spec: V2 Ecommerce Events Documentation - Segment
"
,

            
"
url
"
:
 
"
https://segment.com/docs/connections/spec/ecommerce/v2/
"

        
},

        
"
traits
"
:
 
{

            
"
crossDomainId
"
:
 
"
1d1cd931-bc7d-4e39-a1a7-61563296fb15
"

        
},

        
"
userAgent
"
:
 
"
Mozilla/5.0 (Macintosh; Intel Mac OS X 10_13_2) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/63.0.3239.84 Safari/537.36
"

      
},

      
"
type
"
:
 
"
track
"
,

      
"
message_id
"
:
 
"
ajs-1a6064a677b3c16a01b8055c18f16e0b-clean
"
,

      
"
source_id
"
:
 
"
CRx5M9uk2p
"
,

      
"
timestamp
"
:
 
"
2018-01-05T00:16:35.663Z
"
,

      
"
properties
"
:
 
{

        
"
name
"
:
 
"
Docs
"
,

        
"
page_name
"
:
 
"
Docs
"
,

        
"
path
"
:
 
"
/docs/connections/spec/ecommerce/v2/
"
,

        
"
referrer
"
:
 
"
https://www.google.com/
"
,

        
"
search
"
:
 
""
,

        
"
section
"
:
 
"
Spec
"
,

        
"
title
"
:
 
"
Spec: V2 Ecommerce Events Documentation - Segment
"
,

        
"
topic
"
:
 
"
Spec: V2 Ecommerce Events
"
,

        
"
url
"
:
 
"
https://segment.com/docs/connections/spec/ecommerce/v2/
"

      
},

      
"
event
"
:
 
"
Page Viewed
"
,

      
"
related
"
:
 
{

        
"
users
"
:
 
"
use_RkjG0kW53igMijEISMH0vKBF4sL
"

      
}

    
}

  
],

  
"
cursor
"
:
 
{

    
"
url
"
:
 
"
/v1/spaces/lgJ4AAjFN4/collections/users/profiles/use_RkjG0kW53igMijEISMH0vKBF4sL/events
"
,

    
"
has_more
"
:
 
true
,

    
"
next
"
:
 
"
MTUxMzc1NTQzNjg2NzAwMDAwMDo6YWpzLTcyMWFhNzFjNDM2ZWJhOTUyYmI1ZmNiMzJlZWI3MWMzLWNsZWFu
"

  
}


}



Get a single profile’s metadata within a collection using an 
external_id
.

    GET /v1/spaces/<space_id>/collections/users/profiles/<external_id>/metadata


Request

    curl https://profiles.segment.com/v1/spaces/<space_id>/collections/users/profiles/<external_id>/metadata
      
-X
 GET
      
-u
 
$SEGMENT_ACCESS_SECRET
:


404 Not Found

{

  
"
error
"
:
 
{

    
"
code
"
:
 
"
not_found
"
,

    
"
message
"
:
 
"
Profile was not found.
"

  
}


}



200 OK

{

  
"
metadata
"
:
 
{

    
"
created_at
"
:
 
"
2017-10-23T00:22:42.78Z
"
,

    
"
updated_at
"
:
 
"
2018-01-05T00:16:36.919Z
"
,

    
"
expires_at
"
:
 
null
,

    
"
first_message_id
"
:
 
"
ajs-32ed8dea3980c0c92ed2b8c9c8c5dfb5-clean
"
,

    
"
first_source_id
"
:
 
"
GFu4AJc2bE
"
,

    
"
last_message_id
"
:
 
"
ajs-1a6064a677b3c16a01b8055c18f16e0b-clean
"
,

    
"
last_source_id
"
:
 
"
CRx5M9uk2p
"
,

  
},


}



Get the users linked to an account, or accounts linked to a user, using an 
external_id
.

GET /v1/spaces/<space_id>/collections/users/profiles/<external_id>/links


Request

    curl https://profiles.segment.com/v1/spaces/<space_id>/collections/users/profiles/<external_id>/links
      
-X
 GET
      
-u
 
$SEGMENT_ACCESS_SECRET
:


404 Not Found

{

  
"
error
"
:
 
{

    
"
code
"
:
 
"
not_found
"
,

    
"
message
"
:
 
"
Profile was not found.
"

  
}


}



200 OK

{

    
"
data
"
:
 
[

        
{

            
"
to_collection
"
:
 
"
accounts
"
,

            
"
external_ids
"
:
 
[

                
{

                    
"
id
"
:
 
"
ADGCJE3Y8H
"
,

                    
"
type
"
:
 
"
group_id
"
,

                    
"
source_id
"
:
 
"
DFAAJc2bE
"
,

                    
"
collection
"
:
 
"
accounts
"
,

                    
"
created_at
"
:
 
"
2018-10-06T03:43:26.63387Z
"
,

                    
"
encoding
"
:
 
"
none
"

                
}

            
]

        
},

        
{

            
"
to_collection
"
:
 
"
accounts
"
,

            
"
external_ids
"
:
 
[

                
{

                    
"
id
"
:
 
"
ghdctIwnA
"
,

                    
"
type
"
:
 
"
group_id
"
,

                    
"
source_id
"
:
 
"
DFAAJc2bE
"
,

                    
"
collection
"
:
 
"
accounts
"
,

                    
"
created_at
"
:
 
"
2018-10-07T06:22:47.406773Z
"
,

                    
"
encoding
"
:
 
"
none
"

                
}

            
]

        
}

    
]


}



Best practices

Recommended implementation

The Profile API doesn’t support CORS because it has access to the sum of a customer’s data. Segment also requests that you prevent the Access Token to the public, for example in a client-side application. Engineers implementing this API are advised to create a personalization service in their infrastructure, which other apps, websites, and services communicate with to fetch personalizations about their users.



Example workflow

If you want to display the most relevant blog posts given a reader’s favorite blog category:

Users who take a few minutes to read through an article on the blog will find posts recommended using their historical reading pattern including the post they just read.

External IDs

Segment does not recommend using 
external_ids
 as a lookup field that might contain personally identifiable information (PII), because this can make its way into your server logs that can be hard to find and remove. For this reason, Segment recommends against using 
email
 as an 
external_id
 for Profile API use cases.

Performance

Segment typically sees p95 response times under 200ms for the 
/traits
 endpoint, based on an in-region test in 
us-west
 to retrieve 50 traits. However, if you know which traits you’re looking for, Segment suggests you use the 
/traits?include=
 parameter to provide a list of traits you want to retrieve.

Another best practice to optimize performance in high-throughput applications is to use connection pooling. Your personalization service should share existing connections when making a request to the Profile API, instead of opening and closing a connection for each request. This additional TLS handshake is a common source of overhead for each request.

Segment recommends against blocking the page render to wait for a third party API’s response, as even small slow down can impact the page’s conversion performance. Instead, Segment recommends you to asynchronously request the data from after the page loads and use a server-to-server request for the necessary computed traits. Resulting computed traits can be cached for the second page load.

This page was last modified: 07 Feb 2025

Need support?

Questions? Problems? Need more info? Contact Segment Support for assistance!

Help improve these docs!

Was this page helpful?

Thanks for your feedback!

Can we improve this doc? 
Send us feedback!

Get started with Segment

On this page

Was this page helpful?

Thanks for your feedback!

Can we improve
 this doc? 
Send us feedback!

Product

For Developers

Company

Support

© 2025 Segment.io, Inc.

=== Content from https://segment.com/docs/connections/sources/custom-domain/ ===

Segment-Managed Custom Domain
        

Custom Domain is available to customers on Business tier plans.


See the 
available plans
, or 
contact Support
.

On this page

Custom Domain is a fully-managed service that enables you to configure a first-party subdomain over HTTPS.  You can then track event requests through your own domain (for example, 
cdp.mydomain.com
), instead of the default (
segment.com
). Tracking events through your own domain allows for more secure and complete first-party data collection by reclaiming first-party data lost to browser controls. With a more complete view of your customer behaviors, you can build more robust profiles for greater attribution and ROAS.

Custom Domain is only available to Business Tier customers

Customers not on the Business Tier but who have interest in Custom Domain should 
contact Segment’s sales team
 for assistance with upgrading to a Business Tier plan. Segment also offers an alternative DNS record service, 
Custom Proxy
.



Segment recommends configuring Custom Domain alongside 
Consent Management
 to ensure you are respectful of your end users’ consent preferences.

Segment’s domain delegation solutions

Segment offers two domain delegation solutions: 
Custom Proxy
 and 
Custom Domain
. If you use Custom Domain, you can choose to use either DNS delegation or a Cannonical Name (CNAME). Segment recommends using Custom Domain with DNS delegation, which leads to easy setup, maintenance, and monitoring.

*
If it’s not possible for you to delegate subdomains to Segment, you can use a CNAME instead. Segment encourages users to delegate a DNS subdomain rather than use use CNAME aliasing due to the evolving privacy standards in browsers, but CNAME aliasing remains an option for users not interested in using nameservers.

How DNS subdomain delegation works

DNS subdomain delegation is a process where the control of a specific subdomain is assigned to another DNS server, allowing that server to manage the DNS records for the subdomain. This delegation is useful for distributing the management of DNS records and enables specialized handling of subdomain traffic.

How CNAME records work

When a user tries to access the alias domain, the DNS resolver looks up the CNAME record, finds the canonical name, and resolves it to the IP address of the target. For example, you could alias your subdomain to point to the Segment domain. If a user accesses your site, they are redirected to the Segment domain, but their browser’s address bar still shows the alias domain.

CNAME records provide flexibility and centralized management, making it easier to handle domain redirections and subdomain configurations.

Implementing a Custom Domain using CNAME delegation requires you to add a CNAME and record for two domains that Segment generates on your behalf: one for the Segment CDN and a second for the Tracking API. You must add a CNAME and DNS record for both domains.

Supported sources

Custom Domain supports the following sources:

Getting started

Custom Domain configuration won’t disrupt your event tracking. Default Segment domains will continue to function alongside your custom domains once the setup is complete.

To configure Custom Domain:

FAQ

Can I set up multiple Custom Domains?

Segment recommends creating a different subdomain (for example, 
mysubdomain.mydomain.com
) for each source. You cannot connect multiple custom domains to the same source.

What sources can I use with Custom Domain?

For initial setup, Segment requires an 
Analytics.js
 source. Custom Domain was largely developed to support JavaScript sources. It helps with comprehensive collection of first-party data from your website when accessed over any platform (desktop, mobile, and more). You can use the subdomain for all other non-JavaScript sources as well, for consistency, but it will have no impact on data collection for those sources.

How can I configure non-JavaScript sources to use Custom Domain?

For non-Analytics.js sources, you’ll need to update your implementation to use the subdomain as an endpoint when using the Tracking API. For example:

Is there a benefit in migrating server-side sources over to client-side with Custom Domain?

Server-side tracking is generally more reliable than client-side tracking. For example, when tracking data client-side, you might lose data when users might block all cookies or use tools that interfere with network requests leaving the browser.

For business-critical events, Segment recommends server-side data tracking. This approach means that your data is less susceptible to disruptions from client-side variables, which can result in more accurate and reliable tracking.

Is this a fully-managed solution? What servers or infrastructure do I need to set up on my side for this proxy?

Yes, Custom Domain is a fully-managed solution. However, you must set up the following infrastructure on your end:

First, decide on your subdomain and then delegate it to Segment. Segment then asks you to add a DNS NS record to your DNS with specific values to complete the DNS delegation. From there on, Segment fully manages the infrastructure for serving Analytics.js and ingesting events data through the subdomain.

Can I change my Segment subdomain after the initial setup?

Segment doesn’t recommend that you change the subdomain after the initial setup. If you change the subdomain, Segment must revoke the older certificates for your subdomain and you are required to redo the entire onboarding process, as several underlying components, like certificates, would need to be recreated and reassociated.

Who is responsible for managing the SSL certificate for the Custom Domain?

Segment hosts and manages SSL Certificate on the Custom Domain. At this time, Segment does not support importing a certificate you may already have, as Segment must request a SSL certificate on your behalf using AWS Certificate Manager (ACM) when initially setting up your Custom Domain.

Segment also uses ACM to manage and renew certificates.

Can you rename 
window.analytics
 with Custom Domain?

Yes, Custom Domain allows Segment to rename 
window.analytics
 to a unique name to avoid being blocked by some ad blocking software.

Customers who have access to the Custom Domain feature can rename analytics to 
<workspaceid>/<sourceid>.js
 by choosing an Alias for Analytics.js within the source settings that are available after the workspace is enabled for Custom Domain.

What happens to the Analytics.js cookies already set on the user’s browser prior to a Custom Domain implementation?

Analytics.js cookies are not lost in the transition to Custom Domain. When users revisit your website, the previous Analytics.js cookies continue to be fetched and added to events, if available.

Can I use the same subdomain across multiple workspaces?

No, each workspace requires its own unique subdomain (for example, 
mysubdomain.mydomain.com
).

This page was last modified: 31 Jan 2025

Need support?

Questions? Problems? Need more info? Contact Segment Support for assistance!

Help improve these docs!

Was this page helpful?

Thanks for your feedback!

Can we improve this doc? 
Send us feedback!

Get started with Segment

On this page

Was this page helpful?

Thanks for your feedback!

Can we improve
 this doc? 
Send us feedback!

Product

For Developers

Company

Support

© 2025 Segment.io, Inc.

=== Content from https://segment.com/docs/engage/audiences/linked-audiences-limits/ ===

Linked Audiences Limits
        

Engage Foundations requires a Business tier account and includes Unify.


See the 
available plans
, or 
contact Support
.

On this page

Linked Audiences is an add-on to Twilio Engage. To use 
Linked Audiences
, you must have access to Engage.

To provide consistent performance and reliability at scale, Segment enforces default use limits for Linked Audiences.

Usage limits

The Linked Audiences module provides you the flexibility to create and publish unlimited Linked Audiences within each billing cycle. This means you won’t encounter any limitations or pauses in service related to the number of Linked Audiences you generate.

Linked Audience limits are measured based on Activation Events, which is the number of times profiles are processed to each destination, including audience entered, audience exited, and entity change events. This includes both successful and failed attempts. For example, if you processed an audience of 50k to Braze and Google Ads Conversions, then your total Activation Event usage is 100k records.

Your plan includes a high limit of Activation Events, which ensures that the vast majority of users can use Linked Audiences freely without needing to worry about the limit.

To see how many Activation Events you’ve processed using Linked Audiences, navigate to 
Settings > Usage & billing
 and select the 
Linked Audiences
 tab. If your limit is reached before the end of your billing period, your syncs won’t automatically pause to avoid disruptions to your business. You may be billed for overages in cases of significant excess usage. If you consistently require a higher limit, contact your sales representative to upgrade your plan with a custom limit.

If you have a non-standard or high volume usage plan, you have unique Linked Audiences limits or custom pricing.

Product limits

This page was last modified: 19 Dec 2024

Need support?

Questions? Problems? Need more info? Contact Segment Support for assistance!

Help improve these docs!

Was this page helpful?

Thanks for your feedback!

Can we improve this doc? 
Send us feedback!

Get started with Segment

On this page

Was this page helpful?

Thanks for your feedback!

Can we improve
 this doc? 
Send us feedback!

Product

For Developers

Company

Support

© 2025 Segment.io, Inc.

=== Content from https://segment.com/docs/connections/spec/semantic/ ===

Spec: Semantic Events
        

One of the core components of the Segment 
Spec
 is the 
Track
 call. It describes any arbitrary event that the user has triggered. For some industry verticals and applications, Segment has standardized event names. For Ecommerce tracking, for example, there are 
specific event names and properties
 that we recognize semantically. This semantic meaning allows Segment to specially recognize and transform key events before sending them off to each different tool.

There are a few places where Segment has standardized event names and properties already:

In the future Segment plans to standardize event names from other data sources as well.

This page was last modified: 21 Nov 2023

Need support?

Questions? Problems? Need more info? Contact Segment Support for assistance!

Help improve these docs!

Was this page helpful?

Thanks for your feedback!

Can we improve this doc? 
Send us feedback!

Get started with Segment

Was this page helpful?

Thanks for your feedback!

Can we improve
 this doc? 
Send us feedback!

Product

For Developers

Company

Support

© 2025 Segment.io, Inc.

=== Content from https://segment.com/docs/protocols/schema/ ===

Redirecting…

=== Content from https://segment.com/docs/segment-app/extensions/git/ ===

Git Sync Extension
        

On this page

Segment’s Git extension lets you manage versioning by syncing changes you make in your Segment workspace to a Git repository.

Git Sync supports one-way synchronization from Segment to Git. This sync captures the current state of your workspace through a full sync and includes all new records and changes for supported resources.

Segment doesn’t support syncing changes from Git back to Segment.

Set up Git Sync

Follow these steps to set up Git Sync:

Working with Git Sync

The Git sync extension syncs the following resources from Segment to your Git repository:

The Git sync extension doesn’t support the following resources:

Reach out to 
Segment support
 to request support for additional Git Sync resources.

After you set up the Git sync extension for the first time, Segment performs an initial sync that sends the current state of your Segment workspace to the Git repository you connected. Segment automatically tracks all following workspace updates.

You can manually trigger syncs at any time by clicking 
Full Sync
 on the Git Sync page. To disable Git Sync from the Git Sync page, switch the 
Enabled
 toggle to off.

Git Sync architecture and data model

Because a Segment workspace can represent a distinct environment (testing, staging, production), each workspace is mapped directly to a single Git repository. This direct mapping ensures a clear and organized relationship between workspace resources and a Git repository.

Segment uses its 
Terraform provider
 to manage key functions like tracking changes and retrieving information about those changes in Segment. Segment stores changes in HashiCorp Configuration Language (HCL), the format used by Terraform. To learn more about HCL and how it compares to JSON or YAML, visit 
HashiCorp’s HCL repository on GitHub
.

Using HCL makes it easier to document Segment’s data model, especially for users managing versioning and Git Sync with Terraform. It also helps manage Segment configurations directly from Git. For more details on the Git Sync data model, read 
Segment’s Terraform provider documentation
.

Managing your Segment workspace with Terraform and Git Sync

Segment supports one-way synchronization from Segment to Git, but you can set up two-way synchronization using the Segment Terraform provider.

Terraform offers an open-source way to manage Segment resources through a Git repository as an alternative to a fully managed two-way sync. This method requires third-party tools like 
Atlantis
 for CI integration.

To manage Segment resources using Git and Terraform, follow these steps:

Include the following provider configuration blocks:

 
# providers.tf


 
terraform
 
{

 
required_providers
 
{

     
segment
 
=
 
{

     
source
  
=
 
"segmentio/segment"

     
version
 
=
 
"1.0.4"

     
}

 
}

 
}


 
provider
 
"segment"
 
{

 
# Provide the token directly or load it from an environment variable

 
}



For more information on using Terraform, visit 
Terraform’s documentation
.

Git Connections

Git Connections enable Segment to sync data with your preferred Git repository through supported like SSH and token-based authentication.

Git Sync and the dbt integration operate independently. You don’t need to set up Git Sync to use dbt, and dbt Cloud can trigger its own syncs without relying on Git Sync.

Supported connection types

Segment supports the following credential types for setting up a Git Connection:

Reusing Git Connections

Segment lets you set up multiple Git Connections, allowing you to reuse credentials across both dbt and Git Sync. You can either use the same credential for multiple configurations or create separate Git Connections for each product and environment as needed.

If you plan to reuse a Git token across both dbt and Git Sync, ensure it has the necessary read and write permissions for both integrations.

Troubleshooting Git Sync

When setting up Git Sync, you may run into an access error with the following message: 
“Unable to create Git Sync due to Git connection issues. Please check your configuration and try again
.

This error can occur if there are issues with your Git connection settings or permissions. To resolve the error, verify that:

This page was last modified: 07 Nov 2024

Need support?

Questions? Problems? Need more info? Contact Segment Support for assistance!

Help improve these docs!

Was this page helpful?

Thanks for your feedback!

Can we improve this doc? 
Send us feedback!

Get started with Segment

On this page

Was this page helpful?

Thanks for your feedback!

Can we improve
 this doc? 
Send us feedback!

Product

For Developers

Company

Support

© 2025 Segment.io, Inc.

=== Content from https://segment.com/docs/engage/user-subscriptions/subscription-groups/ ===

Subscription Groups
        

Engage Premier requires a Business tier account and includes Engage Foundations and Unify.


See the 
available plans
, or 
contact Support
.

On this page

Engage Premier entered an End of Sale (EOS) period effective  June 10, 2024. Existing Segment customers will continue to have access and support to Engage Premier until an end-of-life (EOL) date is announced. We recommend exploring the following pages in preparation of a migration or future MCM needs:

Twilio Marketing Campaigns

Preferred ISV Partners:

Airship Blog
 


Bloomreach Blog
 


Braze Blog
 


Insider Blog
 


Klaviyo Blog
 


Twilio Engage Foundations Documentation
 

Subscription groups let your users choose the emails they want to receive from you. This page introduces subscription groups and explains how you can use them with 
Engage email campaigns
.

About subscription groups

A subscription group lets you send email campaigns to specific groups of users. Subscription groups also give your customers the ability to manage their email preferences, ensuring they only get emails they want to receive.

For example, you may want to create a subscription group that will receive only promotional email campaigns. Should a customer decide to opt out of your promotional campaigns, they’ll still be able to receive email campaigns from other subscription groups you’ve created and to which they’ve subscribed.

What your users experience

With subscription groups, your customers can opt in and out of groups on an individual basis instead of unsubscribing from all your campaigns.

Your customers will have the chance to opt in and out of subscription groups on both a subscription preferences page and on the landing page that launches when they unsubscribe.

Customers can access these pages through the 
unsubscribe and manage preference links
 that you include in your 
email templates
.



Using subscription groups

To use a subscription group, you’ll need to first create the group, add subscribers, then create a new email template.

Create a subscription group

Follow these steps to create a subscription group:

Add group subscribers

In addition to adding group subscribers when you first create a subscription group, you can also add subscribers to existing groups with a CSV file upload with these steps:

Validation errors

The following table lists validation errors you may run into with your CSV upload:

View update history

Use the Update History page to view CSV file uploads in your workspace over the last 30 days.

To view the Update History page:

Engage uses the following error codes on the report:

Please note the following limits as you upload CSV files to Twilio Engage:

Create a new email template and send an email

To use subscription groups, you’ll need to create a new email template with new unsubscribe and manage preference links.

Once you’ve created a subscription group and added subscribers to it, follow these steps to send to the group:

Set subscription group status with the Identify call

Segment supports subscription groups for email. You can send statuses for email subscription groups using the 
Identify call
.

To set susbcription groups with the Identify call, you’ll need to include a key-value pair of 
"type": "EMAIL"
 and the 
groups
 object, like in the following sample payload:

{

  
"userId"
:
 
"12aBCDeFg4hIjKlM5OPq67RS8Tu"
,

  
"context"
:
 
{

    
"messaging_subscriptions"
:
 
[

      
{

        
"key"
:
 
"(123) 555-5555"
,

        
"type"
:
 
"SMS"
,

        
"status"
:
 
"SUBSCRIBED"
 
|
 
"UNSUBSCRIBED"
 
|
 
"DID_NOT_SUBSCRIBE"

      
},

      
{

        
"key"
:
 
"(123) 555-5555"
,

        
"type"
:
 
"WhatsApp"
,

        
"status"
:
 
"SUBSCRIBED"
 
|
 
"UNSUBSCRIBED"
 
|
 
"DID_NOT_SUBSCRIBE"

      
},

      
{

        
"key"
:
 
"test@example.com"
,

        
"type"
:
 
"EMAIL"
,

        
"status"
:
 
"SUBSCRIBED"
 
|
 
"UNSUBSCRIBED"
 
|
 
"DID_NOT_SUBSCRIBE"
,

        
"groups"
:
 
[

            
{

               
"name"
:
 
"newsletter"
,

               
"status"
:
 
"SUBSCRIBED"
 
|
 
"UNSUBSCRIBED"
 
|
 
"DID_NOT_SUBSCRIBE"

            
},

            
{

               
"name"
:
 
"marketing updates"
,

               
"status"
:
 
"SUBSCRIBED"
 
|
 
"UNSUBSCRIBED"
 
|
 
"DID_NOT_SUBSCRIBE"

            
}

        
]

      
}

    
],

    
"externalIds"
:
 
[

      
{

        
"id"
:
 
"(123) 555-5555"
,

        
"type"
:
 
"phone"
,

        
"collection"
:
 
"users"
,

        
"encoding"
:
 
"none"

      
}

    
],

    
"traits"
:
 
{

       
"email"
:
 
"test@example.com"

    
}

  
},

  
"integrations"
:
 
{},

  
"traits"
:
 
{}


}



FAQs

Your Engage space includes up to 25 subscription groups.

No. Templates you’ve previously created aren’t compatible with subscription groups. To use subscription groups, you’ll need to create new templates that include new unsubscribe and manage preference links.

If you delete a subscription group, Engage will still maintain the preferences of the group’s end users.

The 
Engage Events Source
 tracks four subscription group events: 
Email Unsubscribed
, 
Email Group Unsubscribed
, 
Channel Subscription Updated
, and 
Group Subscription Updated
.

If a user unsubscribes from all of your subscription groups, they’ll need to re-subscribe by explicitly opting back in to each group.

Yes. Test emails include fully functional unsubscribe and subscription preference links. If a test email recipient unsubscribes using a test email, Segment updates that user’s subscription state. 

Test emails temporarily override an email subscription state. This means that an unsubscribed email address can receive a test email but won’t receive regular email campaigns from which they’ve unsubscribed.

Yes. Keep the following table in mind when you name a subscription group:



This page was last modified: 15 Jul 2024

Need support?

Questions? Problems? Need more info? Contact Segment Support for assistance!

Help improve these docs!

Was this page helpful?

Thanks for your feedback!

Can we improve this doc? 
Send us feedback!

Get started with Segment

On this page

Was this page helpful?

Thanks for your feedback!

Can we improve
 this doc? 
Send us feedback!

Product

For Developers

Company

Support

© 2025 Segment.io, Inc.

=== Content from https://segment.com/docs/connections/spec/ecommerce/v2/ ===

Spec: V2 Ecommerce Events
        

On this page

Segment’s e-commerce spec helps define the journey for a customer as they browse your store, click on promotions, view products, add those products to a cart, and complete a purchase.

Note

Not all destinations support every event listed here and accept arrays as properties. Refer to individual destination documentation for more information on supported events and properties.

Event lifecycles

Here is a list of supported events for our various categories within the customer journey.

Browsing overview

Promotions overview

Core ordering overview

Coupons overview

Wishlisting overview

Sharing overview

Reviewing overview

The following sections list more detail for each lifecycle event as well as an example API call.

Browsing

Browsing lifecycle events represent key events that a customer might have while browsing your apps.

Products Searched

Fire this event when a visitor searches for products.

This event supports the following semantic properties:

Example:

analytics
.
track
(
'
Products Searched
'
,
 
{

  
query
:
 
'
blue roses
'


});



Product List Viewed

Fire this event when a visitor views a product list or category.

Note

Not all destinations accept arrays as properties. Refer to individual destination documentation for more information on supported events and properties.

This event supports the following semantic properties:

Example:

analytics
.
track
(
'
Product List Viewed
'
,
 
{

  
list_id
:
 
'
hot_deals_1
'
,

  
category
:
 
'
Deals
'
,

  
products
:
 
[

    
{

      
product_id
:
 
'
507f1f77bcf86cd799439011
'
,

      
sku
:
 
'
45790-32
'
,

      
name
:
 
'
Monopoly: 3rd Edition
'
,

      
price
:
 
19
,

      
position
:
 
1
,

      
category
:
 
'
Games
'
,

      
url
:
 
'
https://www.example.com/product/path
'
,

      
image_url
:
 
'
https://www.example.com/product/path.jpg
'

    
},

    
{

      
product_id
:
 
'
505bd76785ebb509fc183733
'
,

      
sku
:
 
'
46493-32
'
,

      
name
:
 
'
Uno Card Game
'
,

      
price
:
 
3
,

      
position
:
 
2
,

      
category
:
 
'
Games
'

    
}

  
]


});



Note

The 
Product List Viewed
 event is aliased to the 
Viewed Product Category
 event (from e-commerce v1 spec).

Product List Filtered

Send this event when a visitor filters a product list or category.

Note

Not all destinations accept arrays as properties. Refer to individual destination docs for more information on supported events and properties.

This event supports the following semantic properties:

Example:



analytics
.
track
(
'
Product List Filtered
'
,
 
{

  
list_id
:
 
'
todays_deals_may_11_2019
'
,

  
filters
:
 
[

    
{

      
type
:
 
'
department
'
,

      
value
:
 
'
beauty
'

    
},

    
{

      
type
:
 
'
price
'
,

      
value
:
 
'
under-$25
'

    
},

  
],

  
sorts
:
 
[

    
{

      
type
:
 
'
price
'
,

      
value
:
 
'
desc
'

    
}

  
],

  
products
:
 
[

    
{

      
product_id
:
 
'
507f1f77bcf86cd798439011
'
,

      
sku
:
 
'
45360-32
'
,

      
name
:
 
'
Special Facial Soap
'
,

      
price
:
 
12.60
,

      
position
:
 
1
,

      
category
:
 
'
Beauty
'
,

      
url
:
 
'
https://www.example.com/product/path
'
,

      
image_url
:
 
'
https://www.example.com/product/path.jpg
'

    
},

    
{

      
product_id
:
 
'
505bd76785ebb509fc283733
'
,

      
sku
:
 
'
46573-32
'
,

      
name
:
 
'
Fancy Hairbrush
'
,

      
price
:
 
7.60
,

      
position
:
 
2
,

      
category
:
 
'
Beauty
'

    
}

  
]


});



Promotions

Promotion view and click events help you gather analytics on internal offers within your web or mobile app. For example, when a banner advertisement is shown in your web or app’s home page, you can fire a 
Viewed Promotion
  event. If the user proceeds to click the advertisement, fire the 
Clicked Promotion
 event.

Promotion Viewed

Fire this event when a user views a promotion.

This event supports the following semantic properties:

Example:

analytics
.
track
(
'
Promotion Viewed
'
,
 
{

  
promotion_id
:
 
'
promo_1
'
,

  
creative
:
 
'
top_banner_2
'
,

  
name
:
 
'
75% store-wide shoe sale
'
,

  
position
:
 
'
home_banner_top
'


});



Note

The 
Promotion Viewed
 event is aliased to the 
Viewed Promotion
 event
.

Promotion Clicked

Fire this event when a visitor clicks an internal offer promotion.

This event supports the following semantic properties:

Example:

analytics
.
track
(
'
Promotion Clicked
'
,
 
{

  
promotion_id
:
 
'
promo_1
'
,

  
creative
:
 
'
top_banner_2
'
,

  
name
:
 
'
75% store-wide shoe sale
'
,

  
position
:
 
'
home_banner_top
'


});



Note

The 
Promotion Clicked
 event is aliased to the 
Clicked Promotion
 event
.

Core Ordering

These events represent the customer journey in regards to product ordering.

Product Clicked

Fire this event when a visitor clicks a product.

This event supports the following semantic properties:

Example:

analytics
.
track
(
'
Product Clicked
'
,
 
{

  
product_id
:
 
'
507f1f77bcf86cd799439011
'
,

  
sku
:
 
'
G-32
'
,

  
category
:
 
'
Games
'
,

  
name
:
 
'
Monopoly: 3rd Edition
'
,

  
brand
:
 
'
Hasbro
'
,

  
variant
:
 
'
200 pieces
'
,

  
price
:
 
18.99
,

  
quantity
:
 
1
,

  
coupon
:
 
'
MAYDEALS
'
,

  
position
:
 
3
,

  
url
:
 
'
https://www.example.com/product/path
'
,

  
image_url
:
 
'
https://www.example.com/product/path.jpg
'


});



Note

Product Viewed

Fire this event when a visitor views a product. That view might happen on a page, screen, or preview modal.

This event supports the following semantic properties:

Example:

analytics
.
track
(
'
Product Viewed
'
,
 
{

  
product_id
:
 
'
507f1f77bcf86cd799439011
'
,

  
sku
:
 
'
G-32
'
,

  
category
:
 
'
Games
'
,

  
name
:
 
'
Monopoly: 3rd Edition
'
,

  
brand
:
 
'
Hasbro
'
,

  
variant
:
 
'
200 pieces
'
,

  
price
:
 
18.99
,

  
quantity
:
 
1
,

  
coupon
:
 
'
MAYDEALS
'
,

  
currency
:
 
'
usd
'
,

  
position
:
 
3
,

  
value
:
 
18.99
,

  
url
:
 
'
https://www.example.com/product/path
'
,

  
image_url
:
 
'
https://www.example.com/product/path.jpg
'


});



Note

Product Added

Fire this event when a visitor adds a product to their shopping cart.

This event supports the following semantic properties:

Example:

analytics
.
track
(
'
Product Added
'
,
 
{

  
cart_id
:
 
'
skdjsidjsdkdj29j
'
,

  
product_id
:
 
'
507f1f77bcf86cd799439011
'
,

  
sku
:
 
'
G-32
'
,

  
category
:
 
'
Games
'
,

  
name
:
 
'
Monopoly: 3rd Edition
'
,

  
brand
:
 
'
Hasbro
'
,

  
variant
:
 
'
200 pieces
'
,

  
price
:
 
18.99
,

  
quantity
:
 
1
,

  
coupon
:
 
'
MAYDEALS
'
,

  
position
:
 
3
,

  
url
:
 
'
https://www.example.com/product/path
'
,

  
image_url
:
 
'
https://www.example.com/product/path.jpg
'


});



Note

Product Removed

Fire this event when a visitor removes a product from their shopping cart.

This event supports the following semantic properties:

Example:

analytics
.
track
(
'
Product Removed
'
,
 
{

  
cart_id
:
 
'
ksjdj92dj29dj92d2j
'
,

  
product_id
:
 
'
507f1f77bcf86cd799439011
'
,

  
sku
:
 
'
G-32
'
,

  
category
:
 
'
Games
'
,

  
name
:
 
'
Monopoly: 3rd Edition
'
,

  
brand
:
 
'
Hasbro
'
,

  
variant
:
 
'
200 pieces
'
,

  
price
:
 
18.99
,

  
quantity
:
 
1
,

  
coupon
:
 
'
MAYDEALS
'
,

  
position
:
 
3
,

  
url
:
 
'
https://www.example.com/product/path
'
,

  
image_url
:
 
'
https://www.example.com/product/path.jpg
'


});



Note

Cart Viewed

Fire this event when a visitor views a shopping cart.

Note

Not all destinations accept arrays as properties. Refer to individual destination documentation for more information on supported events and properties.

This event supports the following semantic properties:

Example:

analytics
.
track
(
'
Cart Viewed
'
,
 
{

  
cart_id
:
 
'
d92jd29jd92jd29j92d92jd
'
,

  
products
:
 
[

    
{

      
product_id
:
 
'
507f1f77bcf86cd799439011
'
,

      
sku
:
 
'
45790-32
'
,

      
name
:
 
'
Monopoly: 3rd Edition
'
,

      
price
:
 
19
,

      
position
:
 
1
,

      
category
:
 
'
Games
'
,

      
url
:
 
'
https://www.example.com/product/path
'
,

      
image_url
:
 
'
https://www.example.com/product/path.jpg
'

    
},

    
{

      
product_id
:
 
'
505bd76785ebb509fc183733
'
,

      
sku
:
 
'
46493-32
'
,

      
name
:
 
'
Uno Card Game
'
,

      
price
:
 
3
,

      
position
:
 
2
,

      
category
:
 
'
Games
'

    
}

  
]


});



Checkout Started

Fire this event whenever an order/transaction was started. Fire on the page that the customer lands on after they press the checkout button.

Note

Not all destinations accept arrays as properties. Refer to individual destination documentation for more information on supported events and properties.

Be sure to 
include all items in the cart as event properties
, with the same properties from the previous calls, like so:

This event supports the following semantic properties:

Example:

analytics
.
track
(
'
Checkout Started
'
,
 
{

  
order_id
:
 
'
50314b8e9bcf000000000000
'
,

  
affiliation
:
 
'
Google Store
'
,

  
value
:
 
30
,

  
revenue
:
 
25.00
,

  
shipping
:
 
3
,

  
tax
:
 
2
,

  
discount
:
 
2.5
,

  
coupon
:
 
'
hasbros
'
,

  
currency
:
 
'
USD
'
,

  
products
:
 
[

    
{

      
product_id
:
 
'
507f1f77bcf86cd799439011
'
,

      
sku
:
 
'
45790-32
'
,

      
name
:
 
'
Monopoly: 3rd Edition
'
,

      
price
:
 
19
,

      
quantity
:
 
1
,

      
category
:
 
'
Games
'
,

      
url
:
 
'
https://www.example.com/product/path
'
,

      
image_url
:
 
'
https://www.example.com/product/path.jpg
'

    
},

    
{

      
product_id
:
 
'
505bd76785ebb509fc183733
'
,

      
sku
:
 
'
46493-32
'
,

      
name
:
 
'
Uno Card Game
'
,

      
price
:
 
3
,

      
quantity
:
 
2
,

      
category
:
 
'
Games
'

    
}

  
]


});



Note

Checkout Step Viewed

Fire this event whenever a checkout step is viewed.

This event supports the following semantic properties:

Example:

analytics
.
track
(
'
Checkout Step Viewed
'
,
 
{

  
checkout_id
:
 
'
50314b8e9bcf000000000000
'
,

  
step
:
 
2
,

  
shipping_method
:
 
'
Fedex
'
,

  
payment_method
:
 
'
Visa
'


});



Note

shipping_method
 and 
payment_method
 are semantic properties. If you want to send that information, do so in this exact spelling.

You can have as many or as few steps in the checkout funnel as you’d like. Note that you’ll still need to track the 
Order Completed
 event per Segment’s standard 
e-commerce tracking API
 after you’ve tracked the checkout steps.

Note

The 
Checkout Step Viewed
 event is aliased to the 
Viewed Checkout Step
 event from 
Segment’s GA Enhanced E-Commerce destinations
.

Checkout Step Completed

Fire this event whenever a checkout step is completed.

This event supports the following semantic properties:

Example:

analytics
.
track
(
'
Checkout Step Completed
'
,
 
{

  
checkout_id
:
 
'
50314b8e9bcf000000000000
'
,

  
step
:
 
2
,

  
shipping_method
:
 
'
Fedex
'
,

  
payment_method
:
 
'
Visa
'


});



Note

shipping_method
 and 
payment_method
 are semantic properties. If you want to send that information, do so in this exact spelling.

You can have as many or as few steps in the checkout funnel as you’d like. Note that you’ll still need to track the 
Order Completed
 event per Segment’s standard 
e-commerce tracking API
 after you’ve tracked the checkout steps.

Note

The 
Checkout Step Completed
 event is aliased to the 
Completed Checkout Step
 event from 
Segment’s GA Enhanced E-Commerce destinations
.

Payment Info Entered

Fire this event whenever payment information has been successfully entered.

This event supports the following semantic properties:

Example:

analytics
.
track
(
'
Payment Info Entered
'
,
 
{

  
checkout_id
:
 
'
39f39fj39f3jf93fj9fj39fj3f
'
,

  
order_id
:
 
'
dkfsjidfjsdifsdfksdjfkdsfjsdfkdsf
'


});



Note

shipping_method
 and 
payment_method
 are semantic properties. If you want to send that information, do so in this exact spelling.

You can have as many or as few steps in the checkout funnel as you’d like. Note that you’ll still need to track the 
Order Completed
 event per Segment’s standard 
e-commerce tracking API
 after you’ve tracked the checkout steps.

Order Updated

Fire this event whenever an order/transaction was updated.

Note

Not all destinations accept arrays as properties. Refer to individual destination documentation for more information on supported events and properties.

Be sure to 
include all items in the cart as event properties
, with the same properties from the previous calls, like so:

This event supports the following semantic properties:

Example:

analytics
.
track
(
'
Order Updated
'
,
 
{

      
order_id
:
 
'
50314b8e9bcf000000000000
'
,

      
affiliation
:
 
'
Google Store
'
,

      
total
:
 
27.50
,

      
revenue
:
 
25.00
,

      
shipping
:
 
3
,

      
tax
:
 
2
,

      
discount
:
 
2.5
,

      
coupon
:
 
'
hasbros
'
,

      
currency
:
 
'
USD
'
,

      
products
:
 
[

        
{

          
product_id
:
 
'
507f1f77bcf86cd799439011
'
,

          
sku
:
 
'
45790-32
'
,

          
name
:
 
'
Monopoly: 3rd Edition
'
,

          
price
:
 
19
,

          
quantity
:
 
1
,

          
category
:
 
'
Games
'
,

          
url
:
 
'
https://www.example.com/product/path
'
,

  
image_url
:
 
'
https://www.example.com/product/path.jpg
'

        
},

        
{

          
product_id
:
 
'
505bd76785ebb509fc183733
'
,

          
sku
:
 
'
46493-32
'
,

          
name
:
 
'
Uno Card Game
'
,

          
price
:
 
3
,

          
quantity
:
 
2
,

          
category
:
 
'
Games
'

        
}

      
]

    
});



Note

Order Completed

Fire this event whenever an order/transaction was successfully completed by the customer.

Note

Not all destinations accept arrays as properties. Refer to individual destination documentation for more information on supported events and properties.

Be sure to 
include all items in the cart as event properties
, with the same properties from the previous calls, like so:

This event supports the following semantic properties:

Example:

analytics
.
track
(
'
Order Completed
'
,
 
{

  
checkout_id
:
 
'
fksdjfsdjfisjf9sdfjsd9f
'
,

  
order_id
:
 
'
50314b8e9bcf000000000000
'
,

  
affiliation
:
 
'
Google Store
'
,

  
total
:
 
27.50
,

  
subtotal
:
 
22.50
,

  
revenue
:
 
22.50
,

  
shipping
:
 
3
,

  
tax
:
 
2
,

  
discount
:
 
2.5
,

  
coupon
:
 
'
hasbros
'
,

  
currency
:
 
'
USD
'
,

  
products
:
 
[

    
{

      
product_id
:
 
'
507f1f77bcf86cd799439011
'
,

      
sku
:
 
'
45790-32
'
,

      
name
:
 
'
Monopoly: 3rd Edition
'
,

      
price
:
 
19
,

      
quantity
:
 
1
,

      
category
:
 
'
Games
'
,

      
url
:
 
'
https://www.example.com/product/path
'
,

      
image_url
:
 
'
https:///www.example.com/product/path.jpg
'

    
},

    
{

      
product_id
:
 
'
505bd76785ebb509fc183733
'
,

      
sku
:
 
'
46493-32
'
,

      
name
:
 
'
Uno Card Game
'
,

      
price
:
 
3
,

      
quantity
:
 
2
,

      
category
:
 
'
Games
'

    
}

  
]


});



Note

Order Refunded

Fire this event whenever an order/transaction was refunded.

Be sure to 
include all items in the cart as event properties
, with the same properties from the previous “Order Completed” call.

This event supports the following semantic properties:

Example:

analytics
.
track
(
'
Order Refunded
'
,
 
{

  
order_id
:
 
'
50314b8e9bcf000000000000
'
,

  
total
:
 
30
,

  
currency
:
 
'
USD
'
,

  
products
:
 
[

    
{

      
product_id
:
 
'
507f1f77bcf86cd799439011
'
,

      
sku
:
 
'
45790-32
'
,

      
name
:
 
'
Monopoly: 3rd Edition
'
,

      
price
:
 
19
,

      
quantity
:
 
1
,

      
category
:
 
'
Games
'
,

      
url
:
 
'
https://www.example.com/product/path
'
,

      
image_url
:
 
'
https://www.example.com/product/path.jpg
'

    
},

    
{

      
product_id
:
 
'
505bd76785ebb509fc183733
'
,

      
sku
:
 
'
46493-32
'
,

      
name
:
 
'
Uno Card Game
'
,

      
price
:
 
3
,

      
quantity
:
 
2
,

      
category
:
 
'
Games
'

    
}

  
]


});



Note

The 
sku
 and 
product_id
 don’t have to be different. If they are different, typically the 
product_id
 is a database identifier, like 
9714107479
 and the 
sku
 is a public-facing identifier like 
SEG-02
.

Order Cancelled

Fire this event whenever an order/transaction was cancelled.

Note

Not all destinations accept arrays as properties. Refer to individual destination documentation for more information on supported events and properties.

Be sure to 
include all items in the cart as event properties
, with the same properties from the previous calls.

This event supports the following semantic properties:

Example:

analytics
.
track
(
'
Order Cancelled
'
,
 
{

  
order_id
:
 
'
50314b8e9bcf000000000000
'
,

  
affiliation
:
 
'
Google Store
'
,

  
total
:
 
30
,

  
revenue
:
 
25.00
,

  
shipping
:
 
3
,

  
tax
:
 
2
,

  
discount
:
 
2.5
,

  
coupon
:
 
'
hasbros
'
,

  
currency
:
 
'
USD
'
,

  
products
:
 
[

    
{

      
product_id
:
 
'
507f1f77bcf86cd799439011
'
,

      
sku
:
 
'
45790-32
'
,

      
name
:
 
'
Monopoly: 3rd Edition
'
,

      
price
:
 
19
,

      
quantity
:
 
1
,

      
category
:
 
'
Games
'
,

      
url
:
 
'
https://www.example.com/product/path
'
,

      
image_url
:
 
'
https://www.example.com/product/path.jpg
'

    
},

    
{

      
product_id
:
 
'
505bd76785ebb509fc183733
'
,

      
sku
:
 
'
46493-32
'
,

      
name
:
 
'
Uno Card Game
'
,

      
price
:
 
3
,

      
quantity
:
 
2
,

      
category
:
 
'
Games
'

    
}

  
]


});



Note

The 
sku
 and 
product_id
 don’t have to be different. If they are different, typically the 
product_id
 is a database identifier, like 
9714107479
 and the 
sku
 is a public-facing identifier like 
SEG-02
.

Coupons

These are events that might occur when dealing with coupons in your ecommerce.

Coupon Entered

Fire this event whenever a coupon is entered either on a cart or on an order/transaction.

This event supports the following semantic properties:

Example:

analytics
.
track
(
'
Coupon Entered
'
,
 
{

      
order_id
:
 
'
50314b8e9bcf000000000000
'
,

      
cart_id
:
 
'
923923929jd29jd92dj9j93fj3
'
,

      
coupon_id
:
 
'
may_deals_2016
'

    
});



Note

This has no effect in GA enhanced e-commerce, as that destination pulls from the 
coupon
 field on the order events. Refer to Segment’s 
Google Analytic documentation
 for more information.

Coupon Applied

Fire this event whenever a coupon is successfully applied to either a cart or an order/transaction.

This event supports the following semantic properties:

Example:

analytics
.
track
(
'
Coupon Applied
'
,
 
{

      
order_id
:
 
'
50314b8e9bcf000000000000
'
,

      
cart_id
:
 
'
923923929jd29jd92dj9j93fj3
'

      
coupon_id
:
 
'
may_deals_2016
'
,

      
coupon_name
:
 
'
May Deals 2016
'
,

      
discount
:
 
23.32

    
});



Note

This has no effect in GA enhanced e-commerce, as that destination pulls from the 
coupon
 field on the order events. Refer to Segment’s 
Google Analytic documentation
 for more information.

Coupon Denied

Fire this event whenever a coupon is denied from either a cart or an order/transaction.

This event supports the following semantic properties:

Example:

analytics
.
track
(
'
Coupon Denied
'
,
 
{

      
order_id
:
 
'
50314b8e9bcf000000000000
'
,

      
cart_id
:
 
'
923923929jd29jd92dj9j93fj3
'

      
coupon
:
 
'
may_deals_2016
'
,

      
reason
:
 
'
Coupon expired
'

    
});



Note

This has no effect in GA enhanced e-commerce, as that destination pulls from the 
coupon
 field on the order events. Refer to Segment’s 
Google Analytic documentation
 for more information.

Coupon Removed

Fire this event whenever a coupon is removed from either a cart or an order/transaction.

This event supports the following semantic properties:

Example:

analytics
.
track
(
'
Coupon Removed
'
,
 
{

  
order_id
:
 
'
50314b8e9bcf000000000000
'
,

  
cart_id
:
 
'
923923929jd29jd92dj9j93fj3
'

  
coupon_id
:
 
'
may_deals_2016
'
,

  
coupon_name
:
 
'
May Deals 2016
'
,

  
discount
:
 
23.32


});



Note

This has no effect in GA enhanced e-commerce, as that destination pulls from the 
coupon
 field on the order events. Refer to Segment’s 
Google Analytic documentation
 for more information.

Wishlisting

These events may occur if your ecommerce supports wishlist features.

Product Added to Wishlist

Fire this event when a customer adds a product to their wish list.

This event supports the following semantic properties:

Example:

analytics
.
track
(
'
Product Added to Wishlist
'
,
 
{

  
wishlist_id
:
 
'
skdjsidjsdkdj29j
'
,

  
wishlist_name
:
 
'
Loved Games
'
,

  
product_id
:
 
'
507f1f77bcf86cd799439011
'
,

  
sku
:
 
'
G-32
'
,

  
category
:
 
'
Games
'
,

  
name
:
 
'
Monopoly: 3rd Edition
'
,

  
brand
:
 
'
Hasbro
'
,

  
variant
:
 
'
200 pieces
'
,

  
price
:
 
18.99
,

  
quantity
:
 
1
,

  
coupon
:
 
'
MAYDEALS
'
,

  
position
:
 
3
,

  
url
:
 
'
https://www.example.com/product/path
'
,

  
image_url
:
 
'
https://www.example.com/product/path.jpg
'


});



Note

The 
sku
 and 
product_id
 do not have to be different. If they are different, typically the 
product_id
 is a database identifier, like 
9714107479
 and the 
sku
 is a public-facing identifier like 
SEG-02
.

Product Removed from Wishlist

Fire this event when a customer removes a product from their wish list.

This event supports the following semantic properties:

Example:

analytics
.
track
(
'
Product Removed from Wishlist
'
,
 
{

  
wishlist_id
:
 
'
skdjsidjsdkdj29j
'
,

  
wishlist_name
:
 
'
Loved Games
'
,

  
product_id
:
 
'
507f1f77bcf86cd799439011
'
,

  
sku
:
 
'
G-32
'
,

  
category
:
 
'
Games
'
,

  
name
:
 
'
Monopoly: 3rd Edition
'
,

  
brand
:
 
'
Hasbro
'
,

  
variant
:
 
'
200 pieces
'
,

  
price
:
 
18.99
,

  
quantity
:
 
1
,

  
coupon
:
 
'
MAYDEALS
'
,

  
position
:
 
3
,

  
url
:
 
'
https://www.example.com/product/path
'
,

  
image_url
:
 
'
https://www.example.com/product/path.jpg
'


});



Note

The 
sku
 and 
product_id
 don’t have to be different. If they are different, typically the 
product_id
 is a database identifier, like 
9714107479
 and the 
sku
 is a public-facing identifier like 
SEG-02
.

Wishlist Product Added to Cart

Fire this event when a customer moves a product from their wish list to their cart.

This event supports the following semantic properties:

Example:

analytics
.
track
(
'
Wishlist Product Added to Cart
'
,
 
{

  
wishlist_id
:
 
'
skdjsidjsdkdj29j
'
,

  
wishlist_name
:
 
'
Loved Games
'
,

  
cart_id
:
 
'
99j2d92j9dj29dj29d2d
'
,

  
product_id
:
 
'
507f1f77bcf86cd799439011
'
,

  
sku
:
 
'
G-32
'
,

  
category
:
 
'
Games
'
,

  
name
:
 
'
Monopoly: 3rd Edition
'
,

  
brand
:
 
'
Hasbro
'
,

  
variant
:
 
'
200 pieces
'
,

  
price
:
 
18.99
,

  
quantity
:
 
1
,

  
coupon
:
 
'
MAYDEALS
'
,

  
position
:
 
3
,

  
url
:
 
'
https://www.example.com/product/path
'
,

  
image_url
:
 
'
https://www.example.com/product/path.jpg
'


});



Note

The 
sku
 and 
product_id
 don’t have to be different. If they are different, typically the 
product_id
 is a database identifier, like 
9714107479
 and the 
sku
 is a public-facing identifier like 
SEG-02
.

Sharing

With many ecommerce stores integrating with social apps or other sharing capabilities, these events might be useful if you are tracking customers sharing product information.

Product Shared

Fire this event when a customer shares a product.

This event supports the following semantic properties:

Example:

analytics
.
track
(
'
Product Shared
'
,
 
{

  
share_via
:
 
'
email
'
,

  
share_message
:
 
'
Hey, check out this item
'
,

  
recipient
:
 
'
friend@example.com
'
,

  
product_id
:
 
'
507f1f77bcf86cd799439011
'
,

  
sku
:
 
'
G-32
'
,

  
category
:
 
'
Games
'
,

  
name
:
 
'
Monopoly: 3rd Edition
'
,

  
brand
:
 
'
Hasbro
'
,

  
variant
:
 
'
200 pieces
'
,

  
price
:
 
18.99
,

  
url
:
 
'
https://www.example.com/product/path
'
,

  
image_url
:
 
'
https://www.example.com/product/path.jpg
'


});



Note

The 
sku
 and 
product_id
 don’t have to be different. If they are different, typically the 
product_id
 is a database identifier, like 
9714107479
 and the 
sku
 is a public-facing identifier like 
SEG-02
.

Cart Shared

Fire this event when a customer shares a shopping cart.

Note

Not all destinations accept arrays as properties. Refer to individual destination docs for more information on supported events and properties.

This event supports the following semantic properties:

Example:

analytics
.
track
(
'
Cart Shared
'
,
 
{

  
share_via
:
 
'
email
'
,

  
share_message
:
 
'
Hey, check out this item
'
,

  
recipient
:
 
'
friend@example.com
'
,

  
cart_id
:
 
'
d92jd29jd92jd29j92d92jd
'
,

  
products
:
 
[

    
{
 
product_id
:
 
'
507f1f77bcf86cd799439011
'
 
},

    
{
 
product_id
:
 
'
505bd76785ebb509fc183733
'
 
}

  
]


});



Reviewing

These events can be useful for tracking product related reviews.

Product Reviewed

Fire this event when a customer reviews a product.

This event supports the following semantic properties:

Example:

analytics
.
track
(
'
Product Reviewed
'
,
 
{

  
product_id
:
 
'
507f1f77bcf86cd799439011
'
,

  
review_id
:
 
'
kdfjrj39fj39jf3
'
,

  
review_body
:
 
'
I love this product
'
,

  
rating
:
 
'
5
'


});



This page was last modified: 17 Apr 2024

Need support?

Questions? Problems? Need more info? Contact Segment Support for assistance!

Help improve these docs!

Was this page helpful?

Thanks for your feedback!

Can we improve this doc? 
Send us feedback!

Get started with Segment

On this page

Was this page helpful?

Thanks for your feedback!

Can we improve
 this doc? 
Send us feedback!

Product

For Developers

Company

Support

© 2025 Segment.io, Inc.

=== Content from https://segment.com/docs/connections/destinations/actions/ ===

Destination Actions
        

Destination Actions are available for all Segment plans. Self-service plans are limited to two conditions per Trigger.


See the 
available plans
, or 
contact Support
.

On this page

The Destination Actions framework improves on classic destinations by enabling you to see and control how Segment sends the event data it receives from your sources, to actions-based destinations. Each Action in a destination lists the event data it requires, and the event data that is optional.

You can also choose which event types, event names, or event property values trigger an Action. These Triggers and mappings make it possible to send different versions of the Action, depending on the context from which it is triggered.

Each Actions-framework Destination you see in the Segment catalog represents a feature or capability of the destination which can consume data from your Segment source. The Action clearly lists which data from the events it requires, and which data is optional. For example, Amplitude requires that you always send a  
LogEvent
 , or Slack always requires a 
PostMessage
.  Each Action also includes a default mapping which you can modify.

Benefits of Destination Actions

Available Actions-based Destinations

The following Actions-based Destinations are available:

Destination Actions compatibility

Components of a Destination Action

A Destination Action contains a hierarchy of components, that work together to ensure the right data is sent to the destination.

For example, in the Amplitude (Actions) destination, you define your API and Secret keys in the destination’s global settings. Then, the provided Page Calls mapping:

Set up a destination action

To set up a new Actions-framework destination for the first time:

You must configure and enable at least one mapping to handle a connected source’s event(s) in an Actions-framework destination in order for data to send downstream. 
Events send downstream in the order in which they appear in the mappings UI. There is no mechanism through which you can control the order of events that send to the downstream destinations outside of that.

Migrate a classic destination to an actions-based destination

Moving from a classic destination to an actions-based destination is a manual process. Segment recommends that you follow the procedure below:

Migrate your destination filters from the classic destination to the actions destination

You can only migrate your destination filters using the Public API if you’re on the business tier plan. This functionality isn’t available in the Segment app.

To migrate your destination filters to your actions destination from the classic destination:

Migrate to an actions-based destination using Destination Filters

For a more comprehensive migration from a classic destination to an actions-based destination, follow the steps outlined below. This implementation strategy is only available for customers on a Segment Business Tier plan with access to 
Destination Filters
. By adding additional line of defense with Destination Filters, you remove the possibility of duplicate events or dropped events and ensure that events sent before/after a specified 
received_at
 timestamp are sent to each destination.

This migration strategy involves configuring a destination filter on both the Classic destination and the Actions destination. Configure the classic destination filter to block events by the 
received_at
 field with a certain value, and the Actions destination to drop events until the 
received_at
 timestamp field reaches that same value. Destination Filters within the UI have a limitation where they cannot access any top-level fields, but this is not a limitation for 
Destination Filters
 created by the 
Public API
 using 
FQL
. Because the 
received_at
 is a top-level field in the payload, you’ll need to create a destination filter with the Public API and submit the request with that FQL information described below.

By combining these Filters, Segment sends events through the Classic integration up until a specified time and then blocks events after that time. Then the Actions integration blocks events until that specified time, and only allows events beginning at that specified time.

The following code samples show you how you can create filters for your destinations using the 
Create Filter for Destination
 Public API operation.

Endpoint
: 
POST
 
https://api.segmentapis.com/destination/classic_destination_id_from_url/filters

// JSON BODY : 
{
  "sourceId": "add_source_id_here",
  "destinationId": "classic_destination_id_from_url",
  "title": "drop event after (timestamp) received_at > value April 4, 2023 19:55pm",
  "description": "drop event after (timestamp) received_at > value April 4, 2023 19:55pm",
  "if": "(received_at >= '2023-04-21T19:55:00.933Z')",
  "actions": [
    {
      "type":"DROP"
    }
  ],
  "enabled": true
}


Endpoint
: 
POST
 
https://api.segmentapis.com/destination/actions_destination_id_from_url/filters

// JSON BODY :
{
  "sourceId": "add_source_id_here",
  "destinationId": "actions_destination_id_from_url",
  "title": "drop event before (timestamp) received_at < value April 4, 2023 19:55pm",
  "description": "drop event before (timestamp) received_at < value April 4, 2023 19:55pm",
  "if": "(received_at < '2023-04-21T19:55:00.933Z')",
  "actions": [
    {
      "type":"DROP"
    }
  ],
  "enabled": true
}


After configuring the Destination Filter on both the Classic and Actions destination, see each destination’s Filters tab and enable the filters. After completing the migration, you can disable the Classic destination on the Settings page, and remove each of the filters from both destinations.

Edit a destination action

You can add or remove, disable and re-enable, and rename individual actions from the Actions tab on the destination’s information page in the Segment app. Click an individual action to edit it.

From the edit screen you can change the action’s name and mapping, and toggle it on or off. See 
Customizing mappings
 for more information.



When an Action is created, it’s disabled by default, to ensure that it’s only used after being fully configured. To begin sending data through an Action, enable it on the Actions page by selecting the toggle so that it appears blue.

Disable a destination action

If you find that you need to stop an action from running, but don’t want to delete it completely, you can click the action to select it, then click the toggle next to the action’s name to disable it. This takes effect within minutes, and disables the action until you reenable it.

Delete a destination action

To delete a destination action: click the action to select it, and click 
Delete
 (the trash can icon).

This takes effect within minutes, and removes the action completely. Any data that would have gone to the destination is not delivered. Once deleted, the saved action cannot be restored.

Test a destination action

To test a destination action, follow the instructions in 
Testing Connections
. You must enable a mapping in order to test the destination. Otherwise, this error occurs: 
You may not have any subscriptions that match this event.

You can also test within the mapping itself. To test the mapping:

Test Mapping might not return the events you're looking for

Segment only surfaces a small subset of events for the Test Mapping feature and might not always return the event you’re looking for. If you’d like to test with a specific event, copy a specific event from your 
Source Debugger
 and paste it into the 
Add test event
 interface.

Customize mappings

If you use the default mappings for a destination action, you don’t 
need
 to customize the mapping template for the action. You can edit the fields later if you find that the defaults no longer meet your needs.

Actions-based destinations have a limit of 50 individual mappings.

To create a custom destination action, start from the Actions tab.
If necessary, click 
New Mapping
 to create a new, blank action.

The required fields for a destination mapping appear automatically. Click the + sign to see optional fields.

Suggested mappings

Suggested mappings is fully available for RETL mappings, and is in public beta for event streams and connections.

Segment offers suggested mappings that automatically propose relevant destination fields for both model columns and payload elements. For example, if your model includes a column or payload field named 
transaction_amount
, the feature might suggest mapping it to a destination field like 
Amount
 or 
TransactionValue
. This automation, powered by intelligent autocompletion, matches and identifies near-matching field names to streamline the setup. For more information, see 
Segment’s suggested mappings blogpost
 and the 
Suggested Mappings Nutrition Label
.

Review the suggested mappings for accuracy before finalizing them as the suggestions aren’t guaranteed to be 100% accurate.

Coalesce function

The coalesce function takes a primary value and uses it if it is available. If the value isn’t available, the function uses the fallback value instead.

Replace function

The replace function allows you to replace a string, integer, or boolean with a new value. You have the option to replace up to two values within a single field.

Flatten function

The flatten function allows you to flatten a nested object to an object with a depth of 1. Keys are delimited by the configured separator. For example, an object like {a: { b: { c: 1 }, d: 2 } } will be converted to { ‘a.b.c’: 1, ‘a.d’: 2 }.

Conditions

Self-service users can add a maximum of two conditions per Trigger.

Mapping fields are case-sensitive. The following type filters and operators are available to help you build conditions:

Event property
 (
is
, 
is not
, 
less than
, 
less than or equal to
, 
greater than
, 
greater than or equal to
, 
contains
,  
does not contain
, 
starts with
, 
ends with
, 
exists
, 
does not exist
).  Use these filters to trigger the action only when an event with a specific property occurs.

You can specify nested properties using dot notation, for example 
context.app.name
. If the property might appear in more than one format or location, you can use an ANY statement and add conditions for each of those formats. For example, you might filter for both 
context.device.type = ios
  as well as 
context.os.name = "iPhone OS
"

  The 
does
 
not exist
 operator matches both a 
null
 value or a missing property.

The available operators depend on the property’s data type:

You can combine criteria in a single group using 
ALL
 or 
ANY
.  Use an ANY to “subscribe” to multiple conditions. Use ALL when you need to filter for very specific conditions. You can only create one group condition per destination action. You cannot created nested conditions.

Unsupported Special Characters

Mappings do not support the use of double quotes “ or a tilde ~ in the trigger fields. In mapping fields, the . character is not supported unless it’s being used to access an object key. If a string has a . in it, that is not supported.

Limitations

Mapping fields don’t support dot notation. For example, properties.amount.cost or properties_amount.cost aren’t supported.

Destination Filters

Destination filters are compatible with Destination Actions. Consider a Destination Filter when:

If your use case does not match these criteria, you might benefit from using Mapping-level Triggers to match only certain events.

Duplicate Mappings

You can use the Duplicate Mappings feature to create an exact copy of a mapping. The duplicated mapping has the same configurations and enrichments as your original mapping.

Duplicate Mappings supports 
Actions destinations
, 
Reverse ETL destinations
, and destinations connected to Engage 
Audiences
 and 
Journeys
.

To duplicate your mappings:

Segment creates a disabled mapping with the name “Original Mapping Name (Copy)”. You must enable the mapping for data to flow.

FAQ and troubleshooting

Validation error when using the Event Tester

When you send an event with an actions destination Event Tester that doesn’t match the trigger of any configured and enabled mappings, you’ll see an error message that states, 
You may not have any subscriptions that match this event.
 To resolve the error, create a mapping with a trigger to handle the event being tested, or update the test event’s payload to match the trigger of any existing mappings.

Data not sending downstream

If no mappings are enabled to trigger on an event that has been received from the connected source, the destination will not send any events. Ensure that at least one mapping has been configured and enabled in the destination mappings for an event that you would like to reach downstream.

Events without mappings enabled to handle them display as being discarded due to “No matching mapping” in a destination’s Delivery Overview.

Multiple mappings triggered by the same event

When the same event triggers multiple mappings, a request will be generated for each mapping that’s configured to trigger on an event. For example, for the 
Subscription Updated
 event, if two mappings are enabled and both have conditions defined to trigger on the 
Subscription Updated
 event, the two requests will be generated and sent to the destination for each 
Subscription Updated
 event.

Oauth “access token expired” message shown in Segment UI

Access Tokens that were generated from initial authorization, for example, when you connect a destination via Oauth, are always short-lived. Commonly, the token remains valid for 30 minutes to 1 hour. When Segment receives 401 error responses from the destination after a token has expired, it will automatically make another request to the destination for a new token and will then retry the event. Therefore, 401 responses are sometimes expected and do not indicate an event failure. There are three event flows when events are received and sent to a destination:

The underlying systems for these flows have their own copy of the token, which can expire at different points in time.
Threfore, if you see a 401 error in a sample response, it is likely that you’ll also see another request was made after it, to ask the downstream destination for a new token. Then one more request was made to actually send the data in your payload to the downstream destination.

Is it possible to map a field from one event to another?

Segment integrations process events through mappings individially. This means that no context is held that would allow you to map a value from one event to the field of a subsequent event. Each event itself must contain all of the data you’d like to send downstream in regards to it. For example, you cannot send 
email
 in on an Identify call and then access that same 
email
 field on a Track call that comes in later if that Track call doesn’t also have 
email
 set on it.

I’m getting a ‘Couldn’t load page’ error when viewing or editing a mapping

This issue can occur due to a browser cache conflict or if an event property name includes a 
/
. To resolve it, try clearing your browser cache or accessing the mapping page in an incognito window. Additionally, check if the mapped property name contains a 
/
. If it does, rename the property to remove the 
/
 and update the mapping.

This page was last modified: 09 Jan 2025

Need support?

Questions? Problems? Need more info? Contact Segment Support for assistance!

Help improve these docs!

Was this page helpful?

Thanks for your feedback!

Can we improve this doc? 
Send us feedback!

Get started with Segment

On this page

Was this page helpful?

Thanks for your feedback!

Can we improve
 this doc? 
Send us feedback!

Product

For Developers

Company

Support

© 2025 Segment.io, Inc.

=== Content from https://segment.com/docs/connections/destinations/add-destination/ ===

Sending Segment Data to Destinations
        

On this page

You’ve decided how to format your data, and collected it using 
Segment Sources
. Now what do you do with it? You send the data to Destinations.

Destinations are tools or services which can use the data sent from Segment to power analytics, marketing, customer outreach, and more.

Each Segment Workspace has its own set of destinations, which are connected to the workspace’s sources. When you add or modify a destination, make sure you’re working with the correct workspace.

Healthcare and Life Sciences (HLS) customers can encrypt data flowing into their destinations

HLS customers with a HIPAA eligible workspace can encrypt data in fields marked as Yellow in the Privacy Portal before they flow into an event stream, cloud-mode destination.

To learn more about data encryption, see the 
HIPAA Eligible Segment documentation
.

Adding a destination

There are two ways to add a destination to your deployment: using the Segment web app, or using the 
Public API
.

Some third-party tools (such as Customer.io, Leanplum, and Airship) can both consume Segment data (as destinations), and send their data to Segment Warehouses as 
Cloud-Sources
. When you add a destination, make sure you’re viewing the Destinations tab in the catalog so you add the correct one.

If you have more than one instance of the same destination, you can click 
Copy Settings From Other Destination
 to save yourself time entering the settings values manually.

You can also add a destination directly from the source’s settings page in the Segment web app.

You can use the Segment Public API to add destinations to your workspace using the 
Create Destination endpoint
. The API requires an authorization token, and uses the 
name
 field as a namespace that defines which 
source
 the destination is connected to. You send the rest of the destination’s configuration as a JSON blob. View the documentation page in the Segment Catalog, or query the 
Segment Catalog API
, for a destination to see the available settings.

You must use an authorization token to access the Public API, and these tokens are tied to specific workspaces. If you use the Public API to access multiple workspaces, make sure you’re using the token for the workspace you want to access before proceeding.

What happens when you add a destination

Adding a destination can have a few different effects, depending on which sources you set up to collect your data, and how you configured them.

Analytics.js

If you are using 
Segment’s JavaScript library, Analytics.js
, then Segment handles any configuration changes you need for you. If you’re using Analytics.js in cloud-mode, the library sends its tracking data to the Segment servers, which route it to your destinations. When you change which destinations you send data to, the Segment servers automatically add that destination to the distribution list.

If you’re using Analytics.js in device-mode, then Analytics.js serves as a wrapper around additional code used by the individual destinations to run on the user’s device. When you add a destination, the Segment servers update a list of destinations that the library queries. When a user next loads your site, Analytics.js checks the list of destinations to load code for, and adds the new destination’s code to what it loads. It can take up to 30 minutes for the list to update, due to CDN caching.

You can enable device-mode for some destinations from the destination’s Settings page in the Segment web app. You don’t need to use the same mode for all destinations in a workspace; some can use device-mode, and some can use cloud-mode.

Mobile sources

By default, Segment’s 
mobile sources
 send data to Segment in cloud-mode to help minimize the size of your apps. In cloud-mode the mobile source libraries forward the tracking data to the Segment servers, which route the data to the destinations. Since the Segment servers know which destinations you’re using, you don’t need to take any action to add destinations to mobile apps using cloud-mode.

However, if the destination you’re adding has features that run on the user’s device, you might need to update the app to package that destination’s SDK with the library. Some destinations require that you package the SDK, and some only offer it

Server sources

Segment’s 
server sources
 run on your internal app code, and never have access to the user’s device. They run in cloud-mode only, and forward their tracking calls to the Segment servers, which forward the data to any destinations you enabled.

Destination authentication

When you add a destination in Segment, you must tell Segment how to connect with that destination’s app or endpoints. Most destinations offer an API token or authentication code which you can get from their web app. The documentation for each Segment destination includes information about what you need, and how to find it. Copy this information, and paste it into the Settings for the destination, or include it in the 
create API call
.

Destination settings

Each destination can also have destination settings. These control how Segment transforms the data you send to the destination, and can be used to adapt it to your configuration or enable or disable certain destination features.

Connecting one source to multiple instances of a destination

Multiple-destination support is available for all Segment customers on all plan tiers.

Segment allows you to connect a source to multiple instances of a destination. You can use this to set up a single Segment source that sends data into different instances of your analytics and other tools.

For example, you might set up a single Segment source to send data both to separate instances of Google Analytics for each business unit in your organization, and to another instance for executive-level reporting. You could also use this to make sure that tooling instances for different geographic teams are populated with the same data from the same source.

You can also connect multiple instances of a destination to help you smoothly migrate from one configuration to another. By sending each version the same data, you can check and validate the new configuration without interrupting use of the old one.

However, there are a few considerations:

Device-mode destinations do not support connecting multiple instances of the destination to the same source. If you try to a connect an additional instance of a device-mode destination to your source, the option to add a second instance does not appear.

Mobile sources, and the legacy Project source, can connect to multiple instances of destinations that operate only in cloud-mode. Mobile and Project sources cannot connect to multiple instances of destinations that operate in both cloud-mode and device-mode. Non-mobile sources can only connect to 
one
 device-mode instance of a destination.

Multi-instance support is not available for most hybrid Actions destinations or Web mode Actions destinations.

Segment does not support connecting a single source to multiple instances of a 
Data Lakes
 destination.

Non-mobile sources can only connect to _one_ device-mode instance of a destination

You cannot connect a source to more than one instance of a destination that operates only in device-mode. For more information about device-mode restrictions, see the 
Sending Segment data to Destinations
 documentation.

If your organization is on a Segment Business tier plan, you can use 
Replay
 to send historical data to new instances of a destination.

Connect a source to more than one instance of a destination

To connect a source to more than one instance of a destination in the Segment web app, start by adding the first instance of the destination and giving it a unique name, 
as described above
. To add another instance of the destination, follow either of those two methods and choose another unique name.

You must give each instance of the destination connected to the same source a unique name. Segment recommends that you use descriptive names rather than numbers, so other Segment users can understand which Segment destinations are linked to which tool instances. For example, you might use “Amplitude North America” and “Amplitude South America”, instead of “Amplitude 1” and “Amplitude 2”. You can edit the destination instance name at any time.

If you added the first instance of your destination before multi-instance destinations became available, that instance is automatically named for the destination with no other identifiers, for example “Amplitude”.

Some destinations do not support having multiple instances connected to the same source. In that case, the option to add a second instance of that destination does not appear.

You can create unique destination filters for each destination instance connected to the same source.

Some destinations don’t support multiple instances connected to the same source. If this is the case, you won’t see the option to add a second instance of that destination.

Connect multiple sources to one instance of a destination

It is not possible to connect multiple instances of one source (for example, two website sources) to the same destination. However, you can create another instance of the destination for the other sources, and click 
Copy Settings From Other Destination
  to save yourself time entering the settings values again.

Connect to more than one instance of a destination using the Public API

You can add multiple instances of a destination using the Segment Public API. See the Segment Config 
API documentation
. If a destination does not support multi-instance, the Public API throws an appropriate error.

Multi-instance destinations and Device-mode

Other multi-instance destination considerations

Multiple Data Lakes:
 Segment does not currently support connecting a single source to multiple instances of a data lakes destination. 
Contact Segment Customer Success
 if this would be useful for your organization.

Protocols transformations and multi-instance support:
 Protocols transformations are specific to each source, and operate the same on all instances of a specific destination. Segment does not currently support creating unique protocols transformations for each instance of a destination.

Integrations object considerations:
 A common part of a Segment message is 
the integrations object
, which you can use to explicitly filter to which destinations the call is forwarded, as well as to specify options for different destination tools. If you use the integrations object to filter events or to send destination-specific options, Segment applies its values to all instances. For example:

{

  
"
integrations
"
:
 
{

    
"
Mixpanel
"
:
 
false
,

    
"
Adobe Analytics
"
:
 
{

      
"
marketingCloudVisitorId
"
:
 
"
12345
"

    
}

  
}


}



In this example:

This page was last modified: 17 Jan 2025

Need support?

Questions? Problems? Need more info? Contact Segment Support for assistance!

Help improve these docs!

Was this page helpful?

Thanks for your feedback!

Can we improve this doc? 
Send us feedback!

Get started with Segment

On this page

Was this page helpful?

Thanks for your feedback!

Can we improve
 this doc? 
Send us feedback!

Product

For Developers

Company

Support

© 2025 Segment.io, Inc.

=== Content from https://segment.com/docs/connections/test-connections/ ===

Event Tester
        

On this page

Segment has an Event Tester that enables you to test your connections between Segment and your destination. You can access the Event Tester from your Source Debugger, or from your destination settings.   

Available for server-side event streaming destinations only

This feature is only available for server-side integrations (also known as cloud-mode destinations). You can’t use this for client-side integrations (also known as device-mode destinations).

Use Cases

There are two scenarios where you might want to use the Event Tester:

Ensuring an event is successfully making it to a specific destination

1. Choose an event from the Source Debugger that you want to debug and select “Validate”

Go to your Source Debugger, select an event and in the top right hand side of the debugger view, select “Validate”.



2. Choose the destination you want to test with

Select the destination that you want to test this event with. At this time, you can only use the Event Tester for cloud-mode (server side) destinations.



3. Send event to destination

The event payload from your debugger that you just selected will automatically load in the JSON view. You have the option to edit the payload if you want. Assuming it looks good, select “Send Event” at the bottom right of the screen. 



4. Ensure you’re happy to send the test event to the destination

This is a real event that will appear in your end tool alongside your existing data. If you’re not comfortable with this, then select “Cancel” and do not send the event. 



5. View the Partner API response

On the right hand side of the Event Tester you will see the response from the partner API. At the top, Segment provide of summary of the response. Below is the raw response payload Segment received that you can use for further debugging if necessary. 



If you are receiving an error and are unsure how to fix the issue, visit the partner docs (for example 
https://developers.google.com/analytics/devguides/reporting/core/v3/errors
) or contact the partner support team. 

FAQ

The Event Tester is only accessible to users with write access in their Segment workspace (read-only users will not see the Event Tester in their workspace). 

If you experience an error, 
let Segment know
 and the Segment team will help you troubleshoot the issue.

The Event Tester is not available for Data Lakes.

Events passed into the Event Tester bypass destination filters. Destination filters are applied to events as they are sent to specific destinations. However, the Event Tester is designed to help you troubleshoot your Sources, their configuration, and their downstream destinations by showing a sample of the data available. It allows you to check that data is being sent, and that it’s in the correct format without the filters being applied. This means that when you use the Event Tester, you’re seeing the data before any destination filters or other processing rules are applied, providing a clear view of the raw event data as it comes from the source.

This page was last modified: 13 Jun 2024

Need support?

Questions? Problems? Need more info? Contact Segment Support for assistance!

Help improve these docs!

Was this page helpful?

Thanks for your feedback!

Can we improve this doc? 
Send us feedback!

Get started with Segment

On this page

Was this page helpful?

Thanks for your feedback!

Can we improve
 this doc? 
Send us feedback!

Product

For Developers

Company

Support

© 2025 Segment.io, Inc.

=== Content from https://segment.com/docs/guides/segment-vs-tag-managers/ ===

Segment vs. Tag Managers
        

Tag managers, also known as Tag Management Systems (TMS), were a popular solution before the mainstream adoption of mobile apps. They primarily helped Digital Analytics and Online Marketers manage web tags or “beacons” on a website.

Built on an older technology, tag managers inject either a piece of JavaScript or an ad pixel into a website. They carry out rules that marketers create for each tag, like firing an ad channel pixel when that network refers a website visitor. Every tag requires users to create rules. No data is stored, and no code is eliminated.

In addition to ad networks, today’s data-driven businesses use a variety of tools to optimize their product and marketing spends. In order to a/b test copy, nurture sales leads, email customers, and provide fast support, businesses integrate variety of analytics and marketing tools. Segment makes it easy to install, try, and use them all. Tag managers primarily focus on ad networks, and can’t support modern tools without extensive customization.

Rather than “firing and forgetting,” Segment takes a data-centric, deliberate approach to destinations. You don’t need to set up special parameters for each tool – Segment does that for you. Segment structures your data so we can understand what it is, and can translate it correctly for each destination we send it to.  Segment works because 
all of these tools
 operate on the same customer data: who is on your app and what are they doing. Segment collects this data once, then translates and sends it to every tool you use. Because Segment also archives the data, Segment can 
replay your historical data
 into new tools, and send your raw data to a 
data storage solution
 for later analysis.

Every organization’s data stack and business requirements are unique. Segment also works well in tandem with a tag manager. For example, Segment sends data directly to the 
Google Tag Manager (GTM) destination
.

While you can use Segment’s Analytics.js library through a tag manager, Segment doesn’t recommended this for a few reasons:

A hybrid approach makes it difficult to determine the root cause of technical problems, and complicates troubleshooting. Segment cannot guarantee destination compatibility in a “hybrid” Segment-tag-manager installation, and cannot guarantee support on these installations. All QA and regression testing assumes a native installation of Analytics.js on the page.

One of Segment’s main charters is to not lose data. Our system and cloud infrastructure is designed to ensure that data loss does not happen. If you implement the entry point of data capture (Segment’s libraries) using a Tag Manager, you introduce risk of data loss and make it difficult or impossible to troubleshoot.

This implementation behind a tag manager can introduce major delays and performance issues, which can cause delays with events that need to occur early in your funnel.

The biggest challenge is around triggering cascading events. Browsers are notorious for dropping calls. When you use a TMS to initiate Segment events you are introducing a second point of failure for those events.

This page was last modified: 16 Feb 2023

Need support?

Questions? Problems? Need more info? Contact Segment Support for assistance!

Help improve these docs!

Was this page helpful?

Thanks for your feedback!

Can we improve this doc? 
Send us feedback!

Get started with Segment

Was this page helpful?

Thanks for your feedback!

Can we improve
 this doc? 
Send us feedback!

Product

For Developers

Company

Support

© 2025 Segment.io, Inc.

=== Content from https://segment.com/docs/connections/functions/usage/ ===

Functions usage limits
        

On this page

Functions are billed to your account using the total execution time per month.

An 
individual function’s execution time
 is the total time it takes for the function to process events, including mapping, transformations, and requests to external APIs. Generally, requests to external APIs can greatly add to your total execution time.

Your 
total execution time
 is the execution time for all of your active functions accumulated over the course of a month. You can see your current execution time on the 
Functions tab of the Usage page
 in each workspace. You will receive notifications of your usage when you’ve reached 75%, 90%, and 100% of your allotted execution time.

The amount of time you are allotted changes depending on your 
Segment pricing plan
.

Measuring execution time

Segment measures execution time from when the function first receives an event to the time the function either returns successfully or throws an error. If Segment retries your function (for example, if there was a timeout), those retries also count as billable execution time.

Starting on April 8, 2021 Functions usage is measured in millisecond increments. This makes your usage and billing much more precise. Prior to this change, Functions was measured in 100ms increments, and then rounded up. For example, a function that took 80ms to complete was previously billed as 100ms. Using the new usage calculation, it is billed as 80ms.

Execution timeouts

Functions have a timeout of five seconds. If a function takes longer than five seconds, execution halts and the function is retried periodically for up to four hours.

Estimating execution time

Execution time can vary widely between use cases, so it is extremely difficult to predict. The best way is to look at the function’s actual execution time and multiply it by the event volume.

Another way to provide a rough estimate is to use an expected source function time of 100ms per invocation, and expected destination function time at 200ms per invocation:

Test runs are generally slower than the time it takes a function to run once it's deployed

For more accurate estimates, base your estimates on sending data into a production function, and not on timing the test runs.

You can (and should!) use 
Destination Filters
 to reduce the volume of events reaching your function. Filtering events with a Destination Filter prevents the Function from being invoked for that event entirely.

Improving speed of external requests

In the most cases, functions are slow due to external requests using the 
fetch()
 call. The external API may be under heavy load or it may simply take a long time to process your request.

If you’re making many requests that could be done in parallel, ensure that you’re not doing them sequentially. If the external API takes 400ms to respond and you issue 10 requests, it would take four seconds to do them sequentially versus 400ms to do them in parallel. For example, if you’re waiting for requests to complete inside of a loop you’re making your requests sequentially:

for 
(
const
 
objectId
 
of
 
event
.
properties
.
objects
)
 
{

   
const
 
response
 
=
 
await
 
fetch
(
'
https://example.com/?id=
'
 
+
 
objectId
,
 
{

       
method
:
 
'
POST
'
,

       
body
:
 
event
.
properties

   
})


   
console
.
log
(
response
.
json
())


}



Instead, consider making an array of async requests that are running in parallel and then using 
Promise.all()
 to wait for all of them to complete:

const
 
requests
 
=
 
event
.
properties
.
objects
.
map
(
objectId
 
=>
 
{

    
return
 
fetch
(
'
https://example.com/?id=
'
 
+
 
objectId
,
 
{

        
body
:
 
event
.
properties

    
})


})



const
 
responses
 
=
 
await
 
Promise
.
all
(
requests
)


for 
(
const
 
response
 
of
 
responses
)
 
{

    
console
.
log
(
response
.
json
())


}



If you’re only issuing a single request in your function and it is slow, you might want to contact the owner of the external API for support.

Default limit number

Each workspace has a default limit of 25 Functions in total across Source, Insert, and Destination Functions. If you want to create more, please reach out to 
Segment
.

This page was last modified: 17 Jan 2025

Need support?

Questions? Problems? Need more info? Contact Segment Support for assistance!

Help improve these docs!

Was this page helpful?

Thanks for your feedback!

Can we improve this doc? 
Send us feedback!

Get started with Segment

On this page

Was this page helpful?

Thanks for your feedback!

Can we improve
 this doc? 
Send us feedback!

Product

For Developers

Company

Support

© 2025 Segment.io, Inc.

=== Content from https://segment.com/docs/segment-app/iam/roles/ ===

Roles
        

On this page

A role gives a user access to resources within a workspace. Roles are additive, and can combine to configure a custom policy for a Team Member or a Group. A policy is at least one role plus one resource applied to an individual user or group.

When a user has both User Permissions and Group Permissions, they will have the highest access given to either of those roles.

Global Roles

All Segment workspaces have the following roles, regardless of account type.

Business Tier Roles

The following roles are only available to Segment Business Tier accounts.

Full edit and view access to all entity models and connection details.

Read-only access, with the ability to view entity models.

PII Access

The Segment App doesn’t show detected Personally Identifiable Information (PII) to workspace members if the information matches specific expected formats for PII. When PII Access turns 
off
, detected PII is masked based on 
red or yellow default matchers
 and any 
custom matchers
 defined in the Privacy Portal.

Workspace Owners can grant specific individuals or groups access to PII from their Access Management settings. PII Access only applies to the resources a user or user group has access to; it doesn’t expand a user’s access beyond the original scope. All Workspace Owners have PII access by default.

For example, users with PII Access and Source Admin/Read-Only permissions can view any PII present in the Source Debugger. However, users with the PII Access role don’t have Privacy Portal access.

Only users with the Workspace Owner role can access the Privacy Portal.

Roles for managing Engage destinations

When managing destination connections in an Engage space, you may require additional permissions.

Roles for connecting resources

To connect two resource instances, you must have access to both. You can either grant this access to all resources, or to the specific resources you want to connect.

To connect a source to warehouse
 you must have 
Source Admin
 and 
Warehouse Admin
 access for the source and the warehouse.

To connect source to tracking plan
 requires 
Source Admin
 and 
Tracking Plan Admin
 access for the source and the tracking plan.

Roles for Protocols Transformations

To 
view
 transformations, you need 
Source Read-only
, either for all Sources or the specific Sources using Protocols.

To 
create or edit
 transformations you must have either 
Source Admin
 for all Sources, or for the specific Sources used with Protocols.

Roles for Privacy Portal

The Privacy Portal is only accessible by 
Workspace owners
. To 
view, create or edit
 any section of the Privacy Portal, you need to have the 
Workspace Owner
 role.

This page was last modified: 13 Nov 2024

Need support?

Questions? Problems? Need more info? Contact Segment Support for assistance!

Help improve these docs!

Was this page helpful?

Thanks for your feedback!

Can we improve this doc? 
Send us feedback!

Get started with Segment

On this page

Was this page helpful?

Thanks for your feedback!

Can we improve
 this doc? 
Send us feedback!

Product

For Developers

Company

Support

© 2025 Segment.io, Inc.

=== Content from https://segment.com/docs/guides/duplicate-data/ ===

Handling Duplicate Data
        

On this page

Segment guarantees that 99% of your data won’t have duplicates within an approximately 24 hour look-back window. Warehouses and Data Lakes also have their own secondary deduplication process to ensure you store clean data.

99% deduplication

Segment has a special deduplication service that sits behind the 
api.segment.com
 endpoint and attempts to drop 99% of duplicate data. Segment stores at least 24 hours’ worth of event 
messageId
s, which allows Segment to deduplicate any data that appears with the same 
messageId
 within the stored values.

Segment deduplicates on the event’s 
messageId
, 
not
 on the contents of the event payload. Segment doesn’t have a built-in way to deduplicate data for events that don’t generate 
messageId
s. The message de-duplication is not scoped to a specific source or a workspace, and applies to all events being received by Segment.

Keep in mind that Segment’s libraries all generate 
messageId
s for each event payload, with the exception of the Segment HTTP API, which assigns each event a unique 
messageId
 when the message is ingested. You can override these default generated IDs and manually assign a 
messageId
 if necessary. The 
messageId
 field is limited to 100 characters.

Warehouse deduplication

Duplicate events that are more than 24 hours apart from one another deduplicate in the Warehouse. Segment deduplicates messages going into a Warehouse (
including Profiles Sync data
) based on the 
messageId
, which is the 
id
 column in a Segment Warehouse.

Data Lake deduplication

To ensure clean data in your Data Lake, Segment removes duplicate events at the time your Data Lake ingests data. The Data Lake deduplication process dedupes the data the Data Lake syncs within the last 7 days with Segment deduping the data based on the 
messageId
.

This page was last modified: 02 Aug 2024

Need support?

Questions? Problems? Need more info? Contact Segment Support for assistance!

Help improve these docs!

Was this page helpful?

Thanks for your feedback!

Can we improve this doc? 
Send us feedback!

Get started with Segment

On this page

Was this page helpful?

Thanks for your feedback!

Can we improve
 this doc? 
Send us feedback!

Product

For Developers

Company

Support

© 2025 Segment.io, Inc.

=== Content from https://segment.com/docs/unify/traits/predictions/suggested-predictive-audiences/ ===

Suggested Predictive Audiences
        

Unify Plus requires a business tier account and is included with Engage


See the 
available plans
, or 
contact Support
.

On this page

Suggested Predictive Audiences can help you improve customer engagement, drive higher conversion rates, and reduce ad spend.

This page explains what a Suggested Predictive Audience is, how to build a Suggested Predictive Audience, and what each available Audience targets.

Suggested Predictive Audience basics

A Suggested Predictive Audience is an out-of-the-box Audience template driven by machine learning.

Segment offers 
five templates
 that are prebuilt with 
Predictions
 like likelihood to purchase and lifetime predicted value. Selecting a template generates a Predictive Audience whose members you can engage in a number of ways:

Build a Suggested Predictive Audience

Follow these steps to build a Suggested Predictive Audience:

Your Suggested Predictive Audience is now live.

Suggested Predictive Audience types

Engage offers five Suggested Predictive Audiences. The following table summarizes the customers each Audience targets and the events and traits Engage uses to build the Audience:

Audience descriptions

Choose a 
Ready to buy
 Predictive Audience to target customers who show a high propensity to make a purchase.

Segment builds this Audience with the 
Likelihood to Purchase prediction
. Audience members show encouraging engagement and have a likelihood to buy in the top 20th percentile.

Choose a 
Long shot
 Predictive Audience to target customers who have made a purchase but have a middling likelihood to buy.

Segment builds this Audience with the 
Order Completed
 event and 
Likelihood to Purchase
 trait. Audience members have completed a purchase but currently have a likelihood to buy somewhere between the 25th and 65th percentile.

Choose a 
High lifetime value
 Predictive Audience to target customers that show a high predicted lifetime value.

Segment builds this Audience with the 
Predicted LTV prediction
. Audience members are in the top 10th percentile of predicted lifetime value and Segment expects that they’ll spend the most over the next 90 days.

Choose a 
Potential VIPs
 Predictive Audience to target customers exhibiting several promising marketing behaviors.

Segment builds this Audience with the 
Page Viewed
 event and Likelihood to Purchase and Predicted LTV prediction. Audience members have been active on your site within the last two weeks, have a high predicted lifetime value, and a high propensity to purchase.

Choose a 
Dormant
 Predictive Audience to target inactive customers.

Segment builds this Audience with the 
Page Viewed
 event and the Likelihood to Purchase Predictive Trait. Audience members have a low likelihood to purchase and haven’t been active on your site in the last 60 days.

This page was last modified: 23 Aug 2023

Need support?

Questions? Problems? Need more info? Contact Segment Support for assistance!

Help improve these docs!

Was this page helpful?

Thanks for your feedback!

Can we improve this doc? 
Send us feedback!

Get started with Segment

On this page

Was this page helpful?

Thanks for your feedback!

Can we improve
 this doc? 
Send us feedback!

Product

For Developers

Company

Support

© 2025 Segment.io, Inc.

=== Content from https://github.com/segmentio/segment-docs/issues/new/choose?body=File:%20[index.md](https://segment.com/docs/) ===

Sign in to GitHub

Password login alternatives



 


Sign in with a passkey








            New to GitHub?
              
Create an account

=== Content from https://segment.com/docs/guides/intro-admin/ ===

Segment for Workspace Administrators
        

On this page

If your job is to set up or maintain a Segment Workspace for your organization, or assist other people using the Segment Web App, this guide is for you. If you’re more interested in Segment implementation details, see the 
developer intro guide
.

What is Segment?

If you’ve already read 
an Introduction to Segment
, you can skip ahead.

Segment is a system for sending messages from your websites, mobile apps, and servers. These messages contain event and user data that you can send to other tools or collect in warehouses for further analysis. Segment also gathers information about your users from external systems, like help desk software or CRMs. You can use this collated information to analyze data, build user audiences, and personalize your users’ experiences.

What’s a Workspace?

A workspace is a group of sources that can be administered and billed together. Workspaces help companies manage access for multiple users and data sources. Workspaces let you collaborate with team members, add permissions, and share sources across your whole team using a shared billing account.

When you first log in to your Segment account, you can create a new workspace, or choose to log into an existing workspace if your account is part of an existing organization.

The Workspace Administrator’s Role

You don’t have to be a developer to be a Workspace administrator for an organization, and this guide only covers tasks specifically related to managing a Workspace in the 
Segment App
.

However, many Workspace admins are also involved in the Segment implementation process as there are usually some tasks that must be performed in the Workspace to complete an implementation. If you think you might develop a Segment implementation or help out other developers, first read 
Segment for developers
.

In addition, Workspace administrators set up and maintain the organization’s 
workspace settings
, which include:

Changing a workspace name and slug won’t impact configured sources or destinations, which connect using an internal ID and 
writeKey
.

Workspace administrators might also maintain:

Tasks in Connections

As an administrator, you might be asked to help other members of your organization with tasks related to setting up and troubleshooting your Segment implementation.

Setting up destinations

Destinations are the endpoints to which Segment sends data flowing from your Sources. Destinations can be third-party external tools, like Google Analytics or Mixpanel, or bulk-storage resources, like warehouses.

You can set up a Destination from within the Segment App by navigating to the 
Destination Catalog
 and selecting the tool you want to set up. In most cases, you’ll need an existing API key or token so that Segment can send the data to the correct account. If you’re setting up a Warehouse or other storage destination, more steps might be required; see the 
Warehouses documentation
 for more details.

Troubleshooting

Use these Segment features to keep tabs on your Workspace:

Still stumped? 
Contact support
 for more help troubleshooting.

Have suggestions for this guide? 
Reach out with your feedback
.

This page was last modified: 07 Feb 2025

Need support?

Questions? Problems? Need more info? Contact Segment Support for assistance!

Help improve these docs!

Was this page helpful?

Thanks for your feedback!

Can we improve this doc? 
Send us feedback!

Get started with Segment

On this page

Was this page helpful?

Thanks for your feedback!

Can we improve
 this doc? 
Send us feedback!

Product

For Developers

Company

Support

© 2025 Segment.io, Inc.

=== Content from https://segment.com/docs/getting-started/use-cases/guide// ===

Choosing a Use Case
        

On this page

Segment built Use Cases to streamline the process of implementing Segment for specific business objectives.

This guide will help you navigate through the available use cases and select the one that best aligns with your business goals.

You can onboard to Segment with a Use Case if you’re a new Business Tier customer or haven’t yet connected a source and destination.

Understanding business goals and use cases

Segment supports 25 use cases, organized into 4 main business goals:

These goals represent key ways businesses often use customer data for improved performance and growth.

Looking for a technical breakdown of each use case? View the 
Use Cases Reference
.

Selecting your use case

Follow these steps to identify which use case to implement:

The use case you select will guide your Segment setup, including the events you’ll track and the integrations you’ll implement. However, Segment’s flexibility allows you to adapt and expand your strategy over time as your business needs evolve.

The following sections explore each business goal and associated use cases in detail.

Optimize advertising

The 
Optimize advertising
 business goal focuses on improving the efficiency and effectiveness of your advertising efforts. By using your customer data effectively, you can create more targeted campaigns, reduce wasted ad spend, and increase your return on investment (ROI).

Key considerations for this goal:

Use cases in this category include:

Personalize first conversion

The 
Personalize first conversion
 goal focuses on optimizing the initial interactions a potential customer has with your brand. By personalizing these early touchpoints, you can increase the likelihood of converting prospects into customers.

Key considerations for this goal:

Use cases in this category include:

Boost retention, upsell, and cross-sell

The 
Boost retention, upsell, and cross-sell
 business goal focuses on maximizing the value of your existing customer base. By analyzing customer behavior and preferences, you can create targeted strategies to encourage repeat purchases, introduce customers to higher-value products or services, and increase overall customer lifetime value.

Key considerations for this goal:

Use cases in this category include:

Personalize communications and product experiences

The 
Personalize communications and product experiences
 business goal focuses on creating tailored experiences for your customers across all touchpoints. With this business goal, you can create more relevant and engaging communications and product experiences, leading to increased satisfaction and loyalty.

Key considerations for this goal:

Use cases in this category include:

Next steps

Once you’ve selected a use case, follow the 
Use Cases Setup Guide
, which explains how to set up a use case.

This page was last modified: 08 Oct 2024

Need support?

Questions? Problems? Need more info? Contact Segment Support for assistance!

Help improve these docs!

Was this page helpful?

Thanks for your feedback!

Can we improve this doc? 
Send us feedback!

Get started with Segment

On this page

Was this page helpful?

Thanks for your feedback!

Can we improve
 this doc? 
Send us feedback!

Product

For Developers

Company

Support

© 2025 Segment.io, Inc.

=== Content from https://segment.com/docs/connections/regional-segment/ ===

Redirecting…

=== Content from https://segment.com/docs/engage/user-subscriptions/ ===

User Subscriptions Overview
        

Engage Premier requires a Business tier account and includes Engage Foundations and Unify.


See the 
available plans
, or 
contact Support
.

On this page

Engage Premier entered an End of Sale (EOS) period effective  June 10, 2024. Existing Segment customers will continue to have access and support to Engage Premier until an end-of-life (EOL) date is announced. We recommend exploring the following pages in preparation of a migration or future MCM needs:

Twilio Marketing Campaigns

Preferred ISV Partners:

Airship Blog
 


Bloomreach Blog
 


Braze Blog
 


Insider Blog
 


Klaviyo Blog
 


Twilio Engage Foundations Documentation
 

Segment associates 
subscription states
 with each email address and phone number 
external id
 in your audiences. Subscription states indicate the level of consent end users have given to receive your marketing campaigns.

Knowledge of subscription states will help you understand which users can and cannot receive your campaigns. This page provides an overview of user subscriptions.

The Four Subscription states

Email addresses and phone numbers in your audience have one of the four following user subscription states:

It’s best practice to only send Engage campaigns to users with a 
subscribed
 status. However, if you need to send an email to someone who hasn’t subscribed, you can create an email campaign that you 
send to all users
.

To learn how Segment determines user subscription states, read the 
User Subscription State documentation
.

Setting User Subscriptions

You can set user subscriptions manually when you upload contacts 
using Engage’s CSV uploader
. You can also use a CSV upload to correct contacts with outdated subscription states.

Most user subscriptions update programmatically, using Segment APIs. The Public API and the Identify call handle subscription state changes made when users sign up to or change their subscription status to your marketing materials with online forms or within notification centers.

View the 
Setting User Subscriptions
 page to learn more about user subscription changes.

This page was last modified: 15 Jul 2024

Need support?

Questions? Problems? Need more info? Contact Segment Support for assistance!

Help improve these docs!

Was this page helpful?

Thanks for your feedback!

Can we improve this doc? 
Send us feedback!

Get started with Segment

On this page

Was this page helpful?

Thanks for your feedback!

Can we improve
 this doc? 
Send us feedback!

Product

For Developers

Company

Support

© 2025 Segment.io, Inc.

=== Content from https://segment.com/docs/connections/spec/mobile-packaging-sdks/ ===

Packaging SDKs for Mobile Destinations
        

When it comes to Mobile SDKs, we know that minimizing size and complexity is a priority for our customers. That’s why our core Mobile SDKs are small and offload as much work as possible in handling destinations to our servers. When you install our light-weight SDK, you have access to our entire suite of server-side destinations.

Why do some destinations require bundling their SDKs?

We bundle certain SDKs, instead of just proxying your data to them through our servers, so that you have access to their deeper features that requires direct client manipulation (A/B testing, user surveys, touch heatmapping, etc) or access to rich data such as CPU usage, network data, or raised exceptions. For those types of features, we still need to bundle their native SDK for you so you can make the most of them.

We’ve worked hard to make our mobile SDKs as modular as possible so that you only need to include the SDKs for tools you plan to use.

Custom builds allow us to offer the native functionality of all of our destinations without having to include hefty third-party SDKs by default. This gives you control over size and method bloat. Check out how to use custom builds for both 
Android
 and 
iOS
.

Which SDKs are bundled?

To check if a destination is bundled or not, take a look at our 
documentation
 for that specific destination.

This page was last modified: 14 Jul 2021

Need support?

Questions? Problems? Need more info? Contact Segment Support for assistance!

Help improve these docs!

Was this page helpful?

Thanks for your feedback!

Can we improve this doc? 
Send us feedback!

Get started with Segment

Was this page helpful?

Thanks for your feedback!

Can we improve
 this doc? 
Send us feedback!

Product

For Developers

Company

Support

© 2025 Segment.io, Inc.

=== Content from https://segment.com/docs/unify/insights/ ===

Profiles Insights
        

Unify requires a Business tier account and is included with Engage.


See the 
available plans
, or 
contact Support
.

On this page

With Profiles Insights, you can troubleshoot event data with transparent insight into your 
Unify profiles
. View errors and violations, success logs, and an audit trail for events that flow into your profiles. You can also learn why certain issues occur, and take preventative action against future errors.

Getting started

To get started with Profiles Insights, navigate to 
Unify
 > 
Profiles insights
.

From the Profiles Insights page, you can navigate to these tabs:

Errors and violations

Use the errors and violations tab to view and help you troubleshoot any errors or violations that have occurred in your space.

You can filter results by ID type, time range, specific violations, and more.

To filter by 
external_id
, you must enter an 
external_id
 value in the search bar.



Errors

Errors occur when a message didn’t resolve to a profile because Segment didn’t find a matching identifier or the system behaved unexpectedly. Click on an error log to view the error and next steps that Segment recommends.

Profiles Insights flags the following error:

Profile Insights won’t surface errors if the event was dropped before entering Unify’s 
Identity Resolution
. Learn more about 
Unify ingestion limits
.

Violations

Violations occur when incoming events don’t comply with your 
Identity Resolution Settings
. For example, when Segment drops an anonymous ID (lower priority) to resolve an event based on a matching user ID (higher priority), it results in a violation.

Learn about identifier priorities
 in your Identity Resolution.

For any violations, Segment may drop lower priority identifiers or the identifiers that violate your Identity Resolution Settings. From the grid, you can click a log name to view the violation details and recommended next steps.

You can use the 
Message Payload
 tab to view raw messages for Track events and see exactly where the violation occurred.

Profiles Insights flags the following violations:

Success logs

Success logs provide visibility into a profile’s journey from creation to the point of merging into other profiles.

Use the success logs to view:

You can filter results by ID type, time range, incoming event type, and more.

When you click a specific log, Segment displays merge or mapping details along with the message payload for Track events.

Audit trail

The Audit trail displays all audit actions that occur in your Unify space. This can include, for example, a user creating an access token or modifying Unify settings.

Click an audit log link to view the user who initiated the action, timestamp, and log details.

FAQ

What are the record display and search time limits for Profiles Insights?

Profiles Insights can display up to 500 records and supports searches within a 30-day time frame.

This page was last modified: 23 Apr 2024

Need support?

Questions? Problems? Need more info? Contact Segment Support for assistance!

Help improve these docs!

Was this page helpful?

Thanks for your feedback!

Can we improve this doc? 
Send us feedback!

Get started with Segment

On this page

Was this page helpful?

Thanks for your feedback!

Can we improve
 this doc? 
Send us feedback!

Product

For Developers

Company

Support

© 2025 Segment.io, Inc.

=== Content from https://segment.com/docs/engage/trait-activation/trait-enrichment/ ===

Trait Enrichment
        

Engage Foundations requires a Business tier account and includes Unify.


See the 
available plans
, or 
contact Support
.

On this page

Use Trait Enrichment to access Segment profile traits when you sync Audiences and Journeys to Destinations and Destination Functions. With Trait Enrichment, you can use custom, SQL, computed, and predictive traits to enrich the data you map to your destinations.

Set up Trait Enrichment

Use the following steps to set up Trait Enrichment with Audiences or Journeys.

The setup steps you’ll use for Trait Enrichment depend on the type of destination 
you’ve connected
.

Set up Trait Enrichment with Audiences

To set up Trait Enrichment with 
Audiences
:

Set up Trait Enrichment with Journeys

You can set up Trait Enrichment with Journeys as you’re creating or editing your journey in the 
builder
.

Default setup

Default setup uses default Segment Destination settings without Trait Enrichment. To use the default settings, select 
Default Setup
, then click 
Save
 to resume building your audience or journey.

You can customize event settings at any time.

Customized setup

With Customized setup, you can choose which traits you want to map to your destination or destination function.

Segment sends traits you select for enrichment in the 
traits
 object in Identify calls (
traits.trait_1
, 
traits.trait_2
), and as properties in the 
properties
 object in Track calls (
properties.trait_1
, 
properties.trait_2
).

Here’s an example Identify call payload with traits in the 
traits
 object:

{

  
"messageId"
:
 
"segment-test-message-uozjhr"
,

  
"timestamp"
:
 
"2024-02-22T22:11:15.595Z"
,

  
"type"
:
 
"identify"
,

  
"email"
:
 
"test@example.org"
,

  
"projectId"
:
 
"5kXbpcJxms8WWaEdQUkRWc"
,

  
"traits"
:
 
{

    
"trait1"
:
 
1
,

    
"trait2"
:
 
"test"
,

    
"trait3"
:
 
true

  
},

  
"userId"
:
 
"test-user-cq8idf"


}



And here’s an example Track call payload with properties in the 
properties
 object:

{

  
"messageId"
:
 
"segment-test-message"
,

  
"timestamp"
:
 
"2024-02-22T22:10:13.640Z"
,

  
"type"
:
 
"track"
,

  
"email"
:
 
"test@example.org"
,

  
"projectId"
:
 
"5kXbpcJxms8WWaEdQUkRWc"
,

  
"properties"
:
 
{

    
"property1"
:
 
1
,

    
"property2"
:
 
"test"
,

    
"property3"
:
 
true

  
},

  
"userId"
:
 
"test-user-1tgg9e"
,

  
"event"
:
 
"Segment Test Event Name"


}



Destination requirements

The following are a list of destination-specific requirements for using Trait Enrichment.

You can only sync the following traits to Facebook:

Each trait you select must map to a Facebook key.

email
 is required when syncing to Google, because every payload will send 
email
 (as an identifier) downstream in addition to phone number.

Additionally, you can only map one trait per audience to Google as a phone number.

Destination Actions and Destination Functions setup

If you’re using 
Destination Actions
 or 
Destination Functions
, use the following steps to set up Trait Enrichment.

Configure mappings in your destination

After you add traits, configure how your selected traits will map to your destination.

Keep your Engage Audience open in a separate tab, as you’ll need to return.

Best practices

For best results with Trait Enrichment, Segment recommends:

FAQs

Trait Enrichment
 lets you map the traits data you’ve collected with Engage to use when syncing audiences and Journeys to destinations.

ID Sync
 lets you map the identities data gathered for a profile for use when syncing audiences and Journeys to destinations.

Trait Enrichment on existing audience destinations doesn’t automatically resync the entire audience. Only new data flowing into Segment will adhere to the new trait criteria.

Yes, you can edit mappings in the Destination 
Mappings
 tab at any time. However, changes will only take place in subsequent audience syncs or in new audiences connected to the destination.

No. Segment doesn’t guarantee match rate improvements with Trait Enrichment. Match rates depend on data quality.

This page was last modified: 01 Aug 2024

Need support?

Questions? Problems? Need more info? Contact Segment Support for assistance!

Help improve these docs!

Was this page helpful?

Thanks for your feedback!

Can we improve this doc? 
Send us feedback!

Get started with Segment

On this page

Was this page helpful?

Thanks for your feedback!

Can we improve
 this doc? 
Send us feedback!

Product

For Developers

Company

Support

© 2025 Segment.io, Inc.

=== Content from https://segment.com/docs/connections/spec/screen/ ===

Spec: Screen
        

On this page

The Screen call lets you record whenever a user sees a screen, the mobile equivalent of Page, in your mobile app, along with any properties about the screen. Calling Page or 
Screen
 in one of Segment’s 
sources
 is one of the first steps to getting started with Segment.

Check out our high-level overview of the Screen method in Segment University. (Must be logged in to access.)

Here’s the payload of a typical Screen call, with most 
common fields
 removed:

{

  
"type"
:
 
"screen"
,

  
"name"
:
 
"Home"
,

  
"properties"
:
 
{

    
"Feed Type"
:
 
"private"

  
}


}



And here’s the corresponding Objective-C event that would generate the above payload:

[[
SEGAnalytics
 
sharedAnalytics
]
 
screen
:
@"Home"

                            
properties:
@{
 
@"Feed Type"
:
 
@"private"
 
}];



Based on the library you use, the syntax in the examples might be different. You can find library-specific documentation on the 
Sources Overview
 page.

Beyond the common fields, the Screen call takes the following fields:

Example

Here’s a complete example of a Screen call:

{

  
"anonymousId"
:
 
"3a12eab0-bca7-11e4-8dfc-aa07a5b093db"
,

  
"channel"
:
 
"mobile"
,

  
"context"
:
 
{

    
"ip"
:
 
"8.8.8.8"

  
},

  
"integrations"
:
 
{

    
"All"
:
 
true
,

    
"Mixpanel"
:
 
false
,

    
"Salesforce"
:
 
false

  
},

  
"messageId"
:
 
"022bb90c-bbac-11e4-8dfc-aa07a5b093db"
,

  
"name"
:
 
"Home"
,

  
"properties"
:
 
{

    
"variation"
:
 
"blue signup button"

  
},

  
"receivedAt"
:
 
"2015-02-23T22:28:55.387Z"
,

  
"sentAt"
:
 
"2015-02-23T22:28:55.111Z"
,

  
"timestamp"
:
 
"2015-02-23T22:28:55.111Z"
,

  
"type"
:
 
"screen"
,

  
"userId"
:
 
"97980cfea0067"
,

  
"version"
:
 
"1.1"


}



Create your own Screen call

Use the following interactive code pen to see what your Screen calls would look like with user-provided information:


Sample output goes here!


Identities

The User ID is a unique identifier for the user performing the actions. Check out the 
User ID docs
 for more detail.

The Anonymous ID can be any pseudo-unique identifier, for cases where you don’t know who the user is, but you still want to tie them to an event. Check out the 
Anonymous ID docs
 for more detail.

Note: In our browser and mobile libraries a User ID is automatically added
 from the state stored by a previous 
identify
 call, so you do not need to add it yourself. They will also automatically handle Anonymous IDs under the covers.

Name

Each screen can be tagged with a 
name
. For example, many apps have a “Signup” screen that can be useful to tag so that you can see users as they move through your funnel.

Properties

Properties are extra pieces of information that describe the screen. They can be anything you want.

Segment has reserved some properties with semantic meanings and handles them in special ways. You should 
only use reserved properties for their intended meaning
.

Reserved properties that Segment has standardized:

This page was last modified: 21 Nov 2023

Need support?

Questions? Problems? Need more info? Contact Segment Support for assistance!

Help improve these docs!

Was this page helpful?

Thanks for your feedback!

Can we improve this doc? 
Send us feedback!

Get started with Segment

On this page

Was this page helpful?

Thanks for your feedback!

Can we improve
 this doc? 
Send us feedback!

Product

For Developers

Company

Support

© 2025 Segment.io, Inc.

=== Content from https://segment.com/docs/engage/use-cases/ ===

Use Cases for Twilio Engage Premier
        

Engage Premier requires a Business tier account and includes Engage Foundations and Unify.


See the 
available plans
, or 
contact Support
.

On this page

Use Twilio Engage to build personalized marketing campaigns and improve:

To help you get started with Engage, here are a few example campaigns.

Customer loyalty email campaign

This journey sends an exclusive promo code to repeat customers to promote ongoing loyalty.



Build similar campaigns with SMS or use both email and SMS to contact subscribed users on their preferred channels.

Cart abandonment

This journey sends purchase reminders to cart abandonment users based on the channels they’ve subscribed to.



Onboarding

This journey sends exclusive offers and onboarding emails based on user action.


Low recency campaign

This campaign sends personalized re-engagement email and SMS promo offers to low recency customers.


Company updates and newsletters

Use Engage to send one-time updates as part of a campaign.

This campaign sends a semi-annual newsletter to subscribed users.



This page was last modified: 21 Sep 2023

Need support?

Questions? Problems? Need more info? Contact Segment Support for assistance!

Help improve these docs!

Was this page helpful?

Thanks for your feedback!

Can we improve this doc? 
Send us feedback!

Get started with Segment

On this page

Was this page helpful?

Thanks for your feedback!

Can we improve
 this doc? 
Send us feedback!

Product

For Developers

Company

Support

© 2025 Segment.io, Inc.

=== Content from https://segment.com/docs/privacy/consent-management/configure-consent-management/ ===

Configure Consent Management
        

Consent Management is available to customers on Business tier plans.


See the 
available plans
, or 
contact Support
.

On this page

After setting up your consent management platform (CMP), you can enforce the consent collected from your users by adding the 
consent object
 to your events.

Once you’ve configured consent in the Segment app and updated your sources to contain consent preference in every event, your events are routed only to the categories your end users consented to share data with. Events without the consent preference will continue to flow to destinations without consent enforcement.

Prerequisites

Consent management edit and update capabilities limited to Workspace Owners

Only users with the Workspace Owner role are able to create, edit, and disable consent categories. All other users have read-only access to Consent Management features.

Before you can configure consent in Segment, take the following steps:

Step 1: Create consent categories in the Segment app

Limited availability of destinations

AWS S3 and Engage destinations do not enforce consent preferences.

Segment recommends mapping all destinations to a category

Segment assumes all destinations without a mapping do not require user consent and will receive all events containing a consent object. If a destination is mapped to multiple categories, a user must consent to all categories for data to flow to the destination.

Step 2: Integrating your CMP with Segment

Once you’ve created consent categories in the Segment app, you need to integrate your CMP with Segment.

Segment supports the following CMPs:

*
If you send data to device-mode destinations connected to your Analytics.js source, you must navigate to your Analytics.js source in the Segment app, select 
Settings > Analytics.js
, and enable Destination Filters.

For more information about Segment’s Analytics.js OneTrust wrapper, see the 
Analytics.js OneTrust Wrapper
 documentation.

If you’d like to integrate with any other CMP, Segment requires you to build your own wrapper or use any mechanism provided it meets the following requirements for data and event generation:

To get started building your own wrapper, follow the instructions in the 
@segment/analytics-consent-tools
 repository.

Consent Management is not backwards compatible with Segment's legacy iOS and Android libraries

If you are using one of Segment’s legacy mobile libraries (iOS or Android,) you will need to upgrade to 
Swift
 or 
Kotlin
 before using Consent Management.

Validate your CMP integration

Customers with Analytics.js 2.0 sources can use the 
Segment Inspector
 to confirm that events from their source contain the 
consent object
. Unify and Engage users can also verify that the 
Segment Consent Preference Updated event
 emits every time end users update their consent preferences.

All users can validate that events contain the consent object and that the Segment Consent Preference Updated event is present using Segment’s 
Source Debugger
.

You can also confirm your events flow to destinations or are blocked from destinations according to the consent categories you created in 
Step 1: Create consent categories in the Segment App
, if already connected to the destination.

Edit consent categories

If you need to make changes to your consent categories, you can edit them on the Consent Management page. You may experience some latency between making the changes and having the changes take effect.

The 
Audit Trail
 surfaces information about when a consent category is created, modified, or disabled, and when consent mappings are created or removed.

Disable consent categories

Disabling a consent category means that Segment no longer enforces end user consent preferences for the destinations in the disabled category. Other consent categories aren’t affected.

This page was last modified: 16 Oct 2024

Need support?

Questions? Problems? Need more info? Contact Segment Support for assistance!

Help improve these docs!

Was this page helpful?

Thanks for your feedback!

Can we improve this doc? 
Send us feedback!

Get started with Segment

On this page

Was this page helpful?

Thanks for your feedback!

Can we improve
 this doc? 
Send us feedback!

Product

For Developers

Company

Support

© 2025 Segment.io, Inc.

=== Content from https://segment.com/docs/privacy/consent-management/consent-in-retl/ ===

Consent in Reverse ETL
        

Consent Management is available to customers on Business tier plans.


See the 
available plans
, or 
contact Support
.

On this page

With Consent Management in Reverse ETL, you can enforce your end-users’ consent preferences that are captured by your consent management platform (CMP) and stored in your warehouse.

To enforce consent stored in your warehouse, build a Reverse ETL model that identifies consent categories. You can create a “consent to” column mapping in a new data model or update an existing data model to include a “consent to” mapping.

Consent in Reverse ETL supports Reverse ETL-supported Actions destinations and Segment Connections

At this time, Consent in Reverse ETL does not support adding consent to Segment Profiles using the Segment Profiles destination. To enforce consent data in your classic Segment destinations, use the 
Segment Connections destination
.

Prerequisites

Consent management edit and update capabilities limited to Workspace Owners

Only users with the Workspace Owner role are able to create, edit, and disable consent categories. All other users have read-only access to Consent Management features.

Before you can enforce consent stored in your warehouse, take the following steps:

Step 1: Create consent categories in the Segment app

Limited availability of destinations

Reverse ETL supports the Actions destinations in the 
Reverse ETL catalog
 and 
Segment Connections
.

Segment recommends mapping all Reverse ETL destinations to a category

Segment assumes all destinations without a mapping do not require user consent and will receive all events containing a consent object. If a destination is mapped to multiple categories, a user must consent to all categories for data to flow to the destination.

To edit or disable consent categories, view the 
Configure Consent Management
 documentation.

Step 2: Add your Reverse ETL source

If you’ve already added a 
Reverse ETL source
 to your workspace, you can proceed to 
Step 3: Identify consent columns
.

If you haven’t already configured a Reverse ETL source in your workspace, follow the instructions in the 
Reverse ETL: Add a source
 documentation to add your warehouse as a data source. When you’ve configured your Reverse ETL source, proceed to 
Step 3: Identify consent columns
.

Step 3: Identify consent columns

After you set up consent categories in the Segment app, you must identify the columns in your data warehouse that store end user consent by creating a 
model
, or SQL query that defines the set of data you want to synchronize to your Reverse ETL destinations. When building your data model, Segment recommends that you represent consent as a boolean 
true
 or 
false
 value and map one consent category to one column.

Creating a data model that does not include information about consent preferences results in no consent enforcement

If you create consent categories in your workspace but fail to identify columns that contain consent preferences in your data model, events flow to all destinations in your workspace regardless of end user consent preferences.

Identify consent when building your model

To identify consent when building your model:

Update your Reverse ETL model to include consent

To update an existing Reverse ETL model to include consent enforcement:

You can select the 
Settings
 tab and click 
Consent settings
 to verify that the consent categories in your model match the consent categories you configured in your workspace.

You can store each consent category in its own column in your warehouse, or store your consent information in one single blob column. Segment requires your consent categories to be in their own column in your data model.

The following sample model maps consent categories from each column in your database:

select
 
  
USERID
,
 
  
name
,
 
  
email
,
 
  
distinctid
,

  
Ads
,
 
  
Personalization
,
 
  
Analytics
,
 
  

from
 
CONSENT_PREFERENCES
;



The following sample model maps consent categories from one blob column in your database:

select
 
  
USERID
,
 
  
name
,
 
  
email
,
 
  
distinctid
,
 
  
CAST
(
CONSENT_OBJ
:
consent
.
cookie
.
Advertising
 
as
 
Boolean
)
 
as
 
Ads
,
  
  
CAST
(
CONSENT_OBJ
:
consent
.
cookie
.
Personalization
 
as
 
Boolean
)
 
as
 
Personalization
,
 
  
CAST
(
CONSENT_OBJ
:
consent
.
cookie
.
Analytics
 
as
 
Boolean
)
 
as
 
Analytics
,
 
   

from
 
CONSENT_PREFERENCES
;



Failing to identify consent columns in your warehouse might lead to unintentional data loss

If you have destinations mapped to consent categories in the Segment app but fail to identify a column in your warehouse that stores consent for a category, then consent preference for that category will be considered to be false and 
no data will flow to destinations mapped to the category
.

Step 4: Connect your downstream destinations

After you set up categories in the Segment app and create a SQL model that extracts consent information, connect your downstream destinations to complete the consent enforcement process.

Consent in Reverse ETL supports Reverse ETL-supported Actions destinations and Segment Connections

At this time, Consent in Reverse ETL does not support enforcing consent in the Segment Profiles destination. To enforce consent data in your classic Segment destinations, use the 
Segment Connections destination
.

To add your first destination:

Segment does not count Reverse ETL records filtered by Consent Management toward your Reverse ETL limits

Records filtered out by Consent Management are not counted as part of your Reverse ETL limits. For more information about Reverse ETL limits, see the 
Reverse ETL Limits
 documentation.

Validate your consent mapping

You can validate that you successfully created your consent mapping in Segment Connections or supported Reverse ETL Actions destinations using the following methods.

Segment Connections destination

Segment automatically adds the 
consent object
 to every event that’s routed downstream to your Segment Connections destination. 
Consent enforcement in Connections
 validates that only consenting data flows downstream to any classic Segment destinations connected to your Segment Connections instance.

Open the Source Debugger for your Reverse ETL source and confirm that the 
consent object
 appears on every event and that the consent object has the categories you mapped in 
Step 2: Identify consent columns
.

Reverse ETL Actions destinations

Segment automatically filters out data from users who have not consented to the category mapped to your destination.

To verify that this behavior is working as intended, open 
Delivery Overview
 for a RETL-supported Actions destination and view the events that were successfully delivered to the destination. The events in your destination should only come from users that consented to send data to the category that your supported Actions destination belongs to.

This page was last modified: 25 Jul 2024

Need support?

Questions? Problems? Need more info? Contact Segment Support for assistance!

Help improve these docs!

Was this page helpful?

Thanks for your feedback!

Can we improve this doc? 
Send us feedback!

Get started with Segment

On this page

Was this page helpful?

Thanks for your feedback!

Can we improve
 this doc? 
Send us feedback!

Product

For Developers

Company

Support

© 2025 Segment.io, Inc.

=== Content from https://segment.com/docs/unify/traits/predictions// ===

Predictions
        

Unify Plus requires a business tier account and is included with Engage


See the 
available plans
, or 
contact Support
.

On this page

Predictions, Segment’s artificial intelligence and machine learning feature, lets you predict the likelihood that users will perform any event tracked in Segment.

With Predictions, you can identify users with, for example, a high propensity to purchase, refer a friend, or use a promo code. Predictions also lets you predict a user’s lifetime value (LTV).

Segment saves predictions to user profiles, letting you build Audiences, trigger Journeys, and send data to downstream destinations.

For more details on AI usage and data, see 
Predictions Nutrition Facts Label
.

On this page, you’ll learn how to build a prediction.

Build a prediction



Follow these steps to build a prediction:

Keep the following in mind when you build a prediction:

In the next section, you’ll learn more about the four available predictions.

Choosing a prediction

Segment offers four predictions: Custom Predictive Goals, Likelihood to Purchase, Predicted LTV, and Likelihood to Churn.

Custom Predictive Goals

Custom Predictive Goals require a starting cohort, target event, and quality data.

When you build a Custom Predictive Goal, you’ll first need to select a cohort, or a group of users, for which you want to make a prediction. Traits with small cohorts compute faster and tend to be more accurate. If you want to predict for an entire audience, though, skip cohort selection and move to selecting a target event.

The target event is the Segment event that you want to predict. In creating a prediction, Segment determines the likelihood of the user performing the target event. Segment lets you include up to two target events and an event property in your prediction.

Access and data requirements

In machine learning, better data leads to better predictions. Because Segment prioritizes trust and performance, Segment has a number of data checks to ensure that each prediction is reliable and of high quality. Segment provides guidance in the UI before you create a trait, but some checks only occur during model training. If a trait fails, you’ll see an error message and description in the UI.

This sections lists Segment’s access and data requirements, service limits, and best practices for Predictions.

For example, to predict a customer’s propensity to purchase over the next 30 days, set the Target Window to 30 days and the Target Event to 
Order Completed
 (or the relevant purchase event that you track).

To access Predictions, you must:

This table lists the requirements for a trait to compute successfully:

Some customers want to specifically include or exclude events that get fed into the model. For example, if you track different events from an EU storefront compared to a US storefront and you only want to make predictions using data from the US, you could unselect the events from the EU space. This step is optional, Segment only recommends using it if you have a clear reason in mind for removing events from becoming a factor in the model.

Predictive Traits and anonymous events

Predictive Traits are limited to non-anonymous events, which means you’ll need to include an additional 
external_id
 other than 
anonymousId
 in the targeted events. If want to create Predictive Traits based on anonymous events, reach out to your CSM with your use case for creating an anonymous Predictive Trait and the conditions for trait.

Likelihood to Purchase

Likelihood to Purchase is identical to Custom Predictive Goals, but Segment prefills the 
Order Completed
 event, assuming it’s tracked in your Segment instance.

If you don’t track 
Order Completed
, choose a target event that represents a customer making a purchase.

Predicted Lifetime Value

Predicted Lifetime Value predicts a customer’s future spend over the next 120 days. To create this prediction, select a purchase event, revenue property, and the currency (which defaults to USD). LTV is only calculated for customers that have performed the selected purchase events 2 or more times. The following table contains details for each property:

Likelihood to Churn

Likelihood to Churn proactively identifies customers likely to stop using your product. Segment builds this prediction by determining whether or not a customer will perform a certain action.

To use Likelihood to Churn, you’ll need to specify a customer event, a future time frame for which you want the prediction to occur, and if you want to know whether the customer will or won’t perform the event.

For example, suppose you wanted to predict whether or not a customer would view a page on your site over the next three months. You would select 
not perform
, 
Page Viewed
, and 
at least 1 time within 90 days
.

Churn predictions are only made for eligible customers. In the previous example, only customers that have performed 
Page Viewed
 in the last 90 days would be eligible to recieve this prediction. The Segment app shows you which customers are eligibile to recieve this prediction.

Segment then uses this criteria to build the prediction and create specific percentile cohorts. You can then use these cohorts to target customers with retention flows, promo codes, or one-off email and SMS campaigns.

Use cases

For use cases and information on how Segment builds prediction, read 
Using Predictions
.

This page was last modified: 05 Aug 2024

Need support?

Questions? Problems? Need more info? Contact Segment Support for assistance!

Help improve these docs!

Was this page helpful?

Thanks for your feedback!

Can we improve this doc? 
Send us feedback!

Get started with Segment

On this page

Was this page helpful?

Thanks for your feedback!

Can we improve
 this doc? 
Send us feedback!

Product

For Developers

Company

Support

© 2025 Segment.io, Inc.

=== Content from https://segment.com/docs/getting-started/ ===

What is Segment?
        

With Segment, you can collect, transform, send, and archive your 
first-party customer data
. Segment simplifies the process of collecting data and connecting new tools, allowing you to spend more time using your data, and less time trying to collect it. You can use Segment to track events that happen when a user interacts with the interfaces. “Interfaces” is Segment’s generic word for any digital properties you own: your website, mobile apps, and processes that run on a server or OTT device.

When you capture interaction data in Segment, you can send it (often in real-time) to your marketing, product, and analytics tools, as well as to data warehouses. In most cases, you won’t even need to touch your tracking code to connect to new tools.

Let's walk through the steps to get up and running on Segment. Let's go!

This page was last modified: 14 Dec 2021

Need support?

Questions? Problems? Need more info? Contact Segment Support for assistance!

Help improve these docs!

Was this page helpful?

Thanks for your feedback!

Can we improve this doc? 
Send us feedback!

Get started with Segment

Was this page helpful?

Thanks for your feedback!

Can we improve
 this doc? 
Send us feedback!

Product

For Developers

Company

Support

© 2025 Segment.io, Inc.

=== Content from https://segment.com/docs/connections/storage/data-lakes/sync-history/ ===

Data Lakes Sync History and Health
        

Data Lakes is available for the listed account plans only.


See the 
available plans
, or 
contact Support
.

On this page

The Segment Data Lakes sync history and health tabs generate real-time information about data syncs so you can monitor the health and performance of your data lakes. These tools provide monitoring and debugging capabilities within the Data Lakes UI, so you can identify and proactively address data sync or data pipeline failures.

Sync history

The Sync History table shows detailed information about the latest 100 syncs to the data lake. The table includes the following fields:

Selecting a row in the Sync History table opens a sidebar showing the number of rows from each collection that synced.

To access the Sync History page from the Segment app, open the 
My Destinations
 page and select the data lake. On the data lakes Settings page, select the 
Sync History
 tab.

Health

The health tab provides an overview of the rows that synced to your data lake both today and each day for the last 30 days.

The bar chart, ‘Daily Synced Rows,’ shows an overview of the rows synced for each of the last 30 days. Hovering over a date shows the number of rows that were synced for that day. Selecting a date from the bar chart opens the Daily Row Volume table, which provides a breakdown of which collections synced, how many rows from each collection synced, and the percentage of all synced rows from each collection.

The Daily Row Volume table contains the following information:

Above the Daily Row Volume table is an overview of the total syncs for the current day, showing the number of rows synced, the number of collections that synced, and the current date.

To access the Sync history page from the Segment app, open the 
My Destinations
 page and select the data lake. On the data lakes settings page, select the 
Health
 tab.

Data Lakes reports FAQ

How long is a data point available?

The health tab shows an aggregate view of the last 30 days worth of data, while the sync history retains the last 100 syncs.

How do sync history and health compare?

The sync history feature shows detailed information about the most recent 100 syncs to a data lake, while the health tab shows just the number of rows synced to the data lake over the last 30 days.

What timezone is the time and date information in?

All dates and times on the sync history and health pages are in the user’s local time.

When does the data update?

The sync data for both reports updates in real time.

When do syncs occur?

Syncs occur approximately every two hours. Users cannot choose how frequently the data lake syncs.

This page was last modified: 03 Aug 2023

Need support?

Questions? Problems? Need more info? Contact Segment Support for assistance!

Help improve these docs!

Was this page helpful?

Thanks for your feedback!

Can we improve this doc? 
Send us feedback!

Get started with Segment

On this page

Was this page helpful?

Thanks for your feedback!

Can we improve
 this doc? 
Send us feedback!

Product

For Developers

Company

Support

© 2025 Segment.io, Inc.

=== Content from https://segment.com/docs/engage/journeys/event-triggered-journeys-steps/ ===

Event-Triggered Journeys Steps
        

Engage Foundations requires a Business tier account and includes Unify.


See the 
available plans
, or 
contact Support
.

On this page

Event-Triggered Journeys
 in Engage use steps to control how users move through a journey based on their actions or predefined conditions.

Steps are the building blocks of a journey. This page explains the 
Hold Until
 and 
Send to Destination
 steps, which enable precise control over journey progression and data delivery.

Public Beta

Event-Triggered Journeys is in public beta, and Segment is actively working on this feature. Some functionality may change before it becomes generally available. Event-Triggered Journeys is not currently HIPAA eligible.

Hold Until: smart pauses in journeys

The 
Hold Until
 step adds a deliberate pause in a journey, waiting for specific user actions or a predefined time limit before progressing. This lets you create highly personalized experiences by responding to user behavior (or the lack thereof) at the right moment.

Because the Hold Until step introduces a checkpoint in your journey where the next action depends on user behavior, it creates opportunities for:

How Hold Until works

When a journey reaches a Hold Until step:

Configurable parameters

The following table explains the parameters you can configure for the Hold Until step:

Additional features

The Hold Until step includes optional settings that let you customize how Segment stores and processes events in your journey. These features give you more control over event timing, data inclusion, and journey logic.

The Hold Until step can restart when a specified event reoccurs. This resets the hold duration and updates the 
journey context
 with the most recent event data.

When the same event occurs again, the hold timer resets, and Segment updates the journey context with the latest event data. However, Segment only includes events in the journey context if the profile follows the branch where the event was processed.

For example, in an abandoned cart journey, if a user modifies their cart during the hold period, the cart contents are updated and the two-hour timer resets. This prevents premature follow-ups and keeps the data up-to-date.

Enable this feature by selecting 
Send profiles back to the beginning of this step each time this branch event occurs
 in the step configuration. For more details about how journey context handles triggering events, see 
Destination event payload schema
.

Segment recommends putting branches for recurring events at the top of the list to improve readability.



In this example, users enter the journey when they modify their cart and wait for either a purchase or two hours to pass. If the user modifies their cart again during those two hours, the cart contents are updated, and the two-hour timer resets. As a result, follow-ups reflect the latest information.

Event name aliases let you reuse the same event in multiple branches or steps without losing track of data. This approach encourages data clarity and integrity by preserving event-specific context for each branch or step where the alias is applied.

By default, when the same event is triggered multiple times, the most recent event data overwrites earlier occurrences. When you use aliases, though, each branch or step can maintain its own version of the event for more granular control. This is especially useful in journeys that involve repeated events or complex branching logic.

For example, an onboarding journey with a 
Signup Completed
 event could trigger multiple actions:

As another example, consider the 
Cart_Modified
 event in an abandoned journey:

The destination payload after the Hold Until step would look like this:

{

  
"properties"
:
 
{

    
"journey_context"
:
 
{

      
"Cart_Modified"
:
 
{

        
"organization"
:
 
"Duff Brewery"
,

        
"compression_ratio"
:
 
5.2
,

        
"output_code"
:
 
"not_hotdog"

      
},

      
"Cart_Modified - user updates cart"
:
 
{

        
"organization"
:
 
"Acme Corp"
,

        
"user_name"
:
 
"Homer Simpson"
,

        
"output_code"
:
 
"always_blue"

      
}

    
}

  
}


}



In this example:

Segment generates aliases for each instance of an event by concatenating the event name and branch name (for example, 
Cart_Modified - user updates cart
, like in the previous payload example). This approach allows both branches to retain the specific event context needed for their respective actions.

Segment creates these aliases automatically during setup, and they show up in the journey context and downstream payloads. While you can’t customize alias names, using clear and meaningful branch names helps maintain clarity and precise tracking.

Managing Hold Until steps

Deleting a Hold Until step can impact downstream steps that rely on it. When you delete a configured step, Segment displays a modal that summarizes the potential impact on related branches and steps. Review all dependencies carefully to avoid unintentionally disrupting the journey.

Send to Destination

The 
Send to Destination
 step lets you send journey data to one of your 
configured Engage destinations
, enabling real-time integration with tools like marketing platforms, analytics systems, or custom endpoints.

This step supports Actions Destinations (excluding list destinations) and destination functions. It doesn’t support storage destinations or classic (non-Actions) destinations.

How Send to Destination works

When a journey reaches the Send to Destination step, the journey packages the relevant data and sends it to your chosen destination. This could be a third-party platform, like a marketing tool, or a custom destination built using 
Destination Functions
. The data that Segment sends includes key attributes from the journey context, profile traits, and any mapped fields you’ve configured.

Configure the Send to Destination step

Set a destination up first

Before you add configure this step, make sure you’ve already set up the destination(s) in Engage.

Here’s how to configure this step within a journey:

Before activating the journey, 
send a test event to verify that the payload matches your expectations
 and that it reaches the destination successfully.

Destination event payload schema

The events that Segment sends to destinations from Event-Triggered Journeys include an object called 
journey_context
 within the event’s properties. The 
journey_context
 object contains:

You can also optionally include profile traits to provide richer context for the destination.

Here’s a detailed example of a payload structure, highlighting the journey context and how Segment enriches event data:

{

  
"event"
:
 
"<<YOUR CUSTOM EVENT NAME>>"
,

  
"type"
:
 
"track"
,

  
"userId"
:
 
"test-user-67"
,

  
"timestamp"
:
 
"2025-01-15T02:02:15.908Z"
,

  
"receivedAt"
:
 
"2025-01-15T02:02:15.908Z"
,

  
"originalTimestamp"
:
 
"2025-01-15T02:02:15.908Z"
,

  
"context"
:
 
{

    
"personas"
:
 
{

      
"computation_class"
:
 
"journey_step"
,

      
"computation_id"
:
 
"journey_name__step_name_8943l"
,

      
"computation_key"
:
 
"journey_name__step_name_8943l"
,

      
"event_emitter_id"
:
 
"event_tester_lekqCASsZX"
,

      
"namespace"
:
 
"spa_w5akhv1XwnGj5j2HVT6NWX"
,

      
"space_id"
:
 
"spa_w5akhv1XwnGj5j2HVT6NWX"

    
}

  
},

  
"properties"
:
 
{

    
"journey_context"
:
 
{

      
"triggering_event"
:
 
{

        
"organization"
:
 
"Pied Piper"
,

        
"compression_ratio"
:
 
5.2
,

        
"output_code"
:
 
"not_hotdog"

      
},

      
"event_from_hold_until_step"
:
 
{

        
"organization"
:
 
"Tres Commas"
,

        
"user_name"
:
 
"Russ Hanneman"
,

        
"output_code"
:
 
"always_blue"

      
}

    
},

    
"journey_metadata"
:
 
{

      
"journey_id"
:
 
"2GKsjADZkD"
,

      
"epoch_id"
:
 
"yiC2qPZNIS"

    
},

    
"user_name"
:
 
"Richard Hendricks"
,

    
"coding_style"
:
 
"tabs_only"
,

    
"pivot_count"
:
 
12

  
},

  
"messageId"
:
 
"personas_up0crko4htawmo2c9ziyq"


}



This example shows how data is structured and enriched with contextual details so that destinations receive the information they need to act effectively.

Managing activations

Activations control the configuration for sending data to destinations, including the destination type, selected action, and mapped attributes. Managing activations allow you to adjust how data flows to a destination without altering the overall journey logic.

You can make updates to an existing activation to align mapped attributes with changes in the downstream schema and add or remove profile traits included in the payload.

To edit or delete an activation, click the destination name in the journey canvas and select the 
More
 menu. Changes apply only to new journey entries after saving your updates.

If you delete an activation, future instances of the journey step will fail to send data to that destination. To avoid disruptions, make sure you’ve configured alternative logic or destinations before removing an activation.

Handling missing attributes

There may be cases where events sent to Segment are missing specific properties or when profile traits are unavailable. How Segment handles these scenarios depends on whether the attribute is explicitly mapped.

Carefully configuring mappings and handling missing attributes can help you maintain data integrity and avoid errors in downstream systems.

This page was last modified: 23 Jan 2025

Need support?

Questions? Problems? Need more info? Contact Segment Support for assistance!

Help improve these docs!

Was this page helpful?

Thanks for your feedback!

Can we improve this doc? 
Send us feedback!

Get started with Segment

On this page

Was this page helpful?

Thanks for your feedback!

Can we improve
 this doc? 
Send us feedback!

Product

For Developers

Company

Support

© 2025 Segment.io, Inc.

=== Content from https://segment.com/docs/engage/journeys/journeys-analytics/ ===

Journeys Analytics
        

On this page

Segment maintains analytics for each Journey and its individual steps. As a result, you can view both granular and high-level performance metrics that give you insight into your Journeys.

Journey-Level Analytics

Journeys Level Analytics is a collection of statistics that can help you assess how a Journey is performing.

Where individual messaging analytics give you focused insight into specific Journey events, Journey Level Analytics shows you a high-level overview of a Journey’s effectiveness.

Access a Journey’s Analytics

Follow these steps to view the Analytics for a specific Journey:

Journeys in draft status don’t display Analytics.

Journey-Level Analytics statistics

The following table shows the statistics available for a Journey:

Completed and exits are mutually exclusive. The “Search for a user” search box excludes users who have exited the Journey.

Use the date picker to view a Journey’s analytics over a specific time frame in any 180 day period.

The following table shows descriptions of the time frames you can select:

Step-Level Analytics

Displayed with each step of your Journey, Step-Level Analytics shows you how many users made it to the step you’re viewing. You can use this data to gain context for how users flow through your Journey.

Changing the calculation percentage

With Step-Level Analytics, you can configure two settings that give you granular insight into each step’s performance:

By default, Engage calculates an individual step’s analytics as a percentage of the number of users in the previous step. However, you can also view step analytics as a percentage of the initial number of users in the Journey’s entry step.

For example, suppose your Journey’s entry step contained 100 users, and 50 proceeded to the next step. For both calculation options, Engage would display 
50% and 50
 for the next step. If 25 users from the second step reached step three, however, Engage would display 
50% and 25
 for previous-step based calculations but 
25% and 25
 for entry-step based calculations.

To change this base percentage, select 
Calculate % based on
, then select 
Entry step
 or 
Previous step
.

If you’ve enabled re-entry for your Journey, you can also configure Step-Level Analytics to calculate the step’s percentage based on unique or total users. Selecting 
Unique
 generates a percentage based on unique users, while 
Total
 includes users who have re-entered the Journey.

For more information on re-entry settings in Journeys, view 
Journey re-entry
.

This page was last modified: 30 Apr 2024

Need support?

Questions? Problems? Need more info? Contact Segment Support for assistance!

Help improve these docs!

Was this page helpful?

Thanks for your feedback!

Can we improve this doc? 
Send us feedback!

Get started with Segment

On this page

Was this page helpful?

Thanks for your feedback!

Can we improve
 this doc? 
Send us feedback!

Product

For Developers

Company

Support

© 2025 Segment.io, Inc.

=== Content from https://segment.com/docs/engage/content/organization/ ===

Organizing Your Templates
        

Engage Premier requires a Business tier account and includes Engage Foundations and Unify.


See the 
available plans
, or 
contact Support
.

On this page

Engage Premier entered an End of Sale (EOS) period effective  June 10, 2024. Existing Segment customers will continue to have access and support to Engage Premier until an end-of-life (EOL) date is announced. We recommend exploring the following pages in preparation of a migration or future MCM needs:

Twilio Marketing Campaigns

Preferred ISV Partners:

Airship Blog
 


Bloomreach Blog
 


Braze Blog
 


Insider Blog
 


Klaviyo Blog
 


Twilio Engage Foundations Documentation
 

To add structure to your marketing content, you can organize templates into folders and duplicate them within your Segment space.

Organize with folders

Use folders to organize your Email, SMS/MMS, Push, and WhatsApp content templates. Group related content together to better help you manage and find your marketing resources.

From the Templates overview page you can create, update, view, and delete template folders.

You must have both read and write workspace permissions to create or make changes to folders.

To create a folder:

You can also rename, add templates, or disband your folder from the Templates overview page. Disbanding a folder returns all templates from the folder to the main template list, without deleting any of the templates.

You can only organize templates in your folders according to template type. For example, you can’t group email and SMS templates in the same folder.

Move templates to your folders

From the Templates overview page, you can select individual template(s) to move to your folders.

After you select the templates you’d like to move:

Use the 
Actions
 button in your folder to remove templates or move them to a different location. When you remove a template, Engage returns the template to the Templates overview page, without deleting it.

Duplicate an Engage template

You can clone existing Engage templates to edit and use in your message campaigns.

Duplicate email, SMS, and push templates

To duplicate an email, SMS, or push template:

Learn more about configuring 
email
, 
SMS
, and 
push
 templates.

Duplicate WhatsApp templates

To duplicate WhatsApp templates:

Learn more about 
configuring WhatsApp templates
.

This page was last modified: 15 Jul 2024

Need support?

Questions? Problems? Need more info? Contact Segment Support for assistance!

Help improve these docs!

Was this page helpful?

Thanks for your feedback!

Can we improve this doc? 
Send us feedback!

Get started with Segment

On this page

Was this page helpful?

Thanks for your feedback!

Can we improve
 this doc? 
Send us feedback!

Product

For Developers

Company

Support

© 2025 Segment.io, Inc.

=== Content from https://segment.com/docs/privacy/ ===

Privacy Tools Overview
        

On this page

Segment includes a suite of Privacy tools to help your organization comply with regulations like HIPAA, the GDPR, and the CCPA.

Proactively identify PII with the Privacy Portal

The Privacy Portal helps automate preparing for new privacy regulations.

Take control over whether specific data is allowed to enter Segment

Prepare for GDPR & CCPA

Segment is committed to making it easier for you to comply with the GDPR.

Easily remove your company's end-users from Segment and supported connections.

This page was last modified: 14 Nov 2022

Need support?

Questions? Problems? Need more info? Contact Segment Support for assistance!

Help improve these docs!

Was this page helpful?

Thanks for your feedback!

Can we improve this doc? 
Send us feedback!

Get started with Segment

On this page

Was this page helpful?

Thanks for your feedback!

Can we improve
 this doc? 
Send us feedback!

Product

For Developers

Company

Support

© 2025 Segment.io, Inc.

=== Content from https://segment.com/docs/connections/sources/catalog/libraries/mobile/swift-ios/ ===

Redirecting…

=== Content from https://segment.com/docs/guides/how-to-guides/set-up-notifications-alerts/ ===

How do we set up event-triggered notifications or alerts?
        

On this page

Below you’ll find a bunch of ways to set up notifications for yourself based on the data you’re sending through Segment. 

Connections Alerting

Connections Alerting allows Segment users to receive in-app, email, and Slack notifications related to the performance and throughput of an event-streaming connection.

Connections Alerting allows you to create two different alerts:

For more information about Connections Alerting, see the 
Connections Alerting
 documentation.

Google Analytics custom alerts

You can use Google Analytics Custom Alerts to send yourself emails whenever a specific traffic segment drops below (or above) a threshold you set. 

Learn how to set up email alerts in 
Google’s documentation
.

Analytics email summaries

With tools like Amplitude, Kissmetrics, and Mixpanel, you can set up email reports delivered to you on a daily basis. They are completely customizable, so you can keep an eye on as many events or other metrics you’d like. 

Realtime traffic monitoring

Chartbeat and GoSquared both offer awesome real-time dashboards to see what’s happening right now on your site. They both include the option to get notified when your traffic hits a certain threshold. For example, if your on-site visitors is less than 100 people, or more than 1,000.

GoSquared also offers in-depth historical and user analysis. Chartbeat sticks to realtime anonymous traffic, but offers some sweet features for publishers.

Webhook-based alerts

The last option Segment recommends is to use a monitoring tool like 
PagerDuty
 or 
Datadog
 and point Segment’s 
webhooks
 destination at them. That way you can set up custom alerts in their system.

Event-triggered emails

The last option for alerting based off of Segment events is to use one of the email tools available on the Segment platform that offers event-triggered emails. Your options there are Customer.io, Vero, Autopilot, Outbound, Klaviyo, or Threads.

This page was last modified: 30 May 2024

Need support?

Questions? Problems? Need more info? Contact Segment Support for assistance!

Help improve these docs!

Was this page helpful?

Thanks for your feedback!

Can we improve this doc? 
Send us feedback!

Get started with Segment

On this page

Was this page helpful?

Thanks for your feedback!

Can we improve
 this doc? 
Send us feedback!

Product

For Developers

Company

Support

© 2025 Segment.io, Inc.

=== Content from https://segment.com/docs/protocols/enforce/schema-configuration/ ===

Customize your schema controls
        

Protocols is available as an add-on for Business plans only.


See the 
available plans
, or 
contact Support
.

On this page

The Schema Configuration settings for each source can be used to selectively block events, or omit properties and traits from 
.track()
, 
.identify()
 and 
.group()
 calls. Segment can permanently drop events that are not included in your Tracking Plan, depending on the settings you select. Segment can also block events with invalid properties or invalid property values.

Blocked events not forwarded to a Source are discarded

Blocking is a serious step that you should only do after you have resolved any violations that appear when you first connect a Tracking Plan to a Source. Any blocked events that are not 
forwarded to a separate Source
 are permanently discarded and cannot be recovered.

To enable blocking, go to the 
Settings
 tab for your source and click on 
Schema Configuration
. See below for detailed descriptions for each of the configuration settings.



You can 
export your Source Schema
 as a CSV file to quickly audit events from your Tracking Plan.

Archived events

If you archive events while your source is connected to a Tracking Plan, and then later disconnect your Tracking Plan from that source, any archived events will remain archived, but will be allowed if the Schema Configuration was previously set to block unplanned events when your Tracking Plan was connected to the source.

To view all archived events, go to your 
Source Schema
 page, click 
Filter
 next to the search bar, and select 
Archived
. To unarchive events that have been archived, click 
Unarchive
 in the event column.

Order of Priority in Blocking Options

When setting up Schema Configuration, note that Segment prioritizes blocking controls in the following order:

Standard Schema Controls
: Segment first evaluates incoming events against these controls and your Tracking Plan. Events, properties, or traits not blocked or omitted in this phase then flow to the next level of controls: the Advanced Blocking Controls/Common JSON Schema. 


Advanced Blocking Controls/Common JSON Schema:
 These controls act as a secondary layer, evaluating incoming events against the Common JSON schema included in your Tracking Plan.


Using only the Common JSON Schema to block events

If your Tracking Plan only has Common JSON Schema rules, you only need to use the Advanced Blocking Controls for your source.

If you use the Standard Schema Controls and omit properties or traits that do not exist, the Tracking Plan might not generate violations for the Common JSON Schema, as the entire Tracking Plan has nothing and everything is considered to be “unplanned”.

Track Calls - Unplanned Events

When you set this dropdown to Block Event, Segment drops any events that are not defined in your Tracking Plan. Only allowlisted 
track
 calls in your Tracking Plan flow through Segment to your Destinations.

For example, if you include a 
Subscription Cancelled
 event in your Tracking Plan, the example track call below would be blocked by Protocols because the event name does not match the event name casing in your Tracking Plan.

    
analytics
.
track
(
'
subscription_cancelled
'
)



IMPORTANT: Unplanned event blocking is supported for all device-mode and cloud-mode Analytics.js destinations and Mobile libraries in cloud-mode.

Track Calls - Unplanned Properties

Setting this dropdown to Omit Properties will ensure that properties not defined in your Tracking Plan are removed from the relevant event.

For example, if you include a single 
subscription_id
 property in the 
Subscription Cancelled
 event in your tracking plan, the example track call below would have the 
subscription_name
 property omitted by Protocols.

    
analytics
.
track
(
'
Subscription Cancelled
'
,
 
{
subscription_id
:
 
'
23r90jfs9ej
'
,
 
subscription_name
:
 
'
premium
'
})



IMPORTANT: Unplanned property omission is ONLY supported in cloud-mode Destinations. Unplanned properties will not be omitted when sending to device-mode Destinations.

Block Track Calls - Common JSON Schema Violations

JSON schema violation event blocking only supports cloud-mode destinations

Events with invalid properties are not blocked from device-mode destinations.

To block all Track calls that generate a common JSON schema violation:

Setting the dropdown to 
Block Event
 ensures that all Track events with JSON schema violations (for example, missing required properties, incorrect property value data types, or invalid regex patterns) are blocked. A less aggressive option is to select 
Omit
 from the dropdown which removes the offending property from the events.

This is an advanced feature that requires extensive testing and a squeaky clean data set/Tracking Plan to enable. To get a sense of which events will be blocked, or properties omitted, go to the Violations view for a source and note all events with a violation. For example, if you added a 
subscription_id
 required property to your 
Subscription Cancelled
 event in your Tracking Plan, the below track call would be either blocked by Protocols, or the property would be omitted, depending on your settings.

    
analytics
.
track
(
'
Subscription Cancelled
'
,
 
{
customer_type
:
 
'
enterprise
'
})



IMPORTANT: JSON schema violation event blocking is ONLY supported in cloud-mode Destinations. Events with invalid properties will not be blocked from sending to device-mode Destinations.

Identify Calls - Unplanned Traits

Setting this dropdown to Omit Traits will ensure that traits not defined in your Tracking Plan are removed from the identify call. For example, if you specify three traits in your Tracking Plan (
name
, 
email
, 
join_date
), the below identify call would have the 
first_name
 property omitted by Protocols.

    
analytics
.
identify
(
'
fe923fjid
'
,
 
{
email
:
 
'
roger@example.com
'
,
 
first_name
:
 
'
Roger
'
})



IMPORTANT: Unplanned identify trait blocking is ONLY supported in cloud-mode Destinations. Events with invalid traits will not be blocked from sending to device-mode Destinations.

Block Identify Calls - Common JSON Schema Violations

JSON schema violation event blocking only supports cloud-mode destinations

Events with invalid properties are not blocked from device-mode destinations.

To block all Identify calls that generate a common JSON schema violation:

This page was last modified: 12 Feb 2025

Need support?

Questions? Problems? Need more info? Contact Segment Support for assistance!

Help improve these docs!

Was this page helpful?

Thanks for your feedback!

Can we improve this doc? 
Send us feedback!

Get started with Segment

On this page

Was this page helpful?

Thanks for your feedback!

Can we improve
 this doc? 
Send us feedback!

Product

For Developers

Company

Support

© 2025 Segment.io, Inc.

=== Content from https://segment.com/docs/engage/faqs/ ===

Engage FAQs
        

Engage Foundations requires a Business tier account and includes Unify.


See the 
available plans
, or 
contact Support
.

On this page

Do you have an Audiences API?

Yes. You can learn more about the Audience API by visiting the 
Segment Public API documentation
.

Can I programmatically determine if a user belongs to a particular audience?

Yes. Because Engage creates a trait with the same name as your audience, you can query the Profile API to determine if a user belongs to a particular audience. For example, to determine if the user with an email address of 
bob@example.com
 is a member of your 
high_value_users
 audience, you could query the following Profile API URL:

https://profiles.segment.com/v1/namespaces/<namespace_id>/collections/users/profiles/email:bob@segment.com/traits?include=high_value_users


The following response indicates that Bob is a high-value user:

{

  
"traits"
:
 
{

    
"high_value_users"
:
 
true
,

  
},

  
"cursor"
:
 
{

    
"has_more"
:
 
false
,

  
}


}



For more information on profile queries, visit the 
Profile API documentation
.

Can I modify audience keys?

You can’t change the audience key after it’s created. To change the key, you need to re-create the audience.

Can I reuse audience keys?

Avoid using the same audience key twice, even if you’ve deleted the key’s original audience. Downstream tools and destinations might have trouble distinguishing between different audiences that once shared the same key. This may create mismatch in audience size between Segment and the destination because the destination may count users of the old audience, resulting in a larger audience size.

How do historical lookback windows work?

Engage allows you to compute new traits and audiences of your users based on their entire customer journey, and all historical data you’ve tracked with Segment.

When you create a new computed trait or audience, you include a lookback window that determines how far back into the past the trait or audiences will be computed.



Some important things to keep in mind when setting a lookback window:

Historical lookback windows are based on the event 
timestamp
 field.

Lookback windows are precise down to the hour, so a 90-day lookback window will include any events with a 
timestamp
 timestamp within the last 2,160 hours (24 hr/day * 90 days).

The trait and audience will automatically update going forward as historical events exceed the lookback window.

What are Funnel Audiences?

Funnel Audiences allow you to use 
strict, relative ordering
 for your audience conditions. Common use cases for these audiences are Cart Abandonment (users that triggered the Product Added event but did not trigger the Order Completed event after the Product Added event occurred) and onboarding steps (users that Added Credit Card but did not Subscribe afterward).

To get started with Funnel Audiences, go to:

Audiences > New > Select Funnel Condition
 (“and then did not”/”and then did”)

The funnel condition will now be relative to the parent condition.

The audience in the image below includes all users that have Product Added in the last week, but not Order Completed within a day of doing so.



Funnel Audiences compute based on all instances of the parent event within the lookback period. This means that if you have a user that Product Added ⟶ Order Completed ⟶ Product Added, this user would be entered into the Abandoned Cart state despite having previously completed an order.

What is Engage Merge Protection?

Engage’s merge protection algorithm protects your identity graph from unnecessary merges by finding and removing untrusted external IDs. Here’s an example:



In this example, 
anonymous_id: a1
 is not reset during a 
User Logout
. Without merge protection rules, Segment would merge 
user_id u1
 and 
user_id u2
. Instead, the identity resolution algorithm detects that such a merge would break 
user_id
 uniqueness and prevents the merge.

This is especially helpful for preventing “blob users” that are merged together by non-unique anonymous IDs or by common group emails like 
team@company.com
.

Which destinations support syncing the identity graph?

Most destinations on the Segment Platform are built up around a user model. They assume that a user will have a single userId. Further, most Destinations are not built to handle anonymous traffic.

By default, Segment doesn’t sync the output of the Identity Graph to Destinations. However, Segment computed traits and audiences are based on the entire user profile, including anonymous and merged data. Segment syncs the value of these computations (for example, 
blog_posts_ready_30_days: 10
) using all 
userIds
 on the profile.

For Destinations that support an 
alias
 call (for example, Mixpanel), you can emit an 
alias
 call on merge.

What Sources can I sync to Engage?

The following list shows just some data sources you can sync to Engage:

Can I send audiences to multiple destination accounts?

Yes, Engage supports the ability to send an audience or computed trait to two or more accounts of the same partner. The most common use case is multiple Facebook, or Adwords ad accounts.

Why am I getting alerts about an audience/computed trait sync failure, but when I look at the specific audience/computed trait it shows a successful sync?

An audience/computed trait run or sync may fail on its first attempt, but Engage will retry up to five times before considering it a hard failure that displays on the audience/compute trait’s overview page. As long as the runs/syncs within the specific audience’s overview page indicate success, you can ignore any failure alerts.

How things work internally:

Segment’s Engage scheduler fetches audiences/traits from the compute service and then handles the logic of generating tasks. These compute/sync tasks get scheduled and executed by another worker. These tasks are a list of steps to be executed. Each task has a series of steps that Segment marks as complete by saving a timestamp for the completion. If something disrupts the worker, it picks up at the latest step without a 
completed_at
 timestamp. In some cases, the step or entire task might fail due to timeout or worker disruption. No matter the cause, Segment will retry any failures.

The audit trail’s configuration notifies about every task failure, even if the failure later succeeds. In most cases, you won’t need to track these failures, unless you notice actual computation or sync failures.

If you don’t want to receive notifications for temporary failures, 
reach out to support
. Upon request, Segment can disable temporary failure notifications, which will reduce the number of notifications your workspace receives.

Why is the user count in a journey step greater than the entry/previous step of the journey?

Each step of a Journey is an Engage audience under the hood. The conditions stack, so a user must be a member of the previous step (audience) and meet all conditions to be added to subsequent steps. However, if the user no longer meets entry conditions for a particular step, they’ll exit and you’ll see the user count reduced. For any subsequent steps a user is still a part of, they’ll remain until they no longer meet entry conditions.

Why were multiple audience-entered events triggered for the same user?

Multiple audience events can trigger for a user if any of the following conditions occur:
1) There is a merge on the user.
2) An 
external_id
 was added to the profile.
3) The user has 
multiple identifiers of the same type
. Segment sends one event per identifier for each audience or computed trait event.
4) The 
include anonymous users
 option is selected for an audience. Segment sends an event for every 
anonymousId
 on the user profile.

Why am I not seeing standard source events on the Engage source, even though it has been connected through “Unify -> Unify Settings -> Profile Sources” page?

Based on Engage behavior, standard source events such as Page, Track and Identify calls aren’t visible on the Engage source. The Engage source tracks and manages events related to audiences and computed traits within the Engage space. This includes events generated by changes in audience membership or computed trait calculations or when a user profile has been created in the Engage space. These are distinct from the typical Page calls, Track calls, or Identify calls (user interaction events) that you would observe in a standard Segment source.

Why can’t I connect the audience/computed trait to an existing destination in my workspace?

Engage will not allow you to connect an audience/computed trait to a destination that is already linked to a 
Connections-based source
. Instead, create a new instance of the destination with the correct Engage space selected as the data source.

How are the “5 most common values” for traits calculated?

The “5 most common values” are the most frequently observed values for a given trait across all users, not tied to any individual user.

This page was last modified: 24 Oct 2024

Need support?

Questions? Problems? Need more info? Contact Segment Support for assistance!

Help improve these docs!

Was this page helpful?

Thanks for your feedback!

Can we improve this doc? 
Send us feedback!

Get started with Segment

On this page

Was this page helpful?

Thanks for your feedback!

Can we improve
 this doc? 
Send us feedback!

Product

For Developers

Company

Support

© 2025 Segment.io, Inc.

=== Content from https://segment.com/docs/protocols/enforce/forward-blocked-events/ ===

Forward blocked events
        

Protocols is available as an add-on for Business plans only.


See the 
available plans
, or 
contact Support
.

If you’re concerned about permanently discarding blocked events, you can enable blocked event forwarding on a Segment Source. To set up forwarding, navigate to the settings tab of the Source, then Schema Configuration.

Select the source you’ll forward events to from the Blocked Events and Traits dropdown. Segment recommends that you create a new Source for forwarded events to avoid contaminating production data and enable blocking only when you are confident about the quality of your data.

Since forwarding happens server to server, Segment recommends creating a 
HTTP Tracking API source
, though any server-side source will work.



Blocked events and MTUs

Only blocked events are forwarded to the source, and count toward your MTU limits. Events with omitted traits are not forwarded, and do not contribute to your MTU counts. Instead, Segment inserts a 
context.protocols
 object into the event payload which contains the omitted properties or traits.

This page was last modified: 07 Feb 2025

Need support?

Questions? Problems? Need more info? Contact Segment Support for assistance!

Help improve these docs!

Was this page helpful?

Thanks for your feedback!

Can we improve this doc? 
Send us feedback!

Get started with Segment

Was this page helpful?

Thanks for your feedback!

Can we improve
 this doc? 
Send us feedback!

Product

For Developers

Company

Support

© 2025 Segment.io, Inc.

=== Content from https://segment.com/docs/connections/sources/catalog/libraries/website/javascript/custom-proxy/ ===

Self-Managed Custom Proxy
                

On this page

Custom proxies allow you to proxy Analytics.js and all tracking event requests through your own domain.

You cannot use custom proxy setup for Analytics.js CDN or Tracking API with device-mode destinations because it requires that the destination’s native scripts are loaded onto the client, and the requests are sent directly to the destination.

Business Tier customers can also use Custom Domain

Custom Domain is a fully-managed service that enables you to configure a first-party subdomain over HTTPS to track event requests through your domain instead of tracking events through Segment’s default domain. For more information, see the 
Custom Domain
 documentation.

Segment’s domain delegation solutions

Segment offers two domain delegation solutions: 
Custom Proxy
 and 
Custom Domain
. If you use Custom Domain, you can choose to use either DNS delegation or a Cannonical Name (CNAME). Segment recommends using Custom Domain with DNS delegation, which leads to easy setup, maintenance, and monitoring.

*
If it’s not possible for you to delegate subdomains to Segment, you can use a CNAME instead. Segment encourages users to delegate a DNS subdomain rather than use use CNAME aliasing due to the evolving privacy standards in browsers, but CNAME aliasing remains an option for users not interested in using nameservers.

Custom Proxy prerequisites

To set up a custom proxy, you need:

Custom Proxy Troubleshooting

If you experience issues configuring a custom proxy, contact your organization’s IT department for help. Segment does not have access to the resources you need to configure a custom proxy.

This guide explains how to set up a custom proxy in CloudFront. You can apply these principles to almost any modern CDN that supports proxies.

You need to set up two important parts, regardless of the CDN provider you use:

If you are using a 
Regional Workspace
, please note that instead of using 
api.segment.io
 to proxy the Tracking API, you’ll be using 
events.eu1.segmentapis.com

Segment only has the ability to enable the proxy setting for the Web (Analytics.js) source. Details for mobile source proxies are in the 
Analytics-iOS
 and 
Analytics-Android
 documentation.  It is not currently possible to set up a proxy for server sources using the Segment UI.

Segment loads most integrations through the proxy, except for third-party SDKs

Third-party SDKs are loaded by a partner’s CDN, even with a Segment proxy configured. For example, if you have  a Segment custom proxy enabled and send data to a FullStory destination, FullStory’s CDN would load the FullStory SDK.

Custom Proxy setup

There are two options you can choose from when you set up your custom domain proxy.

Follow the directions listed for 
CloudFront
 or 
use your own CDN setup
. Once you complete those steps and verify that your proxy works for both 
cdn.segment.com
 and 
api.segment.io
, 
contact Segment Product Support
 with the following template email:

Hi,

This is {person} from {company}. I would like to configure a proxy for the following source(s):

**Source URL**: link to the source in your Segment workspace (for example: https://app.segment.com/<your_slug>/sources/<source>/overview)
**Source ID**: navigate to **API Keys** on the left-hand side of the source **Settings** and provide the Source ID 


Double-check the Source URL and the Source ID.

A Segment Customer Success team member will respond that they have enabled this option for your account. When you receive this confirmation, open the source in your workspace, and navigate to Settings > Analytics.js. Update the 
Host Address
 setting from 
api.segment.io/v1
 to 
[your proxy host]/v1
.

The 
Host Address
 field does not appear in source settings until it’s enabled by Segment Customer Success.

There should be no downtime once the setup is complete, as the default Segment domains continue to work alongside the customer’s domains.

Custom CDN / API Proxy

Follow these instructions after setting up a proxy such as 
CloudFront
. Choose between the 
snippet instructions
 or the 
npm instructions
.

If you’ve followed the instructions above to have a Segment team member enable the apiHost settings in the UI, you can skip the instructions in this section.

Snippet instructions

If you’re a snippet user, modify the 
analytics snippet
 located inside the 
<head>
 of your website:

To proxy CDN settings and destination requests that typically go to 
https://cdn.segment.com
, replace:

- t.src="https://cdn.segment.com/analytics.js/v1/" + key + "/analytics.min.js"

+ t.src="https://MY-CUSTOM-CDN-PROXY.com/analytics.js/v1/" + key + "/analytics.min.js"


To proxy API tracking calls that typically go to 
api.segment.io/v1
, replace:

- analytics.load("<MY_WRITE_KEY>")

+ analytics.load("<MY_WRITE_KEY>", { integrations: { "Segment.io": { apiHost: "MY-CUSTOM-API-PROXY.com/v1" }}})


npm instructions

If you’re using the 
npm library
, make the following changes directly in your code:

To proxy settings and destination requests that typically go to 
https://cdn.segment.com
 through a custom proxy:

const
 
analytics
 
=
 
AnalyticsBrowser
.
load
({

  
writeKey
,

  
// GET https://MY-CUSTOM-CDN-PROXY.com/v1/projects/<writekey>/settings --> proxies to

  
// https://cdn.segment.com/v1/projects/<writekey>/settings


  
// GET https://MY-CUSTOM-CDN-PROXY.com/next-integrations/actions/...js  --> proxies to

  
// https://cdn.segment.com/next-integrations/actions/...js

  
cdnURL
:
 
'
https://MY-CUSTOM-CDN-PROXY.com
'

 
})



To proxy tracking calls that typically go to 
api.segment.io/v1
, configure the 
integrations['Segment.io'].apiHost
:

const
 
analytics
 
=
 
AnalyticsBrowser
.
load
(

    
{

      
writeKey
,

      
cdnURL
:
 
'
https://MY-CUSTOM-CDN-PROXY.com
'

    
},

    
{

      
integrations
:
 
{

        
'
Segment.io
'
:
 
{

          
// POST https://MY-CUSTOM-API-PROXY.com/v1/t --> proxies to

          
// https://api.segment.io/v1/t

          
apiHost
:
 
'
MY-CUSTOM-API-PROXY.com/v1
'
,

          
protocol
:
 
'
https
'
 
// optional

        
}

      
}

    
}

  
)



Custom Proxy CloudFront

These instructions refer to Amazon CloudFront, but apply more generally to other providers as well. Before changing the Segment Tracking API or the Segment snippet (Segment CDN) to use your new proxy, complete the custom domain proxy setup on your side to avoid any unexpected behavior.

CDN Proxy

To set up your CDN Proxy:

Take note of the Domain Name for use in the next step.

To add a CNAME record for the Segment proxy to your organizations DNS settings:

Tracking API Proxy

As events travel through the proxy before reaching the tracking API, set up a proxy for the tracking API so that all calls proxy through your domain. To do this, set up a CloudFront distribution that’s similar to the one in the previous section, with the exception of the Origin Domain Name:

To add a CNAME record to your DNS settings:

Common issues

These are some common issues that occur for customers implementing a custom proxy. This is not an exhaustive list, and these CloudFront or Cloudflare settings may change.

A 403 error can mean that you’ve misconfigured your Cloudflare CDN distribution. Try one of the following options to fix the error:

If you have a Cloudflare enterprise plan, create a Page Rule in Cloudflare so that Segment’s CDN doesn’t refuse the requests made through the Cloudflare Proxy. If 
cdn.segment.com
 is another CNAME that resolves to 
xxx.cloudfront.net
, you will need to use a Page Rule in Cloudflare to override the host header to match the hostname for proxy requests. For more information about overriding the host header, see Cloudflare’s 
Rewrite Host headers
 docs.

For customers who are not on the Cloudflare Enterprise plan, use Cloudflare Workers. Workers usually run on the main domain (for example, 
www.domain.com
), but if you want Workers to run on a subdomain, like 
http://segment.domain.com
, you must record the subdomain in your DNS. For more information, see Cloudflare’s 
Routes and domains
 documentation.

When creating a Worker you can use this example provided by Cloudflare in their 
Bulk origin override
 documentation with the origins set to:

const
 
ORIGINS
 
=
 
{


"
yourcdndomain.com
"
:
 
"
cdn.segment.com
"
,


}



In order to resolve a CORS OPTIONS pre-request fetch error, you must specify “Strict (SSL-Only Origin Pull)” as a Cloudflare Page rule for the 
api.segment.io
 proxy. Please see Cloudflare’s 
Encryption modes
 documentation for more details.

If your CloudFront Proxy is returing a 403 error, the following change in CloudFront might resolve the issue:

Before
:


Cache
 
Based
 
on
 
Selected
 
Request
 
Headers
:
 
All



After
:


Cache
 
Based
 
on
 
Selected
 
Request
 
Headers
:
 
None



Alternatively, this setting may solve your issue:

Before
:


Origin
 
request
 
policy
:
 
AllViewer



After
:


Origin
 
request
 
policy
:
 
None



CloudFront CORS issue

To resolve a CORS issue, you might need to add a referrer header in the request you send to Segment. Follow AWS’s 
How do I resolve the “No ‘Access-Control-Allow-Origin’ header is present on the requested resource” error from CloudFront?
 guide, which explains how to add a referrer header.

Self-hosting Analytics.js

To reduce fetching assets from Segment’s CDN, you can bundle Analytics.js with your own code.

To bundle Analytics.js with your own code, you can:

Use Analytics.js as an npm package
.

Use npm to install your destinations
.

Hardcode your settings instead of fetching from the CDN (Segment doesn’t recommend this as it completely bypasses the Segment source GUI).

// npm-only


export
 
const
 
analytics
 
=
 
new
 
AnalyticsBrowser
()


analytics
.
load
({

 
...

 
cdnSettings
:
 
{...}
 
// object from https://cdn.segment.com/v1/projects/<YOUR_WRITE_KEY>/settings'

 
})



Restore the API host to the Segment default

If you wish to restore the proxied API host to it’s original value:

Any changes made to the CDN host must be update manually in your code.

This page was last modified: 27 Jan 2025

Need support?

Questions? Problems? Need more info? Contact Segment Support for assistance!

Help improve these docs!

Was this page helpful?

Thanks for your feedback!

Can we improve this doc? 
Send us feedback!

Get started with Segment

On this page

Was this page helpful?

Thanks for your feedback!

Can we improve
 this doc? 
Send us feedback!

Product

For Developers

Company

Support

© 2025 Segment.io, Inc.

=== Content from https://segment.com/docs/unify/faqs/ ===

Unify FAQs
        

Unify requires a Business tier account and is included with Engage.


See the 
available plans
, or 
contact Support
.

On this page

Does your identity model support multiple external ID types?

Yes, Identity Graph supports multiple external IDs.

Identity Graph automatically collects a rich set of external IDs without any additional code:

If you want Identity Graph to operate on a different custom ID, you can pass it in using 
context.externalIds
 on an 
Identify
 or 
Track call
. If you’re interested in this feature, contact your CSM to discuss the best way to implement this feature.

How does Unify handle identity merging?

Segment analyzes each incoming event and extracts external IDs (like 
user_id
, 
anonymous_id
, 
email
). The simplified algorithm works as follows:

Is all matching deterministic, or is there any support for probabilistic matching?

All Profile matching is deterministic and based on first-party data that you’ve collected.

Segment doesn’t support probabilistic matching. Most marketing automation use cases require 100% confidence that a user is who you think they are (sending an email, delivering a recommendation, and so on). The best way to support this is through a deterministic identity algorithm.

What happens to conflicting and non-conflicting profile attributes?

If two merged user profiles contain conflicting profile attributes, Segment selects the newest, or last updated, attributes when querying the profile.

What identifiers can the merged profile be queried/updated with?

Any of the external IDs can be used to query a profile. When a profile is requested, Segment traverses the merge graph and resolves all merged profiles. The result is a single profile, with the latest state of all traits, events, and identifiers.

Can external IDs be changed or removed from the profiles?

No. As the Identity Graph uses external IDs, they remain for the lifetime of the user profile.

Can I delete specific events from a user profile in Unify?

No. Alternatively, you may delete the entire user profile from Segment using a 
GDPR deletion request
.

How does profile creation affect MTUs, particularly where a profile isn’t merged with the parent profile due to exceeding the merge limit?

Segment determines the Monthly Tracked Users (MTUs) count by the number of unique user IDs and anonymous IDs processed, regardless of how you manage these profiles in Unify and Engage. This count is taken as events are sent to Segment, before they reach Unify and Engage. Therefore, the creation of new profiles or the merging of profiles in Unify doesn’t affect the MTU count. The MTU count only increases when you send new unique user or anonymous IDs to Segment.

What is the event lookback period on the Profile Explorer?

The 
Profile Explorer
 retains event details for a period of up to 2 weeks. If you need event information beyond this timeframe, Segment recommends using 
Profiles Sync
 for comprehensive event analysis and retention.

Can I remove a trait from a user profile?

Yes, you can remove a trait from a user profile by sending an Identify event with the trait value set to 
null
 in the traits object from one of your connected sources. For example:

{

  
"traits"
:
 
{

    
"trait1"
:
 
null

  
}


}



Setting the trait value to an empty string won’t remove the trait, like in this example:

{

  
"traits"
:
 
{

    
"trait2"
:
 
""

  
}


}



Instead, this updates the trait to an empty string within the user profile.

This page was last modified: 18 Nov 2024

Need support?

Questions? Problems? Need more info? Contact Segment Support for assistance!

Help improve these docs!

Was this page helpful?

Thanks for your feedback!

Can we improve this doc? 
Send us feedback!

Get started with Segment

On this page

Was this page helpful?

Thanks for your feedback!

Can we improve
 this doc? 
Send us feedback!

Product

For Developers

Company

Support

© 2025 Segment.io, Inc.

=== Content from https://segment.com/docs/unify/data-graph/linked-events// ===

Linked Events Overview
        

Unify requires a Business tier account and is included with Engage.


See the 
available plans
, or 
contact Support
.

On this page

Use Linked Events to enrich real-time event streams with entities from your data warehouse to your destinations. Insert additional event context for downstream applications for richer data about each event.

Consent enforcement for Linked Events

You can use 
Consent Management
 to enforce consent in your downstream destinations for Linked Events stamped with the 
consent object
. You must enable Consent Management and have consent stamped on events from event streaming sources to use Consent Management. You cannot use Linked Events to enrich events with consent preferences that are stored in your warehouse.

On this page, you’ll learn how to get started with Linked Events.

Linked Events warehouse support

Linked Events supports Snowflake, BigQuery, Redshift, and Databricks.

Use cases

With Linked Events, you can:

Prerequisites

To use Linked Events, you’ll need the following:

Segment stores and processes all data in the United States.

Profiles Sync isn’t required for Linked Events.

Linked Events roles

The following Segment access 
roles
 apply to Linked Events:

Entities Admin Access
: Entities Admins have the ability to view and edit entity models and connection details.

Entities Read-only Access
: Entities Read-only users have the ability to view entity models.

To create models and enrich events in destinations, you need to be a 
Workspace Owner
 or have the following roles:

Step 1: Set up your data warehouse and permissions

Linked Events uses Segment’s 
Reverse ETL
 infrastructure for pulling in data from your warehouse.

To get started, you’ll need to set up your data warehouse and provide the correct access detailed in the set up steps below. Linked Events supports 
BigQuery
, 
Databricks
, 
Snowflake
, and 
Redshift
.

Step 2: Connect your warehouse to the Data Graph

Before getting started with the Data Graph, be sure to set up your warehouse permissions.

Schema

Linked Events uses Reverse ETL to compute the incremental changes to your data directly within your data warehouse. The Unique Identifier column detects data changes, such as new, updated, and deleted records.

For Segment to compute data changes in your warehouse, Segment requires both read and write permissions to the warehouse schema table. At a high level, the extract process requires read permissions for the query being executed. Segment tracks changes to the query results through tables that Segment manages in a dedicated schema (for example, 
_segment_reverse_etl
), which requires some write permissions.

Only sync what you need for enrichment. There may be cost implications to having Segment query your warehouse tables.

Linked Events syncs data from your warehouse approximately once every hour.

Supported data warehouses

The table below shows the data warehouses Linked Events supports. View the Segment docs for your warehouse, then carry out the corresponding steps.

Step 3: Build your Data Graph

The Data Graph is a semantic layer that represents a subset of relevant business data that you’ll use to enrich events in downstream tools. Use the configuration language spec below to add models to build out your Data Graph.

Each Unify space has one Data Graph. The current version is v0.0.6 but this may change in the future as Segment accepts feedback about the process.

Deleting entities and relationships are not yet supported.

Defining entities

Snowflake schemas are case sensitive, so you’ll need to reflect the schema, table, and column names based on how you case them in Snowflake.

An entity is a stateful representation of a business object. The entity corresponds to a table in the warehouse that represents that entity.

# Define an entity and indicate if the entity will be referenced for Linked Events (enrichment_enabled=true)



entity
 
"
account-entity
"
 
{

     
name
 
=
 
"
account
"

     
table_ref
 
=
 
"
CUST.ACCOUNT
"

     
primary_key
 
=
 
"
id
"

     
enrichment_enabled
 
=
 
true


}



Step 4: Add an actions-based destination

To use Linked Events, you’ll need to add an action destination to send enriched events to. Navigate to 
Connections > Destinations
. Select an existing action destination, or click 
+ Add destination
 to add a new action destination.

For Linked Events, Segment supports 
Destination Actions
 in cloud-mode only.

Step 5: Enrich events with entities

With Linked Events, you can select entities and properties from your data warehouse, then add enrichments to map properties to your connected destination.

To enrich events with entities:

Configure the sync schedule

You can schedule how often you want Segment to cache the table data for Linked Events.

To configure your sync schedule:

Add entities

After you load a sample event, you can add entities from the 
Enrich events with entities
 section. You’ll select an entity, then an entity match property.

After you’ve added an entity and match property, add your event enrichments.

Add enrichments

Use enrichments to select the entity you wish to send to your downstream destination.

In the Mappings tab, locate the 
Select Mappings
 section where you can enrich source properties from the entities you’ve selected in the previous step.

At this time, Linked Events doesn’t support a preview of enriched payloads.

Save your enrichments

When you’re satisfied with the mappings, click 
Save
. Segment returns you to the Mappings table.

At this time, when you select mappings or test events, you won’t see enrichment data. Enrichment data is only available with real events.

Enrichment observability

To verify which of your events matched one or more enrichments:

FAQs

To use Linked Events, be sure that you have proper permissions for the Data Warehouse you’re using. Visit the 
BigQuery
, 
Databricks
, 
Snowflake
, and 
Redshift
 setup guides to learn more about updating permissions.

Segment currently syncs once every hour.

For Linked Events, Segment supports all actions-based destinations in cloud-mode. Device-mode destinations are not supported.

Test events don’t send Linked Events. You’ll only see test events that come from the source debugger, which is ahead of the event enrichment.

Linked Events uses the existing Audit Trail in your Segment workspace. To view your Audit Trail, navigate to 
Settings > Admin > Audit Trail
.

You can define a schedule for refreshing the linked data from your data warehouse.

To use entities with Linked Events, you’ll need to set the 
enrichment_enabled
 flag to 
true
. Here’s the sample code:

# Define an entity and indicate if the entity will be referenced for Linked Events (enrichment_enabled=true)



entity
 
"
account-entity
"
 
{

     
name
 
=
 
"
account
"

     
table_ref
 
=
 
"
CUST.ACCOUNT
"

     
primary_key
 
=
 
"
id
"

     
enrichment_enabled
 
=
 
true


}



This page was last modified: 06 Feb 2025

Need support?

Questions? Problems? Need more info? Contact Segment Support for assistance!

Help improve these docs!

Was this page helpful?

Thanks for your feedback!

Can we improve this doc? 
Send us feedback!

Get started with Segment

On this page

Was this page helpful?

Thanks for your feedback!

Can we improve
 this doc? 
Send us feedback!

Product

For Developers

Company

Support

© 2025 Segment.io, Inc.

=== Content from https://segment.com/docs/privacy/portal/ ===

Privacy Portal
        

Privacy Portal is available for the listed account plans only.


See the 
available plans
, or 
contact Support
.

On this page

When preparing for new privacy regulations (like HIPAA, the GDPR, or the CCPA), the
best practice is to create a comprehensive data inventory which includes details
about what personal information you collect, where you collect it from, where
you store the data, and who has access to it. The Privacy Portal helps automate
this process for the data you collect with Segment.

When you use Segment as the single point of collection for your customer data, you can use the Privacy Portal to:

Privacy Portal features are available to all Segment workspaces, however only workspace owners can access the Privacy Portal.

Privacy Detection

The Detection page in the Privacy Portal is where you can find out more about
exactly how data is being detected and classified in your workspace. You can
think of it as the brain behind the entire Privacy Portal, filled with the logic
that detects and classifies the data in the first place.

On this page, you can also modify your Detection settings and 
tell Segment how you want us to match data
, so that it is meets your unique business needs.

Default PII Matchers

Out of the box, the Privacy Portal contains matchers for the most common PII
fields. These matchers scan data coming from your Sources for PII based on both
exact-matching (looking for an exact match, such as a field name) and
fuzzy-matching (looking for both exact matches, and any values which are
similar).

In this section of Segment’s Privacy Portal, you can see the fields we match
against by default. The display lists whether we match on the key (for example
the label “CCN”) or value (for example, the payload 123-456-7890) in the

Matches On
 column. You can also see how we classify these matchers by
default in the 
Default Classification
 column.

Below is a full list of automatically detected restricted fields.

When Segment detects data that meets the criteria for one of the default
matchers (in the list above) in properties in your Web, Mobile, Server, or Cloud
Event Sources, we display it in the 
Privacy Portal Inbox
.

Default PII matchers are currently uneditable. If you want to change the behavior of a default matcher, you can create a custom PII matcher that replicates and overwrites the default matcher.

Custom PII Matchers

This is where you can create your very own matchers to tell Segment what to scan
for in your workspace. You can use this feature to detect properties that are
unique to your company or region, or that aren’t already handled by the default
matchers above. You can have up to 100 custom matchers per workspace. Custom
Matchers detect data in your Web, Mobile, Server, and Cloud Event Sources for 
fields under 
context
, 
traits
 and 
properties
 objects, and
the data they detect appears in the Inbox.

For example, if you have a restricted data point at your company called “SIN”
(for “social insurance number”) you can tell Segment’s Privacy Portal how to
treat that property whenever it is appears in data Segment processes.

To create a Custom Matcher:





Unless the field value pattern is unique, we recommend matching on the Key. For
example, for Credit Card Number, it’s better to detect on Keys that look like
“CCN” or “Credit Card Number” instead of trying to detect any values that look
like “1234567890”, because a 10-digit string can be found in all kinds of data
even when it’s not an CCN. For example, the key “Product_ID” could contain a
10-digit string, even though Product_ID does not actually contain an SSN. A
North American phone number (without country code) is also ten digits.

Use Access Roles to control who has access to the PII identified by your matchers.

Custom Matchers use 
regular expressions
 (using the 
Golang Regex Package
) to provide you great flexibility for your matching patterns.

Regular expressions (or regexes) are a way to describe to a computer a pattern
that we’re looking for, using a series of special symbols. For example, if we
want to match all Gmail emails, we’d write the following regex:

@example.com


This pattern matches 
jane@example.com
, 
mike@example.com
 and so on. Regular
expressions can also contain special symbols. One of them is 
\d
 and it tells
the computer to match a single-digit number. In that case, regular expression

Number \d
 would match 
Number 1
, 
Number 2
 and so on. You can match multiple
digits by adding a plus sign (
+
) at the end: 
Year \d+
. This pattern would
match 
Year 2019
, 
Year 2020
, etc.

If you need to match a specific word, you can type the word without any
modifiers. For example, a regular expression of 
Apple
 would only match strings
that contain 
Apple
. You can also change it to match all 
Apple
 or 
Orange

words, by separating the search terms with a “pipe” character, like so:

Apple|Orange
.

Regular expressions have much more flexibility than we can describe here. Check
out the following resources to learn more about regular expressions so you can
build new custom matchers:

Using Synonyms

Segment’s exact matching and fuzzy matching do not detect all variations in the received keys
and for those scenarios, you can use synonyms.  For example, for the value 
credit card number
, you can add 
credit card no
,

debit card number
, 
debit card no
, or similar variations in the synonyms section to classify those fields.


Privacy Inbox

The Inbox helps you keep track of new restricted data types as they are captured, quickly classify them, and build a data Inventory.

Segment detects these fields by scanning data from your Web, Mobile, Server, and Cloud Event Sources to detect PII based on the 
default PII matchers
. New properties sent into Segment appear in the Inbox in realtime.

When you view the Inbox, it displays every property that was sent into Segment from Web, Mobile, Server, and Cloud Event Sources for the past 7 days. (
Object Cloud Sources
 and 
Reverse ETL Sources
 do not appear in the Inbox at this time.)

You can click a row in the Inbox to learn more about a field and where it was collected. The expanded view shows:



To streamline the classification process, Segment pre-classifies the data in the
Privacy Portal Inbox as 
Red
 (likely highly restricted data), 
Yellow

(likely moderately restricted data), and 
Green
 (likely least restricted
data). These colors indicate how restricted the data is for your business. You
can also send and block data from flowing based on its color classification and
how restricted it is.

Segment makes recommendations about how a field should be classified using
built-in PII matcher 
detection
, however, you can always
update the classification in the Inbox based on your company’s requirements.

Change a recommended classification

You can update the classifications to suit your needs by clicking on the color
dropdown menu to change. For example, you might manually change a field that
does not contain personal information in your implementation from a “Yellow”
classification to “Green.”



When you’re satisfied that the fields have been classified appropriately, you
can click 
Add to Inventory
 to officially apply the classification to the
field. This moves the field into your 
Data Inventory
,
which is a central repository of all of the properties 
you
 classified as Red,
Yellow, and Green. Any time you send this field from a Web, Mobile, Server, or
Cloud Event Source — whether from another Source or event type — the Privacy
Portal automatically classifies it and adds it to the Inventory.

Understanding Classification types:

Red Classification
:
Fields that are classified as ‘Red’ are masked for users that do not have PII Access enabled. These fields are also blocked if you have set Standard Controls under Privacy > Settings section.

Keep in mind that if you have set Standard Controls to block fields from any of your sources, any new classifications you create in the Inbox will start to take affect immediately. For example, if you have a Privacy Control set up to block 
Red
 data from your Android source, any new fields you classify in the Inbox as 
Red
 will be blocked from entering Segment from your Android source.

Yellow Classification
:
Fields that are classified as 
Yellow
 are masked for users that do not have PII Access enabled. You need a Custom Matcher to mask fields other than those in the Default PII Matchers list.

Green Classification
:
Classifying a field as ‘Green’ does not have any impact on the behavior of masking of fields within the Segment App, it is only available for the housekeeping purposes.

Once a field has been classified as “Yellow” or “Red”, marking it “Green” will not make it visible for users that don’t have PII access.

Privacy Inventory

The Inventory is a central repository of all of the properties 
you
 classified as Red, Yellow, and Green. Where the 
Inbox
 shows new, unclassified data with Segment’s 
recommended
 classifications, the Inventory only contains data that you explicitly applied Classifications to.

The Inventory is intended to be a Single Source of Truth so you can answer common regulatory questions about the data you’re sending through Segment, for example:

Once you’ve classified the fields as Red, Yellow, and Green in the Inbox, the classified fields appear in the Inventory. You can use the filter at the top left to filter down to specific categories of data (for example, Red data, data from a production environment, data from specific sources).



Click into a field (for example, 
ip
) in the Inventory to open the Inventory
details. The details sheet displays how many times a specific field has been
sent from each Source it comes from. You can click the Events tab to see which
events contained the event, along with the Sources which sent the event. The
data in the side sheet updates in realtime, and includes a limited historical
view.

You can click 
Connected Destinations
 to see which Destinations are
connected to the Source that contains the field. The Access tab displays a list
of who within your organization has access to this field.



Finally, workspace owners can use the 
Download CSV
 button to export a CSV of
their data Inventory to share with their Data Protection Officer (DPO), Chief
Information Security Officer (CISO), legal teams, and more! Note that the CSV
download button includes 
all
 data from your Inventory, and ignores any filters
you applied in the UI.

This page was last modified: 29 Jan 2025

Need support?

Questions? Problems? Need more info? Contact Segment Support for assistance!

Help improve these docs!

Was this page helpful?

Thanks for your feedback!

Can we improve this doc? 
Send us feedback!

Get started with Segment

On this page

Was this page helpful?

Thanks for your feedback!

Can we improve
 this doc? 
Send us feedback!

Product

For Developers

Company

Support

© 2025 Segment.io, Inc.

=== Content from https://segment.com/docs/connections/reverse-etl/system/ ===

Reverse ETL System
        

On this page

View reference information about how Segment detects data changes in your warehouse and the rate and usage limits associated with Reverse ETL.

Record diffing

Reverse ETL computes the incremental changes to your data directly within your data warehouse. The Unique Identifier column is used to detect the data changes, such as new, updated, and deleted records.

Delete Records Payload

The only value passed for deleted records is their unique ID, which can be accessed as 
__segment_id
. As of September 24, 2024, deleted records also contain all columns selected by your model, with 
null
 values in place of data.

For Segment to compute the data changes within your warehouse, Segment needs to have both read and write permissions to the warehouse schema table. At a high level, the extract process requires read permissions for the query being executed. Segment keeps track of changes to the query results through tables that Segment manages in a dedicated schema (for example, 
_segment_reverse_etl
), which requires some write permissions.

There may be cost implications to having Segment query your warehouse tables.

Limits

To provide consistent performance and reliability at scale, Segment enforces default use and rate limits for Reverse ETL.

Usage limits

Reverse ETL usage limits are measured based on the number of records processed to each destination – this includes both successful and failed records. For example, if you processed 50K records to Braze and 50K records to Mixpanel, then your total Reverse ETL usage is 100K records.

Processed records represents the number of records Segment attempts to send to each destination. Keep in mind that not all processed records are successfully delivered, for example, such as when the destination experiences an issue.

Your plan determines how many Reverse ETL records you can process in one monthly billing cycle. When your limit is reached before the end of your billing period, your syncs will pause and then resume on your next billing cycle. To see how many records you’ve processed using Reverse ETL, navigate to 
Settings > Usage & billing
 and select the 
Reverse ETL
 tab.

If you have a non-standard or high volume usage plan, you may have unique Reverse ETL limits or custom pricing. To see your Reverse ETL limits in the Segment app, select 
Settings > Usage & Billing
.

Configuration limits

Extract limits

The extract phase is the time spent connecting to your database, executing the model query, updating internal state tables and staging the extracted records for loading.

*
: 
If Segment identifies a sync would be larger than 150 million records, Segment extracts 150 million of the records in the initial sync and syncs any additional records during the next scheduled or manual sync.

For example, if a sync would contain 700 million records, Segment would run an initial 150 million record sync, and during the next three scheduled or manual syncs, would sync 150 million records. The fifth scheduled or manual sync would contain the remaining 100 million records.

This page was last modified: 25 Sep 2024

Need support?

Questions? Problems? Need more info? Contact Segment Support for assistance!

Help improve these docs!

Was this page helpful?

Thanks for your feedback!

Can we improve this doc? 
Send us feedback!

Get started with Segment

On this page

Was this page helpful?

Thanks for your feedback!

Can we improve
 this doc? 
Send us feedback!

Product

For Developers

Company

Support

© 2025 Segment.io, Inc.

=== Content from https://segment.com/docs/connections/functions/source-functions/ ===

Source Functions
        

On this page

Source functions allow you to gather data from any third-party applications without worrying about setting up or maintaining any infrastructure.

All functions are scoped to your workspace, so members of other workspaces cannot view or use them.

Functions is available to all customer plan types with a free allotment of usage hours. Read more about 
Functions usage limits
, or see 
your workspace’s Functions usage stats
.



Create a source function

After you click 
Build
, a code editor appears. Use the editor to write the code for your function, configure settings, and test the function’s behavior.

Tip:
 Want to see some example functions? Check out the templates available in the Functions UI, or in the open-source 
Segment Functions Library
. (Contributions welcome!)



Code the source function

Source functions must have an 
onRequest()
 function defined.
This function is executed by Segment for each HTTPS request sent to this function’s webhook.

async
 
function
 
onRequest
(
request
,
 
settings
)
 
{

  
// Process incoming data


}



The 
onRequest()
 function receives two arguments:

Request processing

To parse the JSON body of the request, use the 
request.json()
 method, as in the following example:

async
 
function
 
onRequest
(
request
)
 
{

  
const
 
body
 
=
 
request
.
json
()

  
console
.
log
(
'
Hello
'
,
 
body
.
name
)


}



Use the 
request.headers
 object to get values of request headers.
Since it’s an instance of 
Headers
, the API is the same in both the browser and in Node.js.

async
 
function
 
onRequest
(
request
)
 
{

  
const
 
contentType
 
=
 
request
.
headers
.
get
(
'
Content-Type
'
)

  
const
 
authorization
 
=
 
request
.
headers
.
get
(
'
Authorization
'
)


}



To access the URL details, refer to 
request.url
 object, which is an instance of 
URL
.

async
 
function
 
onRequest
(
request
)
 
{

  
// Access a query parameter (e.g. `?name=Jane`)

  
const
 
name
 
=
 
request
.
url
.
searchParams
.
get
(
'
name
'
)


}



You can send messages to the Segment API using the 
Segment
 object:

async
 
function
 
onRequest
(
request
)
 
{

  
Segment
.
identify
({

    
userId
:
 
'
user_id
'
,

    
traits
:
 
{

      
name
:
 
'
Jane Hopper
'

    
}

  
})


  
Segment
.
track
({

    
event
:
 
'
Page Viewed
'
,

    
userId
:
 
'
user_id
'
,

    
properties
:
 
{

      
page_name
:
 
'
Summer Collection 2020
'

    
}

  
})


  
Segment
.
group
({

    
groupId
:
 
'
group_id
'
,

    
traits
:
 
{

      
name
:
 
'
Clearbit
'

    
}

  
})


  
Segment
.
set
({

    
collection
:
 
'
products
'
,

    
id
:
 
'
product_id
'
,

    
properties
:
 
{

      
name
:
 
'
Nike Air Max
'

    
}

  
})


}



Use 
Identify calls
 to connect users with their actions, and to record traits about them.

Segment
.
identify
({

  
userId
:
 
'
user_id
'
,

  
traits
:
 
{

    
name
:
 
'
Jane Hopper
'

  
}


})



The 
Segment.identify()
 method accepts an object with the following fields:

Track calls
 record actions that users perform, along with any properties that describe the action.

Segment
.
track
({

  
event
:
 
'
Page Viewed
'
,

  
userId
:
 
'
user_id
'
,

  
properties
:
 
{

    
page_name
:
 
'
Summer Collection 2020
'

  
}


})



The 
Segment.track()
 method accepts an object with the following fields:

Group calls
 associate users with a group, like a company, organization, account, project, or team.

Segment
.
group
({

  
groupId
:
 
'
group_id
'
,

  
traits
:
 
{

    
name
:
 
'
Clearbit
'

  
}


})



The 
Segment.group()
 method accepts an object with the following fields:

Page calls
 record whenever a user sees a page of your website, along with any other properties about the page.

Segment
.
page
({

  
name
:
 
'
Shoe Catalog
'
,

  
properties
:
 
{

    
url
:
 
'
https://myshoeshop.com/catalog
'

  
}


})



The 
Segment.page()
 method accepts an object with the following fields:

Screen calls
 record when a user sees a screen, the mobile equivalent of 
Page
, in your mobile app.

Segment
.
screen
({

  
name
:
 
'
Shoe Feed
'
,

  
properties
:
 
{

    
feed_items
:
 
5

  
}


})



The 
Segment.screen()
 method accepts an object with the following fields:

The 
Alias call
 merges two user identities, effectively connecting two sets of user data as one.

Segment
.
alias
({

  
previousId
:
 
'
old-email@example.com
'
,

  
userId
:
 
'
new-email@example.com
'


})



The 
Segment.alias()
 method accepts an object with the following fields:

The Set call uses 
the object API
 to save object data to your Redshift, BigQuery, Snowflake, or other data warehouses supported by Segment.

Segment
.
set
({

  
collection
:
 
'
products
'
,

  
id
:
 
'
product_id
'
,

  
properties
:
 
{

    
name
:
 
'
Nike Air Max 90
'
,

    
size
:
 
11

  
}


})



The 
Segment.set()
 method accepts an object with the following fields:

When you use the 
set()
 method, you won’t see events in the Source Debugger. Segment only sends events to connected warehouses.

Runtime and dependencies

On March 26, 2024, Segment is upgrading the Functions runtime environment to Node.js v18, which is the current long-term support (LTS) release.

This upgrade keeps your runtime current with industry standards. Based on the 
AWS Lambda
 and 
Node.js
 support schedule, Node.js v16 is no longer in 
Maintenance LTS
. Production applications should only use releases of Node.js that are in 
Active LTS
 or 
Maintenance LTS
.

All new functions will use Node.js v18 starting March 26, 2024.

For existing functions, this change automatically occurs as you update and deploy an existing function. Segment recommends that you check your function post-deployment to ensure everything’s working. Your function may face issues due to the change in sytax between different Node.js versions and dependency compatibility.

Limited time opt-out option 


If you need more time to prepare, you can opt out of the update before March 19, 2024. 
 Note that if you opt out: 

- The existing functions will continue working on Node.js v16. 

- You won’t be able to create new functions after July 15, 2024. 

- You won’t be able to update existing functions after August 15, 2024. 

- You won’t receive future bug fixes, enhancements, and dependency updates to the functions runtime. 


Contact Segment
 to opt-out or with any questions. 

Node.js 18 

Segment strongly recommends updating to Node.js v18 to benefit from future runtime updates, the latest security, and performance improvements.

Functions do not currently support importing dependencies, but you can 
contact Segment Support
 to request that one be added.

The following dependencies are installed in the function environment by default.

zlib v1.0.5
 exposed as 
zlib.zlib

 
uuidv5
 is exposed as an object. Use 
uuidv5.uuidv5
 to access its functions. For example:

  
async
 
function
 
onRequest
(
request
,
 
settings
)
 
{

       
uuidv5
 
=
 
uuidv5
.
uuidv5
;

       
console
.
log
(
typeof
 
uuidv5
);


        
//Generate a UUID in the default URL namespace

        
var
 
urlUUID
 
=
 
uuidv5
(
'
url
'
,
 
'
http://google/com/page
'
);

        
console
.
log
(
urlUUID
);


        
//Default DNS namespace

        
var
 
dnsUUID
 
=
 
uuidv5
(
'
dns
'
,
 
'
google.com
'
);

        
console
.
log
(
dnsUUID
);

    
}



zlib
’s asynchronous methods 
inflate
 and 
deflate
 must be used with 
async
 or 
await
. For example:

zlib
 
=
 
zlib
.
zlib
;
  
// Required to access zlib objects and associated functions


async
 
function
 
onRequest
(
request
,
 
settings
)
 
{

  
const
 
body
 
=
 
request
.
json
();


  
const
 
input
 
=
 
'
something
'
;


  
// Calling inflateSync method

  
var
 
deflated
 
=
 
zlib
.
deflateSync
(
input
);


  
console
.
log
(
deflated
.
toString
(
'
base64
'
));


  
// Calling inflateSync method

  
var
 
inflated
 
=
 
zlib
.
inflateSync
(
new
 
Buffer
.
from
(
deflated
)).
toString
();


  
console
.
log
(
inflated
);


  
console
.
log
(
'
Done
'
);

  
}



The following Node.js modules are available:

Other built-in Node.js modules
 aren’t available.

For more information on using the 
aws-sdk
 module, see how to 
set up functions for calling AWS APIs
.

Caching

Basic cache storage is available through the 
cache
 object, which has the following methods defined:

Some important notes about the cache:

const
 
ttl
 
=
 
5
 
*
 
60
 
*
 
1000
 
// 5 minutes


const
 
val
 
=
 
await
 
cache
.
load
(
"
mycachekey
"
,
 
ttl
,
 
async 
()
 
=>
 
{

    
const
 
res
 
=
 
await
 
fetch
(
"
http://echo.jsontest.com/key/value/one/two
"
)

    
const
 
data
 
=
 
await
 
res
.
json
()

    
return
 
data


})



Create settings and secrets

Settings allow you to pass configurable variables to your function, which is the best way to pass sensitive information such as security tokens. For example, you might use 
settings
 as placeholders to use information such as an API endpoint and API key. This way, you can use the same code with different settings for different purposes. When you deploy a function in your workspace, you are prompted to fill out these settings to configure the function.

First, add a setting in 
Settings
 tab in the code editor:



Click 
Add Setting
 to add your new setting.



You can configure the details about this setting, which change how it’s displayed to anyone using your function:

As you change the values, a preview to the right updates to show how your setting will look and work.

Click 
Add Setting
 to save the new setting.

Once you save a setting, it appears in the 
Settings
 tab for the function. You can edit or delete settings from this tab.



Next, fill out this setting’s value in 
Test
 tab, so that you can run the function and check the setting values being passed.

Note, this value is only for testing your function.



Now that you’ve configured a setting and filled in a test value, you can add code to read its value and run the function:

async
 
function
 
onRequest
(
request
,
 
settings
)
 
{

  
const
 
apiKey
 
=
 
settings
.
apiKey

  
//=> "super_secret_string"


}



When you deploy a source function in your workspace, you are prompted to fill out settings to configure the source. You can access these settings later by navigating to the Source Settings page for the source function.



Test the source function

You can test your code directly from the editor in two ways: either by receiving real HTTPS requests through a webhook, or by manually constructing an HTTPS request from within the editor.

The advantage of testing your source function with webhooks is that all incoming data is real, so you can test behavior while closely mimicking the production conditions.

Note: Segment has updated the webhook URL to 
api.segmentapis.com/functions
. To use webhooks with your function, you must:

Testing source functions with a webhook

You can use webhooks to test the source function either by sending requests manually (using any HTTP client such as cURL, Postman, or Insomnia), or by pasting the webhook into an external server that supports webhooks (such as Slack).

A common Segment use case is to connect a Segment 
webhooks destination
 or 
webhook actions destination
 to a test source, where the Webhook URL/endpoint that is used corresponds to the provided source function’s endpoint, then you can trigger test events to send directly to that source, which are routed through your Webhook destination and continue on to the source function: Source → Webhook destination → Source Function.

From the source function editor, copy the provided webhook URL (endpoint) from the “Auto-fill via Webhook” dialog. 

Note
 : When a new source is created that utilizes a source function, the new source’s endpoint (webhook URL) will differ from the URL that is provided in the source function’s test environment.

To test the source function:

Testing source functions manually

You can also manually construct the headers and body of an HTTPS request inside the editor and test with this data without using webhooks.
The 
Content-Type
 Header is required when testing the function:



Save and deploy the function

After you finish building your source function, click 
Configure
 to name it, then click 
Create Function
 to save it.
The source function appears on the 
Functions
 page in your workspace’s catalog.

If you’re editing an existing function, you can 
Save
 changes without updating instances of the function that are already deployed and running.

You can also choose to 
Save & Deploy
 to save the changes, and then choose which already-deployed functions to update with your changes. You might need 
additional permissions
 to update existing functions.

Source functions logs and errors

Your function may encounter errors that you missed during testing, or you might intentionally throw errors in your code (for example, if the incoming request is missing required fields).

If your function throws an error, execution halts immediately. Segment captures the incoming request, any console logs the function printed, and the error, and displays this information in the function’s 
Errors
 tab. You can use this tab to find and fix unexpected errors.



Functions can throw 
an Error or custom Error
, and you can also add additional helpful context in logs using the 
console
 API
.
For example:

async
 
function
 
onRequest
(
request
,
 
settings
)
 
{

  
const
 
body
 
=
 
request
.
json
()

  
const
 
userId
 
=
 
body
.
userId


  
console
.
log
(
'
User ID is
'
,
 
userId
)


  
if 
(
typeof
 
userId
 
!==
 
'
string
'
 
||
 
userId
.
length
 
<
 
8
)
 
{

    
throw
 
new
 
Error
(
'
User ID is invalid
'
)

  
}


  
console
.
log
(
'
User ID is valid
'
)


}



Warning:
 Do not log sensitive data, such as personally-identifying information (PII), authentication tokens, or other secrets. You should especially avoid logging entire request/response payloads. Segment only retains the 100 most recent errors and logs for up to 30 days but the 
Errors
 tab may be visible to other workspace members if they have the necessary permissions.

Error types

Segment only attempts to run your source function again if a 
Retry
 error occurs.

Managing source functions

Source functions permissions

Functions have specific roles which can be used for 
access management
 in your Segment workspace.

Access to functions is controlled by two permissions 
roles
:

You also need additional 
Source Admin
 permissions to enable source functions, connect destination functions to a source, or to deploy changes to existing functions.

Editing and deleting source functions

If you are a 
Workspace Owner
 or 
Functions Admin
, you can manage your source function from the 
Functions
 tab in the catalog.

Connecting source functions

You must be a 
Workspace Owner
 or 
Source Admin
 to connect an instance of your function in your workspace.

From the 
Functions tab
, click 
Connect Source
 and follow the prompts to set it up in your workspace.

After configuring, find the webhook URL - either on the 
Overview
 or 
Settings → Endpoint
 page.

Copy and paste this URL into the upstream tool or service to send data to this source.

Source function FAQs

Segment retries invocations that throw RetryError or Timeout errors up to six times. After six attempts, the request is dropped.
The initial wait time for the retried event is a random value between one and three minutes.
Wait time increases exponentially after every retry attempt. The maximum wait time between attempts can reach 20 minutes.

Retry errors only appear in the source function error logs if the event has exhausted all six retry attempts and, as a result, has been dropped.

The maximum payload size for an incoming webhook payload is 512 KiB.

The execution time limit is five seconds, however Segment strongly recommends that you keep execution time as low as possible. If you are making multiple external requests you can use async / await to make them concurrently, which will help keep your execution time low.

Segment alphabetizes payload fields that come in to 
deployed
 source functions. Segment doesn’t alphabetize payloads in the Functions tester. If you need to verify the exact payload that hits a source function, alphabetize it first. You can then make sure it matches what the source function ingests.

GET
 requests are not supported with a source function. Source functions can only receive data through 
POST
 requests.

No. Tracking Pixels operate client-side only and need to be loaded onto your website directly. Source Functions operate server-side only, and aren’t able to capture or implement client-side tracking code. If the tool you’re hoping to integrate is server-side, then you can use a Source Function to connect it to Segment.

The test function interface has a 4KB console logging limit. Outputs surpassing this limit will not be visible in the user interface.

No, Source Functions can’t send custom responses to the tool that triggered the Function’s webhook. Source Functions can only send a success or failure response, not a custom one.

This error occurs because Segment prevents Source Functions from sending data back to their own webhook endpoint (
https://fn.segmentapis.com
). Allowing this could create an infinite loop where the function continuously triggers itself.

To resolve this error, check your Function code and ensure the URL 
https://fn.segmentapis.com
 is not included. This URL is used to send data to a Source Function and shouldn’t appear in your outgoing requests. Once you remove this URL from your code, you’ll be able to save the Function successfully.

This page was last modified: 17 Jan 2025

Need support?

Questions? Problems? Need more info? Contact Segment Support for assistance!

Help improve these docs!

Was this page helpful?

Thanks for your feedback!

Can we improve this doc? 
Send us feedback!

Get started with Segment

On this page

Was this page helpful?

Thanks for your feedback!

Can we improve
 this doc? 
Send us feedback!

Product

For Developers

Company

Support

© 2025 Segment.io, Inc.

=== Content from https://segment.com/docs/engage/audiences/ ===

Engage Audiences Overview
        

Engage Foundations requires a Business tier account and includes Unify.


See the 
available plans
, or 
contact Support
.

On this page

Audiences let you group users or accounts based on event behavior and traits that Segment tracks.

You can build Audiences from core 
tracking events
, 
traits
, and 
computed traits
. You can then sync Audiences to hundreds of 
Destinations
 or access them with the 
Profile API
.

Building an Audience

You can build an Audience from existing events, traits, computed traits, or other Audiences.

The 
Include Anonymous Users
 checkbox determines which external IDs need to exist on a profile for Segment to include the user in the audience:

Editing an audience before the initial backfill is complete can create technical errors.

Audience Keys

Avoid using the same Audience Key twice, even if you’ve deleted the original Audience.

Events

You can build an Audience from any events connected to Engage, including 
Track
, 
Page
, and 
Screen
 calls. In the Audience builder, Page calls appear as 
Page Viewed
 and Screen calls appear as 
Screen Viewed
.

To refine the audience based on event properties, use the 
+property
 button:

Select 
and not who
 to indicate users that have not performed an event. For example, you might want to look at all users that have viewed a product above a certain price point but not completed the order.



You can also specify two different types of time-windows, 
within
 and 
in between
. The 
within
 property lets you specify an event that occurred in the last 
x
 number of days, while 
in between
 lets you specify events that occurred over a rolling time window in the past. A common use case is to look at all customers that were active 30 to 90 days ago, but have not completed an action in the last 30 days.

Building audiences with traits

You can also build audiences using Custom Traits, Computed Traits, SQL Traits, and audience memberships.

Custom traits
 are user or account-specific attributes. You can collect these traits from your apps when a user completes a form or signs up using an 
Identify call
. You can view these traits in the Profile explorer. Custom Traits are mutable and update to the latest value seen by the user’s Identify events.

When you delete an audience that previously generated Identify events, the data for the audience key stays attached to profiles that entered the audience. This data then becomes visible in Segment as a custom trait.

You can also use computed traits in an audience definition. For example, you can create a 
total_revenue
 computed trait and use it to generate an audience of 
big_spender
 customers that exceed a certain threshold.

Engage supports nested traits, but the Audience builder doesn’t support accessing objects nested in arrays. When you send arrays of objects, they are flattened into strings. As a result, the same conditions that work on strings will work on the array. Within the builder, you can only use string operations like 
contains
 and 
does not contain
 to look for individual characters or a set of characters in the flattened array.

With SQL Traits, you can use data in your warehouse to build an audience. By running SQL queries on this warehouse data, you can import specific traits back into Segment to enhance both Segment audiences and the data you send to downstream destinations.

When you build an audience based on audience membership, you use existing audiences as criteria for creating new audiences. You can include or exclude profiles based on their membership in other audiences, allowing you to generate more specific audience segments.

Time comparison

You can use the following time comparison operators in your audience definition:

Only ISO timestamps can be used with these operators. Additionally, these time comparison operators exclusively apply to custom traits.
If the timestamp is not a valid ISO timestamp (for example, a trailing 
Z
 is missing), Segment won’t process the audience in real-time. Learn more about 
real-time compute compared to batch
.

Note
: Timezones seen in the UI are based on your local timezone, but are converted to UTC on the backend.

Funnel Audiences

Funnel audiences allow you to specify strict ordering between two events. This might be the case if you want an event to happen or not happen within a specific time window, as in the following example:



Dynamic property references

Dynamic Property references give you more flexibility over funnel audiences. Instead of specifying a constant value in both events, like 
product_id = 123
 for both Product Viewed and Order Completed events, you can specify that a child event references an event property of a parent event. You can also compare an event property to a trait variable.



Account-level audiences

If you have a B2B business, you might want to build an Audience of accounts. You can use both account-level traits that you’ve sent through the 
Group
 call, or user-level traits and events. For example, you might want to re-engage a list of at-risk accounts defined as companies which are on a business tier plan and where none of the users in that account have logged in recently. When incorporating user-level events or traits, you can specify 
None of the users
, 
Any users
, or 
All users
.

See 
Account-level Audiences
 for more information.

Send audiences to destinations

You can send audiences and computed traits to third-party services in Segment’s 
Destinations catalog
.

For step-by-step instructions on how to connect an audience to a destination, see 
Send Audience Data to Destinations
.

Understanding compute times

Because a number of factors (like system load, backfills, or user bases) determine the complexity of an Audience, some compute times take longer than others.

As a result, 
Segment recommends waiting at least 24 hours for an Audience to finish computing
 before you resume working with the Audience.

From the Overview page, you can view Audience details including the current compute status and a progress bar for real-time and batch Audiences. Engage updates the progress bar and status for real-time computations approximately every 10 minutes.

Viewing compute progress

When you create a real-time Audience, you’ll see a progress bar, computed percentage, and status updates. For existing Audiences that you edit, Engage displays the compute status but not the progress bar or percentage.

Engage syncs the Overview page for an individual audience more frequently than the Engage Audiences page (
Engage > Audiences
). As a result, you might see temporary discrepancies in Audience details, such as user counts, between these two pages.

Refresh real-time Audiences and Traits

For real-time computations, you can click 
Refresh Audience
 or 
Refresh Trait
 to update user counts, status, and compute progress.

Compute statuses

Engage displays the following compute statuses for Audiences and Traits.

Real-time compute compared to batch

Real-time Compute allows you to update traits and Audiences as Segment receives new events. Real-time Compute unlocks exciting use cases:

By default, Segment creates all Audiences as real-time computations. There are however, a few exceptions which can only be supported as batch computations, one example is 
Funnel Audiences
. The Audience builder will determine and indicate whether the Audience is a real-time or batch computation.

To create a new Audience or Trait:

Go to your 
Computed Traits
 or 
Audiences
 tab in Engage and select 
Create
.

While Engage is computing, use the Audience Explorer to see users or accounts that enter your Audience. Engage displays the Audience as computing in the Explorer until at least one user or account enters.

Facebook Custom Audiences
, 
Marketo Lists
, and 
Adwords Remarking Lists
 impose rate limits on how quickly Segment can update an Audience. Segment syncs at the highest frequency allowed by the tool, which is between one and six hours.

Real-time and batch computation

By default, Segment creates all audiences as real-time computations. However, some conditions require batch computation. For example, 
funnel audiences
 can only be computed in batch mode. The Audience builder determines whether an audience is real-time or batch based on the conditions applied.

Editing Realtime Audiences and Traits

Engage supports the editing of realtime Audiences and Traits, which allows you to make nuanced changes to existing Traits and Audiences in situations where cloning or building from scratch may not suit your use case.

To edit a realtime Trait or Audience, follow these steps:

Engage then processes your realtime Audience or Trait edits. While the edit task runs, the audience remains locked and you can’t make further changes. Once Engage incorporates your changes, you’ll be able to access your updated Audience or Trait.

If your audience includes historical data (Historical Backfill is enabled), editing an audience creates a new backfill task. The backfill task, and therefore the edit task, take longer to process if the audience is connected to a destination with rate limits. Rate-limited destinations dictate how fast Engage can backfill. View a list of 
rate-limited destinations
.

It is not possible to edit an audience to convert it from real-time to batch, or vice-versa. If the computation type needs to be changed, you will need to recreate the audience with the appropriate conditions.

You can’t edit an audience to include anonymous users. If you need to include anonymous profiles, recreate the audience with the appropriate conditions

Access your Audiences using the Profiles API

You can access your Audiences using the Profile API by querying the 
/traits
 endpoint. For example, you can query for 
high_value_user
 property with the following 
GET
 request:

https://profiles.segment.com/v1/spaces/<workspace_id>/collections/users/profiles/email:alex@segment.com/traits?limit=100&include=high_value_user


The query would return the following payload:

    
{

        
"traits"
:
 
{

            
"high_value_user"
:
 
true

        
},

        
"cursor"
:
 
{

            
"url"
:
 
""
,

            
"has_more"
:
 
false
,

            
"next"
:
 
""
,

            
"limit"
:
 
100

        
}

    
}



You can read the 
full Profile API docs
 to learn more.

Download your Audience as a CSV file

You can download a copy of your Audience by visiting the Audiences overview page.

Once Segment generates the CSV, you can download the file directly. You’ll receive an email notification of the CSV completion, with a URL to the Audience overview page.

Note the following limits for the CSV downloader:

Generating a CSV can take a substantial amount of time for large audiences. After you generate the CSV file, leave the modal window open while Segment creates the file. (If the audience recalculates between when you click Generate and when you download the file, you might want to regenerate the file. The CSV is a snapshot from when you clicked Generate, and could be outdated.)

You can’t add account traits and identifiers using the CSV downloader with account level audiences. This is because every row listed in the CSV file is a user, and since account traits and identifiers only exist on accounts, they wouldn’t exist as a user’s custom trait and appear on the CSV.

Identifier Breakdown

The audience summary is a breakdown of the percentages of external_ids of users in the audience. These are the default IDs that Segment includes in the Identity resolution configuration. Segment displays the percentage of the audience with each identifier, which you can use to verify the audience size and profiles are correct. The update of identifier breakdowns on profiles doesn’t occur in real time.

The Identifier Breakdown doesn’t show custom IDs included in the Identity resolution configuration unless those IDs are explicitly selected through 
ID sync
. By default, Segment only displays external IDs in the breakdown.

FAQ

Why do I get a different user count when I use 
$
 on a field?**

Segment recommends using the 
$
 operator when you deal with array properties. However, the 
$
 causes logical conditions to apply independently to each array entry independently. As a result, you’ll get more accurate results by using the 
equals one of
 condition:



How do I populate multiple items off a list for an 
equals one of
 condition? **

The audience builder accepts CSV and TSV lists.

Why am I receiving the error “The audience would create a cycle by referencing another audience”?

This error occurs when creating audiences that reference each other, meaning audience X refers to audience Y in its trigger condition, and later you attempt to modify audience Y’s trigger condition to refer back to audience X. To avoid this error, ensure that the audiences do not reference each other in their conditions.

How does the historical data flag work?

Including historical data lets you take past information into account. You can only exclude historical data for real-time audiences. For batch audiences, Segment includes historical data by default.

This page was last modified: 06 Feb 2025

Need support?

Questions? Problems? Need more info? Contact Segment Support for assistance!

Help improve these docs!

Was this page helpful?

Thanks for your feedback!

Can we improve this doc? 
Send us feedback!

Get started with Segment

On this page

Was this page helpful?

Thanks for your feedback!

Can we improve
 this doc? 
Send us feedback!

Product

For Developers

Company

Support

© 2025 Segment.io, Inc.

=== Content from https://segment.com/docs/guides/how-to-guides/measure-marketing-roi/ ===

Measuring the ROI of Your Marketing Campaigns
        

On this page

The purpose of marketing campaigns is to drive traffic (and sales). But how do you know which campaigns yield the most conversions or what channel across the campaigns was most effective?

This guide provides you with the tools to answer these questions with SQL so that your marketing team can reproduce the hit campaigns and consistently generate loyal customers.

Talk to a product specialist
 to learn how companies like Warby Parker and Crate & Barrel use a data warehouse to increase engagement and sales.

Analyze campaign performance

The goal of marketing campaigns is to drive engagement and conversions. Most commonly performed by attracting traffic to the site, these campaigns use UTM parameters for attribution. In our analysis, we’ll be heavily relying on UTM parameters to analyze not only campaign, but also channel performance.

Learn how to effectively use UTM parameters in your marketing campaign strategies.

For our analysis walkthrough, we’ll use fictitious e-commerce and marketing data from on-demand artisanal toast company, Toastmates.

Toastmates is currently running these two campaigns:

Each of these campaigns used a combination of channels. Here is a table with the channels and corresponding UTM parameters so when we build the SQL query, we can make sure all of the traffic sources are accounted for.



We’ll use SQL below to measure the performance of each campaign and what that means for future marketing activities.

Build the funnel

The following query creates a table where each row is a customer and the columns are the date time when a key funnel event happens that have the 
context_campaign_name
 to match that of the 
UTM_campaign
 . The key funnel events in this analysis are 
Store Visited
(based on a page view to the store URL), 
Product Viewed
 , and 
Order Completed
 . Given that each channel may have some key top of the funnel action that is unique to itself, let’s save that analysis for when we’re analyzing across channels.

Feel free to copy and paste the below query for your analysis so long as you replace 
national-toast-day
 with your own UTM campaign parameter.

 
 
 
 
with



 
 
 
 
users
 
as
 
(


 
 
 
 
 
 
 
 
select
 
 
*


 
 
 
 
 
 
 
 
 
 
from
 
 
toastmates
.
users


 
 
 
 
),



 
 
 
 
page_viewed
 
as
 
(


 
 
 
 
 
 
 
 
select
 
 
p
.
received_at
 
as
 
page_viewed_at
,


 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
p
.
context_campaign_name
,


 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
p
.
user_id


 
 
 
 
 
 
 
 
 
 
from
 
 
toastmates
.
pages
 
p


 
 
 
 
 
left
 
join
 
 
users
 
u


 
 
 
 
 
 
 
 
 
 
 
 
on
 
 
u
.
id
 
=
 
p
.
user_id


 
 
 
 
 
 
 
 
 
where
 
 
p
.
context_campaign_name
 
is
 
not
 
null


 
 
 
 
 
 
 
 
 
 
 
and
 
 
p
.
url
 
ilike
 
'%toastmates.com/store%'


 
 
 
 
),



 
 
 
 
product_viewed
 
as
 
(


 
 
 
 
 
 
 
 
select
 
 
v
.
received_at
 
as
 
product_viewed_at
,


 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
v
.
context_campaign_name
,


 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
v
.
user_id


 
 
 
 
 
 
 
 
 
 
from
 
 
toastmates
.
product_viewed
 
v


 
 
 
 
 
left
 
join
 
 
users
 
u


 
 
 
 
 
 
 
 
 
 
 
 
on
 
 
u
.
id
 
=
 
v
.
user_id


 
 
 
 
),



 
 
 
 
order_completed
 
as
 
(


 
 
 
 
 
 
 
 
select
 
 
c
.
received_at
 
as
 
order_completed_at
,


 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
c
.
context_campaign_name
,


 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
c
.
user_id


 
 
 
 
 
 
 
 
 
 
from
 
 
toastmates
.
order_completed
 
c


 
 
 
 
 
left
 
join
 
 
users
 
u


 
 
 
 
 
 
 
 
 
 
 
 
on
 
 
u
.
id
 
=
 
c
.
user_id


 
 
 
 
)



 
 
 
 
 
 
 
 
select
 
 
p
.
user_id
 
as
 
user_id
,


 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
page_viewed_at
,


 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
product_viewed_at
,


 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
order_completed_at
,


 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
p
.
context_campaign_name


 
 
 
 
 
 
 
 
 
 
from
 
 
page_viewed
 
p


 
 
 
 
 
left
 
join
 
 
product_viewed
 
v


 
 
 
 
 
 
 
 
 
 
 
 
on
 
 
p
.
user_id
 
=
 
v
.
user_id


 
 
 
 
 
left
 
join
 
 
order_completed
 
c


 
 
 
 
 
 
 
 
 
 
 
 
on
 
 
p
.
user_id
 
=
 
l
.
user_id


 
 
 
 
 
 
order
 
by
 
 
5
 
desc



Here are the first four rows of the resulting table:



Then, we can use tweak the query above into the one below to perform some simple 
COUNT
 and 
SUM
 on the previous table to get conversion metrics as well as total revenue derived from the campaign.

 
 
 
 
with



 
 
 
 
users
 
as
 
(


 
 
 
 
 
 
 
 
select
 
 
*


 
 
 
 
 
 
 
 
 
 
from
 
 
toastmates
.
users


 
 
 
 
),



 
 
 
 
page_viewed
 
as
 
(


 
 
 
 
 
 
 
 
select
 
 
p
.
received_at
 
as
 
page_viewed_at
,


 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
p
.
context_campaign_name
,


 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
p
.
user_id


 
 
 
 
 
 
 
 
 
 
from
 
 
toastmates
.
pages
 
p


 
 
 
 
 
left
 
join
 
 
users
 
u


 
 
 
 
 
 
 
 
 
 
 
 
on
 
 
u
.
id
 
=
 
p
.
user_id


 
 
 
 
 
 
 
 
 
where
 
 
p
.
context_campaign_name
 
is
 
not
 
null


 
 
 
 
 
 
 
 
 
 
 
and
 
 
p
.
url
 
ilike
 
'%toastmates.com/store%'


 
 
 
 
),



 
 
 
 
product_viewed
 
as
 
(


 
 
 
 
 
 
 
 
select
 
 
v
.
received_at
 
as
 
product_viewed_at
,


 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
v
.
context_campaign_name
,


 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
v
.
user_id


 
 
 
 
 
 
 
 
 
 
from
 
 
toastmates
.
product_viewed
 
v


 
 
 
 
 
left
 
join
 
 
users
 
u


 
 
 
 
 
 
 
 
 
 
 
 
on
 
 
u
.
id
 
=
 
v
.
user_id


 
 
 
 
),



 
 
 
 
order_completed
 
as
 
(


 
 
 
 
 
 
 
 
select
 
 
c
.
received_at
 
as
 
order_completed_at
,


 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
c
.
context_campaign_name
,


 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
c
.
total
,


 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
c
.
user_id


 
 
 
 
 
 
 
 
 
 
from
 
 
toastmates
.
order_completed
 
c


 
 
 
 
 
left
 
join
 
 
users
 
u


 
 
 
 
 
 
 
 
 
 
 
 
on
 
 
u
.
id
 
=
 
c
.
user_id


 
 
 
 
)



 
 
 
 
 
 
 
 
select
 
 
p
.
context_campaign_name
,


 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
count
(
page_viewed_at
)
 
as
 
store_visits
,


 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
count
(
product_viewed_at
)
 
as
 
product_views
,


 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
count
(
order_completed_at
)
 
as
 
orders_completed
,


 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
sum
(
total
)
 
as
 
total_revenue


 
 
 
 
 
 
 
 
 
 
from
 
 
page_viewed
 
p


 
 
 
 
 
left
 
join
 
 
product_viewed
 
v


 
 
 
 
 
 
 
 
 
 
 
 
on
 
 
p
.
user_id
 
=
 
v
.
user_id


 
 
 
 
 
left
 
join
 
 
order_completed
 
c


 
 
 
 
 
 
 
 
 
 
 
 
on
 
 
p
.
user_id
 
=
 
l
.
user_id


 
 
 
 
 
 
group
 
by
 
 
5


 
 
 
 
 
 
order
 
by
 
 
5
 
desc



Here is the resulting table:



This analysis not only gives us a great snapshot of the conversion points along each campaign’s funnel, but also shows that we’ve generated $3,100.37 from the National Toast Day campaign and $3,824.68 from the Toast Your Friend campaign. Also we can see that the quality of the traffic from the National Toast Day is higher, but we’ve had more total traffic from Toast Your Friend, which makes sense since it’s an ongoing campaign.

But this is not yet ROI, since we haven’t incorporated the spend—the labor of your marketing team and the paid acquisition channels to source part of this traffic—that went into these channels.

Add campaign costs

The main costs that are incorporated in an ROI calculation are salaries (pro-rated by person-hour) and media spend. While we could conceivably create a custom, static table in SQL that contains the spend information over time, the faster and more practical way would be a back of the envelope calculation.

The costs associated with a given campaign consist of two major pieces: the person-hour cost and any associated media spend.

Calculating the pro-rated person-hour is an estimate of the number of hours and people used to set up and manage the campaign, then multiplied by the hourly rates based off their annual salaries.

The media spend is the advertising cost for distributing creatives to generate traffic to your store

Want to easily export advertising data from
 
Google Adwords
 
or
 
Facebook Ads
?
 
Check out
 
Segment Sources
.

When we have the aggregate cost numbers, the formula for ROI is:

Campaign ROI = (Profit Attributed to Campaign – Campaign Cost) / Campaign Cost


Here is a spreadsheet to illustrate the ROI calculation for both campaigns:



Though ROI numbers are one success metric, it’s an important benchmark for comparing performance when launching new campaigns or comparing against past campaigns.

But how can we go one step further and see what worked and what didn’t? One approach is to see which channels convert better, so you know how to adjust your marketing spend or media buys in your current campaigns or future ones.

Analyze channel performance

A single campaign can include a wide variety of channels: email, display ads, push notifications, forums, etc. all of which yields different engagement and conversion rates. Effective marketers will keep a pulse on each channel throughout the duration of the campaign to understand whether a target audience is being saturated, a creative refresh is needed (for advertising), or how to efficiently allocate future spend towards a source that converts.

The analysis is similar to measuring the performance across a single campaign, with the only change being finding events where we focus on 
context_campaign_medium
 or 
context_campaign_source
 instead of 
context_campaign_name
 . The SQL below measures the conversion rates at key funnel events for 
national-toast-day
 , but broken down by 
utm_medium
 .

You can copy the below into your favorite editor, as long as you change out the 
context_campaign_name
 and 
context_campaign_medium
 parameters to ones that applies to your business.

 
 
 
 
with



 
 
 
 
users
 
as
 
(


 
 
 
 
 
 
 
 
select
 
 
*


 
 
 
 
 
 
 
 
 
 
from
 
 
toastmates
.
users


 
 
 
 
),



 
 
 
 
page_viewed
 
as
 
(


 
 
 
 
 
 
 
 
select
 
 
p
.
received_at
 
as
 
page_viewed_at
,


 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
p
.
context_campaign_name
,


 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
p
.
user_id


 
 
 
 
 
 
 
 
 
 
from
 
 
site
.
pages
 
p


 
 
 
 
 
left
 
join
 
 
users
 
u


 
 
 
 
 
 
 
 
 
 
 
 
on
 
 
u
.
id
 
=
 
p
.
user_id


 
 
 
 
 
 
 
 
 
where
 
 
p
.
context_campaign_name
 
=
 
'national-toast-day'


 
 
 
 
 
 
 
 
 
 
 
and
 
 
p
.
context_campaign_medium
 
is
 
not
 
null


 
 
 
 
 
 
 
 
 
 
 
and
 
 
p
.
url
 
ilike
 
'%toastmates.com/store%'


 
 
 
 
),



 
 
 
 
product_viewed
 
as
 
(


 
 
 
 
 
 
 
 
select
 
 
v
.
received_at
 
as
 
product_viewed_at
,


 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
v
.
context_campaign_medium
,


 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
v
.
user_id


 
 
 
 
 
 
 
 
 
 
from
 
 
toastmates
.
product_viewed
 
v


 
 
 
 
 
left
 
join
 
 
users
 
u


 
 
 
 
 
 
 
 
 
 
 
 
on
 
 
u
.
id
 
=
 
v
.
user_id


 
 
 
 
),



 
 
 
 
order_completed
 
as
 
(


 
 
 
 
 
 
 
 
select
 
 
c
.
received_at
 
as
 
order_completed_at
,


 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
c
.
context_campaign_medium
,


 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
c
.
user_id
,


 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
c
.
total


 
 
 
 
 
 
 
 
 
 
from
 
 
toastmates
.
order_completed
 
c


 
 
 
 
 
left
 
join
 
 
users
 
u


 
 
 
 
 
 
 
 
 
 
 
 
on
 
 
u
.
id
 
=
 
c
.
user_id


 
 
 
 
)



 
 
 
 
 
 
 
 
select
 
 
p
.
context_campaign_medium
 
as
 
utm_medium
,


 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
count
(
page_viewed_at
)
 
as
 
store_visits
,


 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
count
(
product_viewed_at
)
 
as
 
product_views
,


 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
count
(
order_completed_at
)
 
as
 
orders_completed
,


 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
sum
(
c
.
total
)
 
as
 
total_revenue


 
 
 
 
 
 
 
 
 
 
from
 
 
page_viewed
 
p


 
 
 
 
 
left
 
join
 
 
product_viewed_at
 
v


 
 
 
 
 
 
 
 
 
 
 
 
on
 
 
p
.
user_id
 
=
 
c
.
user_id


 
 
 
 
 
left
 
join
 
 
order_completed
 
c


 
 
 
 
 
 
 
 
 
 
 
 
on
 
 
p
.
user_id
 
=
 
c
.
user_id


 
 
 
 
 
 
group
 
by
 
 
1


 
 
 
 
 
 
order
 
by
 
 
1
 
desc



The resulting table:



Since the National Toast Day campaign is relatively new, the majority of the traffic is from the email and an article (“news”). But we can see that the social channels have a lower conversion from store visits to product views. Email has the best overall conversion to revenue, which may be attributed to the recipients already familiar with the Toastmates brand or having previously had a stellar end-to-end shopping experience.

We can further breakdown this analysis by seeing which email, display ads, and social channels performed the best, by adding 
utm_source
 and 
utm_content
 ,assuming that you’ve properly added them in your earned and paid media links. Also note that this preliminary analysis in SQL doesn’t account for double-counted users, who had impressions with our brand on multiple channels (e.g. someone seeing a display ad, yet converted on the email outreach). Fortunately, there are multi-touch attribution models that can be applied to better understand the weights of each activity towards conversion.

Learn more about multi-touch attribution models.

Build repeatable hit marketing campaigns

Measuring the ROI and performance of marketing campaigns and marketing channels tells a compelling story about what types of campaigns resonate with your audience. How does your audience like to be engaged? Text, push notifications, email? What campaign messaging hooks work the best in getting them back at your store?

You can apply this analytical approach and performance measurement techniques to a wide variety of marketing activities, such as offline marketing, billboards, or sponsoring events. These insights can empower your team to focus on what works and eliminate what doesn’t.

Talk to a product specialist
 
to learn how companies like Warby Parker and Crate & Barrel use a data warehouse to increase engagement and sales.

This page was last modified: 21 Apr 2023

Need support?

Questions? Problems? Need more info? Contact Segment Support for assistance!

Help improve these docs!

Was this page helpful?

Thanks for your feedback!

Can we improve this doc? 
Send us feedback!

Get started with Segment

On this page

Was this page helpful?

Thanks for your feedback!

Can we improve
 this doc? 
Send us feedback!

Product

For Developers

Company

Support

© 2025 Segment.io, Inc.

=== Content from https://segment.com/docs/connections/sources/about-cloud-sources/ ===

Cloud Sources
        

On this page

Cloud-App Sources (often shortened to Cloud Sources) allow you to pull in data from third-party tools so you can use it in Segment. There are two types of Cloud Apps: 
Object
 and 
Event
 sources.

As in the basic tracking API, 
objects
 usually contain information about a person or group which is updated over time, while 
event
 data happens once, and is appended to a list.

Event Cloud-App Sources

Event Cloud Sources can export their data both into Segment warehouses, and into other enabled Segment integrations that work with event data.

Object Cloud-App Sources

Object Cloud App Sources can export data and import it directly into a Segment warehouse. You 
must
 have a Segment warehouse enabled before you enable these. From the warehouse, you can analyze your data with SQL, use 
Reverse ETL
 to extract data, or use Engage SQL Traits to build audiences. Some examples of Object Cloud sources are Salesforce (account information), Zendesk (support cases), and Stripe (payments information).

You can send data from website, mobile, and server sources to a warehouse 
or
 to destinations. You can only send object cloud app source data to warehouses.

How do cloud sources work?

Sources are functionally comprised of either one or both of the following components: a “sync” component and a “streaming” component. They work together to populate logical collections of data based on upstream resource availability and following data normalization best practices. These collections may be either events (append only data streams, akin to “facts” in data warehousing parlance) or objects (dimensional values that may be updated based on changes in state upstream).

Sync frequency

You enable a cloud source from the Segment web app, and grant Segment access by pasting an API key or authenticating with OAuth. Segment then starts a scheduled job on your behalf which makes requests to the downstream tool, normalizes and transforms the data, and forwards that data to the Segment API.

Cloud sources attempt to use as few API calls as possible, and (where possible) only fetch data that changed since the last sync. The syncs might take a long time (especially on the first sync), so the cloud source syncs have robust retry and rate limiting logic.

Contact Segment Product Support
 if you’d like to change the cadence of your source’s sync frequency.

API call use and collection selection

We make an effort to be respectful of your API call allotments and limits. For example, in the case of Salesforce, we issue only one query per collection per run, using the absolute minimum number of API calls possible (typically about 350/day).

Moreover, we’re deliberate about which collections we pull, striking a balance between allowing you to get a full picture of your users and reducing extraneous data (like administrative and metadata tables).

Soon, we’ll allow you to specify which collections you care about during the source set up phase, so if you need to cut down on calls, you’ll be able to just deselect collections.

Streaming

Streaming components are used to listen in real time to webhooks from downstream cloud sources, normalize and transform the data, and forward it to our APIs.

Both sync and streaming components can forward data to our event tracking and objects upsertion API processing layers, but generally sync components are used to fetch objects and streaming components listen for events.

Set up a cloud source

To use cloud sources, we suggest going through the following steps.

Before you connect a source, check out the 
sources documentation
. See what kind of credentials you will need to enable the source. Different sources require different levels of permissioning.

Next, you’ll also need to get the credentials for your 
warehouse
.

Once you have the necessary credentials (or are logged in to OAuth for your cloud source), you should be ready to go!

Based on your plan, you can schedule a certain number of syncs per day. We suggest setting these up so your dashboards and reports are fresh for reporting, but not at the same time of day that a lot of people are querying your database.

Troubleshooting cloud sources

The most common reason cloud sources have trouble because of authentication or permission issues. When the issue is related to authentication, you’ll see an “access denied” connection error in your source details. When this happens, Segment quits the process early and does not make any further attempts on any collections.

When you successfully authenticate, but your user lacks the required permissions (for example, if you use an agent login instead of an administrator for Zendesk), Segment attempts to pull each collection and reports errors on a per-collection basis. This helps you troubleshoot why source runs fail, because sometimes permission-based denials are scoped to specific resources from the upstream tool.

Segment attempts to make the errors displayed in the UI clear enough so we don’t need to document all of them. However, if it’s not clear what to do to fix an error you encounter, 
contact support
 and let them know.

Sometimes, when the sync job fails due to an unhandled error or is mysteriously hanging for too long, we’ll kill the job and report a failure with instructions to contact support. When this happens, our support and engineering teams have already been notified of the failure and have the complete set of logs to set about debugging and remediating the issue, but  don’t hesitate to get in touch so they can keep you in the loop!

Using Cloud Source data

What kind of data does Segment pull from each source?

In general, we’ve focused on pulling all of the collections directly related to the customer experience. We do not automatically pull all collections available from a partner API, since many of them aren’t relevant to the customer journey. You can see a list of the collections we pull in the docs 
for each cloud source
. Each collection reflects a table in your database.

Contact Segment Product Support
  if you need additional data collected, or to change the schema to do the analysis you want. We’d love to know what analysis you’re trying to run, what additional data you need, and we’ll share with the product team to evaluate.

What questions can you answer with data from cloud, web, and mobile sources combined in a single warehouse?

Querying source data

Generally, you need intermediate- to advanced SQL experience to explore and analyze cloud source data in a warehouse. The following resources can help you get up and running more quickly!

Joining IDs
:  As you start to get into joining across different types of sources, you’ll need a way to join user IDs. This 
help article
 explains how to do this in detail.

Partner Dashboards
: Segment’s BI partners at Mode, Looker, BIME, Periscope, and Chartio have created out of the box dashboards that work on top of our source schemas.

This page was last modified: 13 Feb 2025

Need support?

Questions? Problems? Need more info? Contact Segment Support for assistance!

Help improve these docs!

Was this page helpful?

Thanks for your feedback!

Can we improve this doc? 
Send us feedback!

Get started with Segment

On this page

Was this page helpful?

Thanks for your feedback!

Can we improve
 this doc? 
Send us feedback!

Product

For Developers

Company

Support

© 2025 Segment.io, Inc.

=== Content from https://segment.com/docs/protocols/validate/review-violations/ ===

Review and Resolve Event Violations
        

Protocols is available as an add-on for Business plans only.


See the 
available plans
, or 
contact Support
.

Upon connecting your Tracking Plan to a Source, you will be able to view violations grouped by event. To view violations, click on the Violations button located on the Schema tab in a Source. A filter can be applied to only show events with violations within the past hour, 24 hours, and 7 days.



To view detailed violations for an event, click on the specific event. Specific violations include:

In the event detail violations view, a filter can be applied to only show violations in the past hour, 24 hours, and 7 days.



To view a specific violation, simply click on the violation to view recent sample payloads that generated the violation. These payloads can then be used to help engineering quickly pinpoint the root cause and release a fix.

If you want to analyze or build custom alerts based on inbound violations, you can 
enable violation forwarding here
.

Track violations use the 
event
 field for aggregation, while Page and Screen violations use the 
name
 field. If these are not properly implemented in your events, violations cannot be aggregated correctly.

This page was last modified: 27 Feb 2024

Need support?

Questions? Problems? Need more info? Contact Segment Support for assistance!

Help improve these docs!

Was this page helpful?

Thanks for your feedback!

Can we improve this doc? 
Send us feedback!

Get started with Segment

Was this page helpful?

Thanks for your feedback!

Can we improve
 this doc? 
Send us feedback!

Product

For Developers

Company

Support

© 2025 Segment.io, Inc.

=== Content from https://segment.com/docs/protocols/apis-and-extensions/typewriter/ ===

Typewriter
        

On this page

Typewriter for analytics.js and analytics-node will receive no new features and only critical maintenance updates from Segment. Typewriter for other libraries and SDKs are not actively maintained by Segment. Typewriter is available on 
GitHub
 under the MIT license for the open-source community to fork and contribute.

Typewriter
 is a tool for generating strongly-typed Segment analytics libraries based on your pre-defined 
Tracking Plan
 spec.

At a high-level, Typewriter can take an event from your Tracking Plan like this 
"Order Completed"
 event:



Typewriter uses the event to generate a typed analytics call in different languages:

// Example client in your web app


import
 
typewriter
 
from
 
'
./analytics
'
  


typewriter
.
orderCompleted
({

  
orderID
:
 
'
ck-f306fe0e-cc21-445a-9caa-08245a9aa52c
'
,

  
total
:
   
39.99


})



// Example client in your iOS app


SEGTypewriterAnalytics
.
orderCompleted
(

  
orderID
:
 
"ck-f306fe0e-cc21-445a-9caa-08245a9aa52c"
,

  
total
:
 
39
.
99


)



Typewriter can generate clients for 
analytics.js
, 
analytics-node
, 
analytics-swift
 and 
analytics-kotlin
.

These generated clients are embedded with metadata from your Tracking Plan, which contextualizes your analytics instrumentation, and reduces (or entirely eliminates!) incorrect instrumentations in your production environments. In your editor, you can access event names, descriptions, property names, types and more:



You can also configure Typewriter to validate analytic events at runtime, which can alert you to instrumentation errors during development and testing. Typewriter can warn you about missing required properties, invalid enum values, regex mismatches, and any other advanced 
JSON Schema
 you configure in your Tracking Plan.



You can use this with a test suite to automatically fail your unit tests if the instrumentation generates any violations:



If you use a statically typed language (such as TypeScript, Java, Objective-C, or Swift), you get access to compile-time warnings about your instrumentation:



Typewriter also helps teams adopt 
analytics best practices
, such as avoiding autogenerated event names, and carefully considering what properties are tracked.

To get started, check out one of the quickstart guides below:

For use with the Analytics-iOS and Analytics-Android SDK, use 
Typewriter v7
.

Have feedback on Typewriter? Consider opening a 
GitHub Issue in the @segmentio/typewriter
 repository.

Prerequisites

Typewriter is built using 
Node.js
, and requires node >= 14.x

You can check if you have Node and NPM installed by running the following commands in your command-line window:

$ 
node 
--version

v14.x


If you don’t have these, 
you’ll need to install 
node
. Installing 
node
 also installs 
npm
 and 
npx
 for you. If you’re on macOS, you can install it with 
Homebrew
:

$ 
brew 
install 
node


Once you’ve installed Node and NPM, run the 
--version
 commands again to verify that they were installed correctly.

Browser Quickstart

To get started with Typewriter in your browser:

Once you’ve got 
analytics.js
 installed, add Typewriter as a developer dependency in your project:

 
$ 
npm 
install
 
--save-dev
 typewriter


   
$ 
npm 
install
 
--save-dev
 @segment/analytics-next


Run 
npx typewriter init
 to use the Typewriter quickstart wizard that generates a 
typewriter.yml
 configuration, along with your first Typewriter client. When you run the command, it creates a 
typewriter.yml
 file in your project. For more information on the format of this file, see the 
Typewriter Configuration Reference
.
 The command also adds a new Typewriter client in 
./analytics
 (or whichever path you configured). You can import this client into your project, like so:

 
// Import your auto-generated Typewriter client:

 
import
 
typewriter
 
from
 
'
./analytics
'


 
// Issue your first Typewriter track call!

 
typewriter
.
orderCompleted
({

   
orderID
:
 
'
ck-f306fe0e-cc21-445a-9caa-08245a9aa52c
'
,

   
total
:
   
39.99

 
})



### Configuration for snippet + TypeScript users

 
// Optional but recommended: you can improve your developer experience by adding typings for the global analytics object.

 
import
 
type
 
{
 
AnalyticsSnippet
 
}
 
from
 
'
@segment/analytics-next
'

    
 
declare
 
global
 
{

   
interface
 
Window
 
{

     
analytics
:
 
AnalyticsSnippet
;

 
}



### Configuration for NPM users

 
// As an npm user, you *must* explicitly pass in your analytics instance.

 
import
 
{
 
AnalyticsBrowser
 
}
 
from
 
'
@segment/analytics-next
'

    
 
const
 
analytics
 
=
 
AnalyticsBrowser
.
load
({
 
writeKey
:
 
'
YOUR_WRITE_KEY
'
 
})


 
typewriter
.
setTypewriterOptions
({

   
analytics
:
 
analytics

 
})



Run 
npx typewriter
 to regenerate your Typewriter client. You need to do this each time you update your Tracking Plan.

To help you minimize your bundle size, Typewriter supports 
tree-shaking
 using named exports. All generated analytics calls generate and export automatically, so you can import them like so:

import
 
{
 
orderCompleted
 
}
 
from
 
'
./analytics
'



orderCompleted
({

  
orderID
:
 
'
ck-f306fe0e-cc21-445a-9caa-08245a9aa52c
'
,

  
total
:
   
39.99


})



Typewriter wraps your analytics calls in an 
ES6 
Proxy
, which helps protect your application from crashing if you make analytics calls with a generated function that doesn’t exist. For example, if an 
Order Completed
 event didn’t exist in your Tracking Plan in the first example above, then your app would crash with a 
TypeError: typewriter.orderCompleted is not a function
. However, since Typewriter dynamically proxies the underlying function calls, it can detect if a function doesn’t exist, and handle it for you. Typewriter logs a warning message, then fires an 
Unknown Analytics Call Fired
 event into your source. This helps to prevent regressions when you migrate JavaScript projects to Typewriter in bulk. Keep in mind that proxying doesn’t work with named exports.

Node.js Quickstart

To get started with Node.js:

Once you have 
analytics-node
 installed, add Typewriter as a developer dependency in your project:

 
$ 
npm 
install
 
--save-dev
 typewriter


Run 
npx typewriter init
 to use the Typewriter quickstart wizard that generates a 
typewriter.yml
 configuration, along with your first Typewriter client. When you run the command, it creates a 
typewriter.yml
 file in your repo. For more information on the format of this file, see the 
Typewriter Configuration Reference
. The command also adds a new Typewriter client in 
./analytics
 (or whichever path you configured). You can import this client into your project, like so:

 
// Initialize analytics-node, per the analytics-node guide above.

 
import
 
{
 
Analytics
 
}
 
from
 
'
@segment/analytics-node
'


 
const
 
analytics
 
=
 
new
 
Analytics
({
 
writeKey
:
 
'
<MY_WRITE_KEY>
'
 
})


 
app
.
post
(
'
/login
'
,
 
(
req
,
 
res
)
 
=>
 
{

    
analytics
.
identify
({

       
userId
:
 
req
.
body
.
userId
,

       
previousId
:
 
req
.
body
.
previousId

   
})

   
res
.
sendStatus
(
200
)

 
})

    
 
app
.
post
(
'
/cart
'
,
 
(
req
,
 
res
)
 
=>
 
{

   
analytics
.
track
({

     
userId
:
 
req
.
body
.
userId
,

     
event
:
 
'
Add to cart
'
,

     
properties
:
 
{
 
productId
:
 
'
123456
'
 
}

   
})

    
res
.
sendStatus
(
201
)

 
});



Run 
npx typewriter
 to regenerate your Typewriter client. You need to do this each time you update your Tracking Plan.

Typewriter wraps your analytics calls in an 
ES6 
Proxy
, which helps protect your application from crashing if you make analytics calls with a generated function that doesn’t exist. For example, if an 
Order Completed
 event didn’t exist in your Tracking Plan in the first example above, then your app would crash with a 
TypeError: typewriter.orderCompleted is not a function
. However, since 
typewriter
 dynamically proxies the underlying function calls, it can detect if a function does not exist, and handle it for you. Typewriter logs a warning message, then fires an 
Unknown Analytics Call Fired
 event into your source. This helps to prevent regressions when you migrate JavaScript projects to Typewriter in bulk. Keep in mind that proxying doesn’t work with named exports.

Swift Quickstart

For use with the 
analytics-ios
 SDK, use 
Typewriter v7
.

To get started using Typewriter with Swift:

Run 
npx typewriter init
 to use the Typewriter quickstart wizard that generates a 
typewriter.yml
 configuration, along with your first Typewriter client. When you run the command, it creates a 
typewriter.yml
 file in your repo. For more information on the format of this file, see the 
Typewriter Configuration Reference
.

 
Note:
 Run 
npx typewriter
 to regenerate your Typewriter client. You need to do this each time you update your Tracking Plan.

Import your new Typewriter client into your project using XCode. If you place your generated files into a folder in your project, import the project as a group not a folder reference.

 When you add the generated client to your Xcode Project you can use as a Swift extension method on any Analytics client object:

 
Analytics
.
main
.
orderCompleted
(
OrderCompleted
(

   
orderID
:
 
"ck-f306fe0e-cc21-445a-9caa-08245a9aa52c"
,

   
total
:
 
39.99

 
))



Kotlin Quickstart

For use with the 
analytics-android
 SDK, use 
Typewriter v7
.

To get started using Typewriter with Kotlin:

Typewriter creates the class file with the package name 
typewriter
. Segment recommends you to enter the right package name during 
npx typewriter init
 by choosing to review the Advanced Options for Kotlin. You can also enter the right package name directly in 
typewriter.yml
:

  
client
:

    
language
:
 
kotlin

    
sdk
:
 
kotlin

    
languageOptions
:

      
package
:
 
com.segment.typewriter



Run 
npx typewriter
 to regenerate your Typewriter client. You need to do this each time you update your Tracking Plan.

You can now use your Typewriter client in your Android Kotlin or Java application as extensions to any 
Analytics
 object:

Kotlin:

// Import your auto-generated Typewriter client:


import
 
com.segment.generated.*


analytics
.
orderCompleted
(
OrderCompleted
(
orderID
 
=
 
"110"
,
 
total
 
=
 
39.98
))



Java:

// Import your auto-generated Typewriter client:


import
 
com.segment.generated.*



// Issue your first Typewriter track call!


TypewriterAnalytics
.
with
(
this
).
orderCompleted
(

  
OrderCompleted
.
Builder
()

    
.
orderID
(
"ck-f306fe0e-cc21-445a-9caa-08245a9aa52c"
)

    
.
total
(
39.99
)

    
.
build
()


);



React Native Quickstart

To get started with React Native:

Add 
typewriter
 as a dev dependency in your project once you have the library installed in your project.

 $ npm install --save-dev typewriter


Run 
npx typewriter init
 to use the Typewriter quickstart wizard that generates a 
typewriter.yml
 configuration along with your first Typewriter client.

This command creates a 
typewriter.yml
 file in your repo. For more information on the format of this file, see the 
Typewriter Configuration Reference
. The command also adds a new Typewriter / Segment client in 
./analytics
 (or whichever path you configured). You can use this interchangeably as a normal React Native Segment client. It contains additional methods for your tracking plan:

 
import
 
{

   
createClient
,

   
AnalyticsProvider
,

 
}
 
from
 
'
../typewriter
'
;
 
// Remember to import the methods from your typewriter generated file!

 
const
 
segmentClient
 
=
 
createClient
({

   
writeKey
:
 
'
SEGMENT_API_KEY
'

 
});

 
const
 
App
 
=
 
()
 
=>
 
(

   
<
AnalyticsProvider
 
client
=
{
segmentClient
}
>

     
<
Content
 
/>

   
<
/AnalyticsProvider
>

 
);



From there you can use it with hooks:

 
import
 
React
 
from
 
'
react
'
;

 
import
 
{
 
Text
,
 
TouchableOpacity
 
}
 
from
 
'
react-native
'
;

 
import
 
{
 
useAnalytics
 
}
 
from
 
'
../typewriter
'
;
 
// Important! To

 
const
 
Button
 
=
 
()
 
=>
 
{

   
const
 
{
 
orderCompleted
 
}
 
=
 
useAnalytics
();

   
return 
(

     
<
TouchableOpacity

       
style
=
{
styles
.
button
}

       
onPress
=
{()
 
=>
 
{

         
orderCompleted
({
orderID
:
 
"
111
"
,
 
total
:
 
39.99
});

       
}}

     
>

       
<
Text
 
style
=
{
styles
.
text
}
>
Press
 
me
!<
/Text
>

     
<
/TouchableOpacity
>

   
);

 
};



Or directly through the client:

 
segmentClient
.
orderCompleted
({
orderID
:
 
"
111
"
,
 
total
:
 
39.99
});

 
// Remember this is just an extended client with the typewriter methods so all the normal segment methods still work!

 
segmentClient
.
track
(
'
Untyped event
'
);



Run 
npx typewriter
 to regenerate your Typewriter client. You need to do this each time you update your Tracking Plan.

Adding Events

To update or add a new event to a Typewriter client, first apply your changes to your Tracking Plan. Then run the following:

# Run this in the directory with your repo's `typewriter.yml`.


$ 
npx typewriter


API Token Configuration

Typewriter requires a Segment API token to fetch Tracking Plans from the 
Segment Public API
.

You must be a workspace owner to create Segment API tokens.

To create an API token:

Typewriter looks for an API token in three ways, in the following order:

The quickstart wizard prompts you for an API token and stores it in 
~/.typewriter
 for you.

Segment recommends you use a 
Token Script
 to share an API token with your team. When you use a token script, you can supply your API token as an environment variable (
echo $TYPEWRITER_TOKEN
), from an 
.env.
 file (
source .env; echo $TYPEWRITER_TOKEN
) or using any other CLI tool for providing secrets.

Segment also recommends you to pipe through your API Token as this will let you keep your token secret, but it also allows you to share it across your team.

Segment is temporarily keeping the Token Script execution for compatibility purposes. Segment might deprecate this feature in the future, and encourages you to execute your script and pipe in the token. For example, 
echo $TW_TOKEN | typewriter build
.

Editor Configuration

To make the most of Typewriter, Segment recommends installing a few extensions:

JavaScript

Typewriter clients include function documentation adhering to the 
JSDoc
 specification. Install the relevant extension below for JSDoc support in your editor:

TypeScript

For intellisense in TypeScript clients, install the relevant extension below for TypeScript support in your editor. If your project is a mix between JavaScript and TypeScript, then you should also install the plugins in the JavaScript section above so that your editor will also support JSDoc intellisense.

iOS

XCode does not require any extra configuration and shows intellisense out-of-the-box.

Android

Android Studio does not require any extra configuration and shows intellisense out-of-the-box.

Best Practices

Segment 
strongly recommends
 that you store your Tracking Plan (
plan.json
) in a version control system. This guarantees that Typewriter will generate the same client, regardless of any changes you make to your Tracking Plan in the Segment app. Otherwise, changes to your Tracking Plan could lead to broken builds.

Segment recommends that you only check in the 
plan.json
, and generate your Typewriter client during the application build step (by calling 
npx typewriter
). You can do this in 
git
 with the following 
.gitignore
:

# Make sure to update `analytics` to the full path to your Typewriter client.

analytics/
*


!
analytics/plan.json


If this isn’t possible you can also check in the full generated client. Segment, however, doesn’t recommend this method.

Configuration Reference

Typewriter stores its configuration in a 
typewriter.yml
 file in the root of your repo. A sample configuration might look like this:

# Segment Typewriter Configuration Reference (https://github.com/segmentio/typewriter)


# Just run `npx typewriter` to re-generate a client with the latest versions of these events.



scripts
:

  
# You can supply a Segment API token using a `script.token` command. See `Token Script` below.

  
token
:
 
source .env; echo $TYPEWRITER_TOKEN

  
# You can format any of Typewriter's auto-generated files using a `script.after` command.

  
# See `Formatting Generated Files` below.

  
after
:
 
./node_modules/.bin/prettier --write analytics/plan.json



client
:

  
# Which Segment SDK you are generating for.

  
# Valid values: analytics.js, analytics-node, analytics-react-native, swift, kotlin.

  
sdk
:
 
analytics-node

  
# The target language for your Typewriter client.

  
# Valid values: javascript, typescript, kotlin, swift.

  
language
:
 
typescript



trackingPlans
:

  
# The Segment Protocols Tracking Plan that you are generating a client for.

  
# Provide your workspace slug and Tracking Plan id, both of which can be found

  
# in the URL when viewing the Tracking Plan editor. For example:

  
# https://app.segment.com/segment-demo/protocols/tracking-plans/rs_QhWHOgp7xg8wkYxilH3scd2uRID

  
# You also need to supply a path to a directory to save your Typewriter client.

  
-
 
id
:
 
rs_QhWHOgp7xg8wkYxilH3scd2uRID

    
workspaceSlug
:
 
segment-demo

    
path
:
 
./analytics



At any time, you can regenerate this file by running the Typewriter quickstart wizard:

$ 
npx typewriter init


Token Script

Segment is keeping the Token Script execution for compatibility purposes only in v8 of Typewriter. Segment might deprecate this feature in the future, and encourages you to execute your script and pipe in the token. For example, 
echo $TW_TOKEN | typewriter build
.

If your team has a standard way to supply secrets (passwords and tokens) in development environments, whether that’s an 
.env
 file or an AWS-backed secret store, you can configure Typewriter to use it to get a Segment API token.

To configure this, create a token script called 
scripts.token
 in your 
typewriter.yml
. This script is a string that contains a shell command that, when executed, outputs a valid Segment API token. Here’s an 
insecure
, example:

scripts
:

  
# NOTE: NEVER commit a Segment API token to your version control system.

  
token
:
 
echo "OIEGO$*hf83hfh034fnosnfiOEfowienfownfnoweunfoiwenf..."



To give a real example, Segment stores secrets in 
segmentio/chamber
 which is backed by 
AWS Parameter Store
. Providing access to a token in 
chamber
 looks like this:

scripts
:

  
token
:
 
aws-okta exec dev-privileged -- chamber export typewriter | jq -r .typewriter_token



To learn more about the 
typewriter.yml
 configuration format, see the 
Configuration Reference
.

Formatting Generated Files

In your 
typewriter.yml
, you can configure a script (
scripts.after
) that fires after generating a Typewriter client. You can use this to apply your team’s style guide to any of Typewriter’s auto-generated files.

For example, if you want to apply your 
prettier
 formatting to 
plan.json
 (the local snapshot of your Tracking Plan), you can use an 
after
 script like this:

scripts
:

  
after
:
 
./node_modules/.bin/prettier --write ./analytics/plan.json



To learn more about the 
typewriter.yml
 configuration format, see the 
Configuration Reference
.

Connecting to CI

As mentioned in the 
Best Practices
 section above, Segment recommends that you only check in the 
plan.json
, and not the generated clients, into your version control. Instead, Segment recommends building these clients as part of the build step for your application.

In your CI environment, this usually involves a step to build the Typewriter client. Make sure to build the production client before deploying the application, as explained in the 
Tracking Plan Violation Handling
 section below.

# An example (simplified) CircleCI configuration:


jobs
:

  
test
:

    
steps
:

      
-
 
npx typewriter development

      
-
 
yarn run test


  
deploy
:

    
steps
:

      
-
 
npx typewriter production

      
-
 
yarn run deploy



Tracking Plan Violation Handling

You can also configure Typewriter to validate analytic events at runtime, which can alert you to instrumentation errors during development and testing. By default, Typewriter generates a “development” build, which means that it includes this logic. You can generate a “production” build that omits this logic:

# To build a development client (the default, if not supplied):


$ 
npx typewriter development

# To build a production client:


$ 
npx typewriter production


Run-time validation support

Not all languages support run-time validation. Currently, 
analytics.js
 and 
analytics-node
 support it using 
AJV
 (both for JavaScript and TypeScript projects) while 
analytics-ios
 and 
analytics-android
 do not yet support run-time validation. Typewriter also doesn’t support run-time validation using Common JSON Schema. For languages that don’t support run-time validation, the development and production clients are identical.

Segment recommends you to use a development build when testing your application locally, or when running tests. Segment generally recommends 
against
 using a development build in production, since this includes a full copy of your Tracking Plan which can increase the size of the application.

You can provide a custom handler that fires whenever a violation is seen. By default, this handler logs a warning.

For 
analytics.js
 and 
analytics-node
 clients, you can configure this handler with 
setTypewriterOptions
:

import
 
typewriter
 
from
 
'
./analytics
'



const
 
yourViolationHandler
 
=
 
(
message
,
 
violations
)
 
=>
 
{

  
console
.
error
(
`Typewriter Violation found in 
${
message
.
event
}
`
,
 
violations
)


}



typewriter
.
setTypewriterOptions
({

  
onViolation
:
 
yourViolationHandler


})



A common use case for this handler is to configure Typewriter to detect when your tests are running and if so, throw an error to fail your unit tests. For example:

const
 
typewriter
 
=
 
require
(
'
./analytics
'
)



const
 
yourViolationHandler
 
=
 
(
message
,
 
violations
)
 
=>
 
{

  
if 
(
process
.
env
.
IS_TESTING
 
===
 
'
true
'
)
 
{

    
throw
 
new
 
Error
(
`Typewriter Violation found in 
${
message
.
event
}
`
)

  
}


}



typewriter
.
setTypewriterOptions
({

  
onViolation
:
 
yourViolationHandler


})



Typewriter is preconfigured in 
analytics-node
 environments to throw an error if 
NODE_ENV=test
, which is set by most Node.js testing libraries such as 
ava
 and 
jest
.

Another common use case is to customize how violations are reported to your team. For example, Segment customized this handler to show a 
toast notification
 to developers in-app:



import
 
typewriter
 
from
 
'
./analytics
'


import
 
{
 
toaster
 
}
 
from
 
'
evergreen-ui
'



typewriter
.
setTypewriterOptions
({

  
// Note that this handler only fires in development mode, since we ship the production build

  
// of Typewriter to customers.

  
onViolation
:
 
(
msg
,
 
violations
)
 
=>
 
{

    
toaster
.
warning
(
`"
${
msg
.
event
}
" Fired with Tracking Plan Violation`
,
 
{

      
description
:
 
violations
[
0
].
message

    
})

  
}


})



Known Limitations

Typewriter only supports 
track
 calls. However, you can continue to use the underlying (untyped) analytics instance to perform 
identify
, 
group
, 
page
, 
screen
, and 
alias
 calls.

Not all languages support run-time validation. Currently, 
analytics.js
 and 
analytics-node
 support it using 
AJV
 (both for JavaScript and TypeScript projects) while 
analytics-swift
 and 
analytics-kotlin
 don’t support run-time validation. Typewriter also does not support event validation using the Common JSON Schema.

Contributing

If you’re interested in contributing, 
open an issue on GitHub
 and Segment can help provide you pointers to get started.

Feedback

Segment welcomes feedback you may have on your experience with Typewriter. To contact Segment, 
open an issue on GitHub
.

This page was last modified: 07 Feb 2025

Need support?

Questions? Problems? Need more info? Contact Segment Support for assistance!

Help improve these docs!

Was this page helpful?

Thanks for your feedback!

Can we improve this doc? 
Send us feedback!

Get started with Segment

On this page

Was this page helpful?

Thanks for your feedback!

Can we improve
 this doc? 
Send us feedback!

Product

For Developers

Company

Support

© 2025 Segment.io, Inc.

=== Content from https://segment.com/docs/segment-app/iam/ ===

Identity & Access Management Overview
        

Advanced Access Management is available for all Business plans.


See the 
available plans
, or 
contact Support
.

On this page

Segment’s access management tools let Workspace Owners manage which users can access different parts of their Segment workspaces. The Access Management page has three tabs: 
Users (team members)
, 
User Groups
, and 
Tokens
.

Access settings are applied at the workspace level. A Segment user can have access to one or more workspaces and can have different 
roles
 in each workspace.
Users access their Segment account with either email/password credentials, their 
Twilio credentials
, or by using 
Single Sign On
.

Exporting a workspace’s user list

Workspace Owners
 can download a CSV list of users who have access to a specific workspace (including their roles) from the Access Management page in the Segment app.

You can select a user in the table to see their 
roles
. Check out the 
Roles documentation
 for more details.

Twilio Unified Login

With Twilio Unified Login, Twilio users can use their Twilio email, password, and authentication settings to access several Twilio products, including Twilio Messaging, SendGrid, and Segment. You can also use Sign up With Google to create your Twilio account. Once you link your Segment account to your Twilio credentials, you can access Segment directly from the Twilio console using the 
Twilio Product Switcher
.

Twilio Sign Up

Segment invitations and sign ups that are redirected to Twilio’s sign up page must adhere to Twilio’s 
minimum password and 2FA requirements
. To learn more, view Twilio’s 
Account Management
 documentation.

Any existing Segment user must adhere to existing password requirements and 2FA settings set at the Workspace level.

Twilio Product Switcher

You can access Segment from the Twilio Console using the Product Switcher. For more information, view the Twilio support article 
Getting Started with the Unified Login and Product Switcher
.

User settings

Twilio Unified Login users can manage their Segment user settings, including name, email, password, and 2FA settings, directly in their Twilio account. To learn more about Twilio’s user and password policies, review Twilio’s 
Account Management
 documentation.

Segment Users and SSO/SCIM

Existing Segment users can still use their credentials to access Segment.

Segment continues to support 
SSO
 and SCIM, as users who need to access an SSO enabled workspace will be directed to authenticate through the configured Identity Provider.

Quick links

Learn how to add members to your workspace, and manage their permissions.

Learn manage workspace members in bulk.

This page was last modified: 22 Feb 2024

Need support?

Questions? Problems? Need more info? Contact Segment Support for assistance!

Help improve these docs!

Was this page helpful?

Thanks for your feedback!

Can we improve this doc? 
Send us feedback!

Get started with Segment

On this page

Was this page helpful?

Thanks for your feedback!

Can we improve
 this doc? 
Send us feedback!

Product

For Developers

Company

Support

© 2025 Segment.io, Inc.

=== Content from https://segment.com/docs/?ref=nav ===

Segment Documentation
          

Learn how to use Segment to collect, responsibly manage, and integrate your customer data with hundreds of tools.

Learn about Segment, plan and work through a basic implementation, and explore features and extensions.

How can Segment help you?

Integrate the tools you need for analytics, growth, marketing, and more.

Prevent data quality issues with a tracking schema and enforcement with Protocols.

Build audiences and journeys from real-time customer data to personalize experiences on every channel.

Keep customer data private with Segment's data discovery and policy enforcement tools.

Get Data into Segment

The Segment Spec helps you identify, capture, and format meaningful data for use with Segment libraries and APIs as well as downstream tools.

Use Track, Page, Identify, and other Segment tracking calls.

Save time by letting Segment calls collect information for you.

Use our business-case specs to ensure that your tools get the most from your data.

Learning about Segment

The basics of your Segment implementation.

Over a dozen how-to guides that help you accomplish common tasks.

Connect your app to Segment

Additional Resources

Segment's Analytics Academy walks you through the wide world of analytics, including best practices, an overview of the most popular tools, and case studies of how other developers have achieved success.

For a more hands-on tutorial of Segment, check out Segment University. It offers step-by-step instructions, starting with first steps and going through some of our more advanced features.

Need support?

Questions? Problems? Need more info? Contact Segment Support for assistance!

Help improve these docs!

Was this page helpful?

Thanks for your feedback!

Can we improve this doc? 
Send us feedback!

Get started with Segment

Product

For Developers

Company

Support

© 2025 Segment.io, Inc.

=== Content from https://segment.com/docs/guides/how-to-guides/automated-multichannel-reengagement/ ===

Automating Multi-Channel Re-Engagement Campaigns
        

On this page

Compelling and engaging brands delight their customers at every interaction. As customers move seamlessly across channels—such as email, push notifications, display ads—brands must similarly meet them with tailored and consistent messages.

With Segment, you can craft a tailored message while using a combination of AdRoll, Customer.IO, and other tools to dynamically switch between channels.

Talk to a product specialist today
 about using data to tailor your brand experience.

Tools used

Retargeting with 
AdRoll
: 
AdRoll is a retargeting and prospecting tool that allows you to show display ads to a behaviorally-defined cohort

Push notifications with 
Braze
: 
Braze is a multi-channel marketing campaign focused on the mobile experience

Emails with 
Customer.io
: 
Customer.io is a flexible email provider that allows you to create cohorts based on customer actions. You can build complex onboarding emails, nurture email campaigns, as well as marketing automation workflows.

There are other email tools on Segment’s platform, such as
 
Bronto
, 
SendGrid
, 
and
 
Mailchimp
. 
Check out the
 
full list of email tools
.

It’s important to register for these tools and enable them on your Segment source project. When Segment collects tracking data, it’ll also route it to all of your enabled tools. Then your tools, especially ones like Customer.io, Braze, and AdRoll, where you can define cohorts of your users, will be working off a dynamic, yet consistent data set. This is paramount in getting the dynamic messaging to update accordingly.

Set it up



When you send tracking data from your app or website to Segment, Segment will send the same data to all of your tools. Segment also collects key messaging events like Push Notification Opened and Email Opened from Braze and Customer.io, respectively, and sends that to other tools. By defining cohorts based on these events, you can create dynamic campaign audiences, to which customers can add and remove themselves.

In each of your destinations—Braze, Facebook, Customer.io, AdRoll—you can create custom campaigns to show display ads or send emails to a specific segment of users who have performed (or not performed) a given action, or “event.” In this cross-channel re-engagement example, we’ll start with push notifications.

1st line of defense: the push notification

In Braze, create a segment of customers who added a product to their cart, but did not check out. The segment definition, in this case, should be people who have performed 
Product Added
, but have not performed 
Order Completed
 . Send a push notification to these customers with a message that the cart was abandoned and that they can complete the transaction with, for example, a 10% coupon.



2nd line of defense: the email reminder

Because Segment automatically collects second-party data from Braze, you now also have push notification event data, like 
Push Notification Opened
 and 
Push Notification Received
 in Segment. You can use the 
properties
 on each of these events to define a property called 
campaign_name
 so you can tie these activities to a given campaign.



This is helpful because now, you can define segments in Customer.io for customers who have triggered 
Push Notification Received
, but not 
Push Notification Opened
 . You’ve now automated the process of targeting customers who don’t open your push notifications. In Customer.io, you can create a campaign that sends an email to those people asking them to check their push notifications and offering them a coupon to complete their order.

3rd line of defense: paid advertising

Since Segment collects email event data, like 
Email Opened
, from Customer.io, you can similarly create segments in Facebook Ads and AdRoll for when customers don’t open your email. Create a segment where users have an 
Email Delivered
 event, but no 
Email Opened
 event. When users meet these criteria, they’ll get automatically added to your retargeting campaigns. You can then serve them custom creatives about them neglecting to open your emails and, again, perhaps offer them a coupon to complete the transaction.



When users do not open an activation email, add them to a specific retargeting campaign that contains messaging to remind them to activate.

With Segment, automate not just switching across channels, but also the messaging in each channel so that the entire experience is cohesive. The added benefit is that we can create specifically targeted retargeting campaigns for people who no longer open our emails or push notifications. Automating these processes with Segment makes channel-switching more seamless for your customers.

Create an engaging and consistent brand experience

This is just a simple cart abandonment example that dynamically follows customers as they switch between channels. Because Segment collects and routes the second party data of emails and push notifications being opened, you can create specific campaigns with messaging that targets your customers as they interact with your brand.

With over 200+ different tools on Segment’s platform, you can take this idea and create other tailored shopping experiences to re-engage your customers.

Talk to a product specialist today
 
about using data to tailor your brand experience.

This page was last modified: 06 Oct 2023

Need support?

Questions? Problems? Need more info? Contact Segment Support for assistance!

Help improve these docs!

Was this page helpful?

Thanks for your feedback!

Can we improve this doc? 
Send us feedback!

Get started with Segment

On this page

Was this page helpful?

Thanks for your feedback!

Can we improve
 this doc? 
Send us feedback!

Product

For Developers

Company

Support

© 2025 Segment.io, Inc.

=== Content from https://segment.com/docs/engage/ ===

Engage Introduction
        

Engage Foundations requires a Business tier account and includes Unify.


See the 
available plans
, or 
contact Support
.

On this page

Powered by real-time data, Twilio Engage is a customizable personalization platform with which you can build, enrich, and activate Audiences.

Engage Channels builds on top of these Audiences, helping you connect with and market to your customers through email, SMS, and WhatsApp campaigns.

What can you do with Engage?

Engage uses 
Segment Identity Resolution
 to take event data from across devices and channels and intelligently merge it into complete user- or account-level profiles. This gives your organization a single view of your customer base. To learn more, read the 
Identity Resolution documentation
.

Support teams rely on Segment's unified profiles to make real-time and informed decisions about customers when answering tickets or taking support calls. Read about how the support team at Frame.io reduced ticket response time by 80%.

Add detail to user profiles with new traits and use them to power personalized marketing campaigns. You can add new traits to your user or account profiles in Engage using:

Create lists of users or accounts that match specific criteria. For example, after creating an 
inactive accounts
 audience that lists paid accounts with no logins in 60 days, you can push the audience to your analytics tools or send an SMS, email, or WhatsApp campaign with Engage Channels. Learn more about 
Engage audiences
.

Once you create your Computed Traits and Audiences, Engage sends them to your Segment Destinations in just a few clicks. You can use these Traits and Audiences to personalize messages across channels, optimize ad spend, and improve targeting. You can also use the 
Profile API
 to build in-app and onsite personalization. Learn more about 
using Engage data
 and the 
Profile API
.

Marketing teams use Engage to run real-time multi-channel marketing campaigns based off specific user attributes they've computed in Engage. Read about how Drift used Engage to increase prospect engagement by 150% in two months.

Market to customers with Engage Premier and Channels

To send email, SMS, and WhatsApp campaigns with Engage Channels, you’ll connect a 
Twilio messaging service
, 
SendGrid subuser account
, and 
WhatsApp messaging service
 to your Segment Engage space. Use existing accounts, or create new ones.

View the 
onboarding steps
 for more on how to connect Twilio and SendGrid accounts.

Use Engage to build email, SMS, and WhatsApp campaigns within 
Journeys
. Send campaigns to 
subscribed users
 based on event behavior and profile traits. With 
message analytics
, you can track the performance of your campaigns.

Send Email
: 
Build email campaigns
 with existing templates, or create a new email template within Journeys. Before you send the email, test the template and set 
conversion goals
.

Send SMS messages
: 
Build SMS campaigns
 to message users in real-time as a step in a Journey. For example, create an abandoned cart campaign that texts users a reminder to complete their purchase, along with a promo code. Add 
merge tags
 and set conversion goals.

Send WhatsApp messages
: 
Build WhatsApp campaigns
 that deliver messages to your customers on the world’s most used messaging app.

To learn more, visit the 
CSV Uploader
 documentation.

Build personalized 
email
, 
SMS
, and 
WhatsApp
 templates in Twilio Engage for use in your campaigns. Design email templates with a WYSIWYG 
Drag and Drop Editor
 or the 
HTML Editor
. Engage saves the templates for you to preview, edit, and reuse throughout Journeys.

Insert real-time user profile traits from merge tags to personalize each message. For example, address recipients by name or highlight new products from a user’s favorite brand.

Use the CSV uploader to add or update user profiles and 
subscription states
. To learn more, visit the 
CSV Uploader
 documentation.

Set user subscription states in two ways:

Use Engage to add subscription states to user email addresses and phone numbers. Subscription states help determine which users you can send campaigns to in Engage. You can set user subscription states with a 
CSV file upload
, or programmatically with Segment’s 
Public API
.

With analytics in Engage, you can monitor real-time conversion data. Track message performance and customer interaction beyond clicks and opens. Use campaign dashboards to view events such as 
Email Delivered
, 
Unsubscribed
, 
Spam Reported
, and more.

For each message step in a Journey, you can set conversion conditions with events and properties in your Segment space. Then, define a duration after message delivery to track goals.

For example, track users who perform the event 
Order Completed
 with a promo code that you send them.

Visit 
Message Analytics
 to learn more.

This page was last modified: 23 Aug 2023

Need support?

Questions? Problems? Need more info? Contact Segment Support for assistance!

Help improve these docs!

Was this page helpful?

Thanks for your feedback!

Can we improve this doc? 
Send us feedback!

Get started with Segment

On this page

Was this page helpful?

Thanks for your feedback!

Can we improve
 this doc? 
Send us feedback!

Product

For Developers

Company

Support

© 2025 Segment.io, Inc.

=== Content from https://segment.com/docs/segment-app/iam/scim/ ===

System for Cross-domain Identity Management (SCIM) Configuration Guide
        

Single Sign-on is only available in Business plan workspaces.


See the 
available plans
, or 
contact Support
.

On this page

The SCIM specification is designed to make managing user identities in cloud-based applications like Segment easier. SCIM allows your Identity Provider (IdP) to manage users and groups within your Segment workspace.

Most IdPs offer SCIM, and it complements SAML. You can think of SAML as a way for your employees to authenticate and SCIM as a way to make sure they have the appropriate permissions.

Requirements

Before you start, remember that SSO is only available to Business Tier customers, and that only workspace owners may configure SSO connections.

To set up SCIM, you must first create an SSO connection. Once you 
create your SSO connection
, log back in to Segment using SSO.

Configuration Instructions

Segment officially supports 
Okta
, 
Microsoft Entra ID
, and 
OneLogin
. Each link includes specific setup instructions for that IdP. You should read the 
features
 section of this page to understand which features of SCIM Segment supports.

You may still be able to use SCIM with another Identity Provider (IdP) by adapting the following instructions.

Base URL

Your IdP needs to know where to send SCIM requests. The Segment base URL is: https://scim.segmentapis.com/scim/v2

API Key

The other value you need is an API key (sometimes referred to as an Authorization Header). To generate one, go to 
Settings > Advanced Settings
 in the Segment app, and find the 
SSO Sync
 section. Click 
Generate SSO Token
 and copy the generated token. Use this token for the API key or Authorization Header in your IdP.

You can find this page in the 
settings sidebar of your Segment app
.



Features

It’s important to remember that Segment has a multi-tenant user/workspace relationship, meaning that users can be part of more than one workspaces. In most cases these workspaces are all related to a single customer (for example, a single company might have individual workspaces for different brands or subsidiaries). However, some users can be members of workspaces for different Segment customers, such as with contractors or consultants.

Because of this, Segment must balance the autonomy of users with the desired level of control of a Workspace Owner.

Creating Users

Even though Segment users exist separately from workspaces, your IdP can create a new Segment user or add an existing Segment user to your workspace using the same IdP workflow. This process is transparent to the IdP and to you as the customer. In other words, you don’t need to know if a user exists before adding them to your workspace.

If the person you want to add does not have a Segment account, your IdP will create one. If the person already has a Segment account, you can still add them to your Workspace using your IdP, but it 
does not create a new Segment account
.

You can create new users and set their 
userName
 (email) and 
displayName
 (single value field that represents a user’s full name) using your IdP.

If a user already has a Segment account, you can add them using their email address using your IdP. However, Segment ignores the 
displayName
 sent by the IdP, and instead uses the name chosen by the user when they created their account.

Updating User Attributes

Segment user profiles only contain a 
userName
 (email) and 
displayName
. Once you create a user, you cannot update these attributes using SCIM. They can only be updated by the user through the Segment UI.

Deleting or Deactivating Users

Segment workspace owners 
cannot
 
delete
 Segment workspace member accounts using SCIM, the web UI, or the Segment API. A user must delete their own account using the Segment app. Workspace owners 
can
 
remove members from the workspace
 using SCIM, the web UI, or the Segment API.

Some IdPs want to set users as “inactive” or “active.” Segment does not have an “inactive” state for user accounts. Similar functionality can be achieved by removing a user from your workspace. Setting an existing Segment user to “active” is similar to adding that user to the workspace.

When your IdP updates a user to set 
active: false
, or attempts to delete a user, Segment removes the user from your Segment workspace. If your IdP attempts to create a user with an existing email, or set 
active: true
 on an existing user, Segment adds that existing user account to your workspace.

Any Segment group memberships 
must be reassigned
 when a user is removed and re-added from your workspace. Newly added workspace users have the “Minimal Workspace Access” permission by default. The “Minimal Workspace Access” role does not have access to any sources, destinations, etc.

This reassignment might happen automatically depending on how you configured your IdP. If the user was assigned groups using your IdP, your IdP should automatically re-add the user within Segment. For this reason, Segment 
strongly
 recommends that you create groups in your IdP, push them into Segment, and maintain an active link between your IdP and Segment.

Creating Groups

Your IdP can create new groups in Segment using SCIM. All groups created using SCIM start with “Minimal Workspace Access.” The “Minimal Workspace Access” permission does not have access to any sources, destinations, etc. To add more permissions to a group you must use the Segment web app.

Updating Groups

Your IdP can add or remove workspace members from existing groups using SCIM. Your IdP can also update Segment group names.

Deleting Groups

Your IdP can use SCIM to delete groups from your Segment workspace. Deleting a group in Segment does 
not
 remove its members from your workspace. To remove members from the workspace, unassign the users from Segment from your IdP, then Segment removes them from the workspace.

Attribute Mapping

When you integrate Segment SCIM and your IdP you might need to map attributes for users. The only attributes that Segment SCIM supports are 
userName
 and 
displayName
. You should leave any existing mapping for the 
email
 SAML attribute, which you might have set up during your initial SSO setup. This mapping supports SAML authentication, and is separate from setting up SCIM, but may be within the same page depending on your IdP.

You’ll need to map an email (IdP) to 
userName
 (Segment). Depending on your IdP this attribute might be called 
email
 or 
mail
. If your IdP uses emails for usernames, you can map 
userName
 (IdP) to 
userName
 (Segment).

If your IdP supports the 
displayName
 attribute, you can map it directly to the Segment 
displayName
 attribute. If it does not, most IdPs can create a “macro mapping” which allows you to map more than one field to a single field in Segment.

For example, you might map 
{firstName} {lastName}
 from your IdP to 
displayName
 in Segment. If your IdP doesn’t support this, you can map 
firstName
 (IdP) to 
displayName
 (Segment).

Okta Setup Guide

Generate an API key
, then copy and paste this value into the 
API Token
 field in Okta, and click 
Save
.



Click the 
Assignments
 tab. You can now assign people or groups. Before you continue, read through the 
features section
 in this doc to make sure you understand how groups work. Segment recommends that you assign users to the Segment app by Okta group. This allows you to manage which groups in your organization can authenticate to Segment. You can also assign users individually.



Once you assign your users, push the assigned Okta groups to Segment.



Tip
: You can also link Okta groups to an existing group from in the Segment app using the Okta UI.



Microsoft Entra ID Setup Guide

Instructions for configuring Microsoft Entra ID can be found on the Microsoft Docs website.

Complete the Microsoft Entra ID setup guide for SSO

Complete the Microsoft Entra ID setup guide for SCIM

To make Azure compatible with Segment’s SCIM v2 implementation, append the flag 
?aadOptscim062020
 to the tenant URL as explained in the 
Microsoft Entra ID documentation
. By appending the flag to your tenant URL, your request has the correct structure when you remove a user from a group.

OneLogin Setup Guide

Instructions for configuring OneLogin can be found on the OneLogin Docs website.

Add and configure the Segment SSO integration from within the OneLogin application.

Complete the OneLogin setup Guide for SCIM

This page was last modified: 20 Jun 2024

Need support?

Questions? Problems? Need more info? Contact Segment Support for assistance!

Help improve these docs!

Was this page helpful?

Thanks for your feedback!

Can we improve this doc? 
Send us feedback!

Get started with Segment

On this page

Was this page helpful?

Thanks for your feedback!

Can we improve
 this doc? 
Send us feedback!

Product

For Developers

Company

Support

© 2025 Segment.io, Inc.

=== Content from https://segment.com/docs/connections/functions/destination-functions/ ===

Destination Functions
        

On this page

Destination functions allow you to transform and annotate your Segment events and send them to any external tool or API without worrying about setting up or maintaining any infrastructure.

All functions are scoped to your workspace, so members of other workspaces can’t view or use them.

Functions is available to all customer plan types with a free allotment of usage hours. Read more about 
Functions usage limits
, or see 
your workspace’s Functions usage stats
.



Destination functions don’t accept data from 
Object Cloud sources
 or support 
IP Allowlisting
.

Create a destination function

After you click 
Build
, a code editor appears. Use the editor to write the code for your function, configure settings, and test the function’s behavior.

Tip:
 Want to see some example functions? Check out the templates available in the Functions UI, or in the open-source 
Segment Functions Library
. (Contributions welcome!)

Code the destination function

Segment invokes a separate part of the function (called a “handler”) for each event type that you send to your destination function.

Your function isn’t invoked for an event if you’ve configured a 
destination filter
, and the event doesn’t pass the filter.

The default source code template includes handlers for all event types. You don’t need to implement all of them - just use the ones you need, and skip the ones you don’t.

Destination functions can define handlers for each message type in the 
Segment spec
:

Each of the functions above accepts two arguments:

The example below shows a destination function that listens for “Track” events, and sends some details about them to an external service.

async
 
function
 
onTrack
(
event
)
 
{

  
await
 
fetch
(
'
https://example-service.com/api
'
,
 
{

    
method
:
 
'
POST
'
,

    
headers
:
 
{

      
'
Content-Type
'
:
 
'
application/json
'

    
},

    
body
:
 
JSON
.
stringify
({

      
event_name
:
 
event
.
event
,

      
event_properties
:
 
event
.
properties
,

      
timestamp
:
 
event
.
timestamp

    
})

  
})


}



To change which event type the handler listens to, you can rename it to the name of the message type. For example, if you rename this function 
onIdentify
, it listens for “Identify” events instead.

Functions’ runtime includes a 
fetch()
 polyfill using a 
node-fetch
 package. Check out the 
node-fetch documentation
 for usage examples.

Errors and error handling

Segment considers a function’s execution successful if it finishes without error. You can also 
throw
 an error to create a failure on purpose. Use these errors to validate event data before processing it, to ensure the function works as expected.

You can 
throw
 the following pre-defined error types to indicate that the function ran as expected, but that data was not deliverable:

The examples show basic uses of these error types.

async
 
function
 
onGroup
(
event
)
 
{

  
if 
(
!
event
.
traits
.
company
)
 
{

    
throw
 
new
 
InvalidEventPayload
(
'
Company name is required
'
)

  
}


}



async
 
function
 
onPage
(
event
)
 
{

  
if 
(
!
event
.
properties
.
pageName
)
 
{

    
throw
 
new
 
ValidationError
(
'
Page name is required
'
)

  
}


}



async
 
function
 
onAlias
(
event
)
 
{

  
throw
 
new
 
EventNotSupported
(
'
Alias event is not supported
'
)


}



async
 
function
 
onTrack
(
event
)
 
{

  
let
 
res

  
try
 
{

    
res
 
=
 
await
 
fetch
(
'
http://example-service.com/api
'
,
 
{

      
method
:
 
'
POST
'
,

      
headers
:
 
{

        
'
Content-Type
'
:
 
'
application/json
'

      
},

      
body
:
 
JSON
.
stringify
({
 
event
 
})

    
})

  
}
 
catch 
(
err
)
 
{

    
// Retry on connection error

    
throw
 
new
 
RetryError
(
err
.
message
)

  
}

  
if 
(
res
.
status
 
>=
 
500
 
||
 
res
.
status
 
===
 
429
)
 
{

    
// Retry on 5xx and 429s (ratelimits)

    
throw
 
new
 
RetryError
(
`HTTP Status 
${
res
.
status
}
`
)

  
}


}




If you don’t supply a function for an event type, Segment throws an 
EventNotSupported
 error by default.

You can incorporate a a 
try-catch
 block to ensure smooth operation of functions even when fetch calls fail. This allows for the interception of any errors during the API call, enabling the application of specific error handling procedures, such as error logging for future debugging, or the assignment of fallback values when the API call is unsuccessful. By positioning the continuation logic either outside the 
try-catch
 block or within a 
finally
 block, the function is guaranteed to proceed with its execution, maintaining its workflow irrespective of the outcome of the API call.

You can read more about 
error handling
 below.

Runtime and dependencies

On March 26, 2024, Segment is upgrading the Functions runtime environment to Node.js v18, which is the current long-term support (LTS) release.

This upgrade keeps your runtime current with industry standards. Based on the 
AWS Lambda
 and 
Node.js
 support schedule, Node.js v16 is no longer in 
Maintenance LTS
. Production applications should only use releases of Node.js that are in 
Active LTS
 or 
Maintenance LTS
.

All new functions will use Node.js v18 starting March 26, 2024.

For existing functions, this change automatically occurs as you update and deploy an existing function. Segment recommends that you check your function post-deployment to ensure everything’s working. Your function may face issues due to the change in sytax between different Node.js versions and dependency compatibility.

Limited time opt-out option 


If you need more time to prepare, you can opt out of the update before March 19, 2024. 
 Note that if you opt out: 

- The existing functions will continue working on Node.js v16. 

- You won’t be able to create new functions after July 15, 2024. 

- You won’t be able to update existing functions after August 15, 2024. 

- You won’t receive future bug fixes, enhancements, and dependency updates to the functions runtime. 


Contact Segment
 to opt-out or with any questions. 

Node.js 18 

Segment strongly recommends updating to Node.js v18 to benefit from future runtime updates, the latest security, and performance improvements.

Functions do not currently support importing dependencies, but you can 
contact Segment Support
 to request that one be added.

The following dependencies are installed in the function environment by default.

zlib v1.0.5
 exposed as 
zlib.zlib

 
uuidv5
 is exposed as an object. Use 
uuidv5.uuidv5
 to access its functions. For example:

  
async
 
function
 
onRequest
(
request
,
 
settings
)
 
{

       
uuidv5
 
=
 
uuidv5
.
uuidv5
;

       
console
.
log
(
typeof
 
uuidv5
);


        
//Generate a UUID in the default URL namespace

        
var
 
urlUUID
 
=
 
uuidv5
(
'
url
'
,
 
'
http://google/com/page
'
);

        
console
.
log
(
urlUUID
);


        
//Default DNS namespace

        
var
 
dnsUUID
 
=
 
uuidv5
(
'
dns
'
,
 
'
google.com
'
);

        
console
.
log
(
dnsUUID
);

    
}



zlib
’s asynchronous methods 
inflate
 and 
deflate
 must be used with 
async
 or 
await
. For example:

zlib
 
=
 
zlib
.
zlib
;
  
// Required to access zlib objects and associated functions


async
 
function
 
onRequest
(
request
,
 
settings
)
 
{

  
const
 
body
 
=
 
request
.
json
();


  
const
 
input
 
=
 
'
something
'
;


  
// Calling inflateSync method

  
var
 
deflated
 
=
 
zlib
.
deflateSync
(
input
);


  
console
.
log
(
deflated
.
toString
(
'
base64
'
));


  
// Calling inflateSync method

  
var
 
inflated
 
=
 
zlib
.
inflateSync
(
new
 
Buffer
.
from
(
deflated
)).
toString
();


  
console
.
log
(
inflated
);


  
console
.
log
(
'
Done
'
);

  
}



The following Node.js modules are available:

Other built-in Node.js modules
 aren’t available.

For more information on using the 
aws-sdk
 module, see how to 
set up functions for calling AWS APIs
.

Caching

Basic cache storage is available through the 
cache
 object, which has the following methods defined:

Some important notes about the cache:

const
 
ttl
 
=
 
5
 
*
 
60
 
*
 
1000
 
// 5 minutes


const
 
val
 
=
 
await
 
cache
.
load
(
"
mycachekey
"
,
 
ttl
,
 
async 
()
 
=>
 
{

    
const
 
res
 
=
 
await
 
fetch
(
"
http://echo.jsontest.com/key/value/one/two
"
)

    
const
 
data
 
=
 
await
 
res
.
json
()

    
return
 
data


})



Create settings and secrets

Settings allow you to pass configurable variables to your function, which is the best way to pass sensitive information such as security tokens. For example, you might use 
settings
 as placeholders to use information such as an API endpoint and API key. This way, you can use the same code with different settings for different purposes. When you deploy a function in your workspace, you are prompted to fill out these settings to configure the function.

First, add a setting in 
Settings
 tab in the code editor:



Click 
Add Setting
 to add your new setting.



You can configure the details about this setting, which change how it’s displayed to anyone using your function:

As you change the values, a preview to the right updates to show how your setting will look and work.

Click 
Add Setting
 to save the new setting.

Once you save a setting, it appears in the 
Settings
 tab for the function. You can edit or delete settings from this tab.



Next, fill out this setting’s value in the 
Test
 tab, so you can run the function and verify that the correct setting value is passed. (This value is only for testing your function.)

Now that you’ve configured a setting and entered a test value, you can add code to read its value and run the function, as in the example below:

async
 
function
 
onTrack
(
request
,
 
settings
)
 
{

  
const
 
apiKey
 
=
 
settings
.
apiKey

  
//=> "super_secret_string"


}



When you deploy your destination function in your workspace, you fill out the settings on the destination configuration page, similar to how you would configure a normal destination.

You must pass the settings object to the function at runtime. Functions can’t access the settings object when it’s stored as a global variable.

Test the destination function

You can test your code directly from the editor in two ways:

Use sample events for testing

Click 
Use Sample Event
 and select the source to use events from.



Click 
Run
 to test your function with the event you selected.

Test using manual input

You can also manually include your own JSON payload of a Segment event, instead of fetching a sample from one of your workspace sources.



If your function fails, you can check the error details and logs in the 
Output
 section.

Batching the destination function

Batch handlers are an extension of destination functions. When you define an 
onBatch
 handler alongside the handler functions for single events (for example: 
onTrack
 or 
onIdentity
), you’re telling Segment that the destination function can accept and handle batches of events.

Batching is available for destination and destination insert functions only.

When to use batching

Consider creating a batch handler if:

If a batched function receives too low a volume of events (under one event per second) to be worth batching, Segment may not invoke the batch handler.

Define the batch handler

Segment collects the events over a short period of time and combines them into a batch. The system flushes them when the batch reaches a certain number of events, or when the batch has been waiting for a specified wait time.

To create a batch handler, define an 
onBatch
 function within your destination function.
You can also use the “Default Batch” template found in the Functions editor to get started quickly.

async
 
function
 
onBatch
(
events
,
 
settings
){

  
// handle the batch of events


}



The 
onBatch
 handler is an optional extension. Destination functions must still contain single event handlers as a fallback, in cases where Segment does not receive enough events to execute the batch.

The handler function receives an array of events. The events can be of any supported type and a single batch may contain more than one event type. Handler functions can also receive function settings. Here is an example of what a batch can look like:

[

    
{

      
"type"
:
 
"identify"
,

      
"userId"
:
 
"019mr8mf4r"
,

      
"traits"
:
 
{

        
"email"
:
 
"jake@yahoo.com"
,

        
"name"
:
 
"Jake Peterson"
,

        
"age"
:
 
26

      
}

    
},

    
{

      
"type"
:
 
"track"
,

      
"userId"
:
 
"019mr8mf4r"
,

      
"event"
:
 
"Song Played"
,

      
"properties"
:
 
{

        
"name"
:
 
"Fallin for You"
,

        
"artist"
:
 
"Dierks Bentley"

      
}

    
},

    
{

      
"type"
:
 
"track"
,

      
"userId"
:
 
"971mj8mk7p"
,

      
"event"
:
 
"Song Played"
,

      
"properties"
:
 
{

        
"name"
:
 
"Get Right"
,

        
"artist"
:
 
"Jennifer Lopez"

      
}

    
}


]



For example, you could send the array of events to an external services batch endpoint:

async
 
function
 
onBatch
(
events
,
 
settings
)
 
{

  
await
 
fetch
(
'
https://example-service.com/batch-api
'
,
 
{

    
method
:
 
'
POST
'
,

    
headers
:
 
{

      
'
Content-Type
'
:
 
'
application/json
'

    
},

    
body
:
 
JSON
.
stringify
(
events
)
 
// send a JSON array

  
})


}



Configure the event types within a batch

Segment batches together any event 
of any type
 that it sees over a short period of time to increase batching efficiency and give you the flexibility to decide how batches are created. If you want to split batches by event type, you can implement this in your functions code by writing a handler.

If your downstream endpoint requires events of a single type, you can write a handler that groups events by type, and then handles the events.

async
 
function
 
onBatch
(
events
,
 
settings
)
 
{

  
// group events by type

  
const
 
eventsByType
 
=
 
{}

  
for 
(
const
 
event
 
of
 
events
)
 
{

    
if 
(
!
(
event
.
type
 
in
 
eventsByType
))
 
{

      
eventsByType
[
event
.
type
]
 
=
 
[]

    
}

    
eventsByType
[
event
.
type
].
push
(
event
)

  
}


  
// concurrently process sub-batches of a specific event type

  
const
 
promises
 
=
 
Object
.
entries
(
eventsByType
).
map
(([
type
,
 
events
])
 
=>
 
{

    
switch 
(
type
)
 
{

    
case
 
'
track
'
:

      
return
 
onTrackBatch
(
events
,
 
settings
)

    
case
 
'
identify
'
:

      
return
 
onIdentifyBatch
(
events
,
 
settings
)

    
// ...handle other event types here...

    
}

  
})

  
return
 
Promise
.
all
(
promises
)


}



async
 
function
 
onTrackBatch
(
events
,
 
settings
)
 
{

  
// handle a batch of track events


}



async
 
function
 
onIdentifyBatch
(
events
,
 
settings
)
 
{

  
// handle a batch of identify events


}



Configure your batch parameters

By default, Functions waits up to 10 seconds to form a batch of 20 events. You can increase the number of events included in each batch (up to 400 events per batch) by contacting 
Segment support
. Segment recommends users who wish to include fewer than 20 events per batch use destination functions without the 
onBatch
 handler.

Test the batch handler

The 
Functions editing environment
 supports testing batch handlers.

To test the batch handler:

The Sample Event option tests single events only. You must use Manual Mode to add more than one event so you can test batch handlers.

The editor displays logs and request traces from the batch handler.

The 
Public API
 Functions/Preview endpoint also supports testing batch handlers. The payload must be a batch of events as a JSON array.

Handling batching errors

Standard 
function error types
 apply to batch handlers. Segment attempts to retry the batch in the case of Timeout or Retry errors. For all other error types, Segment discards the batch. It’s also possible to report a partial failure by returning status of each event in the batch. Segment retries only the failed events in a batch until those events are successful or until they result in a permanent error.

[

	
{

		
"status"
:
 
200

	
},

	
{

		
"status"
:
 
400
,

		
"errormessage"
:
 
"Bad Request"

	
},

	
{

		
"status"
:
 
200

	
},

	
{

		
"status"
:
 
500
,

		
"errormessage"
:
 
"Error processing request"

	
},

	
{

		
"status"
:
 
500
,

		
"errormessage"
:
 
"Error processing request"

	
},

	
{

		
"status"
:
 
200

	
},


]



For example, after receiving the responses above from the 
onBatch
 handler, Segment only retries 
event_4
 and 
event_5
.

Save and deploy the function

Once you finish building your destination function, click 
Configure
 to name it, then click 
Create Function
 to save it.

Once you do that, the destination function appears on the 
Functions
 page in your workspace’s catalog.

If you’re editing an existing function, you can 
Save
 changes without updating instances of the function that are already deployed and running.

You can also choose to 
Save & Deploy
 to save the changes, and then choose which of the already-deployed functions to update with your changes. 
You might need additional permissions
 to update existing functions.

Destination functions logs and errors

If your function throws an error, execution halts immediately. Segment captures the event, any outgoing requests/responses, any logs the function might have printed, as well as the error itself.

Segment then displays the captured error information in the 
Event Delivery
 page for your destination. You can use this information to find and fix unexpected errors.

You can throw 
an error or a custom error
 and you can also add helpful context in logs using the 
console
 API
. For example:

async
 
function
 
onTrack
(
event
,
 
settings
)
 
{

  
const
 
userId
 
=
 
event
.
userId


  
console
.
log
(
'
User ID is
'
,
 
userId
)


  
if 
(
typeof
 
userId
 
!==
 
'
string
'
 
||
 
userId
.
length
 
<
 
8
)
 
{

    
throw
 
new
 
ValidationError
(
'
User ID is invalid
'
)

  
}


  
console
.
log
(
'
User ID is valid
'
)


}



Warning:
 Do not log sensitive data, such as personally-identifying information (PII), authentication tokens, or other secrets. Avoid logging entire request/response payloads. The 
Function Logs
 tab may be visible to other workspace members if they have the necessary permissions.

Caching in destination functions

Functions execute only in response to incoming data, but the environments that functions run in are generally long-running. Because of this, you can use global variables to cache small amounts of information between invocations. For example, you can reduce the number of access tokens you generate by caching a token, and regenerating it only after it expires. Segment cannot make any guarantees about the longevity of environments, but by using this strategy, you can improve the performance and reliability of your Functions by reducing the need for redundant API requests.

This example code fetches an access token from an external API and refreshes it every hour:

const
 
TOKEN_EXPIRE_MS
 
=
 
60
 
*
 
60
 
*
 
1000
 
// 1 hour


let
 
token
 
=
 
null


async
 
function
 
getAccessToken
 
()
 
{

  
const
 
now
 
=
 
new
 
Date
().
getTime
()

  
if 
(
!
token
 
||
 
now
 
-
 
token
.
ts
 
>
 
TOKEN_EXPIRE_MS
)
 
{

    
const
 
resp
 
=
 
await
 
fetch
(
'
https://example.com/tokens
'
,
 
{

      
method
:
 
'
POST
'

    
}).
then
(
resp
 
=>
 
resp
.
json
())

    
token
 
=
 
{

      
ts
:
 
now
,

      
value
:
 
resp
.
token

    
}

  
}

  
return
 
token
.
value


}



Managing destination functions

Functions permissions

Functions have specific roles which can be used for 
access management
 in your Segment workspace.

Access to functions is controlled by two permissions 
roles
:

You also need additional 
Source Admin
 permissions to enable source functions, connect destination functions to a source, or to deploy changes to existing functions.

Editing and deleting functions

If you are a 
Workspace Owner
 or 
Functions Admin
, you can manage your function from the 
Functions
 page.

Monitoring destination functions

You can use 
Destination Event Delivery
 to understand if Segment encounters any issues delivering your source data to destinations. Errors that the Function throws appear here.

If any of your deployed function instances are failing consistently, they will also appear in 
Connection Health
.

Data control

In addition to using 
Destination Filters
 and the 
Privacy Portal
 to manage which events and properties are sent to your destination function, you can reference the destination function directly in the integrations object of the Segment payload. For example:

...


"integrations"
:
 
{

  
"All"
:
 
false
,

  
"Amplitude"
:
 
true
,

  
"Customer.io"
:
 
true
,

  
"Google Analytics"
:
 
true
,

  
"My Destination Function (My Workspace)"
:
 
true


}


...



In the example above, the integrations object directly references and enables the destination function (
My Destination Function
), located inside your workspace (
My Workspace
). Include the workspace name in parentheses, as shown in the example above. Like all items in the integration object, destination function and workspace names are case sensitive.

Destination functions FAQs

Yes, Functions access is logged in the 
Audit Trail
, so user activity related to functions appears in the logs.

Yes, Segment retries invocations that throw RetryError or Timeout errors (temporary errors only). Segment’s internal system retries failed functions API calls for four hours with a randomized exponential backoff after each attempt. This substantially improves delivery rates.

Retries
 work the same for both functions and cloud-mode destinations in Segment.

No, Segment can’t guarantee the order in which the events are delivered to an endpoint.

No, destination functions are currently available as cloud-mode destinations only. Segment is in the early phases of exploration and discovery for supporting customer “web plugins” for custom device-mode destinations and other use cases, but this is unsupported today.

If you are a partner, looking to publish your destination and distribute your app through Segment catalog, visit the 
Developer Center
 and check out the Segment 
partner docs
.

The 
Event Delivery tab
 continues to show metrics for individual events, even if they are batched by your function code. For more information, see 
Destination functions logs and errors
.

A function’s use depends on the number of times it’s invoked, and the amount of time it takes to execute. When you enable batching, Segment invokes your function 
once per batch
 rather than once per event. The volume of events flowing through the function determines the number of batches, which determines the number of invocations.

If you’re sending your batch to an external service, the execution time of the function depends on the end-to-end latency of that service’s batch endpoint, which may be higher than an endpoint that receives a single event.

When data leaves Segment’s servers to go to various destinations (not including warehouses), Segment uses Amazon Web Services (AWS) and utilizes many different machines in order to send requests.

The IP addresses that are used to send these requests can be found 
on Amazon’s website
. If you want to allowlist these specific IP addresses, you need to allowlist all of the IP addresses from your workspace’s location range. Below are the ranges:

Yes, to do so, remove the 
messageId
 and the 
writeKey
 from the payload in your Function code. Leaving either field on your payload will cause unexpected behavior that may cause your event to be delivered to the wrong source or to not be delivered at all.

Incorporating console.log() statements in your Destination Function code aids in debugging. However, logs generated by these statements will only be accessible in the 
Event Delivery
 view if the payloads encounter errors during processing. Logs from successfully processed payloads are not displayed.

The test function interface has a 4KB console logging limit. Outputs larger than this limit are not visible in the user interface.

This page was last modified: 17 Jan 2025

Need support?

Questions? Problems? Need more info? Contact Segment Support for assistance!

Help improve these docs!

Was this page helpful?

Thanks for your feedback!

Can we improve this doc? 
Send us feedback!

Get started with Segment

On this page

Was this page helpful?

Thanks for your feedback!

Can we improve
 this doc? 
Send us feedback!

Product

For Developers

Company

Support

© 2025 Segment.io, Inc.

=== Content from https://segment.com/docs/guides/how-to-guides/collect-on-client-or-server/ ===

Collecting Data on the Client or Server
        

One of the most common questions Segment receives is: “Should I use one of your client-side libraries or one of your server-side libraries?”

This is such an important topic that you’ll find an in-depth article in Segment’s Analytics Academy:  
When to Track on the Client vs Server
. It’s worth a read. Below, you can also read some quick logic around why you may want to choose either option.

Client-side

Good things to send from the client-side are things that you wouldn’t usually store in your database. Things like page views, button clicks, page scroll length, mouse movements, social shares, and likes.

Things like UTM tags, operating system, device type, or cookied data like returning visitors are all easiest to track client-side. Of course, some things like mouse movements are only available on the client-side so you should definitely track that there.

Some destinations can only accept data when the event is sent from the browser. They require events on the client since they rely on cookies and most of those tools do not have an API that Segment can send server-side data to. More on this in Segment’s 
Analytics.js docs
.

Server-side

Charging customers often happens when they aren’t online, and accuracy for payments is so important. Server-side tracking tends to be more accurate than user devices since it’s a more controlled environment.

In general client-side data is fine for watching general trending, but it’s never going to be perfect. Especially if your customers are likely to use things like adblock or old/non-standard browsers.

For example, if you’re sending triggered emails based on events, it’s probably a good idea to make sure your user profiles are sent through Segment’s servers so no one gets left out or mis-emailed.

Another good type of data to send server-side are things that need to be calculated from a database query. This might be something like “Friend Count” if your site or app is a social network.

Sensitive information is also best kept out of browsers. Any data you don’t want exposed to users should be sent server-side.

Selecting Destinations

Each Segment library allows an 
integrations
 object either as a top level object or nested in options object. 

This flag may be especially useful in Legacy source types, where an event might be triggered on both the client and server for various reasons. The following will cause the payload to be sent to all enabled tools 
except
 Facebook Pixel:

    
analytics
.
identify
(
'
user_123
'
,
 
{

      
email
:
 
'
jane.kim@example.com
'
,

      
name
:
 
'
Jane Kim
'

      
},
 
{

        
integrations
:
 
{

          
'
Facebook Pixel
'
:
 
false

        
}

      
});



This page was last modified: 07 Mar 2024

Need support?

Questions? Problems? Need more info? Contact Segment Support for assistance!

Help improve these docs!

Was this page helpful?

Thanks for your feedback!

Can we improve this doc? 
Send us feedback!

Get started with Segment

Was this page helpful?

Thanks for your feedback!

Can we improve
 this doc? 
Send us feedback!

Product

For Developers

Company

Support

© 2025 Segment.io, Inc.

=== Content from https://segment.com/docs/engage/analytics/ ===

Analytics Overview
        

Engage Premier requires a Business tier account and includes Engage Foundations and Unify.


See the 
available plans
, or 
contact Support
.

On this page

Twilio Engage provides you with analytics that give you insight into the performance of your 
email, SMS, and WhatsApp campaigns
.

On this page, you’ll learn how Engage calculates campaign analytics and which messaging metrics you can view.

Access a campaign’s analytics

You’ll find a campaign’s analytics within its parent Journey, using the following instructions:

How Engage measures campaign analytics

Understanding when and how Engage measures campaign analytics will help you as you review your analytics reports.

Engage begins tracking campaign performance after you send a campaign. As a result, a campaign’s analytics only reflect conversions that occurred after campaign publication. For example, suppose you send an email campaign promoting a sale in your online store. If a customer purchases a qualifying product 
before
 receiving your campaign, their purchase would not count as a conversion.

Changing a campaign’s base metrics

Clicking on a campaign in a Journey opens a side panel that shows your campaign’s analytics. You can change both the date range and the base percentage type for any campaign.



The date range picker initially inherits the date range set in the campaign’s parent Journey, but you can use the campaign date range picker to define a range for the specific campaign you’re viewing. Changing a campaign’s date range won’t impact the parent Journey’s date range.

By default, Engage bases a campaign’s metrics on the number of sent messages. The 
Calculate metrics using
 dropdown lets you change this denominator value so that Engage bases analytics on the number of delivered messages. Changing the denominator won’t impact the 
Converted
 metric, though, since conversions are already based on delivered messages.

Email metrics

The following table lists the email campaign metrics that Engage tracks:

SendGrid powers Engage’s email campaign event analytics. For more details on email metrics, view SendGrid’s 
Marketing Campaigns Statistics Overview
.

Understanding machine opens

Machine opens occur when an email client automatically opens an email, giving the impression that a user opened your email even if they haven’t.

Segment tracks machine opens and subtracts them from the total number of opened emails. Segment displays this number in the 
True Opened
 tile, which gives a more reliable count of how many real users opened your campaign.

Hover over the 
True Opened
 tile for any campaign to see a full comparison of machine opens and true opens.

SMS metrics

The following table lists the SMS campaign metrics that Engage tracks:

WhatsApp metrics

The following table lists the WhatsApp campaign metrics that Engage tracks:

This page was last modified: 21 Jun 2023

Need support?

Questions? Problems? Need more info? Contact Segment Support for assistance!

Help improve these docs!

Was this page helpful?

Thanks for your feedback!

Can we improve this doc? 
Send us feedback!

Get started with Segment

On this page

Was this page helpful?

Thanks for your feedback!

Can we improve
 this doc? 
Send us feedback!

Product

For Developers

Company

Support

© 2025 Segment.io, Inc.

=== Content from https://segment.com/docs/privacy/complying-with-the-gdpr/ ===

Complying with the GDPR
        

On this page

On May 25, 2018 businesses faced the greatest regulatory change in data privacy policy since the 1995 EU Data Protection Directive was enacted: the 
EU General Data Protection Regulation
 (GDPR). The European Union began enforcing the GDPR on May 25, 2018 in an effort to strengthen the security and protection of personal data of EU residents.

In keeping with Segment’s ongoing commitment to privacy and security, Segment updated its practices to be GDPR compliant before the May 25, 2018, enforcement date. But that’s not all. As the central record for your customer data, Segment is also committed to making it easier for you to comply with the GDPR.

Specifically, here is how Segment supports its customers:

An updated Data Protection Addendum (DPA) to reflect the requirements of the GDPR and to ensure compliant data transfer with storage outside the EU. Existing customers can enter into the updated Data Protection Addendum using 
the opt-in process
.

New product capabilities to help you be compliant when users request you delete or suppress their data.

Check out Segment’s 
GDPR blog post
 to learn about Segment’s plan for GDPR readiness.

How does the GDPR impact your business?

The GDPR has different requirements depending on how your business interacts with personal data. Companies can be data controllers, data processors, or, in some cases, both a controller and a processor. Data controllers are businesses that collect their end users’ data and decide why and how that data is processed. On Segment’s 
marketing website
, for example, Segment is considered a 
data controller
. As a vendor, however, the more meaningful way Segment is impacted by the GDPR is as a 
data processor
, as Segment is a company that helps its customers with the processing of their customer data.

In addition to damaging your customers’ trust, failure to comply with the GDPR can result in fines of €20 million or 4% of global annual turnover for the previous year (whichever is greater).

What are your responsibilities as a data controller?

If you collect data about EU residents and decide why and how those data are collected and processed, you may be considered a 
data controller
 under the GDPR. Data controllers are responsible for implementing adequate technical, organizational, and operational measures to ensure and demonstrate that all data collection and processing is performed in accordance with the GDPR, including entering into a relevant data processing agreement. Moreover, you must fulfill data subjects’ rights with respect to their data along the following principles:

Segment recommends reading the full text of the 
GDPR
 to better understand these rights and seeking independent legal advice regarding your obligations under the GDPR. You can also check out publications by data privacy associations such as the 
International Association of Privacy Professionals (IAPP)
 for the latest news. 

Things you can do to address GDPR

In addition to seeking independent legal advice regarding your obligations under the GDPR, here are some tips to get you started:

Educate yourself on the provisions of the 
GDPR
 to understand how they may differ from your existing data protection obligations and practices.

If you don’t have dedicated data privacy or security personnel in-house, consider appointing a directly responsible individual (DRI) or small team to manage your company’s GDPR compliance efforts.

Create an up-to-date inventory of personal data that you collect and manage. -

For data flowing through Segment, you can start with the Overview page in your workspace to understand where you are collecting (Sources) and routing (Destinations) customer data. Next, visit the Schema page within each of your Sources to understand the type of data you’re sending to Segment.

Be sure to consider the data that is not flowing through Segment. You’ll need to make sure the same bar for compliance is met across your organization.

Create a list of vendors who you send data to (analytics tools, CRMs, email tools, etc.), and understand whether they are a controller or a processor. Then, determine what their obligations are, and make sure they have a plan to be ready for the GDPR.

Develop a plan for obtaining and managing 
consent
 in accordance with the GDPR or establish other lawful grounds for using personal data.

Determine if your company needs to appoint a 
Data Protection Officer
 (DPO). If you will be appointing a DPO, begin searching for the best person for the role.

Becoming GDPR compliant takes time, and will require you to rethink how you collect and manage customer data. If you have any questions about the GDPR or want to learn how Segment can help you prepare, 
let us know
!

Opting into the Data Protection Addendum and Standard Contractual Clauses

Segment offers a Data Protection Addendum (DPA) and Standard Contractual (SCCs) as a means of meeting contractual requirements of applicable data privacy laws and regulations, such as GDPR, and to address international data transfers. Segment’s online 
Data Protection Addendum
 (DPA) is already part of and incorporated into the 
Terms of Service
. If you have a separate written agreement with Segment that does not include a Data Protection Addendum (DPA) or you would like to replace the existing Data Protection Addendum (DPA) that is attached to your separate written agreement with Segment’s latest Data Protection Addendum (DPA), please contact your account team or 
customer support
.

Segment offers a Data Protection Addendum (DPA) and Standard Contractual Clauses (SCCs) as a means of meeting the regulatory contractual requirements of GDPR in its role as processor and also to address international data transfers.

Schrems II

Despite the CJEU’s July 2020 ruling invalidating Privacy Shield as a means of validly transferring data to the USA from the EU, these developments are not expected to disrupt Segment’s ability to provide services to its EU customers as the European Court of Justice has reaffirmed that the Standard Contractual Clauses (SCC) remain valid as a method of transfer. Segment’s standard Data Protection Addendum includes a provision whereby should Privacy Shield ever be invalidated (as is the case now) then the SCCs will automatically apply.

This page was last modified: 07 Feb 2025

Need support?

Questions? Problems? Need more info? Contact Segment Support for assistance!

Help improve these docs!

Was this page helpful?

Thanks for your feedback!

Can we improve this doc? 
Send us feedback!

Get started with Segment

On this page

Was this page helpful?

Thanks for your feedback!

Can we improve
 this doc? 
Send us feedback!

Product

For Developers

Company

Support

© 2025 Segment.io, Inc.

=== Content from https://segment.com/docs/connections/sources/debugger/ ===

Using the Source Debugger
        

The Source Debugger is a real-time tool that helps you confirm that API calls made from your website, mobile app, or servers arrive to your Segment Source, so you can troubleshoot your Segment set up even quicker. With the Debugger, you can check that calls are sent in the expected format without having to wait for any data processing.

The Source Debugger’s event order may not reflect how events send downstream or are received by connected destinations. The Debugger primarily confirms incoming data and provides a basic view of its structure. For a reliable record of the data you send to Segment, Segment advises you to attach a raw storage destination to your sources.



The Debugger is separate from your workspace’s data pipeline and is not an exhaustive view of all the events ever sent to your Segment workspace. The Debugger only shows a sample of the events that the Source receives in real time, with a cap of 500 events. The Debugger is a great way to test specific parts of your implementation to validate that events are being fired successfully and arriving to your Source.

To see a more complete view of all your events, Segment recommends that you set up a 
warehouse
 or an 
S3 destination
.

The Debugger shows a live stream of sampled events arriving into the Source, but you can also pause the stream from displaying new events by toggling “Live” to “Pause”. Events continue to arrive to your Source while you Pause the stream.

You can search in the Debugger to find a specific payload using any information you know is available in the event’s raw payload. You can also use advanced search options to limit the results to a specific event.



Two views are available when viewing a payload:

This page was last modified: 11 Apr 2024

Need support?

Questions? Problems? Need more info? Contact Segment Support for assistance!

Help improve these docs!

Was this page helpful?

Thanks for your feedback!

Can we improve this doc? 
Send us feedback!

Get started with Segment

Was this page helpful?

Thanks for your feedback!

Can we improve
 this doc? 
Send us feedback!

Product

For Developers

Company

Support

© 2025 Segment.io, Inc.

=== Content from https://segment.com/docs/engage/settings/ ===

Engage Settings
        

On this page

The 
Engage settings
 tab contains key information about the services you’ve connected to your Engage space.

Channels settings

The Engage settings 
Channels
 tab shows the credentials that you used to connect Engage with the SendGrid and Twilio services that power 
Engage campaigns
.

To change the SendGrid or Twilio accounts associated with your Channels campaigns, select the pencil icon next to the service you need to modify. Engage then displays the email or SMS service setup screen with the fields that you’ll need to edit.

If you see no credentials listed under the Channels tab, it means you’ve not yet 
set up Channels
; refer to the 
Twilio Engage onboarding page
 for instructions on how to connect Engage to both Twilio and SendGrid.

Destinations settings

The 
Destinations
 tab lists the downstream tools receiving your Engage data. Selecting a destination from the list gives you a detailed view of the audiences, computed traits, and journeys that Segment sends to the destination.

To add a destination, select the 
+ Add destination
 button, or navigate to 
Connections > Destinations
 within your Segment workspace. To learn more about sending Engage information to Segment destinations, view the 
Using Engage Data
 documentation.

You can delete a destination from the Destinations tab in the Engage settings (
Engage > Engage settings > Destinations
).

Warehouse sources

By connecting your existing warehouses to Engage, you can import customer or account data and use it to build SQL traits. The Warehouse sources tab displays the warehouses sending data to Engage.

To add a new data warehouse, select the 
+ Add warehouse source
 button. For more information on working with your imported warehouse data in Engage, read the 
Engage SQL traits
 guide.

Engage Events source

The Engage Events source lets you sync subscription states, messaging events, and marketing analytics to downstream destinations. To find your Engage Events source in your Segment workspace, navigate to 
Connections > Sources
 and select 
Engage Events
.

For more information, view the 
Engage Events Source documentation
.

This page was last modified: 30 Jan 2024

Need support?

Questions? Problems? Need more info? Contact Segment Support for assistance!

Help improve these docs!

Was this page helpful?

Thanks for your feedback!

Can we improve this doc? 
Send us feedback!

Get started with Segment

On this page

Was this page helpful?

Thanks for your feedback!

Can we improve
 this doc? 
Send us feedback!

Product

For Developers

Company

Support

© 2025 Segment.io, Inc.

=== Content from https://segment.com/docs/segment-app/support-access/ ===

Support Access
        

On this page

To best assist you after you submit a support ticket, Segment’s Customer Success Engineers may request temporary access to your workspace. Once you grant access to your workspace, CSEs can access your workspace for up to 7 days, or until you revoke access.

Support Access is not available for workspaces using forced-SSO at this time.

Granting a Segment Support Engineer access to your account

Support Access is available for all Segment Workspace Owners and can be found on the 
Support Access
 tab of the  
Workspace Settings
 page.

Click 
Grant Access
 to allow a Segment Support Engineer to access your account.

Support privileges

When you grant Support Access, the Segment Support Engineers can do everything that you can do in your workspace.

How do I know it’s working?

Workspace Owners of Business Tier workspaces can view any Support Access actions in the 
Audit Trail
. You can see and monitor:

This page was last modified: 10 Nov 2022

Need support?

Questions? Problems? Need more info? Contact Segment Support for assistance!

Help improve these docs!

Was this page helpful?

Thanks for your feedback!

Can we improve this doc? 
Send us feedback!

Get started with Segment

On this page

Was this page helpful?

Thanks for your feedback!

Can we improve
 this doc? 
Send us feedback!

Product

For Developers

Company

Support

© 2025 Segment.io, Inc.

=== Content from https://segment.com/docs/engage/journeys/send-data/ ===

Send Journeys data to a Destination
        

Engage Foundations requires a Business tier account and includes Unify.


See the 
available plans
, or 
contact Support
.

On this page

When you send data to destinations, you send a series of events or user lists, depending on the destination type.

Before you begin

Ensure you have connected and enabled destinations in your Space.

Send data to destinations

To include an advertising destination in a Journey, ensure you have connected and enabled the destination within your Space, then utilize the 
Show an Ad
 step.

Test event payloads

With the Engage event tester, you can send a test event payload to a Destination. As a result, you can confirm that you’ve correctly configured Journey Audiences before you publish your Journey.

Follow these steps to send a test event:

If your Destination successfully handled the event, Segment displays a 
200 OK
 HTTP status code along with the full response. If an error occurred, Segment displays any available details in the Event lifecyle section.

Use Trait Activation with Journeys

Use Trait Enrichment and ID Sync to configure sync payloads that you send from Journeys to your destination.

To use Trait Activation with Journeys:

Use Segment’s 
Duplicate mappings
 feature to create an exact copy of an existing mapping. The copied mapping has the same configurations and enrichments as your original mapping.

What events are sent to destinations?

The data type you send to a destination depends on whether the destination is an Event destination or a List destination.

To view the events that get generated by an Engage Space’s Journeys, navigate to 
Unify settings > Debugger
 to view the list of sources that are configured to generate events for 
each destination instance
. Each source generates events only to its connected destinations. Under the source’s Debugger tab, you’ll find the most recent events generated by that source according the connected destinations’ audiences and computed traits.

The full JSON body of a journey event will have the journey’s specific details found under the 
context.personas
 object. These fields can be useful when building out 
Destination Filters
, 
Actions destination mappings
, and 
Functions
.

The integrations object in these payloads will appear as 
{"All" : false,}
 and only list some destinations. This is due to the fact that each source has multiple destinations connected, while each journey may only have a subset of destinations connected to it. See 
Filtering with the Integrations Object
 for more information. The integrations object routing specific events to its specified destinations is also why a destination’s 
Delivery Overview
 tab will show a large number of events under the 
Filtered at destination
 box, as that destination will only receive the events intended to be sent to it according to the journeys that are connected to that specific destination.

Event destination

The format in which the destination receives updates depends on the call type.

When the user enters the step:

{

  
"context"
:
 
{

    
"personas"
:
 
{

      
"computation_class"
:
 
"audience"
,
 
//
 
the
 
type
 
of
 
computation

      
"computation_id"
:
 
"aud_###"
,
 
//
 
the
 
audience's
 
ID
,
 
found
 
in
 
the
 
URL

      
"computation_key"
:
 
"j_o_###"
,
 
//
 
the
 
configured
 
journey
 
key
 
that
 
appears
 
on
 
user
 
profile

      
"namespace"
:
 
"spa_###"
,
 
//
 
the
 
Engage
 
Space's
 
ID

      
"space_id"
:
 
"spa_###"
 
//
 
the
 
Engage
 
Space's
 
ID

    
}

  
},

  
"type"
:
 
"track"
,

  
"event"
:
 
"Audience Entered"
,

  
"properties"
:
 
{

    
"j_o_first_purchase__opened_email_dje83h"
:
 
"true"

  
}


}



When the user enters the step:

{

  
"context"
:
 
{

    
"personas"
:
 
{

      
"computation_class"
:
 
"audience"
,
 
//
 
the
 
type
 
of
 
computation

      
"computation_id"
:
 
"aud_###"
,
 
//
 
the
 
audience's
 
ID
 
found
 
in
 
the
 
URL

      
"computation_key"
:
 
"j_o_###"
,
 
//
 
the
 
configured
 
journey
 
key
 
that
 
appears
 
on
 
user
 
profile

      
"namespace"
:
 
"spa_###"
,
 
//
 
the
 
Engage
 
Space's
 
ID

      
"space_id"
:
 
"spa_###"
 
//
 
the
 
Engage
 
Space's
 
ID

    
}

  
},

  
"type"
:
 
"identify"
,

  
"traits"
:
 
{

    
"j_o_first_purchase__opened_email_dje83h"
:
 
"true"

  
}


}



List destination

The destination receives a list of users who qualify for the associated journey step. Unlike lists associated with Engage Audiences, users who are added to a journey list cannot be subsequently removed. See 
best practices
 for techniques to suppress targeting with journey lists. List destinations do not have access to the Event tester.

For more information, see 
Using Engage Data
.

This page was last modified: 11 Sep 2024

Need support?

Questions? Problems? Need more info? Contact Segment Support for assistance!

Help improve these docs!

Was this page helpful?

Thanks for your feedback!

Can we improve this doc? 
Send us feedback!

Get started with Segment

On this page

Was this page helpful?

Thanks for your feedback!

Can we improve
 this doc? 
Send us feedback!

Product

For Developers

Company

Support

© 2025 Segment.io, Inc.

=== Content from https://segment.com/docs/connections/storage/warehouses/faq/ ===

Warehouse FAQs
        

On this page

Can I control what data is sent to my warehouse?

Yes. Customers on Segment’s 
Business plan
 can choose which sources, collections, and properties sync to your data warehouse using 
Warehouse Selective Sync
.

Selective Sync helps manage the data Segment sends to each warehouse, allowing you to sync different sets of data from the same source to different warehouses.

When you disable a source, Segment no longer syncs data from that source. The historical data from the source remains in your warehouse, even after you disable a source. When you re-enable a source, Segment will automatically sync all events since the last successful data warehouse sync.

When you disable and then re-enable a collection or a property, Segment does not automatically backfill the events since the last successful sync. The only data in the first sync following the re-enabling of a collection or property is any data generated after you re-enabled the collection or property. To recover any data generated while a collection or property was disabled, please reach out to 
friends@segment.com
.

You can also use the 
Integration Object
 to control whether or not data is sent to a specific warehouse.

Don’t send data to any Warehouse

integrations
:
 
{

 
All
:
 
true
,

 
Warehouses
:
 
{

    
all
:
 
false

 
}


}



Send data to all Warehouses

integrations
:
 
{

 
All
:
 
false
,

 
Warehouses
:
 
{

    
all
:
 
true
,

 
}


}



Send data to specific Warehouses

integrations
:
 
{

 
All
:
 
false
,

 
Warehouses
:
 
{

    
warehouseIds
:
 
[
"
<id1>
"
,
 
"
<id2>
"
]

 
}


}



Can we add, tweak, or delete some of the tables?

You have full admin access to your Segment Warehouse. However, don’t tweak or delete Segment generated tables, as this may cause problems for the systems that upload new data.

If you want to join across additional datasets, feel free to create and upload additional tables.

Can we transform or clean up old data to new formats or specs?

This is a common question if the data you’re collecting has evolved over time. For example, if you used to track the event 
Signup
 but now track 
Signed Up
, you’d probably like to merge those two tables to make querying simple and understandable.

Segment does not have a way to update the event data in the context of your warehouse to retroactively merge the tables created from changed events. Instead, you can create a “materialized” view of the unioned events. This is supported in 
Redshift
, 
Postgres
, 
Snowflake
, and others, but may not be available in 
all
 warehouses.

Protocols customers can also use 
Transformations
 to change events at the source, which applies to all cloud-mode destinations (destinations that receive data from the Segment servers) 
including
 your data warehouse. Protocols Transformations offer an excellent way to quickly resolve implementation mistakes and help transition events to a Segment spec.

Note
: Transformations are currently limited to event, property and trait name changes, and do 
not
 apply to historical data.

Can I change the data type of a column in the warehouse?

Yes. Data types are initially set up in your warehouse based on the first value that comes in from a source, but you can request data type changes by reaching out to 
Segment support
 for assistance.

Keep in mind that Segment only uses 
general data types
 when loading data in your warehouse. Therefore, some of the common scenarios are:

More granular changes (such as the examples below) wouldn’t normally be handled by the Support team, thus they often need to be made within the warehouse itself:

Can the data type definitions in Protocols be enforced in a warehouse schema?

The data type definitions in Protocols have no impact on the warehouse schema.

How do I find my source slug?

Your source slug can be found in the URL when you’re looking at the source destinations page or live debugger. The URL structure will look like this:

https://segment.com/[my-workspace]/sources/[my-source-slug]/overview

How do I find my warehouse id?

Your warehouse id appears in the URL when you look at the 
warehouse destinations page
. The URL structure looks like this:

app.segment.com/[my-workspace]/warehouses/[my-warehouse-id]/overview

How fresh is the data in Segment Warehouses?

Data is available in Warehouses within 24-48 hours, depending on your tier’s sync frequency. For more information about sync frequency by tier, see 
Sync Frequency
.

Real-time loading of the data into Segment Warehouses would cause significant performance degradation at query time. To optimize for your query speed, reliability, and robustness, Segment guarantees that your data will be available in your warehouse within 24 hours. The underlying datastore has a subtle tradeoff between data freshness, robustness, and query speed. For the best experience, Segment needs to balance all three of these.

What if I want to add custom data to my warehouse?

You can freely load data into your Segment Warehouse to join against your source data tables.

The only restriction when loading your own data into your connected warehouse is that you should not add or remove tables within schemas generated by Segment for your sources. Those tables have a naming scheme of 
<source-slug>.<table>
 and should only be modified by Segment. Arbitrarily deleting columns from these tables may result in mismatches upon load.

If you want to insert custom data into your warehouse, create new schemas that are not associated with an existing source, since these may be deleted upon a reload of the Segment data in the cluster.

Segment recommends scripting any sort of additions of data you might have to warehouse, so that you aren’t doing one-off tasks that can be hard to recover from in the future in the case of hardware failure.

Which IPs should I allowlist?

Segment recommends enabling IP allowlists for added security. All Segment users with workspaces hosted in the US who use allowlists in their warehouses must update those allowlists to include the following ranges:

Users with workspaces in the EU must allowlist 
3.251.148.96/29
.

Will Segment sync my historical data?

Segment loads up to two months of your historical data when you connect a warehouse.

For full historical backfills you’ll need to be a Segment Business plan customer. If you’d like to learn more about our Business plan and all the features that come with it, 
check out our pricing page
.

What do you recommend for Postgres: Amazon or Heroku?

Heroku’s simple set up and administration process make it a great option to get up and running quickly.

Amazon’s service has some more powerful features and will be more cost-effective for most cases. However, first time users of Amazon Web Services (AWS) will likely need to spend some time with the documentation to get set up properly.

How do I prevent a source from syncing to some or all warehouses?

When you create a new source, the source syncs to all warehouse(s) in the workspace by default. You can prevent the source from syncing to some or all warehouses in the workspace in two ways:

After a source is created, you can enable or disable a warehouse sync within the Warehouse Settings page.

Can I be notified when warehouse syncs fail?

If you enabled activity notifications for your storage destination, you’ll receive notifications in the Segment app for the fifth and 20th consecutive warehouse failures for all incoming data. Segment does not track failures on a per connection (‘source<>warehouse’) basis. Segment’s notification structure also identifies global issues encountered when connecting to your warehouse, like bad credentials or being completely inaccessible to Segment.

To sign up for warehouse sync notifications:

How is the data formatted in my warehouse?

Data in your warehouse is formatted into 
schemas
, which involve a detailed description of database elements (tables, views, indexes, synonyms, etc.)
and the relationships that exist between elements. Segment’s schemas use the following template: 
<source>.<collection>.<property>
, for example,

segment_engineering.tracks.user_id
, where source refers to the source or project name (segment_engineering), collection refers to the event (tracks),
and the property refers to the data being collected (user_id). 
Note:
 It is not possible to have different sources feed data into the same schema in your warehouse. While setting up a new schema, you cannot use a duplicate schema name.

Schema data for Segment warehouses is represented in snake case.

For more information about Warehouse Schemas, see the 
Warehouse Schemas
 page.

If my syncs fail and get fixed, do I need to ask for a backfill?

If your syncs fail, you do not need to reach out to Segment Support to request a backfill. Once a successful sync takes place,
Segment automatically loads all of the data generated since the last successful sync occurred.

Can I change my schema names once they’ve been created?

Segment stores the name of your schema in the 
SQL Settings
 page. Changing the name of your schema in the app without updating the name in your data warehouse causes a new schema to form, one that doesn’t contain historical data.

To change the name of your schema without disruptions:

Note
: This will set the schema name for all existing and future destinations. The new name must be lowercase and may include underscores.

Can I selectively filter data/events sent to my warehouse based on a property?

At the moment, there isn’t a way to selectively filter events that are sent to the warehouse. The warehouse connector works quite differently from our streaming destinations and only has the 
selective sync
 functionality that allows you to enable/disable specific properties or events.

Can data from multiple sources be synced to the same database schema?

It’s not possible for different sources to sync data directly to the same schema in your warehouse. When setting up a new schema within the Segment UI, you can’t use a schema name that’s already in use by another source. Segment recommends syncing the data separately and then joining it downstream in your warehouse.

For more information about Warehouse Schemas, see the 
Warehouse Schemas
 page.

This page was last modified: 04 Feb 2025

Need support?

Questions? Problems? Need more info? Contact Segment Support for assistance!

Help improve these docs!

Was this page helpful?

Thanks for your feedback!

Can we improve this doc? 
Send us feedback!

Get started with Segment

On this page

Was this page helpful?

Thanks for your feedback!

Can we improve
 this doc? 
Send us feedback!

Product

For Developers

Company

Support

© 2025 Segment.io, Inc.

=== Content from https://segment.com/docs/guides/intro-impl/ ===

Segment for Developers
        

On this page

This guide explains all you need to know to get started with your Segment implementation, and directs you to more resources depending on your specific needs.

If you haven’t already, you should read the 
detailed explanation of Segment
 on the previous page!

See a quick example of Segment working on an ecommerce website. (Must be logged in to access.)

What does Segment do?

Segment sends messages about activities in your mobile apps, websites or servers, receives those messages, and translates and forwards the message content to Destination tools. It also can send the contents of those messages to a bulk storage destination for archiving. In more complicated implementations, Segment can serve as a wrapper to trigger messages directly to other APIs, and can inspect, correct, classify and block the message contents.

Types of Segment messages

Segment’s libraries generate and send messages to our tracking API in JSON format, and provide a standard structure for the basic API calls. We also provide recommended JSON structure (also known as a schema, or ‘Spec’) that helps keep the most important parts of your data consistent, while allowing great flexibility in what other information you collect and where.

There are six calls in the basic tracking API, which answer specific questions:

Among these calls, you can think of Identify, Group, and Alias as similar types of calls, all to do with updating our understanding of the user who is triggering Segment messages. You can think of these calls as adding information to, or updating an object record in a database. 
Objects
 are described using “traits”, which you can collect as part of your calls.

The other three, Track, Page, and Screen, can be considered as increasingly specific types of events. 
Events
 can occur multiple times, but generate separate records which append to a list, instead of being updated over time.

A Track call is the most basic type of call, and can represent any type of event. Page and Screen are similar and are triggered by a user viewing a page or screen, however Page calls can come from both web and mobile-web views, while Screen calls 
only
 occur on mobile devices. Because of the difference in platform, the context information collected is very different between the two types of calls.

Tip
! Segment recommends that you always use the Page and Screen calls when recording a page-view, rather than creating a “Page Viewed” event, because the Page/Screen calls automatically collect much better context information.

Anatomy of a Segment message

The most basic Segment message requires only a 
userID
 or 
anonymousID
; all other fields are optional to allow for maximum flexibility. However, a normal Segment message has three main parts: the 
common fields
, the 
“context” object
, and the properties (if it’s an event) or traits (if it’s an object).

The common fields include information specific to how the call was generated, like the timestamp and library name and version. The fields in the context object are usually generated by the library, and include information about the environment in which the call was generated: page path, user agent, OS, locale settings, etc. The properties and traits are optional and are where you customize the information you want to collect for your implementation.

Another common part of a Segment message is 
the 
integrations
 object
, which you can use to explicitly filter which destinations the call is forwarded to. However this object is optional, and is often omitted in favor of non-code based filtering options.

Message schemas, Blocks, and Specs

The Segment “Specs” provide recommended message schemas - the information we recommend that you collect - for each type of call. These are recommendations not requirements, but if you follow these schema guidelines the Segment servers can more easily identify parts of your messages, and translate them to downstream tools.

In addition to the recommended message schemas, Segment also provides “blocks”: recommendations on what information to collect and how to format it, for different industries and use cases. These are recommendations only, but by collecting all of the information in these blocks, you can ensure that common tools used in that use-case have the information they need to function.

A third section of the Spec is the “industry specs” which provide recommendations that include an explicit translation or mapping in the Segment servers, to best power the downstream Destinations commonly used in these industries.

Sources and Destinations

When you start out, you create a Workspace, which serves as a container for all of your Sources and Destinations.

Segment has 
Sources
 and 
Destinations
. Sources send data 
into
 Segment, while Destinations receive data 
from
 Segment.

Segment has five types of sources: Web (Analytics.js), Mobile, Server, and Cloud App, plus a fifth type: User-created 
Source Functions
. Web, Mobile, and Server sources send first-party data from your digital properties. Cloud-app sources send data about your users from your connected web apps, for example a ticketing system such as 
Zendesk
, a payments system such as 
Stripe
, or a marketing tool like 
Braze
.

Connection modes

Segment has several types of sources, and many destinations can accept data from all of them. However, some are only compatible with specific source types (for example, web only, or server only). To find out which source types a specific destination can accept data from, check the documentation for that destination for a “Supported Sources and Connection Modes” section.

Segment’s web source (Analytics.js), and native client-side libraries (iOS, Android, React-native) allow you to choose how you send data to Segment from your website or app. There are two ways to send data:

Cloud-mode
: The sources send data directly to the Segment servers, which then translate it for each connected downstream destination, and send it on. Translation is done on the Segment servers, keeping your page size, method count, and load time small.

Healthcare and Life Sciences (HLS) customers can encrypt data flowing into their destinations

 HLS customers with a HIPAA eligible workspace can encrypt data in fields marked as Yellow in the Privacy Portal before they flow into an event stream, cloud-mode destination.
    
To learn more about data encryption, see the 
HIPAA Eligible Segment documentation

Device-mode
: You include additional code on your website or mobile app which allows Segment to use the data you collect on the device to make calls directly to the destination tool’s API, without sending it to the Segment servers 
first
. (You still send your data to the Segment servers, but this occurs asynchronously.) This is also called 
wrapping
 or 
bundling
, and it might be required when the source has to be loaded on the page to work, or loaded directly on the device to function correctly. When you use Analytics.js, you can change the device-mode destinations that a specific source sends from within the Segment web app, without touching any code.



If you use Server source libraries, they only send data directly to Segment in Cloud-mode. Server library implementations operate in the server backend, and can't load additional destination SDKs.

To learn more about connection modes and when you should use each, see the 
details in the Destinations docs
.

Planning your Segment implementation

The journey of a thousand miles begins, ideally, with a plan. Regardless of if you’re a new company just implementing analytics for the first time, or a multi–national corporation modernizing your analytics stack, it’s a great idea to 
start with a Tracking Plan
. For new implementations, this can be as simple as a document where you write down these four things for each item you track:

If you’re a large or long-established organization and you’re replacing existing tools, you’ll want to spend more time on this to maintain analytic parity and continuity of tooling. We highly recommend 
reading up on tracking plans
 and 
schemas
 for 
Protocols
, our tool for managing and sharing tracking plans and enforcing schemas.

Regardless of your organization’s size or age, you’ll want to take an inventory of the destination tools you’ll be using with Segment, and make a list of the connection modes each one accepts. This makes it easier to check off when you’ve implemented each one, so you’re not missing anything.

How do I test if it’s working?

There are several ways to check if your data is flowing. One is the 
Debugger tab in each Source
 in the Segment web app, where you can see data coming from a source into Segment. Another is the 
Event Delivery tool
 which shows which data is arriving at specific destinations.

For monitoring purposes, you’ll also see alerts in the 
Workspace Health
 tool if your sources or destinations produce repeated errors.

How do I filter my data?

There are several different ways to ensure that you can collect your data once, but filter it out of specific destinations. See 
Filtering Data
 for a list of the available methods and descriptions.

Troubleshooting

If you’re seeing errors thrown by your destinations, you might have an implementation issue. See the 
Integration Error Codes list
 or 
contact our Success engineering team
 for help.

Have suggestions for things to add to this guide? 
Drop us a line
.

Segment Terraform Provider

Segment has a 
Terraform
 provider, powered by the Public API, that you can use to manage Segment resources, automate cloud deployments, and change control. Take a look at the 
Segment provider documentation
 on Terraform to see what’s supported.

This page was last modified: 09 Apr 2024

Need support?

Questions? Problems? Need more info? Contact Segment Support for assistance!

Help improve these docs!

Was this page helpful?

Thanks for your feedback!

Can we improve this doc? 
Send us feedback!

Get started with Segment

On this page

Was this page helpful?

Thanks for your feedback!

Can we improve
 this doc? 
Send us feedback!

Product

For Developers

Company

Support

© 2025 Segment.io, Inc.

=== Content from https://segment.com/docs/engage/journeys/key-terms/ ===

Journeys Key Terms
        

Engage Foundations requires a Business tier account and includes Unify.


See the 
available plans
, or 
contact Support
.

On this page

Keep the following terms in mind as you begin to explore Journeys.

General

Steps

Statuses

Steps with Audiences

Steps without Audiences

Analytics

This page was last modified: 06 Feb 2024

Need support?

Questions? Problems? Need more info? Contact Segment Support for assistance!

Help improve these docs!

Was this page helpful?

Thanks for your feedback!

Can we improve this doc? 
Send us feedback!

Get started with Segment

On this page

Was this page helpful?

Thanks for your feedback!

Can we improve
 this doc? 
Send us feedback!

Product

For Developers

Company

Support

© 2025 Segment.io, Inc.

=== Content from https://segment.com/docs/connections/storage/warehouses/warehouse-errors/ ===

Warehouse Errors
        

ERROR: Schema “XXX” does not exist. (SQLSTATE 3F000)

This is a permissioning issue. To learn how to set up proper permissions, you can check out our 
Postgres
 and 
Redshift
 guides.

ERROR: Cannot execute query because system is in resize mode (SQLSTATE 57014)

This error occurs when your cluster is currently resizing. The warehouse will continue on its scheduled run interval - once the resize is complete, we’ll load all data from the failed run.

ERROR: 1040 (SQLSTATE XX000)

This is a Redshift 500 - an internal server error. This is usually caused by having too many tables or too many columns. If you’re seeing this error, 
contact the Segment Support team
.

read tcp XXX.XX.XX.XXXX:XXXX-XXX.XX.XX.XXXX:XXXX: read: connection timed out

This is a networking error that typically arises when Redshift doesn’t close properly close the connection or gets rebooted.

If you see this error on consecutive syncs, 
contact us
.

pq: role “XXX” is not permitted to log in

This is a permissioning issue. To learn how to set up proper permissions, you can check out our 
postgres
 and 
redshift
 guides.

pq: password authentication failed for user “XXX”;

This is a credential issue. To fix your credentials, head over to Warehouse > Settings > Connection.

dial tcp: lookup XXX-hostname on 10.50.0.2:53: no such host

This is a credential issue. To fix your credentials, head over to Warehouse > Settings > Connection.

dial tcp XX.XXX.XXX.XXX:XXXX: getsockopt: connection timed out / refused

This is a networking error. The connection times out because Redshift doesn’t close properly or gets rebooted.

If you see this error on consecutive syncs, 
contact us
.

ERROR: syntax error at or near “ENCODE”; (SQLSTATE 42601)

This occurs when a Postgres database is incorrectly connected as Redshift. To resolve this, delete the warehouse and reconnect, using the Postgres set up option.

Error during deduping step for collectionXXX: EOF

This error is generally a network error when Redshift closes the connection. If the problem persists on multiple runs, 
contact us
.

ERROR: permission denied for relation update (SQLSTATE 42501)

This is a permissioning issue. To learn how to set up proper permissions, you can check out our 
postgres
 and 
redshift
 guides.

EOF

This error is generally a network error when Redshift closes the connection. If the problem persists on multiple runs, 
contact us
.

ERROR: failed to create table: 002318 (42601): SQL compilation error: invalid column definition name “XXX” (ANSI reserved)

This error indicates that a column that is attempting to sync has the same title as a reserved keyword in Snowflake. More information regarding Snowflake’s reserved keywords can be found 
in Snowflake’s docs
.

This page was last modified: 18 Oct 2023

Need support?

Questions? Problems? Need more info? Contact Segment Support for assistance!

Help improve these docs!

Was this page helpful?

Thanks for your feedback!

Can we improve this doc? 
Send us feedback!

Get started with Segment

Was this page helpful?

Thanks for your feedback!

Can we improve
 this doc? 
Send us feedback!

Product

For Developers

Company

Support

© 2025 Segment.io, Inc.

=== Content from https://segment.com/docs/engage/content/mobile-push/ ===

Mobile Push Template
        

Engage Premier requires a Business tier account and includes Engage Foundations and Unify.


See the 
available plans
, or 
contact Support
.

On this page

Engage Premier entered an End of Sale (EOS) period effective June 10, 2024. Existing Segment customers will continue to have access and support to Engage Premier until an end-of-life (EOL) date is announced. Segment recommends exploring the following pages in preparation of a migration or future MCM needs:

Twilio Marketing Campaigns

Preferred ISV Partners:

Airship Blog
 


Bloomreach Blog
 


Braze Blog
 


Insider Blog
 


Klaviyo Blog
 


Twilio Engage Foundations Documentation
 

Use Twilio Engage to build mobile push templates to include throughout your marketing campaigns.

Mobile push template types

You can choose between two mobile push template types:

Build a mobile push message template

To build mobile push templates in Engage, first 
configure Engage for mobile
.

Follow these steps to build a mobile push template:

Click behaviors

When you build a mobile push template, you can choose between three click behaviors, which determine what happens when a user taps on the mobile push:

Test your mobile push template

Push tokens

Push tokens are unique identifiers Segment associates with each profile. For mobile push, you’ll need to configure identity resolution settings for the push tokens 
ios.push_token
 and 
android.push_token
. Using the Profile explorer, you can find a profile’s push tokens by opening a profile and then selecting the Identities tab. You can only send mobile pushes to profiles with push tokens enabled.

Follow these steps to test your mobile push:

Segment verifies that the profile you’re sending a test to has a push token, then sends the test. If the test mobile push doesn’t work as expected, confirm that the profile you’re sending to has a push token.

Advanced settings

Badge count settings

Badge counts appear in the corner of an app icon on your user’s device. Badge counts show the number of unread notifications. During push notification setup, you can set badge count behavior from the badge count dropdown.

Choose from these badge count settings:

Action buttons

Action buttons sit below a push notification and let your users take action on the push. You can use action buttons to encourage users to make a purchase, visit a website, or share content on social media, for example.

Follow these steps to add an action button:

You can add up to three action buttons for each push notification.

Personalize with merge tags

Personalize mobile push content in Engage using profile traits as merge tags in your messages.

To personalize a mobile push, click 
Add merge tags
 in the template builder and select the profile traits to include in your message.

Engage inserts the selected traits inside merge tags based on cursor placement in the message. This allows you to personalize each mobile push you send to recipients. You can also use 
liquid templating
 to create dynamic content in the template editor.

To learn more about profile traits, visit Segment’s 
Computed Traits
 and 
SQL Traits
 documentation.

Next steps

Now that you’ve built a mobile push template, you’re ready to begin 
sending mobile push campaigns
.

This page was last modified: 15 Jul 2024

Need support?

Questions? Problems? Need more info? Contact Segment Support for assistance!

Help improve these docs!

Was this page helpful?

Thanks for your feedback!

Can we improve this doc? 
Send us feedback!

Get started with Segment

On this page

Was this page helpful?

Thanks for your feedback!

Can we improve
 this doc? 
Send us feedback!

Product

For Developers

Company

Support

© 2025 Segment.io, Inc.

=== Content from https://segment.com/docs/connections/rate-limits/ ===

Product Limits
        

On this page

These limits were updated on January 25, 2024.

Event properties ingestion limit

Events ingested by Segment have a limit of 
10,000
 properties per individual event received. For example, two Track events named “Page Viewed” and “Signup completed” each have their own limit. Segment will not persist properties beyond this limit, and will drop any corresponding values.

Inbound data ingestion API rate limit

If any sources send more than 1,000 events per second in a workspace without prior arrangement, Segment reserves the right to queue any additional events and process those at a rate that doesn’t exceed this limit. To request a higher limit, contact 
Segment
.

Engage rate limit

Engage has a limit of 1,000 events per second for inbound data. Visit the 
Engage Default Limits documentation
 to learn more.

Outbound downstream destination rate limits

Most destinations have their own rate limits that Segment cannot control. In some instances, Segment is able to ingest and attempt to deliver data faster than the downstream destination is able to accept data. Outbound requests to a destination may also fail for other reasons outside of Segment’s control. When requests to downstream destinations fail, Segment makes additional attempts to deliver the data (retries). However, when more than 1,000 requests per second to a downstream destination fail or when the failure rate for a downstream destination exceeds 50% for more than 72 hours, Segment reserves the right to reduce the number of retries until the condition is resolved.

This page was last modified: 25 Jan 2024

Need support?

Questions? Problems? Need more info? Contact Segment Support for assistance!

Help improve these docs!

Was this page helpful?

Thanks for your feedback!

Can we improve this doc? 
Send us feedback!

Get started with Segment

On this page

Was this page helpful?

Thanks for your feedback!

Can we improve
 this doc? 
Send us feedback!

Product

For Developers

Company

Support

© 2025 Segment.io, Inc.

=== Content from https://segment.com/docs/help/ ===

Segment Resources
        

Technical Support

Email support is available for all 
Segment support plans
. If you’re experiencing problems, have questions about implementing Segment, or want to report a bug, you can fill out the 
support contact form
 and the Success Engineering team will get back to you.

You need a Segment account to file a support request. If you don’t have one, sign up for a free workspace and then send your request.

Segment Support Business Hours

For more information about the  Segment Support Team’s hours and holidays see 
Twilio Support business hours
.

Segment University

Segment University
 is Segment’s free, online classroom for learning the basics of Segment.

Analytics Academy

Analytics Academy
  is a series of lessons designed to help you understand the value of analytics as a discipline. Analytics Academy will help you think through your analytics needs so you can start creating robust and flexible analytics systems.

Recipes

Wondering what you can do with Segment? Check out 
Segment Recipes
 for inspiration on what you can achieve by connecting your Segment workspace to different Destinations, from tailored onboarding emails to joining and cleaning your data with third-party tools.

Other Resources

Head over to 
Segment Resources
 for Segment case studies, webinars, courses, and more.

Status Page

The 
Segment Status Page
 offers details about system metrics and reports on uptime for each part of the Segment product. You can subscribe to receive updates about ongoing incidents and view 
information about past incidents
.

This page was last modified: 07 Feb 2025

Get started with Segment

Product

For Developers

Company

Support

© 2025 Segment.io, Inc.

=== Content from https://segment.com/docs/engage/journeys/journeys-logic/ ===

Understanding Journeys Logic
        

Engage Foundations requires a Business tier account and includes Unify.


See the 
available plans
, or 
contact Support
.

On this page

Journeys are powered by a series of Audiences and Computed Traits. This guide defines the logic used to create sequential campaigns.

By the end of this guide, you’ll understand how and why users progress through your Journey. You’ll also gain familiarity with the following key Journeys concepts:

Entry conditions and step behavior

Journeys begin with an entry condition that computes like standard 
Engage Audiences
. This entry condition queries your customer data in Segment to find users who meet your specified criteria.

After users meet the Journey’s entry condition, their progress through the Journey depends on satisfying the criteria of subsequent Journey steps.

Journey steps operate based on the following behaviors:

The combination of these traits, audiences, and business rules allows you to create an enforced funnel with the following implications:

Step membership

To enter a Journey, users must satisfy the entry conditions.

To enter each subsequent step, three conditions must be true:

Condition steps

“Add a condition” steps operate like 
Engage Audiences
. The defined conditions provide criteria for each step’s membership.

Wait times

When you add a “Wait” step to a Journey, Segment automatically includes wait times in the membership criteria of the next condition step.  Journeys represents wait times in relation to the 
preceding_step_joined_time trait
, which must be at least N time ago.

The following table summarizes the three step membership conditions and their equivalents in written logic:

Real-Time step membership

For every step after the entry step, Journeys leverages 
the Engage real-time compute system
.

When a user’s traits change or they exceed time-based conditions (for example, “within 7 days”), they may no longer fulfill the conditions of a previously joined step. If a user joins a step but no longer meets its conditions, Journeys removes them from that step’s preview and analytics.  The user does, however, continue to progress through the Journey.

Consider the following example of Journey conditions for a cart abandonment campaign:

If a user makes a purchase during the wait time of 5 days, the system would automatically update membership to 
false
 for the audience created from the entry condition, Step 1. However, the user could still satisfy Step Condition 3 based on the three step membership conditions:

To maintain best practices and enforce your funnel, re-check or modify audience conditions that follow wait steps.  For example, adding a 
purchases = 0
 condition to Step 3 results in Segment not advancing users who made a purchase during the wait time:

Send to Destination steps

Unless a Journey has an exit condition configured, Journey members permanently remain in Destination sync steps. Segment neither sends 
Audience Exit
 events to Destinations nor removes users from Destinations lists.
Exit conditions will lead to users being removed from all Journey steps and Destinations.

FAQs

What happens when a user reaches a single or Multi-Split Condition step and the conditions evaluates to 
false
?

Each step’s membership conditions evaluate in real time, which means that users remain in a step until the immediate next step’s conditions becomes true.

Can users exit and re-enter a Journey?

Yes. To allow users to re-enter Journeys that they’ve exited, 
enable re-entry
 during initial Journey setup.

What happens to traits and audiences when I delete a Journey?

Deleting a Journey removes its underlying audiences from profile views in the Profile explorer. However, the Journey’s True/False traits remain in the user’s last recorded state.

Note

Cloning a Journey generates new, unique traits and sync keys.  Deleting the original Journey won’t impact any cloned Journeys.

Are splits mutually exclusive?

True/false splits enforce mutual exclusivity by ensuring that once users enter either side of a split, they can’t enter the other.

Multi-branch splits don’t enforce mutual exclusivity. Users can enter multiple branches of a split if they satisfy the split conditions.

How does “Use Historical Data” backfill work?

Use Historical Data backfills the entry condition to “prime” the Journey.  Future events and existing trait memberships trigger all Journey conditions, except for entry.  As a result, event-based conditions only evaluate events that occurred after you published the Journey.

If you want to check for events that occurred before you published your Journey, base your conditions on computed traits instead.

For example, to evaluate if a user already in a Journey has ever used a discount code, create a Computed Trait for 
discount_used
, and set it to 
true
 or 
false
.

How do time windows within step conditions work?

With time windows within step conditions, you can designate a timeframe for Segment to evaluate whether or not a user has met the condition.  Segment calculates the time window from the current point in time, not relative to any other steps in your Journey.

This page was last modified: 09 Jan 2024

Need support?

Questions? Problems? Need more info? Contact Segment Support for assistance!

Help improve these docs!

Was this page helpful?

Thanks for your feedback!

Can we improve this doc? 
Send us feedback!

Get started with Segment

On this page

Was this page helpful?

Thanks for your feedback!

Can we improve
 this doc? 
Send us feedback!

Product

For Developers

Company

Support

© 2025 Segment.io, Inc.

=== Content from https://segment.com/docs/guides/how-to-guides/import-historical-data/ ===

Importing Historical Data
        

On this page

When transitioning over to Segment, customers commonly want to import historical data into tools they are migrating to or evaluating.

Note:
 Historical imports can only be done into destinations that can accept historical timestamped data. Most analytics tools like Mixpanel, Amplitude, or Kissmetrics can handle that type of data just fine. One common destination that doesn’t accept historical data is Google Analytics, since their API cannot accept historical data.

Method 1: Using a Custom Solution

General Instructions

Use any 
server-side library
, which sends requests in batches to improve performance. Once you have data to import, follow the steps below:

Export or collect the data to be imported.

Include timestamp data in your export if the data needs to appear in end tools in a historical reference. For instance, if you’re importing emails and it’s relevant to know when someone joined your email list, you may need to export the timestamp. 
If no timestamp is specified when importing, the data will show a timestamp from the time the data was received
.

Decide which destinations need to receive the data.

By default, data coming into Segment will be forwarded to all destinations connected to a given source.
 To limit data to specific destinations, the 
integrations
 object must be modified. With historical data, you often only want to send the data to a specific destination or into your data warehouse. For example, in 
Node.js
 set the 
integrations
 object as follows.

analytics
.
track
({

    
event
:
 
'
Upgraded Membership
'
,

    
userId
:
 
'
97234974
'
,

    
integrations
:
 
{
 
'
All
'
:
 
false
,
 
'
Vero
'
:
 
true
,
 
'
Google Analytics
'
:
 
false
 
}

 
})



Once you’ve done that, you’ll need to write an application or worker to send the data to Segment.

You will need to cycle through each set of data and map it to a Segment server-side library method or build an array matching the 
HTTP Import API format
.

Tip
: Segment recommends using a Segment library for this process, as they set contextual message fields like 
message_id
 (used for deduping) and 
sent_at
 (used for correctly client clock skew) that Segment’s API uses to correct behavior upon ingestion.

Tip
: The server-side libraries will automatically batch requests to optimize for performance and prevent linear request volume. This batching behavior is modifiable, and some of the underlying libraries implement a configurable max queue size that may discard messages if you enqueue requests much faster than the client can flush them. We recommend overriding the max queue size parameter for the library to a high value you’re comfortable you can remain under in your batch job.

Demo projects

The following projects are open-source and do not have official Segment support. If you encounter issues, the best way to get help is by opening an issue on the project’s GitHub page. 
Feel free to clone the repository and adjust the code to suit your unique needs.

One of Segment’s Success Engineers wrote an alpha prototype Node.js app for importing data utilizing the HTTP API, which we’ve included below:

Example Node.js import application

Additionally, one of Segment’s Software Engineers developed a React App with more out of the box functionality for importing events. The features include a modern UI, transformations, and event format checking prior to import:

Desktop React CSV uploader

MarketLytics
 has documented their experience using the alpha prototype importer and offer some 
helpful visuals and tips
.

Alternative solution

If a server-side library doesn’t meet your needs, you can use the Segment 
bulk import HTTP API
 directly.

Note:
 When you use the HTTP API to export historical data to upload to Segment, remove all the original 
sent_at
, 
message_id
, and 
project_id
 fields from the archived message before forwarding them back to Segment.

Method 2: Using Reverse ETL

Please refer to the 
Reverse ETL guide
 for more details.

This page was last modified: 14 May 2024

Need support?

Questions? Problems? Need more info? Contact Segment Support for assistance!

Help improve these docs!

Was this page helpful?

Thanks for your feedback!

Can we improve this doc? 
Send us feedback!

Get started with Segment

On this page

Was this page helpful?

Thanks for your feedback!

Can we improve
 this doc? 
Send us feedback!

Product

For Developers

Company

Support

© 2025 Segment.io, Inc.

=== Content from https://segment.com/docs/connections/ ===

Connections Overview
        

On this page

Connections is Segment’s core product offering: you can collect event data from your mobile apps, websites, and servers with one API, then pull in contextual data from cloud apps like your CRM, payment systems, and internal databases to build a unified picture of your customers.

Sources

In Segment, you create a source (or more than one!) for each website or app you want to track. We 
highly recommend
 creating a Source for each unique source of data (each site, app, or server), though this isn’t required.

Sources belong to a workspace, and the URL for a source looks something like this:

https://segment.com/<my-workspace>/sources/<my-source-name>/

You can create new sources using the button in the workspace view. Each source you create has a write key, which is used to send data to that source. For example, to load 
analytics.js
, the Segment JavaScript library
 on your page, the snippet on the 
Quickstart Guide
 includes:

analytics
.
load
(
"
YOUR_WRITE_KEY
"
);



Learn more about sources from the 
sources overview page
.

Destinations

Destinations are business tools or apps that you can connect to the data flowing through Segment. Some of Segment’s most popular destinations are Google Analytics, Mixpanel, Kissmetrics, Customer.io, Intercom, and KeenIO.

All of these tools run on the same data: who are your customers and what are they doing? But each tool requires that you send that data in a slightly different format, which means that you’d have to write code to track all of this information, again and again, for each tool, on each page of your app or website.

Enter Segment. Do it once.

Segment eliminates this process by introducing an abstraction layer. You send your data to Segment, and Segment understands how to translate it so we can send it along to any destination. You enable destinations from the catalog in the Segment App, and user data immediately starts flowing into those tools. No extra code required!

Segment supports many categories of destinations, from advertising to marketing, email to customer support, CRM to user testing, and even data warehouses. You can view a complete list of available 
destinations
 or check out the 
destination page
 for a searchable list broken down by category.

Warehouses

A warehouse is a central repository of data collected from one or more sources. This is what commonly comes to mind when you think about a relational database: structured data that fits neatly into rows and columns.

In Segment, a Warehouse is a special type of destination. Instead of streaming data to the destination all the time, we load data to them in bulk at regular intervals. When we load data, we insert and update events and objects, and automatically adjust their schema to fit the data you’ve sent to Segment.

Reverse ETL

With 
Reverse ETL
, your data warehouse acts as your source, enabling you to send data from your warehouse to your destinations.

Information on sources and destinations pages

The Sources and Destinations pages allow each user to decide what information appears in their personal view for each page.

On both pages, you can click the stack icon in the upper right-hand corner of the table to see and select Source properties to show. You can select up to five columns of properties.

The following information is available for Sources:

On the Destinations page, you can choose among the following properties:

You can then sort or filter each column to just the values you care about, by clicking on the arrow next to each displayed column.

FAQs

My source was disabled and it wasn’t done by anyone in my workspace

Sources without any enabled destinations are auto-disabled after 14 days. However, the workspace owner is notified by email before Segment disables the source. Data that flows into Segment but does not flow to any downstream tools is not valuable to you and unnecessarily takes up space.

Segment understands there may be cases to keep a source active. If you’d like to add your sources to an exception list, you can do so by filling out this 
Airtable form
.

Can I request Segment add an integration tool?

Yes, you are able to submit an integration request here https://segment.com/requests/integrations/.

This page was last modified: 29 Feb 2024

Need support?

Questions? Problems? Need more info? Contact Segment Support for assistance!

Help improve these docs!

Was this page helpful?

Thanks for your feedback!

Can we improve this doc? 
Send us feedback!

Get started with Segment

On this page

Was this page helpful?

Thanks for your feedback!

Can we improve
 this doc? 
Send us feedback!

Product

For Developers

Company

Support

© 2025 Segment.io, Inc.

=== Content from https://segment.com/docs/connections/functions/aws-apis/ ===

Set up functions for calling AWS APIs
        

The 
aws-sdk
 module is built-in, which allows you to make calls to AWS services in your own AWS accounts. The AWS SDK requires additional setup to ensure access to your AWS resources is secure. This page describes the process for allowing your functions to securely call AWS APIs in your AWS account.

To set up your functions to call AWS APIs:

 
{

   
"Version"
:
 
"2012-10-17"
,

   
"Statement"
:
 
[

     
{

       
"Effect"
:
 
"Allow"
,

       
"Principal"
:
 
{

         
"AWS"
:
 
"<PRINCIPAL_ACCOUNT_ID>"

       
},

       
"Action"
:
 
"sts:AssumeRole"
,

       
"Condition"
:
 
{

         
"StringEquals"
:
 
{

           
"sts:ExternalId"
:
 
"<EXTERNAL_ID>"

         
}

       
}

     
}

   
]

 
}



Below is an example destination function that uploads each event received to an S3 bucket (configured using additional “S3 Bucket” and “S3 Bucket Region” settings). It uses the built-in local cache to retain S3 clients between requests to minimize processing time and to allow different instances of the function to use different IAM roles.

 
async
 
function
 
getS3
(
settings
)
 
{

   
const
 
ttl
 
=
 
30
 
*
 
60
 
*
 
1000
;
 
// 30 minutes

   
const
 
key
 
=
 
[
settings
.
iamRoleArn
,
 
settings
.
s3Bucket
].
join
();


   
return
 
cache
.
load
(
key
,
 
ttl
,
 
async 
()
 
=>
 
{

     
const
 
sts
 
=
 
new
 
AWS
.
STS
();


     
const
 
opts
 
=
 
await
 
sts

       
.
assumeRole
({

         
RoleArn
:
 
settings
.
iamRoleArn
,

         
ExternalId
:
 
settings
.
iamRoleExternalId
,

         
RoleSessionName
:
 
'
segment-function
'

       
})

       
.
promise
()

       
.
then
(
data
 
=>
 
{

         
return
 
{

           
region
:
 
settings
.
s3BucketRegion
,

           
accessKeyId
:
 
data
.
Credentials
.
AccessKeyId
,

           
secretAccessKey
:
 
data
.
Credentials
.
SecretAccessKey
,

           
sessionToken
:
 
data
.
Credentials
.
SessionToken

         
};

       
});


     
return
 
new
 
AWS
.
S3
();

   
});

 
}


 
async
 
function
 
onTrack
(
event
,
 
settings
)
 
{

   
const
 
s3
 
=
 
await
 
getS3
(
settings
);


   
return
 
s3

     
.
putObject
({

       
Bucket
:
 
settings
.
s3Bucket
,

       
Key
:
 
`
${
event
.
type
}
/
${
Date
.
now
()}
.json`
,

       
Body
:
 
JSON
.
stringify
(
event
)

     
})

     
.
promise
()

     
.
then
(
data
 
=>
 
{

       
console
.
log
(
data
);

     
});

 
}



This page was last modified: 11 May 2022

Need support?

Questions? Problems? Need more info? Contact Segment Support for assistance!

Help improve these docs!

Was this page helpful?

Thanks for your feedback!

Can we improve this doc? 
Send us feedback!

Get started with Segment

Was this page helpful?

Thanks for your feedback!

Can we improve
 this doc? 
Send us feedback!

Product

For Developers

Company

Support

© 2025 Segment.io, Inc.

=== Content from https://segment.com/docs/engage/user-subscriptions/set-user-subscriptions/ ===

Set User Subscriptions
        

Engage Premier requires a Business tier account and includes Engage Foundations and Unify.


See the 
available plans
, or 
contact Support
.

On this page

Engage Premier entered an End of Sale (EOS) period effective  June 10, 2024. Existing Segment customers will continue to have access and support to Engage Premier until an end-of-life (EOL) date is announced. We recommend exploring the following pages in preparation of a migration or future MCM needs:

Twilio Marketing Campaigns

Preferred ISV Partners:

Airship Blog
 


Bloomreach Blog
 


Braze Blog
 


Insider Blog
 


Klaviyo Blog
 


Twilio Engage Foundations Documentation
 

Segment associates a 
user subscription state
 with each email address and phone number in your Engage audiences. Subscription states give you insight into the level of consent a user has given you to receive your Engage campaigns.

You can set a user’s subscription state using a CSV file or, programmatically, using Reverse ETL or Segment’s APIs. On this page, you’ll learn how and when to use these processes.

Setting user subscriptions with a CSV file upload

Setting user subscriptions by uploading a CSV file proves useful when you’re importing batch contacts to Segment for the first time or when you need to change a specific user’s subscription status.

For example, you may want to add contacts to Segment using an audience list sourced from a third-party tool, or you may have gathered a large number of contacts from an in-person event.

Subscription state CSV fields

To learn how to upload a CSV file to Segment, view the 
Engage CSV Uploader page
.

To change the subscription status of an email or phone number with a CSV file, include at least one of the following user subscription columns next to the contact column:

These columns take the following values:

Engage accepts both uppercase and lowercase subscription status values.

Refer to the 
User Subscription States documentation
 for detailed explanations of each subscription state.

Manage user subscriptions with Segment APIs

With Segment’s APIs, you can manage user subscriptions programmatically on a real-time basis. Use the APIs for ongoing subscription status updates, like when users subscribe to your marketing campaigns on a website form or modify their subscription from within a notification center.

Choosing between the Identify call and the Public API

To update Engage user subscriptions with Segment’s APIs, first choose between 
the Identify call
, for non-critical subscription updates, or the 
Public API
, for critical updates that require immediate confirmation, like unsubscribes.

When you use the Identify call, Segment replies with a standard HTTP 
200 OK
 status response code if it successfully received the request. Because the Identify call updates user traits asynchronously, though, the 
200 OK
 code indicates that Segment has received, but not yet processed, the request. As a result, use the Identify call for non-critical subscription updates, like form signups on your website or adding a subscription from within the user’s notification center.

When you update user subscriptions with Segment’s Public API, however, you’ll get an immediate response that confirms that Segment both received and processed the request. Use the Public API, then, for unsubscribes, so users immediately find out if their subscription updated.

Format the Identify call payload

For Segment to process the subscription status request, your Identify call payload must include at least one object that contains an email address or phone number, its subscription type, and its subscription status. Engage accepts both uppercase and lowercase subscription statuses in Identify calls.

The following example payload shows an Identify call with a 
context
 object, which you’ll add to the Identify call to update user subscriptions. The 
context
 object contains a 
messaging_subscriptions
 array with three objects that update SMS, WhatsApp, and email subscription statuses:

{

  
"userId"
:
 
"12aBCDeFg4hIjKlM5OPq67RS8Tu"
,

  
"context"
:
 
{

    
"messaging_subscriptions"
:
 
[

      
{

        
"key"
:
 
"(123) 555-5555"
,

        
"type"
:
 
"SMS"
,

        
"status"
:
 
"SUBSCRIBED"
 
|
 
"UNSUBSCRIBED"
 
|
 
"DID_NOT_SUBSCRIBE"

      
},

      
{

        
"key"
:
 
"(123) 555-5555"
,

        
"type"
:
 
"WhatsApp"
,

        
"status"
:
 
"SUBSCRIBED"
 
|
 
"UNSUBSCRIBED"
 
|
 
"DID_NOT_SUBSCRIBE"

      
},

      
{

        
"key"
:
 
"test@example.com"
,

        
"type"
:
 
"EMAIL"
,

        
"status"
:
 
"SUBSCRIBED"
 
|
 
"UNSUBSCRIBED"
 
|
 
"DID_NOT_SUBSCRIBE"

      
}

    
],

    
"externalIds"
:
 
[

      
{

        
"id"
:
 
"(123) 555-5555"
,

        
"type"
:
 
"phone"
,

        
"collection"
:
 
"users"
,

        
"encoding"
:
 
"none"

      
}

    
],

    
"traits"
:
 
{

       
"email"
:
 
"test@example.com"

    
}

  
},

  
"integrations"
:
 
{},

  
"traits"
:
 
{}


}



For successful requests, Segment instantly updates subscription states in your workspace. You can then display successful updates or error messages with users in your notification center.

While SMS and WhatsApp share the same number, you must add a separate subscription state for both of them.

Setting user subscriptions with Reverse ETL

Engage Premier Subscriptions users
 can use Reverse ETL to sync subscription data from warehouses to destinations. To get started with using Reverse ETL for managing subscriptions, see  
Reverse ETL for Engage Premier Subscriptions
.

This page was last modified: 15 Jul 2024

Need support?

Questions? Problems? Need more info? Contact Segment Support for assistance!

Help improve these docs!

Was this page helpful?

Thanks for your feedback!

Can we improve this doc? 
Send us feedback!

Get started with Segment

On this page

Was this page helpful?

Thanks for your feedback!

Can we improve
 this doc? 
Send us feedback!

Product

For Developers

Company

Support

© 2025 Segment.io, Inc.

=== Content from https://segment.com/docs/engage/journeys/use-cases/ ===

Example Journeys Use Cases
        

Engage Foundations requires a Business tier account and includes Unify.


See the 
available plans
, or 
contact Support
.

On this page

To help you get underway, you can reference these sample Journeys.

E-commerce use cases

Repeat purchase campaign

This journey focuses on converting one-time buyers into repeat purchasers by delivering communications in their preferred channels.



Low recency purchase winback

This journey represents a campaign designed to drive returning purchases based on intent and lifetime value goals.



B2B use cases

Trial to paid conversion

This journey creates an acquisition campaign designed to convert trial accounts to paid accounts with a unified owned and paid media strategy.



Onboarding flow

This journey creates an onboarding flow designed to maintain new user engagement through the onboarding experience.



Media use cases

Paid subscription acquisition

This journey creates an acquisition campaign designed to convert trial subscriptions to paid subscriptions with a unified owned and paid media strategy.



Re-engagement Campaign

This journey aims to bring back users with personalized messaging while conserving ad spend based on user preferences.



This page was last modified: 27 Sep 2022

Need support?

Questions? Problems? Need more info? Contact Segment Support for assistance!

Help improve these docs!

Was this page helpful?

Thanks for your feedback!

Can we improve this doc? 
Send us feedback!

Get started with Segment

On this page

Was this page helpful?

Thanks for your feedback!

Can we improve
 this doc? 
Send us feedback!

Product

For Developers

Company

Support

© 2025 Segment.io, Inc.

=== Content from https://segment.com/docs/guides/how-to-guides/collect-pageviews-serverside/ ===

Collecting Pageviews on the Server Side
        

Segment believes that client-side collection is appropriate for collection of basic pageviews.

If you’d like to track 
page
 calls from your server to Segment, Segment recommends doing it in addition to any client side tracking you’re doing with analytics.js, and doing it in a separate “source” so that you can configure where to send the (probably redundant, albeit higher-fidelity) data.

With this approach, you might use a request “middleware” to log a 
pageview
 with every page load from your server.

There are a few things to be mindful of if you want to make sure you can attribute these (anonymous) page views to the appropriate user in your client-side source (eg, for effectively joining these tables together to do down-funnel behavioral attribution). You’ll want to ensure they share an 
anonymousId
 by respecting one if it’s already there, and setting it yourself if not. To do that, you can read and modify the 
ajs_anonymous_id
 cookie value in the request.

Be sure to pass through as many fields as you can in Segment’s 
Page
 and 
Common
 spec, so that you get full functionality in any downstream tools you choose to enable. Segment recommends specifically ensuring you pass the 
url, path, host, title, search, and referrer
 in the message 
properties
 and 
ip and user-agent
 in the message 
context
 .

Here’s an example of an express middleware function that covers all those edge cases:

If you have any questions or would like help generally adopting this method for other languages and frameworks, be sure to 
get in touch
.

import
 
express
 
from
 
'
express
'


import
 
Analytics
 
from
 
'
analytics-node
'


import
 
{
 
stringify
 
}
 
from
 
'
qs
'



const
 
app
 
=
 
express
()


const
 
analytics
 
=
 
new
 
Analytics
(
'
write-key
'
)



app
.
use
((
req
,
 
res
,
 
next
)
 
=>
 
{

  
const
 
{
 
search
,
 
cookies
,
 
url
,
 
path
,
 
ip
,
 
host
 
}
 
=
 
req


  
// populate campaign object with any utm params

  
const
 
campaign
 
=
 
{}

  
if 
(
search
.
utm_content
)
 
campaign
.
content
 
=
 
search
.
utm_content

  
if 
(
search
.
utm_campaign
)
 
campaign
.
name
 
=
 
search
.
utm_campaign

  
if 
(
search
.
utm_medium
)
 
campaign
.
medium
 
=
 
search
.
utm_medium

  
if 
(
search
.
utm_source
)
 
campaign
.
source
 
=
 
search
.
utm_source

  
if 
(
search
.
utm_term
)
 
campaign
.
keyword
 
=
 
search
.
utm_term


  
// grab userId if present

  
let
 
userId
 
=
 
null

  
if 
(
cookies
.
ajs_user_id
)
 
userId
 
=
 
cookies
.
ajs_user_id


  
// if no anonymousId, send a randomly generated one

  
// otherwise grab existing to include in call to segment

  
let
 
anonymousId

  
if 
(
cookies
.
ajs_anonymous_id
)
 
{

    
anonymousId
 
=
 
cookies
.
ajs_anonymous_id

  
}
 
else
 
{

    
anonymousId
 
=
 
=
 
uuid
.
v4
()

    
res
.
cookie
(
'
ajs_anonymous_id
'
,
 
anonymousId
 
)

  
}


  
const
 
referrer
 
=
 
req
.
get
(
'
Referrer
'
)

  
const
 
userAgent
 
=
 
req
.
get
(
'
User-Agent
'
)


  
const
 
properties
 
=
 
{

    
search
:
 
stringify
(
query
)

    
referrer
,

    
path
,

    
host
,

    
url

    
/* ++ any custom props (eg. title) */

  
}


  
const
 
context
 
=
 
{

    
campaign
,

    
userAgent
,

    
ip

  
}


  
// send a call to segment

  
analytics
.
page
(

    
anonymousId
,
 
// either random (matching cookie) or from client

    
userId
,
 
// might be null

    
properties
,

    
context

  
)


  
// proceed!

  
next
()


})



This page was last modified: 10 Oct 2023

Need support?

Questions? Problems? Need more info? Contact Segment Support for assistance!

Help improve these docs!

Was this page helpful?

Thanks for your feedback!

Can we improve this doc? 
Send us feedback!

Get started with Segment

Was this page helpful?

Thanks for your feedback!

Can we improve
 this doc? 
Send us feedback!

Product

For Developers

Company

Support

© 2025 Segment.io, Inc.

=== Content from https://segment.com/docs/connections/reverse-etl/reverse-etl-source-setup-guides/postgres-setup/ ===

Postgres Reverse ETL Setup
        

On this page

Set up Postgres as your Reverse ETL source.

At a high level, when you set up Postgres for Reverse ETL, the configured user/role needs read permissions for any resources (databases, schemas, tables) the query needs to access. Segment keeps track of changes to your query results with a managed schema (
__SEGMENT_REVERSE_ETL
), which requires the configured user to allow write permissions for that schema.

Postgres Reverse ETL sources support Segment's dbt extension

If you have an existing dbt account with a Git repository, you can use 
Segment’s dbt extension
 to centralize model management and versioning, reduce redundancies, and run CI checks to prevent breaking changes.

Segment supports the following Postgres database providers:

Segment only supports these Postgres database providers. Postgres databases from other providers aren’t guaranteed to work. For questions or concerns about Segment-supported Postgres providers, 
contact Segment Support
.

Set up guide

To set up Postgres with Reverse ETL:

Run the SQL commands below to create a user named 
segment
.

 
-- create a user named "segment" that Segment will use when connecting to your Postgres cluster.

 
CREATE
 
USER
 
segment
 
PASSWORD
 
'<enter password here>'
;


 
-- allows the "segment" user to create new schemas on the specified database. (this is the name you chose when provisioning your cluster) 

 
GRANT
 
CREATE
 
ON
 
DATABASE
 
"<enter database name here>"
 
TO
 
"segment"
;



Extra permissions

Give the 
segment
 user read permissions for any resources (databases, schemas, tables) the query needs to access.

Give the 
segment
 user write permissions for the Segment managed schema (
__SEGMENT_REVERSE_ETL
), which keeps track of changes to the query results.

After you’ve successfully added your Postgres source, 
add a model
 and follow the rest of the steps in the Reverse ETL setup guide.

This page was last modified: 10 Jun 2024

Need support?

Questions? Problems? Need more info? Contact Segment Support for assistance!

Help improve these docs!

Was this page helpful?

Thanks for your feedback!

Can we improve this doc? 
Send us feedback!

Get started with Segment

On this page

Was this page helpful?

Thanks for your feedback!

Can we improve
 this doc? 
Send us feedback!

Product

For Developers

Company

Support

© 2025 Segment.io, Inc.

=== Content from https://segment.com/docs/guides/usage-and-billing/billing/ ===

Billing and Account FAQs
        

On this page

What is a billing cycle?

On the Segment monthly Team plan, your billing cycle starts the day after your 14-day trial ends. You’re billed on this day for each month while you’re on this plan. 

On the Segment annual Team plan, you’re billed at the end of your 14-day trial for the amount of an entire year of service including a specific number of 
MTUs
. Annual plan subscribers are billed for MTU overages at the end of each monthly cycle.

How do I change my plan?

If you already have a Segment workspace, you can change which plan your workspace is on by navigating to 
Settings > Usage & Billing > Plans
.

If you cancel or downgrade your plan during the 2-week trial period, you don’t incur any charges.

What if I cancel my paid plan before the end of the month?

If you cancel your plan or downgrade to a free account before the end of your official billing period on the monthly Team plan, you’ll receive a final bill for the prorated amount for the $120 base + a charge for any MTUs you’ve used over the allotted 10,000 at the rates posted on the 
pricing page
.

Segment doesn’t issue refunds for the pre-paid portion of your annual bill after your trial ends.

Be aware that if you notify Segment of wanting to cancel your annual plan, but continue to send data to Segment’s servers, you may incur overage charges in any given month. You should fully delete your workspace or cycle your write keys to stop all data flow into Segment to avoid future charges.

Will Segment charge sales tax on my invoice?

All Segment customers with a US business address may be subject to state and local sales taxes. The applicable tax law applies based on your business location address, which may be different from your billing address. Customers who purchase a taxable product or service, and are located in a jurisdiction where Segment currently charges sales tax, will see the calculated sales tax on their invoice.

Segment collects Value Added Tax (VAT) and Goods and Services Tax (GST) on the services sold to its international customers located in certain foreign jurisdictions.

For more information about sales tax, VAT, and GST, see the 
Segment VAT/GST FAQs
.

Do I qualify for a tax exemption?

If you believe your organization qualifies for a sales tax exemption (for example, because of a nonprofit or government status), you can contact 
tax@segment.com
 with the appropriate form.

I submitted a form for tax exemption, why am I still charged sales tax?

Tax might still be charged on your bill if either:

Do you offer refunds?

In most cases Segment doesn’t offer refunds, as noted in the 
Terms of Service
. 
Contact support
 if you feel that you’re in a unique situation.

Is there a free trial for paid plans?

Segment offers a 
2-week trial on the Team plan
 to let you try the plan before you purchase it.

Segment also offers the Free plan, which includes up to 1,000 MTUs, at no cost to you. 

Find out more about the 
different plans
 and which one suits your needs best.  

What happens when I exceed the Free plan limit?

The Free plan includes up to 1,000 MTUs at no cost. If you exceed the 1,000 MTU limit once in a 6-month period, Segment locks your account but data is still able to flow through Segment. To unlock your account, you can choose from these options:

If you exceed the 1,000 MTU limit twice in a 6-month period, Segment locks your account and also stops sending and receiving data. You can unlock your account by following option 2 above to upgrade to the 
Team plan
 2-week free trial.

Team Trial FAQ

What is the Team trial?

The Team trial is a 14-day free trial of Segment’s 
Team plan
, and it includes all the features associated with a Team plan, including unlimited sources, two warehouse syncs per day, 10 seats, and 10,000 MTUs (with the ability to track more MTUs as needed).

How do I get a two-week Team trial?

You automatically receive a 2-week trial when you sign up for a Team plan. 

Do I have to be a “new” customer to receive the free Team trial?

The free trial is available to all customers who have never had a Team plan. This includes new customers as well as customers who have previously been on the Free plan. 

Do you have to include your payment information when signing up for a Team trial?

If you’re upgrading from a Free Plan to a Team Plan, you’re required to add your payment information. 

If you’re signing up for a new Team plan, you don’t have to add your payment information during sign up. If you would like to continue to use the Team plan after the 14-day trial, add your credit card information on the “Payment Information” page in your workspace before the trial ends. 

What happens when the two-week trial ends?

If you added your payment information, your subscription automatically continues at the regular rate after the trial period expires. You can 
delete your workspace
 or 
downgrade to a Free plan
 any time during the trial to avoid charges.

To activate the free trial, add your payment information.

This page was last modified: 24 Oct 2024

Need support?

Questions? Problems? Need more info? Contact Segment Support for assistance!

Help improve these docs!

Was this page helpful?

Thanks for your feedback!

Can we improve this doc? 
Send us feedback!

Get started with Segment

On this page

Was this page helpful?

Thanks for your feedback!

Can we improve
 this doc? 
Send us feedback!

Product

For Developers

Company

Support

© 2025 Segment.io, Inc.

=== Content from https://segment.com/docs/engage/content/email/template/ ===

Email Template
        

Engage Premier requires a Business tier account and includes Engage Foundations and Unify.


See the 
available plans
, or 
contact Support
.

On this page

Engage Premier entered an End of Sale (EOS) period effective  June 10, 2024. Existing Segment customers will continue to have access and support to Engage Premier until an end-of-life (EOL) date is announced. We recommend exploring the following pages in preparation of a migration or future MCM needs:

Twilio Marketing Campaigns

Preferred ISV Partners:

Airship Blog
 


Bloomreach Blog
 


Braze Blog
 


Insider Blog
 


Klaviyo Blog
 


Twilio Engage Foundations Documentation
 

Use Twilio Engage to build personalized email templates to store and use throughout marketing campaigns.

Build an email template from scratch using the 
Drag and Drop Editor
 or the 
HTML Editor
. Include 
personalized content
 in the subject line, preview text, and email body to engage with users based on their profile traits and actions.

Build an Email template

Navigate to 
Engage > Content
 and use the Email Templates page to preview and edit existing templates.

To configure an email template, click 
Create Template
.

You must first connect a 
SendGrid subuser account
 to your Segment space to build email templates in Engage. Visit the 
onboarding steps
 for more information.

Engage content validation

For all content editors in Engage, you’ll see alerts for any issues in your template, such as invalid profile traits or incorrect liquid syntax. Engage both flags template issue(s), and displays recommended next steps. While you can save these templates, you must fix any issues before using them in Engage campaigns.

Test the Email template

You can send test emails before you include a template in marketing campaigns.

You can also test email templates directly from a 
Send an Email step
 in Journeys.

Dynamic sender using merge tags

Engage supports dynamic sending using merge tags. Personalize email content by adding real-time profile traits in merge tags to the following fields:

As you configure the template, click 
Merge Tags
 and select the profile traits to include. Engage inserts the merge tags based on cursor placement.

The following table contains a description and some best practices for all fields in the email template. Asterisks indicate required fields.

You can also add merge tags in the heading or body text as you design an email with the 
Drag and Drop
 or 
HTML
 editors. Engage supports 
liquid templating
 to create dynamic content in the email design editor.

Use liquid statements with an image URL

If you’re using the 
image content module
 in the Drag and Drop Editor, you can’t use liquid statements in the 
Image URL
 field. 
To use liquid statements with an image, Segment recommends using an 
HTML block
 with the following syntax: 


<img src=“{{profile.traits.imageLink | default: '<insert your default URL here>'}}”
, where 
profile.traits.imageLink
 is an example profile trait representing personalized image links for each recipient.

To learn more about profile traits, visit Segment’s 
Computed Traits
 and 
SQL Traits
 documentation.

Include unsubscribe and manage preference links

When you build an email template, you’ll need to include links that your customers can access to unsubscribe and manage their email preferences. You’ll find both in the 
Special Links
 dropdown menu of the 
Insert/Edit link
 window.

Unsubscribe links

When you build email templates, it’s your responsibility to include an unsubscribe link in your message. Add unsubscribe links to an email template from the Drag and Drop or HTML editors.

When a recipient clicks on an unsubscribe link, they’ll see a confirmation page and the recipient’s subscription state is updated.

Learn more about 
User Subscriptions
 in Twilio Engage.

Manage preference links

The manage preference link lets your customers opt in and out of email groups on an individual basis instead of unsubscribing from all your campaigns.

For more information, see 
subscription groups
.

Arrays and objects in Broadcasts

Segment doesn’t support profile traits in object and array datatypes in 
Broadcasts
, but you cam use them in 
Journeys
.

Next steps

View some 
email deliverability tips and tricks
 from SendGrid.

You can also use the Templates screen in Engage to 
build SMS templates
.

FAQs

Do updates to an email template automatically apply to Journey steps using it?

When you add a template to a Journey step, it becomes a copy specific to that step. Changes made to the original template won’t update the Journey version, and edits made in the Journey step won’t affect the original template. This keeps your Journey changes separate while preserving the original for reuse.

This page was last modified: 07 Feb 2025

Need support?

Questions? Problems? Need more info? Contact Segment Support for assistance!

Help improve these docs!

Was this page helpful?

Thanks for your feedback!

Can we improve this doc? 
Send us feedback!

Get started with Segment

On this page

Was this page helpful?

Thanks for your feedback!

Can we improve
 this doc? 
Send us feedback!

Product

For Developers

Company

Support

© 2025 Segment.io, Inc.

=== Content from https://segment.com/docs/connections/spec/b2b-saas/ ===

Spec: B2B SaaS
        

On this page

This guide explains how B2B SaaS companies should send core user and account lifecycle data to Segment.

Overview

Most B2B SaaS companies have a few common, core lifecycle events for users and accounts. We understand that account hierarchies can be unique and complex, but by following this spec you can take advantage of account-based tools on Segment platform, and B2B SaaS data products by Segment.

Events

The B2B SaaS category has the following semantic events:

Account Created

This event should be sent when a new account is created.

This event supports the following semantic properties:

{

  
"
userId
"
:
 
"
019mr8mf4r
"
,

  
"
type
"
:
 
"
track
"
,

  
"
event
"
:
 
"
Account Created
"
,

  
"
properties
"
:
 
{

    
"
account_name
"
:
 
"
Initech
"

  
},

  
"
context
"
:
 
{

    
"
groupId
"
:
 
"
acct_123
"

  
}


}



Account Deleted

This event should be sent when an account is deleted.

This event supports the following semantic properties:

{

  
"
userId
"
:
 
"
019mr8mf4r
"
,

  
"
type
"
:
 
"
track
"
,

  
"
event
"
:
 
"
Account Deleted
"
,

  
"
properties
"
:
 
{

    
"
account_name
"
:
 
"
Initech
"

  
},

  
"
context
"
:
 
{

    
"
groupId
"
:
 
"
acct_123
"

  
}


}



Signed Up

This event should be sent when a user signs up for your service.

Good to know
: Segment’s best practice is to use an “Object-Action” naming convention, which in this case would be “User Signed Up”. However, because in the B2B case this may not be a specific user, we omit that noun in our example. You may include or omit it as needed for your implementation.

This event supports the following semantic properties:

{

  
"userId"
:
 
"019mr8mf4r"
,

  
"type"
:
 
"track"
,

  
"event"
:
 
"Signed Up"
,

  
"properties"
:
 
{

    
"type"
:
 
"organic"
,

    
"first_name"
:
 
"Peter"
,

    
"last_name"
:
 
"Gibbons"
,

    
"email"
:
 
"pgibbons@example.com"
,

    
"phone"
:
 
"410-555-9412"
,

    
"username"
:
 
"pgibbons"
,

    
"title"
:
 
"Mr"

  
},

  
"context"
:
 
{

    
"groupId"
:
 
"acct_123"

  
}


}



Signed In

This event should be sent when a user signs in to your service.

Good to know
: Segment’s best practice is to use an “Object-Action” naming convention, which in this case would be “User Signed In”. However, because in the B2B case this may not be a specific user, we omit that noun in our example. You may include or omit it as needed for your implementation.

This event supports the following semantic properties:

{

  
"userId"
:
 
"019mr8mf4r"
,

  
"type"
:
 
"track"
,

  
"event"
:
 
"Signed In"
,

  
"properties"
:
 
{

    
"username"
:
 
"pgibbons"

  
},

  
"context"
:
 
{

    
"groupId"
:
 
"acct_123"

  
}


}



Signed Out

This event should be sent when a user signs out for your service. You should also call 
analytics.reset()
 to refresh the cookie when a Signed Out event occurs.

Good to know
: Segment’s best practice is to use an “Object-Action” naming convention, which in this case would be “User Signed Out”. However, because in the B2B case this may not be a specific user, we omit that noun in our example. You may include or omit it as needed for your implementation.

This event supports the following semantic properties:

{

  
"
userId
"
:
 
"
019mr8mf4r
"
,

  
"
type
"
:
 
"
track
"
,

  
"
event
"
:
 
"
Signed Out
"
,

  
"
properties
"
:
 
{

    
"
username
"
:
 
"
pgibbons
"

  
},

  
"
context
"
:
 
{

    
"
groupId
"
:
 
"
acct_123
"

  
}


}



Invite Sent

This event should be sent when a user invites another user.

This event supports the following semantic properties:

{

  
"
userId
"
:
 
"
019mr8mf4r
"
,

  
"
type
"
:
 
"
track
"
,

  
"
event
"
:
 
"
Invite Sent
"
,

  
"
properties
"
:
 
{

    
"
invitee_email
"
:
 
"
pgibbons@example.com
"
,

    
"
invitee_first_name
"
:
 
"
Peter
"
,

    
"
invitee_last_name
"
:
 
"
Gibbons
"
,

    
"
invitee_role
"
:
 
"
Owner
"

  
},

  
"
context
"
:
 
{

    
"
groupId
"
:
 
"
acct_123
"

  
}


}



Account Added User

This event should be sent when a user is added to a group.

This event supports the following semantic properties:

{

  
"
userId
"
:
 
"
019mr8mf4r
"
,

  
"
type
"
:
 
"
track
"
,

  
"
event
"
:
 
"
Account Added User
"
,

  
"
properties
"
:
 
{

    
"
role
"
:
 
"
Owner
"

  
},

  
"
context
"
:
 
{

    
"
groupId
"
:
 
"
acct_123
"

  
}


}



Account Removed User

This event should be sent when a user is removed from a group or account.

This event supports the following semantic properties:

{

  
"
userId
"
:
 
"
019mr8mf4r
"
,

  
"
type
"
:
 
"
track
"
,

  
"
event
"
:
 
"
Account Removed User
"
,

  
"
properties
"
:
 
{},

  
"
context
"
:
 
{

    
"
groupId
"
:
 
"
acct_123
"

  
}


}



Trial Started

This event should be sent when a trial is started.

This event supports the following semantic properties:

{

  
"
userId
"
:
 
"
019mr8mf4r
"
,

  
"
type
"
:
 
"
track
"
,

  
"
event
"
:
 
"
Trial Started
"
,

  
"
properties
"
:
 
{

    
"
trial_start_date
"
:
 
"
2018-08-28T04:09:47Z
"
,

    
"
trial_end_date
"
:
 
"
2018-09-20T04:09:47Z
"
,

    
"
trial_plan_name
"
:
 
"
Business
"

  
},

  
"
context
"
:
 
{

    
"
groupId
"
:
 
"
acct_123
"

  
}


}



Trial Ended

This event should be sent when a trial ends.

This event supports the following semantic properties:

{

  
"
userId
"
:
 
"
019mr8mf4r
"
,

  
"
type
"
:
 
"
track
"
,

  
"
event
"
:
 
"
Trial Ended
"
,

  
"
properties
"
:
 
{

    
"
trial_start_date
"
:
 
"
2018-08-28T04:09:47Z
"
,

    
"
trial_end_date
"
:
 
"
2018-09-20T04:09:47Z
"
,

    
"
trial_plan_name
"
:
 
"
Business
"

  
},

  
"
context
"
:
 
{

    
"
groupId
"
:
 
"
acct_123
"

  
}


}



This page was last modified: 19 Jul 2022

Need support?

Questions? Problems? Need more info? Contact Segment Support for assistance!

Help improve these docs!

Was this page helpful?

Thanks for your feedback!

Can we improve this doc? 
Send us feedback!

Get started with Segment

On this page

Was this page helpful?

Thanks for your feedback!

Can we improve
 this doc? 
Send us feedback!

Product

For Developers

Company

Support

© 2025 Segment.io, Inc.

=== Content from https://segment.com/docs/connections/storage/data-lakes/lake-formation/ ===

Lake Formation
        

Data Lakes is available for the listed account plans only.


See the 
available plans
, or 
contact Support
.

On this page

Lake Formation is a fully managed service built on top of the AWS Glue Data Catalog that provides one central set of tools to build and manage a Data Lake. These tools help import, catalog, transform, and deduplicate data, as well as provide strategies to optimize data storage and security. To learn more about Lake Formation features, see 
Amazon Web Services documentation
.

The security policies in Lake Formation use two layers of permissions: each resource is protected by Lake Formation permissions (which control access to Data Catalog resources and S3 locations) and IAM permissions (which control access to Lake Formation and AWS Glue API resources). When any user or role reads or writes to a resource, that action must pass a both a Lake Formation and an IAM resource check: for example, a user trying to create a new table in the Data Catalog may have Lake Formation access to the Data Catalog, but if they don’t have the correct Glue API permissions, they will be unable to create the table.

For more information about security practices in Lake Formation, see Amazon’s 
Lake Formation Permissions Reference
 documentation.

Configure Lake Formation

You can configure Lake Formation using the 
IAMAllowedPrincipals
 group
 or by 
using IAM policies for access control
. Configuring Lake Formation using the 
IAMAllowedPrincipals
 group is an easier method, recommended for those exploring Lake Formation. Setting up Lake Formation using IAM policies for access control is a more advanced setup option, recommended for those who want additional customization options.

Permissions required to configure Data Lakes

To configure Lake Formation, you must be logged in to AWS with data lake administrator or database creator permissions.

Configure Lake Formation using the IAMAllowedPrincipals group

To verify that you’ve configured Lake Formation, open the 
AWS Lake Formation service
, select 
Data lake permissions
, and verify the 
IAMAllowedPrincipals
 group is listed with “All” permissions.

Configure Lake Formation using IAM policies

Granting Super permission to IAM roles

If you manually configured your database, assign the 
EMR_EC2_DefaultRole
 Super permissions in step 8. If you configured your database using Terraform, assign the 
segment_emr_instance_profile
 Super permissions in step 8.

This page was last modified: 07 Feb 2025

Need support?

Questions? Problems? Need more info? Contact Segment Support for assistance!

Help improve these docs!

Was this page helpful?

Thanks for your feedback!

Can we improve this doc? 
Send us feedback!

Get started with Segment

On this page

Was this page helpful?

Thanks for your feedback!

Can we improve
 this doc? 
Send us feedback!

Product

For Developers

Company

Support

© 2025 Segment.io, Inc.

=== Content from https://segment.com/docs/connections/event-delivery/ ===

Event Delivery
        

On this page

The Event Delivery tool helps you understand if data is reaching your destinations, and also helps you to see if Segment encountered any issues delivering your source data. 

Segment sends billions of events to destinations every week. If Segment encounters any errors when attempting to deliver your data, Segment reports them in the Event Delivery tool.

Available for server side event streaming destinations only

This feature is only available for server side integrations (also known as cloud-mode destinations). You will not be able to use this for client side integrations (also known as device-mode destinations) because device-mode data is sent directly to the destination tool’s API. In order to report on deliverability, the data must be sent to destinations using a server side connection.

Not available for Warehouses or Amazon S3
. These destinations work differently from other destinations in Segment, and aren’t supported at this time.

Here’s an example of what the Event Delivery tool looks like:



When to use Event Delivery

Scenarios when this tool will be useful: 

Where do I find it? 

Event Delivery can be accessed within any supported destination in the App. It’s located on the tab under “Settings” for each destination. 



How do I use Event Delivery?

The UI consists of three key parts that report on Segment’s ability to deliver your source data - Key Metrics, Error Details, and Delivery Trends. Reporting on core functionality from top to bottom: 

1. Time period

There’s a drop down menu to select your time period. All of the UI is updated based on the time period you select. 



2. Key Metrics

From left to right in the above graphic:

Delivered:
 This tells you the number of messages Segment successfully delivered to a destination in the time period you selected.

Not Delivered:
 This count tells you the number of messages Segment was unable to deliver. If this number is higher than zero, you will see the reasons for this failure in the errors table below. 

P95 Latency:
 This is the time it takes for Segment to deliver the slowest 5% of your data (known as P95 latency). The latency reported is end-to-end: the event being received through the Segment API to the event being delivered to partner API. This helps tell you if there is a delay in your data and how bad it is.

3. Error details

The purpose of the table is to provide you a summary of the different errors we’ve seen in a given period and the most important information on them. All of the rows in the table are clickable and expand to give you more information. 



View Segment’s list of 
Integration Error Codes
 for more information about what might cause an error.

Error detail view

When there’s an error, Segment wants to give you as much information as possible to help you resolve the issue. See below for an example of what this view looks like. 



This view includes: 

The event delivery UI provides a human-friendly summary of the error, based on the payload Segment received back from the partner.

These are actions you can take, based on what Segment knows about the issue. 

This section displays links to any documentation that might be helpful to you. 

To help you debug, Segment provides sample payloads from every step of the data’s journey:

You Sent
 - the data you sent to Segment’s API.

Request to Destination
 - the request Segment made to the Partner API. This payload will likely be different from what you sent it because Segment is mapping your event to the partner’s spec to ensure the message is successfully delivered. 

Response from Destination
 - the response Segment received from the Partner API. This will have the raw partner error. If you need to troubleshoot an issue with a Partner’s Success team, this is usually something they’ll want to see.

You can opt in to receive email alerts regarding failed events/syncs by going to your workspaace Settings > User Preferences > Activity Notifications > Destiantions. Then you can select the option to be alerted by Email and/or In-app.

4. Trends

When debugging, it’s helpful to see when issues start, stop, and trend over time. 



Delivered
: The number of events that were successfully delivered in the time period you selected. 

Not delivered
: The number of events that were not successfully delivered in the time period you selected. 

How P95 latency has trended over the time period you selected.

This page was last modified: 05 Mar 2024

Need support?

Questions? Problems? Need more info? Contact Segment Support for assistance!

Help improve these docs!

Was this page helpful?

Thanks for your feedback!

Can we improve this doc? 
Send us feedback!

Get started with Segment

On this page

Was this page helpful?

Thanks for your feedback!

Can we improve
 this doc? 
Send us feedback!

Product

For Developers

Company

Support

© 2025 Segment.io, Inc.

=== Content from https://segment.com/docs/privacy/consent-management/consent-in-segment-connections/ ===

Consent in Segment Connections
        

Consent Management is available to customers on Business tier plans.


See the 
available plans
, or 
contact Support
.

On this page

Segment Connections users can add the 
consent object
 to their sources to stamp events with the end user consent preferences captured by your consent management platform (CMP) and send them downstream to destinations in categories that an end user consented to share data with. Events without the consent object continue to flow to destinations without consent enforcement.

With the 
Destination Actions framework
, you can send current end user consent preferences to flow to your destination alongside customer interactions so your destinations know when an end user revokes their consent.

For more information about sharing current end user consent preferences with your downstream destinations, see the 
Sharing consent with Actions destinations
 documentation.

For more information about configuring consent categories, see the 
Configure Consent Management
 documentation.

If your sources also contain the integrations object, Segment will look at the consent object first, and then take into account the integrations object according to the table in the 
Reconcile consent object and integrations object
 documentation.

Unify users must send an additional event to add consent preferences to Profiles

If you use Unify, see the 
Consent in Unify
 documentation for more information about the Segment Consent Preference Updated event, which Segment uses with the consent object to add consent preference to Profiles.

Consent object

Segment requires every event from all of your sources to include the end user consent preferences, captured by your CMP or your application logic, in the form of the 
consent object
. The consent object is a JSON object nestled inside of the 
context object
 with the following format:

The JSON keys in the consent object should represent the 
categoryId
 for each consent category.

{


"context"
:
 
{

  
"consent"
:
 
{

    
"categoryPreferences"
:
 
{

        
"Advertising"
:
 
true
,

        
"Analytics"
:
 
false
,

        
"Functional"
:
 
true
,

        
"DataSharing"
:
 
false

      
}

    
}

  
}


}




Events without the consent object will continue to flow to destinations without consent enforcement.

Reconcile consent conflicts

Segment resolves conflicts between your 
consent object and your integration object
 and between your 
CMP and the consent categories you configured in the Segment app
.

Reconcile consent object and integrations object conflicts

You can add both the integrations object and the consent object to your Segment payloads for greater control over how Segment routes data to your downstream destinations.

For more information about the Integrations object, please see 
Filtering your Segment Data
.

If an event includes both an integrations and consent object, Segment will look at the consent object first, and then take into account the integrations object according to the following table:

Reconcile CMP and Segment consent category conflicts

If you have a category configured in your consent management tool (for example, 
advertising
) and there is no category with the same ID in Segment, the data will flow to unmapped destinations. If destinations are mapped to a different category in the Segment app, data flow will honor end user consent for that category.

If there is a category configured in Segment (
functional
) that is not mapped in your CMP, data will not flow to destinations mapped to the 
functional
 category.

Consent observability

Events discarded due to consent preferences appear in 
Delivery Overview
 at the “Filtered at destination” step with the discard reason 
Filtered by end user consent
.

This page was last modified: 02 Aug 2024

Need support?

Questions? Problems? Need more info? Contact Segment Support for assistance!

Help improve these docs!

Was this page helpful?

Thanks for your feedback!

Can we improve this doc? 
Send us feedback!

Get started with Segment

On this page

Was this page helpful?

Thanks for your feedback!

Can we improve
 this doc? 
Send us feedback!

Product

For Developers

Company

Support

© 2025 Segment.io, Inc.

=== Content from https://segment.com/docs/engage/audiences/linked-audiences/ ===

Linked Audiences
        

Engage Foundations requires a Business tier account and includes Unify.


See the 
available plans
, or 
contact Support
.

On this page

Linked Audiences empowers marketers to effortlessly create targeted audiences by combining behavioral data from the Segment Profile and warehouse entity data within a self-serve, no-code interface.

This tool accelerates audience creation, enabling precise targeting, enhanced customer personalization, and optimized marketing spend without the need for constant data team support.

With Linked Audiences, you can:

To learn more about specific use cases you can set up with Linked Audiences, see 
Linked Audiences Use Cases
.

Prerequisites

Before you begin setting up your Linked Audience, ensure you have:

Setting up Linked Audiences

To set up your Linked Audience, complete the following steps:

Step 1: Build a Linked Audience

Linked Audiences allows you to filter based on properties like 
profile traits
, 
relational data
 mapped to the 
Data Graph
, 
events
, and existing 
audiences
.



To build a Linked Audience:

After creating your Linked Audience, you will be brought to the Overview page with the Linked Audience in a disabled state.

Linked Audience conditions

The linked audiences builder sources profile trait and event keys from the data warehouse. This data must be synced to the data warehouse through 
Profiles Sync
 before you can reference it in the linked audience builder. If there is a profile trait that exists in the Segment Profile that hasn’t successfully synced to the data warehouse yet, it will be grayed out so that it can’t be selected.

The linked audience builder also returns a subset of available entity property key values, event property and context key values, and profile trait key values that you can select in the input field drop-down. This eliminates the need to type in the exact value you want to filter on. If the value you’re looking for isn’t listed, you can manually enter it into the input field. Manually entered values are case-sensitive.

Segment displays:

You can duplicate your conditions in the audience builder into the same condition group.You can only create nested entity conditions up to six levels in depth. For example, an entity condition that queries for relationships between Profiles, Accounts, Credit Cards, and Transactions has four levels of depth.

As you’re building your Linked Audience, you can choose from the following conditions:

The entity and event condition type supports these configurations: 
at least: supports 1 or greater,

exactly: supports 0 or greater, 
at most: supports 0 or greater.

*When filtering by 0, you can’t filter on by entity properties or on additional nested entities.

You can create audience definitions using either 
AND
 or 
OR
 operators across all condition levels. You can switch between these operators when filtering on multiple entity or event properties, between conditions within a condition group, and between condition groups.

Example:



If you have defined entity conditions in your audience definition, you will see a “Matched Entities” tab in the audience preview to help you understand what entities qualified a user to be a part of an audience.

This information appears when you click the user profile generated from the audience preview. The contextual information encompasses entity relationships as well as entity column values that were used as filtering criteria in the audience definition. By default, Segment includes the entity ID. The data being returned is truncated - 10 entities at each level, 6 levels of depth. If you want to opt out of this functionality, contact Segment Support.



Event conditions

When filtering on event properties, you can dynamically reference the value of another profile trait, or enter a constant value. These operators support dynamic references: equals, not equals, less than, greater than, less than or equal, greater than or equal, contains, does not contain, starts with, ends with.

Entity conditions

When filtering on entity properties, you can dynamically reference the value of another entity column (from the same entity branch at the same level or above it), profile trait, or enter a constant value. You can only dynamically reference properties of the same data type. Dynamic references are supported for specific operators depending on the data type, as in the following table:

Step 2: Activate your Linked Audience

After you build your Linked Audience, you can send events to your chosen destinations and use them for personalizing your customer communications.

To activate your Linked Audience:

Step 2a: Connecting to a destination

Destinations
 are the business tools or apps that Segment forwards your data to. Adding a destination allows you to act on your data and learn more about your customers in real time. To fully take advantage of Linked Audiences, you must connect and configure at least one destination.

Linked Audiences destinations

Linked Audiences only supports 
Actions Destinations
. List destinations aren’t supported.

Note:
 Ensure your 
destination has been enabled
 in Segment before you begin the steps below.

Step 2b: Select your Destination Actions

The 
Destination Actions
 framework allows you to see and control how Segment sends the event data it receives from your sources to actions-based destinations. Each Action in a destination lists the event data it requires and the event data that is optional. Segment displays available Actions based on the destination you’ve connected to your Linked Audience. You can see details of each option and how to use it in the 
Actions Destinations Catalog
 documentation.

Select the Destination Action to call when the event happens, then click 
Next
.

Step 2c: Define how and when to trigger an event to your destination

Configure how and when events are produced with each audience run. Select the entities referenced in the audience builder to act as a trigger for your events.

Step 2d: Configure the event

After you select an action, Segment attempts to automatically configure the data fields that will be sent to the destination. You can review and adjust these settings before enabling this event.

Select additional traits and properties to include when the event is sent.

Click 
Copy to use in Braze Cloud Mode (Actions)
 to copy the personalization syntax for the selected traits and properties to use in your destination messaging templates.

This feature is in beta for customers using Braze. Some functionality may change before it becomes generally available. This feature is governed by Segment’s 
First Access and Beta Preview Terms
.

As you’re enriching your events in Linked Audiences, you should view a preview of the event payload schema based on the properties you select. It might look like the following:



Only required fields are displayed. All optional & pre-filled fields are hidden, though you can view hidden fields by clicking 
Show hidden fields
.

These fields are pre-filled with properties configured by default.

Step 3: Send a test event to your destination

Send a test event to ensure that everything is connected properly and your destination receives the event.

Enter the destination User id for the profile you want to use to test the event, then click 
Send test event to destination
.

The Event content drop-down shows you a preview of what the data sent to your destination might look like.

Step 4: Enable your Linked Audience

After building your Linked Audience, choose 
Save and Enable
. You’ll be redirected to the Audience Overview page, where you can view the audience you created. Segment automatically disables your audience so that it doesn’t start computing until you’re ready. A run is when Segment runs the audience conditions on your data warehouse and sends events downstream.

To enable your audience, select the 
Enabled
 toggle, then select 
Enable audience
.

Run Now

You can trigger a run for your audience if you want to send events to your destination without waiting for the next scheduled run. To do so, select 
Run Now
. This triggers a run for the audience and sends events downstream.

Set a run schedule

Use the Audience Overview page to view the audience profile count, current run schedule, run status, and upcoming run time.

Determine when an audience should run and send data to enabled destinations with a run schedule:

You can maintain your run schedule at any time from the audience’s 
Settings
 tab.

You can also click 
Run Now
 on the Audience Overview page at any time (even if the run schedule is 
Interval
 Overview 
Day and time
) to manually trigger a run on your warehouse and send data to enabled destinations.

There may be up to a 5 minute delay from the configured start time for audiences that are configured with the 
Interval
 and 
Day and time
 run schedules. For example, if you configured an audience with the 
Day and time
 compute schedule to run on Mondays at 8am, it can compute as late as Monday at 8:05am. This is to help us better manage our system load.

Step 5: Monitor your activation

With your Linked Audience activated, follow these steps to monitor your activation:

Delivery Overview for Linked Audiences

Delivery Overview shows you four steps in your data activation pipeline:

Maintaining Linked Audiences

You can maintain your Linked Audience by accessing these tabs on the main page of your Linked Audience:

This page was last modified: 23 Jan 2025

Need support?

Questions? Problems? Need more info? Contact Segment Support for assistance!

Help improve these docs!

Was this page helpful?

Thanks for your feedback!

Can we improve this doc? 
Send us feedback!

Get started with Segment

On this page

Was this page helpful?

Thanks for your feedback!

Can we improve
 this doc? 
Send us feedback!

Product

For Developers

Company

Support

© 2025 Segment.io, Inc.

=== Content from https://segment.com/docs/segment-app/iam/mfa/ ===

Multi-Factor Authentication (MFA)
        

MFA is available to all Segment plans, but cannot be used with SSO.


See the 
available plans
, or 
contact Support
.

On this page

Multi-factor Authentication (MFA) provides an additional layer of security when logging into your Segment account. When MFA is enabled, users must enter their username and password, and a one-time use code. Users can either enable MFA for their own account, or workspace owners can require that all users in a workspace use MFA. These security settings are available in the workspace from the “Advanced Settings” section.

You can configure your Segment workspace to send a text message code (U.S. and Canada only), or use an authentication app to generate a time-based token (for example 
Google Authenticator
, 
1Password
, or 
Authy
). You can also log in using a recovery code in case you don’t have your MFA device available. When you configure MFA, be sure to save your recovery code in safe place so you can access your Segment account in the event you lose your MFA device.

We highly recommend that you 
choose a strong password
 and also enable MFA for the email account that you use to log into Segment. If someone is able to gain access to your email, they will be able to access your Segment account even if your Segment account has MFA enabled.

Who can use MFA?

MFA is available to all Segment customers that are not 
logged in using SSO
. If your company uses SSO to sign in to Segment, you should enable MFA at the SSO provider. Contact your company’s IT team if you have questions about your company’s SSO configuration.

Enabling MFA

Once MFA is enabled, Segment prompts you for one of these methods every time you log in.

Recovering MFA

Your recovery code can be used bypass in the event you do not have your MFA device. If you no longer have access to your recovery code, you can choose to send a recovery code to your email to re-access your Account.

This page was last modified: 03 Aug 2023

Need support?

Questions? Problems? Need more info? Contact Segment Support for assistance!

Help improve these docs!

Was this page helpful?

Thanks for your feedback!

Can we improve this doc? 
Send us feedback!

Get started with Segment

On this page

Was this page helpful?

Thanks for your feedback!

Can we improve
 this doc? 
Send us feedback!

Product

For Developers

Company

Support

© 2025 Segment.io, Inc.

=== Content from https://segment.com/docs/api/config-api/fql/ ===

Redirecting…

=== Content from https://segment.com/docs/getting-started/06-testing-debugging/ ===

Testing and Debugging
        

On this page

One of the most important questions you’ll ask early on is 
“How do I know if Segment is working?”

There are several ways to check if your data is flowing. One is the 
Debugger tab in each Source
 in the Segment web app, where you can see data coming from a source into Segment. Another is the 
Event Delivery tool
 which shows which data is arriving at specific destinations.

For monitoring purposes, you’ll also see alerts in the 
Workspace Health
 tool if your sources or destinations produce repeated errors.

Want more? Check out our course on debugging and troubleshooting. (Must be logged in to access.)

The Source Debugger

The Source Debugger is a real-time tool that helps you confirm that API calls made from your website, mobile app, or servers arrive to your Segment Source, so you can troubleshoot your Segment set up even quicker. With the Debugger, you can check that you’re sending calls in the expected format, without having to wait for any data processing.



The Debugger is separate from your workspace’s data pipeline and is not an exhaustive view of all the events ever sent to your Segment workspace. The Debugger only shows a sample of the events that the Source receives in real time, with a cap of 500 events. The Debugger is a great way to test specific parts of your implementation to validate that events are being fired successfully and arriving to your Source.

To see a more complete view of all your events, we recommend that you set up either a 
warehouse
 or an 
S3 destination
.

The Debugger shows a live stream of sampled events arriving into the Source, but you can also pause the stream from displaying new events by toggling “Live” to “Pause”. Events continue to arrive to your Source while you Pause the stream.

You can search in the Debugger to find a specific payload using any information you know is available in the event’s raw payload. You can also use advanced search options to limit the results to a specific event.



Two views are available when viewing a payload:

Event Delivery

The Event Delivery tool helps you see if Segment is encountering issues delivering your data from your sources to their connected destinations. 

Segment sends billions of events to destinations every week. If our systems encounter errors when trying to deliver your data, we report them in the Event Delivery tool.

Here is an example of what the Event Delivery tool looks like:



Event Delivery is most useful when: 

You can access the Event Delivery tool from the destination 
Settings
 tab in any supported destination.



Event Delivery is only available for cloud-mode destinations, which receive data through the Segment servers. Device-mode destinations receive data through an API endpoint outside the Segment servers, where we cannot monitor or report on it. 
Event delivery is not available for Warehouses or Amazon S3 destinations
.

Using Event Delivery

The UI shows three parts that report on Segment’s ability to deliver your source data: Key Metrics, Error Details, and Delivery Trends.

Before you begin,
 select a time period from the drop down menu at the right. The Event Delivery display updates to show only information about your selected time period.



This panel displays quantitative information about the destination’s data flow:

Delivered:
 The number of messages Segment successfully delivered to the destination in the selected time period.

Not Delivered:
 The number of messages Segment was unable to deliver. If this number is greater than zero, the reasons for these failures appear in the errors table below. 

P95 Latency:
 The time it takes for Segment to deliver the slowest 5% of your data (known as P95 latency). The latency reported is end-to-end: from the event being received through the Segment API, to the event being delivered to partner API. This helps tell you if there is a delay in your data pipeline, and how severe it is.

The Error details table displays a summary of the errors in a given period, and the most important information about them. You can click any row in the table to expand it to show more information. 



The Error Details view gives you as much information as possible to help you resolve the issue. The example below shows an example Error Details panel. 



This view includes: 

You Sent
 - the data you sent to Segment’s API.

Request to Destination
 - the request Segment made to the Partner API. This payload will likely be different from what you sent it because Segment is mapping your event to the partner’s spec to ensure the message is successfully delivered. 

Response from Destination
 - the response Segment received from the Partner API. This will have the raw partner error. If you need to troubleshoot an issue with a Partner’s Success team, this is usually something they’ll want to see. 

View Segment’s list of 
Integration Error Codes
 for more information about what might cause an error.

Trends

When debugging, it’s helpful to see when issues start, stop and how they trend over time. 

The Event Delivery view shows a graph with the following information:

Delivered
: The number of events that were successfully delivered in the time period you selected. 

Not delivered
: The number of events that were not successfully delivered in the time period you selected. 

The Latency view shows the end-to-end P95 latency during the time period you selected.



Unlock the power of Segment with Destinations

Learn about what you can do next with Segment

This page was last modified: 06 Jul 2022

Need support?

Questions? Problems? Need more info? Contact Segment Support for assistance!

Help improve these docs!

Was this page helpful?

Thanks for your feedback!

Can we improve this doc? 
Send us feedback!

Get started with Segment

On this page

Was this page helpful?

Thanks for your feedback!

Can we improve
 this doc? 
Send us feedback!

Product

For Developers

Company

Support

© 2025 Segment.io, Inc.

=== Content from https://segment.com/docs/privacy/data-controls/ ===

Privacy Controls & Alerts
        

Privacy Portal is available for the listed account plans only.


See the 
available plans
, or 
contact Support
.

On this page

The Privacy Portal gives you control over whether specific data is allowed to
enter Segment.

The controls are available to all customers, across all plan types. With
these, you can block data at the source level, which means
data from those sources never enters Segment once blocked. This does not support blocking to device-mode destinations.

For example, if you want to prevent certain types of PII (like Credit Card
Number) from ever being ingested by Segment, you can block it using standard
controls by classifying those fields as “Red” in your Data Inventory,
and then blocking them in your standard controls settings. Blocking fields
blocks the properties from entering Segment, but does not block the rest of the
event data from flowing through. Remember that any data you classify as “Red” in your Data Inventory is
blocked if you enable these controls.

If you block Segment data at the source level using these controls, the data
does not enter Segment and we can not Replay it. Additionally, if you have
Privacy Controls configured to change how you route Red and
Yellow data into or out of Segment, the standard controls respect the rules set
by those Controls. For example, if you have a Privacy Control set up to block

Red
 data at the Source-level, any new fields you classify in the Data Inventory as

Red
 are also blocked from that Source. Only fields added to the Data Inventory are blocked by a Privacy Control.

Privacy Alerts

Alerts notify you when a new, unclassified data type appears in your Privacy
Portal inbox. You can set up Slack alerts or a generic Source to pipe alerts to
other tools you might prefer for events and notifications. We recommend setting
up alerts to help you ensure your Inventory is always up to date.



To set up a Slack Alert:

you’re all set!

To send alerts to a Segment Source:

you’re all set!

This page was last modified: 24 Apr 2024

Need support?

Questions? Problems? Need more info? Contact Segment Support for assistance!

Help improve these docs!

Was this page helpful?

Thanks for your feedback!

Can we improve this doc? 
Send us feedback!

Get started with Segment

On this page

Was this page helpful?

Thanks for your feedback!

Can we improve
 this doc? 
Send us feedback!

Product

For Developers

Company

Support

© 2025 Segment.io, Inc.

=== Content from https://segment.com/docs/segment-app/workspace-home/ ===

Workspace Home
        

On this page

Segment’s Workspace Home serves as a dashboard that gives you a single consolidated view of the workspace, its health and status, and metrics for specific integrations over time.

Availability

The Workspace Home is visible to users who have the Workspace Owner 
role
. If you have access to the Home page, it is the first screen you see when you log in to your Segment Workspace.

The Home page shows a summary of errors in the workspace’s sources and destinations, a list of “favorite” integrations that you can configure, and information about recent audit logged events, and your billing plan usage.



The Workspace Home page shows when the data it represents was last updated. You can click the 
Updated
 line to refresh the display.

You can also use the drop-down menu at the right to choose between a 24-hour and 7-day rolling view of the data. The page automatically updates the graphs and statistics when you change your selection.

Sources - Violations

The Sources section of the Home pages shows a summary of the event volume flowing through Segment.

If your Segment plan includes Protocols, the dashboard shows 
Event Violations
 occurring in the workspace, including a graph over time. You can click into the violations section to see a list of sources ordered from highest number of violations, to lowest. You can click into individual sources to see more details and go to their individual source pages, or 
go to the Violations page
.



Sources - Events Received

If your Segment plan does not include Protocols, an Events Received chart is shown to reflect the number of events received across sources.

Destinations - Event Delivery

The Destinations section of the Home page shows a summary of the Event Delivery for the workspace, including a graph over time. You can click the 
destinations
 link to see a list of destinations with delivery problems, ordered from highest to lowest error rate. You can click a destination to see more details about the delivery failures, and from the details panel click the destination name to go directly to its configuration page.



Favorite integrations

If you have access to the Workspace Home page, you can customize it for yourself by bookmarking or saving “favorite” integrations. These could be sources, destinations, storage destinations, and functions that you work with regularly, or want to monitor closely.

These Home page favorites are specific to your user account in the workspace. The are not shared among administrators in the same workspace, and are not accessible from other workspaces you might be an administrator for.

To create a favorite, click 
Add Favorite
 or the plus icon, and select the integration(s) to bookmark. The list that appears displays a status icon for each integration, so you can tell which ones are healthy (green) and unhealthy (red), and which ones are inactive (gray).

Once you create a favorite, the section displays your favorites in tabs so you can view sources only, destinations only (including storage destinations), or all of them at once. Each favorite displays important configuration details and summary statistics for that integration. You can click the 
more
 (…) menu to jump directly to one of the configuration or detail pages for that integration.

To delete a favorite, click the 
more
 (…) menu and select 
Remove favorite
.

Recent Activity

For users with Business Tier workspaces, the recent activity section displays the most recent items logged to the Segment Audit trail. This includes workspace membership changes and requests, changes to the configuration of different Segment features (including sources, destinations, and to Engage and Protocols configurations), and data storage sync failures.

Usage

The Usage section shows a summary of the workspace’s plan utilization for the current billing period. This includes billing information for all parts of your Segment plan. This includes 
MTUs or API call volume
 (as applicable), Functions usage time (if applicable), and Engage details (if applicable). Click 
view all
 to go to the workspace’s billing page for more detailed statistics.



This page was last modified: 21 Apr 2023

Need support?

Questions? Problems? Need more info? Contact Segment Support for assistance!

Help improve these docs!

Was this page helpful?

Thanks for your feedback!

Can we improve this doc? 
Send us feedback!

Get started with Segment

On this page

Was this page helpful?

Thanks for your feedback!

Can we improve
 this doc? 
Send us feedback!

Product

For Developers

Company

Support

© 2025 Segment.io, Inc.

=== Content from https://segment.com/docs/unify/csv-upload/ ===

Add or Update Profiles and Traits with a CSV
        

Unify requires a Business tier account and is included with Engage.


See the 
available plans
, or 
contact Support
.

On this page

You can use the Profiles CSV Uploader to add or update user profiles and traits. This page contains guidelines for your CSV upload and explains how to upload a CSV file to Unify.

CSV file upload guidelines

Keep the following guidelines in mind as you upload CSV files:

Upload a CSV file

Use the 
Upload CSV
 page to upload a CSV file in your Segment space:

1. Download your CSV template

Click 
Download Template
 to download a CSV template with identifier columns from your identity resolution configuration.

2. Fill out your CSV file

Enter values for the identifiers in your CSV file.

3. Upload your CSV file

You can upload a CSV file in two ways:

Work with the CSV template

Keep the following in mind as you fill out your CSV template.

Allowed CSV file characters

You can use these characters in your CSV file:

àáâäǎæãåāçćčċďðḍèéêëěẽēėęğġgg͟hħḥh̤ìíîïǐĩīıįķk͟hłļľl̥ṁm̐òóôöǒœøõōřṛr̥ɽßşșśšṣs̤s̱sțťþṭt̤ʈùúûüǔũūűůŵýŷÿźžżẓz̤ÀÁ
ÄǍÆÃÅĀÇĆČĊĎÐḌÈÉÊËĚẼĒĖĘĞĠGG͟HĦḤH̤ÌÍÎÏǏĨĪIĮĶK͟HŁĻĽL̥ṀM̐ÒÓÔÖǑŒØÕŌŘṚR̥ɌSẞŚŠŞȘṢS̤S̱ȚŤÞṬT̤ƮÙÚÛÜǓŨŪŰŮŴÝŶŸŹŽŻẒZ

View Update History

Use the Update History page to view CSV file uploads in your workspace over the last 30 days.

To view the Update History page:

Validation errors

The following table lists validation errors you may run into with your profiles and traits CSV upload:

This page was last modified: 12 Mar 2024

Need support?

Questions? Problems? Need more info? Contact Segment Support for assistance!

Help improve these docs!

Was this page helpful?

Thanks for your feedback!

Can we improve this doc? 
Send us feedback!

Get started with Segment

On this page

Was this page helpful?

Thanks for your feedback!

Can we improve
 this doc? 
Send us feedback!

Product

For Developers

Company

Support

© 2025 Segment.io, Inc.

=== Content from https://segment.com/docs/unify/data-graph/setup-guides/databricks-setup// ===

Databricks Data Graph Setup
        

Unify requires a Business tier account and is included with Engage.


See the 
available plans
, or 
contact Support
.

On this page

On this page, you’ll learn how to connect your Databricks data warehouse to Segment for the 
Data Graph
.

Databricks credentials

Segment assumes that you already have a workspace that includes the datasets you’d like to use for the Data Graph. Sign in to Databricks with admin permissions to create new resources and provide the Data Graph with the necessary permissions.

Step 1: Create a new Service Principal user

Segment recommends setting up a new Service Principal user and only giving this user permissions to access the required catalogs and schemas.

If you already have a Service Principal user you’d like to use, grant it “Can use” permissions for your data warehouse and proceed to 
Step 2
.

1a) Create a new Service Principal user

1b) Add your Service Principal user to Warehouse User Lists

Step 2: Create a catalog for Segment to store checkpoint tables

Segment requires write access to this catalog for internal bookkeeping and to store checkpoint tables for the queries that are executed. Therefore, Segment recommends creating a new catalog for this purpose.
 This is also the catalog you’ll be required to specify when connecting Databricks with the Segment app.

Segment recommends creating a new database for the Data Graph.
If you choose to use an existing database that has also been used for 
Segment Reverse ETL
, you must follow the 
additional instructions
 to update user access for the Segment Reverse ETL catalog.

CREATE
 
CATALOG
 
IF
 
NOT
 
EXISTS
 
`SEGMENT_LINKED_PROFILES_DB`
;


-- Copy the saved Client ID from previously generated secret


GRANT
 
USAGE
 
ON
 
CATALOG
 
`SEGMENT_LINKED_PROFILES_DB`
 
TO
 
`${client_id}`
;


GRANT
 
CREATE
 
ON
 
CATALOG
 
`SEGMENT_LINKED_PROFILES_DB`
 
TO
 
`${client_id}`
;


GRANT
 
SELECT
 
ON
 
CATALOG
 
`SEGMENT_LINKED_PROFILES_DB`
 
TO
 
`${client_id}`
;



Step 3: Grant read-only access to the Profiles Sync catalog

Run the following SQL to grant the Data Graph read-only access to the Profiles Sync catalog:

GRANT
 
USAGE
,
 
SELECT
,
 
USE
 
SCHEMA
 
ON
 
CATALOG
 
`${profiles_sync_catalog}`
 
TO
 
`${client_id}`
;



Step 4: Grant read-only access to additional catalogs for the Data Graph

Run the following SQL to grant your Service Principal user read-only access to any additional catalogs you want to use for the Data Graph.

-- ********** REPEAT THIS COMMAND FOR EACH CATALOG YOU WANT TO USE FOR THE DATA GRAPH **********


GRANT
 
USAGE
,
 
SELECT
,
 
USE
 
SCHEMA
 
ON
 
CATALOG
 
`${catalog}`
 
TO
 
`${client_id}`
;



(Optional) Step 5: Restrict read-only access to schemas

Restrict access to specific schemas by running the following SQL:

GRANT
 
USAGE
 
ON
 
CATALOG
 
`${catalog}`
 
TO
 
`${client_id}`
;


USE
 
CATALOG
 
`${catalog}`
;


GRANT
 
USAGE
,
 
SELECT
 
ON
 
SCHEMA
 
`${schema_1}`
 
TO
 
`${client_id}`
;


GRANT
 
USAGE
,
 
SELECT
 
ON
 
SCHEMA
 
`${schema_2}`
 
TO
 
`${client_id}`
;


...



Step 6: Validate the permissions of your Service Principal user

Sign in to the 
Databricks CLI with your Client ID secret
 and run the following SQL to verify the Service Principal user has the correct permissions for a given table.

If this command succeeds, you can view the table.

USE
 
DATABASE
 
$
{
linked_read_only_database
}
 
;


SHOW
 
SCHEMAS
;


SELECT
 
*
 
FROM
 
$
{
schema
}.
$
{
table
}
 
LIMIT
 
10
;



Step 7: Connect your warehouse to Segment

To connect your warehouse to the Data Graph:

Update user access for Segment Reverse ETL catalog

If Segment Reverse ETL has ever run in the catalog you are configuring as the Segment connection catalog, a Segment-managed schema is already created and you need to provide the new Segment user access to the existing catalog. Run the following SQL if you run into an error on the Segment app indicating that the user doesn’t have sufficient privileges on an existing 
_segment_reverse_etl
 catalog.

GRANT
 
ALL
 
PRIVILEGES
 
ON
 
SCHEMA
 
$
{
segment_internal_catalog
}.
__segment_reverse_etl
 
TO
 
`${client_id}`
;



This page was last modified: 05 Dec 2024

Need support?

Questions? Problems? Need more info? Contact Segment Support for assistance!

Help improve these docs!

Was this page helpful?

Thanks for your feedback!

Can we improve this doc? 
Send us feedback!

Get started with Segment

On this page

Was this page helpful?

Thanks for your feedback!

Can we improve
 this doc? 
Send us feedback!

Product

For Developers

Company

Support

© 2025 Segment.io, Inc.

=== Content from https://segment.com/docs/guides/how-to-guides/join-user-profiles/ ===

Joining User Profiles
        

On this page

One of the first questions we get when our customers start querying all of their data is, how do I join all this data together? For example, let’s say you’d like to know if support interactions in Zendesk increase revenue in Stripe, or which percentage of users opened your email campaign and visited your website or mobile app? The key to answering these advanced questions is tying your data together across these sources. To do that, you need a common user identifier.

What is the user ID problem?

Each SaaS tool you use has its own way of identifying users with a unique primary key. And, you will find each of these different IDs across different collections of tables in your database. So, when you want to start matching Joe Smith who entered a ticket in Zendesk and also clicked through a campaign in Mailchimp, it starts to get tricky.



For example, Stripe keeps track of users with a 
customer_id
, Segment requires a
user_id
, and Marketo uses 
email
 to uniquely identify each person.

To effectively join across these sources, you need to understand how each id maps to each other. The best way to do this is to create a common identifier across tools.

Use a common identifier when possible

When you install a new tool (or use 
Segment
 to install all of them at once), you need to choose what you will put in the ID field. There are lots of different options for this: emails, twitter handles, usernames, and more.

However, we suggest using the same ID you generate from your production database when you create a new user. Database IDs never change, so they are more reliable than emails and usernames that users can switch at their leisure. If you use this same database ID across as many tools as possible, it will be easier to join identities down the road. (In MongoDB, it would look something like this 
507f191e810c19729de860ea
.)

analytics
.
identify
(
'
1e810c197e
'
,
 
{
 
// that's the user ID from the database


 
 
name
:
 
'
Jane Kim
'
,


 
 
email
:
 
'
jane.kim@example.com
'
// also includes email


 
 
});



Though we wish you could use a database ID for everything, some tools force you to identify users with an email. Therefore, you should make sure to send email along to all of your other tools, so you can join on that trait as a fallback.

For Segment Destination Users

Integrating as many tools as possible through Segment will make your joins down the road a little easier. When you use Segment to 
identify
 users, we’ll send the same ID and traits out to all the destinations you turn on in our interface. (More about 
Segment destinations
.

A few of our destination partners accept an external ID, where they will insert the same Segment user ID. Then you can join tables in one swoop. For example, Zendesk saves the Segment User ID as 
external_id
, making a Segment-Zendesk join look like this:

SELECT
 
zendesk
.
external_id
,
 
users
.
user_id


FROM
 
zendesk
.
tickets
 
zendesk


JOIN
 
segment
.
usersusers


ON
 
zendesk
.
tickets
.
external_id
 
=
 
segment
.
user_id



Here’s a look at the Segment destinations that store the Segment User ID:

How to merge identities

Whether you’re using Segment or not, we suggest creating a master user identities table that maps IDs for each of your sources.

This table will cut down on the number of joins you have to do because some IDs may only exist in one out of many tables related to a source.

Here’s sample query to create a master user identities table:

CREATE
 
TABLE
 
user_identities
 
AS
 
(


select


segment
.
id
 
as
 
segment_id
,


segment
.
email
 
as
 
email
,


zendesk
.
id
 
as
 
zendesk_id
,


stripe
.
id
 
as
 
stripe_id
,


salesforce
.
id
 
as
 
salesforce_id
,


intercom
.
id
 
as
 
intercom_id



from
 
segment
.
users
 
segment



–
 
Zendesk


leftjoin
 
zendesk
.
users
 
zendesk
 
on

 
(
 
zendesk
.
external_id
 
=
 
segment
.
id
–
 
if
 
enabled
 
through
 
Segment


or
 
zendesk
.
email
 
=
 
segment
.
email
 
)
 
–
 
fallback
 
if
 
not
 
enabled
 
through
 
Segment



–
 
Stripe


left
 
join
 
stripe
.
customers
 
stripe
 
on

 
stripe
.
email
 
=
 
segment
.
email



–
 
Salesforce


left
 
join
 
salesforce
.
leads
 
salesforce
 
on

 
salesforce
.
email
 
=
 
segment
.
email



–
 
Intercom


left
 
join
 
intercom
.
users
 
intercom
 
on

 
(
 
intercom
.
user_id
 
=
 
segment
.
id
–
 
if
 
enabled
 
through
 
Segment


or
 
intercom
.
email
 
=
 
segment
.
email
 
)
 
–
 
fallback
 
if
 
not
 
enabled
 
through
 
Segment



group
 
by
 
1
,
2
,
3
,
4
,
5
,
6



)



You’ll spit out a user table that looks something like this:

While creating this table in SQL is a good strategy, we’d be remiss not to point out a few drawbacks to this approach. First, you need to run this nightly or at some regular interval. And, if you have a large user base, it might take a while to run. That said, it’s probably still worth it.

How to run a query with your joined data

So what can you do once you have all of your ID’s mapped? Answer some pretty nifty questions that is. Here are just a few SQL examples addressing questions that incorporate more than one source of customer data.

Segment + Zendesk

-- Which referral source is sending us the most tickets?


SELECTsegment
.
referral_source
,


COUNT
(
zendesk
.
ticket_id
)
 
AS
 
count_of_tickets


FROM
 
zendesk
.
tickets
 
zendesk


LEFT
 
JOIN
 
segment
.
userssegment


ONusers
.
segment_id
 
=
 
segment
.
user_id


GROUP
 
BY
 
1


ORDER
 
BY
 
2
 
desc



Stripe + Zendesk

-- How many tickets do we receive across each pricing tier?



SELECT
 
stripe
.
plan_name
 
AS
 
plan_name
,


COUNT
(
zendesk
.
ticket_id
)
 
AS
 
count_of_tickets



-- Start with Zendesk


FROM
 
zendesk
.
tickets
 
zendesk



-- Merge Users


LEFT
 
JOIN
 
user_identities
 
users


ON
 
zendesk
.
id
 
=
 
users
.
zendesk_id



-- Add Stripe


LEFT
 
JOIN
 
stripe
.
charges
 
stripe


ON
 
users
.
stripe_id
 
=
 
stripe
.
customer_id



-- Group by plan name, from most tickets to least


GROUPBY1


ORDERBY2desc



Advanced Tips

An alternative to the lookup user table in SQL would be writing a script to grab user IDs across your third-party tools and dump them into your database.

You’d have to ping the APIs of each tool with something like an email, and ask them to return the key or id for the corresponding user in their tool.

A sample script, to run on a nightly cron job, would look something like this:

var
 
request
 
=
 
require
(
'
superagent
'
);
 
// https://www.npmjs.com/package/superagent



var
 
username
 
=
 
'
<your-username>
'
;


var
 
password
 
=
 
'
<your-password>
'
;


var
 
host
 
=
 
'
https://segment.zendesk.com/api/v2/
'
;



/**
 * Gets the user object in Zendesk by email address.
 *
 * @param {String} email
 * @param {Function} fn
 */



functiongetUserIds
(
email
,
 
fn
)
 
{


 
request


 
.
get
(
host
 
+
 
'
users/search.json?query=
'
 
+
 
email
)


 
.
auth
(
username
,
 
password
)


 
.
end
(
fn
);


}



/**
 * Get the first Zendesk user that matches 'kanye@kimye.com'
 */



getUserIds
(
'
kanye@kimye.com
'
,
 
function
(
err
,
 
res
)
 
{


if 
(
err
)
 
return
 
err
;


// res.body.users will be an Array


// res.body.users[0].id will return the `id` of the first user


});



Here is the documentation for Zendesk’s API for more information
.

This page was last modified: 21 Apr 2023

Need support?

Questions? Problems? Need more info? Contact Segment Support for assistance!

Help improve these docs!

Was this page helpful?

Thanks for your feedback!

Can we improve this doc? 
Send us feedback!

Get started with Segment

On this page

Was this page helpful?

Thanks for your feedback!

Can we improve
 this doc? 
Send us feedback!

Product

For Developers

Company

Support

© 2025 Segment.io, Inc.

=== Content from https://segment.com/docs/segment-app/iam/audit-trail/ ===

Audit Trail
        

Audit Trail is only available in Business plan workspaces.


See the 
available plans
, or 
contact Support
.

On this page

The Audit Trail allows you to view the last 90 days of user and system activity, filter activity for specific actions or actors, and export your data to an event streams source or CSV file.

For any requests exceeding the 90-day timeframe, contact 
Segment Support
 for assistance.

Viewing the Audit Trail requires Workspace Owner permissions

You must have the Workspace Owner role to view the Audit Trail page. For more information about roles and permissions within Segment, see the 
Roles documentation
.

To view the Audit Trail:

Audit Trail events

The Audit Trail returns information about the following Segment product areas:

To view a list of all events Segment surfaces in the Audit Trail, open the Audit Trail, click 
Filters
, and select the 
Events
 dropdown.

Filtering events

Use the Filters dropdown to refine your search results and filter by actions or actors to see who made changes on specific resources in the app. Actors include both logged-in users and access tokens.

Audit forwarding

You can forward events in your workspace to an 
event streams source
 to set up real-time alerts and quickly revert changes (like a user unintentionally disabling a warehouse) that could cause unwanted downstream effects.

Segment recommends creating a dedicated source for Audit Trail events

Segment recommends forwarding all events to an instance of the 
HTTP API
 source.  Segment passes all forwarded events through its entire processing pipeline. This ensures that Tracking Plans, Filters, and other features work with the audit events, and also ensures you can send those events to multiple downstream destinations.

To forward Audit Trail events to an event streams source:

When you forward audit events to a source, Segment passes those events through its entire processing pipeline. This ensures that tracking plans, filters, and other features work with the audit events, and also ensures you can send those events to multiple downstream destinations.

Frequently asked questions

Engage

Why am I getting alerts about an audience/computed trait sync failure, but when I look at the specific audience/computed trait it shows a successful sync?

An audience/computed trait Run or a Sync may fail on its first attempt, but Engage will retry up to 5 times before considering it a hard failure and display on that audience/compute trait’s Overview page. As long as the runs/syncs within the specific Audience’s Overview page say they are successful, then these can be safely ignored.

How things work internally:

Segment Engage scheduler fetches audiences/traits from compute service and then handles the logic of generating tasks. These compute/sync tasks get scheduled and executed by another worker. Essentially, these tasks are a list of steps to be executed. Each task has a series of steps that are marked as complete by saving a timestamp for the completion. If the worker is disrupted, it picks up at the latest step, which has no completed_at timestamp. In some cases, the step may fail or the entire task may fail (for example, due to timeout or the worker disruption as there are many moving parts). In either case, these failures will be retried.

These tasks are a part of internal Segment process, and there are systems in place to retry failed tasks. In most cases, it is not necessary to track these failures, as long as there are no actual computation or sync failures.

The Audit Trail logic, however, is configured to notify you about every task failure, even if it then later succeeds.

If you would like to avoid receiving the notifications for transient failures, 
reach out to support
 to request enabling a setting to reduce the number of notifications your workspace receives.

This page was last modified: 06 Aug 2024

Need support?

Questions? Problems? Need more info? Contact Segment Support for assistance!

Help improve these docs!

Was this page helpful?

Thanks for your feedback!

Can we improve this doc? 
Send us feedback!

Get started with Segment

On this page

Was this page helpful?

Thanks for your feedback!

Can we improve
 this doc? 
Send us feedback!

Product

For Developers

Company

Support

© 2025 Segment.io, Inc.

=== Content from https://segment.com/docs/engage/product-limits/ ===

Engage Default Limits
        

Engage Foundations requires a Business tier account and includes Unify.


See the 
available plans
, or 
contact Support
.

On this page

To provide consistent performance and reliability at scale, Segment enforces default use and rate limits within Engage. Most customers do not exceed these limits.

To learn more about custom limits and upgrades, contact your dedicated Customer Success Manager or 
friends@segment.com
.

Beginning August 18, 2023, Segment has 
updated product limits
 that apply to new Engage and Unify users.

Default limits

Audiences and Computed Traits

SQL Traits

Journeys

Channels

This page was last modified: 31 Oct 2024

Need support?

Questions? Problems? Need more info? Contact Segment Support for assistance!

Help improve these docs!

Was this page helpful?

Thanks for your feedback!

Can we improve this doc? 
Send us feedback!

Get started with Segment

On this page

Was this page helpful?

Thanks for your feedback!

Can we improve
 this doc? 
Send us feedback!

Product

For Developers

Company

Support

© 2025 Segment.io, Inc.

=== Content from https://segment.com/docs/connections/storage/warehouses/redshift-faq/ ===

Redshift cluster and Redshift connector limitations
        

On this page

“Are there limitations of Redshift clusters and our Redshift connector?”

While Redshift clusters are incredibly scalable and efficient, limitations are imposed to ensure that clusters maintain performance.

Reserved words

Redshift does not allow you to create tables or columns using reserved words. To avoid naming convention issues, we prepend a 
_
 to any reserved word names. If you’re having trouble finding a column or table, you can check the list of 
Redshift reserved words
 or search for the table with a prepended underscore like 
_open
.

Table count limitations

Redshift sets the maximum number of tables you can create in a cluster to 9,900 including temporary tables. While it’s rare to reach that limit, we recommend keeping an eye on the number of tables our warehouse connector is creating in your cluster. Keep in mind that a new table is created for each unique event you send to Segment, which becomes an issue if events are being dynamically generated.

Cluster node limitations

When setting up your Redshift cluster, you can select between dense storage (ds2) and dense compute (dc1) cluster types. Dense compute nodes are SSD based which allocates only 200GB per node, but results in faster queries. Dense storage nodes are hard disk based which allocates 2TB of space per node, but result in slower queries. When scaling up your cluster by adding nodes, it’s important to remember that adding more nodes will not add space linearly. As you add more dc1 nodes, the amount of preallocated space for each table increases. For example, if you have a table with 10 columns, Redshift will preallocate 20mb of space (10 columns X 2 slices) per node. That means that the same table will preallocate 20mb of space in a single ds2 cluster, and 200mb in a 10 node dc1 cluster.

Column type changes

Like with most data warehouses, column data types (string, integer, float, etc.) must be defined at the time the column is created. Unlike most data warehouses, Redshift does not allow for easy column type changes after the column has been created. Additionally, we store a record of what the tables and column types should be set to in a local database, and validate the structure on each connector run. Currently, column type changes (i.e. change an integer column to float) are only available to our business tier customers on an ad-hoc basis.

VARCHAR size limits

All Segment-managed schemas have a default VARCHAR size of 512 in order to keep performance high. If you wish to increase the VARCHAR size, you can run the following query.

  
ALTER
 
TABLE
 
table_name
 
ALTER
 
COLUMN
 
column_name
 
column_type
;



Example:

  
ALTER
 
TABLE
 
segment_prod
.
identifies
 
ALTER
 
COLUMN
 
account_id
 
TYPE
 
VARCHAR
(
1024
);



Increasing the default size can impact query performance as it needs to process more data to accomodate the increased column size. See 
Amazon’s Redshift Documentation
 for more details.

Blocklisted track call properties

While almost all event properties are valid, we are unable to pass through properties that have naming conflicts with the default key/value pairs included in a standard raw JSON call. For example, if you send through a property in a track call named “timestamp” or “event”, it will cause a conflict and you likely wont see it appear in your warehouse. To be more specific, if you send the following track call, {‘event’:’birthday’} will likely be dropped when syncing the data to your data warehouse.

analytics.track('selected gift', {'event':'birthday', 'type':'cake'})

This page was last modified: 11 Mar 2021

Need support?

Questions? Problems? Need more info? Contact Segment Support for assistance!

Help improve these docs!

Was this page helpful?

Thanks for your feedback!

Can we improve this doc? 
Send us feedback!

Get started with Segment

On this page

Was this page helpful?

Thanks for your feedback!

Can we improve
 this doc? 
Send us feedback!

Product

For Developers

Company

Support

© 2025 Segment.io, Inc.

=== Content from https://segment.com/docs/connections/integration_error_codes/ ===

Integration Error Codes
        

This page was last modified: 06 Jul 2022

Need support?

Questions? Problems? Need more info? Contact Segment Support for assistance!

Help improve these docs!

Was this page helpful?

Thanks for your feedback!

Can we improve this doc? 
Send us feedback!

Get started with Segment

Was this page helpful?

Thanks for your feedback!

Can we improve
 this doc? 
Send us feedback!

Product

For Developers

Company

Support

© 2025 Segment.io, Inc.

=== Content from https://segment.com/docs/engage/content/email/editor/ ===

Drag and Drop Editor
        

Engage Premier requires a Business tier account and includes Engage Foundations and Unify.


See the 
available plans
, or 
contact Support
.

On this page

Engage Premier entered an End of Sale (EOS) period effective  June 10, 2024. Existing Segment customers will continue to have access and support to Engage Premier until an end-of-life (EOL) date is announced. We recommend exploring the following pages in preparation of a migration or future MCM needs:

Twilio Marketing Campaigns

Preferred ISV Partners:

Airship Blog
 


Bloomreach Blog
 


Braze Blog
 


Insider Blog
 


Klaviyo Blog
 


Twilio Engage Foundations Documentation
 

Use Twilio Engage to build email templates with a 
what you see is what you get
 (WYSIWYG) Drag and Drop Editor. Use drag and drop tools to design the template layout and include user profile traits to personalize the message for each recipient.

You can navigate to the Drag and Drop Editor from the 
Select Editor
 screen:

From the 
Select Editor
 screen, select 
Drag and Drop Editor
 and click 
Build Email
.

The Drag and Drop Editor consists of a 
left sidebar
 with design modules and an 
email canvas
.

Left sidebar

The left sidebar contains the following tools:

Click and drag the tools you want to use from the left sidebar in the email canvas.

Email canvas

Use the canvas to organize and preview the email template for both desktop and mobile. Drag and drop content modules from the sidebar into the canvas and arrange the layout as desired.

Select content in the canvas and return to the left sidebar to set properties for the selected content.

Design for desktop or mobile

Navigate between desktop or mobile to design the email template for both formats. Toggle between desktop or mobile in the left sidebar or use the buttons in the bottom right corner of the email canvas.

Mobile view doesn’t contain all design options that are available for desktop. For example, upload image capabilities are only available when you edit for desktop. However, content that you add for desktop will also display for mobile devices by default. See 
content modules
 for more on content that’s only available to edit for desktop.

Responsive design

Use responsive design settings in the sidebar to hide specific content for either desktop or mobile.

Add content modules

Select from the following content modules:

Add blank columns or predefined content blocks

Use the Blocks tool to add both blank columns and pre-existing content blocks to the email. Drag empty column blocks in the canvas to organize the layout, then drag and drop 
content tools
 inside the column blocks.

Predefined content blocks allow you to add content such as:

Email body attributes

Use the body tool to apply general style and link attributes to the entire email canvas.

Email body attributes include:

To modify style attributes for specific content in the email, select a content block in the canvas and edit attributes in the left sidebar.

Add an image

Use the images tool to add images to your email. Scroll through available images in the left sidebar or use the search tool.

Select and drag and image into the canvas, then return to the sidebar to set image properties:

Upload an image

Use the Uploads tool to upload an image for the email template. Click 
Upload Image
 to select an image stored locally or drag and drop images in the sidebar dropzone.

The maximum image file size you can upload is 10 MB.

Link actions

Use the 
Action Type
 drop down menu in the sidebar to select the action that occurs when a recipient clicks on the link, button, or image in the email template.

Select from the following link actions:

Add unsubscribe links

It’s always best practice to include an unsubscribe link in the emails you build. Engage adds an unsubscribe link to templates, which you can edit at any time. For more on email unsubscribe links, view SendGrid’s 
best practices
.

Add an unsubscribe link as a button:

Add an unsubscribe link to text:

You can alternatively add a 
predefined unsubscribe
 link content block.

Add a manage preference link

Engage also adds a manage preference link to templates. The manage preference link lets your customers opt in and out of email groups on an individual basis instead of unsubscribing from all your campaigns. For more information, see 
subscription groups
.

Personalize with merge tags

Add merge tags in the Drag and Drop Editor to personalize your message with user profile traits.

Engage supports liquid templating to create dynamic content in the email design editor and the SMS editor.

For example, use  
{% if %}
, 
{% elseif %}
, and 
{% else %}
 tags to call a product by name if known, or use a default message:

{% if profile.traits.product_title == "Sneakers" %}
  Hey, view our latest sneakers!
{% elsif profile.traits.product_title == "Sandals" %}
  Hey, check out these sandals!
{% else %}
  Hey, check out our latest footwear.
{% endif %}


To view more examples related to your use case, visit the 
LiquidJS docs
.

Content validation

For all content editors in Engage, you’ll see alerts for any issues in your template, such as invalid profile traits or incorrect 
liquid syntax
. Engage both flags template issue(s), and displays recommended next steps. While you can save these templates, you must fix any issues before using them in Engage campaigns.

Save the template

After you design the email, click 
Create Email Template
.

Next steps

Learn more about 
building email templates
 to include in your Engage campaigns.

You can learn about the 
HTML Editor
 for both code and visual editing capabilities from a single view.

once you create an email with the Drag and Drop Editor, you can’t modify it with the HTML Editor, and vice versa.

This page was last modified: 15 Jul 2024

Need support?

Questions? Problems? Need more info? Contact Segment Support for assistance!

Help improve these docs!

Was this page helpful?

Thanks for your feedback!

Can we improve this doc? 
Send us feedback!

Get started with Segment

On this page

Was this page helpful?

Thanks for your feedback!

Can we improve
 this doc? 
Send us feedback!

Product

For Developers

Company

Support

© 2025 Segment.io, Inc.

=== Content from https://segment.com/docs/engage/using-engage-data/ ===

Using Engage Data
        

Engage Foundations requires a Business tier account and includes Unify.


See the 
available plans
, or 
contact Support
.

On this page

You can send your Computed Traits and Audiences to your Segment Destinations, which allows you to personalize messages across channels, optimize ad spend, and improve targeting. This page provides an overview of different ways to activate Engage data in Segment Destinations.

You can also use the 
Profile API
 to activate Engage data programmatically.

Engage Destination types: Event vs. List

There are two ways to send data to Engage Destinations: as 
Events
 and as 
Lists
.

Event Destinations
 receive data one by one, on a streaming basis as 
events
, which are behaviors or traits tied to a user and a point in time. Every time a piece of data (such as a track event or identify call) is received in Segment — for example, from your website or your mobile app — Segment then sends this piece of data to the Destination right away.

List Destinations
 periodically receive data in batches, and these batches contain lists of users. In most cases, Segment sends data to a list destination every hour, and sends all data accumulated since the last batch was sent.

Some Destinations, such as Salesforce Marketing Cloud have both “event” and “list” destination types that you can use.

Engage sends computed traits and audiences to destinations in different ways depending on whether the destination is an Event or List type
:

Computed Traits
 are always sent to Event destinations either using an identify call for user traits, a group call for account-level computed traits, or a track event.

With 
Audiences
, Engage sends the audience either as a boolean (true or false) 
user property
 to Event Destinations, or as a 
user list
 to List Destinations. If you are a B2B company creating account audiences (where each account represents a group of users, like employees at a business) and sending them to list destinations, Engage sends the list of all users within an account that satisfies the audience criteria.

Event Destinations

Event Destinations and Computed traits

Computed traits can only be sent to Event destinations.
When Engage sends a computed trait to an Event destination, it uses an identify call to send user traits, or a group call to send account-level computed traits.

Event Destinations and Audiences

Segment sends an identify or track call for every user in the audience when the audience is first created. Later syncs only send updates for those users who were added or removed from the audience since the last sync.

Most destinations require that you configure a column in your schema to receive the audience data, however, some destinations (like Braze and Iterable) allow you to send audiences without doing this. This depends on the individual destination, so consult the destination’s documentation for details.

List Destinations

List destinations can only receive Audiences, and cannot receive computed traits.

When syncing to a list destination Engage uploads lists of users directly to the destination. When you first create an audience, Segment uploads the entire list of audience users to the destination. Later syncs only upload the users that have been added or removed since the last sync.

User-list destinations can have individual limits on how often Segment can sync with them. For example, an AdWords audience is updated once every six hours or more, because that’s what AdWords recommends.

What do the payloads look like for Engage data?

The payloads sent from your Engage space to your destinations will be different depending on if you configured the destination to receive identify or track calls, and whether the payload is coming from a computed trait or audience. As a reminder, identify calls usually update a trait on a user profile or table, whereas track calls send a point-in-time event that can be used as a campaign trigger or a detailed record of when a user’s audience membership or computed trait value was calculated.

To view the events generated by an Engage Space’s audience or computed traits, navigate to 
Unify settings > Debugger
 and view the list of sources that are configured to generate events per 
each destination instance
. Each source will only generate events to connected destinations. From the source’s Debugger tab, you’ll find the most recent events generated by that source per the connected destinations’ audiences and computed traits.

In the full json body of an audience, computed trait, or journey, you’ll find specific details under the 
context.personas
 object. These fields can be useful when building out 
Destination Filters
, 
Actions destination mappings
, and 
Functions
.

The integrations object in the payload displays as 
{"All" : false,}
 and only lists some destinations. This is due to the fact that each source has multiple destinations connected while each audience/trait may only have a subset of destinations connected to it. See 
Filtering with the Integrations Object
 for more information. The integrations object routing specific events to its specified destinations is also why a destination’s 
Delivery Overview
 tab will show a large number of events under the 
Filtered at destination
 box, as that destination will only receive the events intended to be sent to it by audiences, traits, or journeys that are connected to that specific destination.

Computed Trait generated events

Identify
 events generated by a Computed Trait have the trait name set to the Computed Trait value:

{

  
"
context
"
:
 
{

    
"
personas
"
:
 
{

      
"
computation_class
"
:
 
"
trait
"
,
 
// the type of computation

      
"
computation_id
"
:
 
"
tra_###
"
,
 
// the trait's id found in the URL

      
"
computation_key
"
:
 
"
aud_###
"
,
 
// the configured trait key that appears on user profile

      
"
namespace
"
:
 
"
spa_###
"
,
 
// the Engage Space's ID

      
"
space_id
"
:
 
"
spa_###
"
 
// the Engage Space's ID

    
}

  
},

  
"
type
"
:
 
"
identify
"
,

  
"
userId
"
:
 
"
u123
"
,

  
"
traits
"
:
 
{

     
"
total_revenue_180_days
"
:
 
450.00

  
}


}



Track
 events generated by a Computed Trait have a key for the trait name, and a key for the Computed Trait value. The default event name is 
Trait Computed
, but you can change it.

{

  
"
context
"
:
 
{

    
"
personas
"
:
 
{

      
"
computation_class
"
:
 
"
trait
"
,
 
// the type of computation

      
"
computation_id
"
:
 
"
tra_###
"
,
 
// the trait's id found in the URL

      
"
computation_key
"
:
 
"
aud_###
"
,
 
// the configured trait key that appears on user profile

      
"
namespace
"
:
 
"
spa_###
"
,
 
// the Engage Space's ID

      
"
space_id
"
:
 
"
spa_###
"
 
// the Engage Space's ID

    
}

  
},

  
"
type
"
:
 
"
track
"
,

  
"
event
"
:
 
"
Trait Computed
"
,

  
"
userId
"
:
 
"
u123
"
,

  
"
properties
"
:
 
{

     
"
trait_key
"
:
 
"
total_revenue_180_days
"
,

     
"
total_revenue_180_days
"
:
 
450.00

  
}


}



Engage only sends events to the destination if the Computed Trait value has changed for the user. Engage doesn’t send a payload for every user in your trait every time the trait computes.

Audience generated events

Identify
 events generated by an Audience have the Audience key set to 
true
 or 
false
 based on whether the user is entering or exiting the audience:

{

  
"
context
"
:
 
{

    
"
personas
"
:
 
{

      
"
computation_class
"
:
 
"
audience
"
,
 
// the type of computation

      
"
computation_id
"
:
 
"
aud_###
"
,
 
// the audience's id found in the URL

      
"
computation_key
"
:
 
"
aud_###
"
,
 
// the configured audience key that appears on user profile

      
"
namespace
"
:
 
"
spa_###
"
,
 
// the Engage Space's ID

      
"
space_id
"
:
 
"
spa_###
"
 
// the Engage Space's ID

    
}

  
},

  
"
type
"
:
 
"
identify
"
,

  
"
userId
"
:
 
"
u123
"
,

  
"
traits
"
:
 
{

     
"
first_time_shopper
"
:
 
true
 
// false when a user exits the audience

  
}


}



Track
 events generated by an Audience have a key for the Audience name, and for the Audience value:

{

  
"
context
"
:
 
{

    
"
personas
"
:
 
{

      
"
computation_class
"
:
 
"
audience
"
,
 
// the type of computation

      
"
computation_id
"
:
 
"
aud_###
"
,
 
// the audience's id found in the URL

      
"
computation_key
"
:
 
"
aud_###
"
,
 
// the configured audience key that appears on user profile

      
"
namespace
"
:
 
"
spa_###
"
,
 
// the Engage Space's ID

      
"
space_id
"
:
 
"
spa_###
"
 
// the Engage Space's ID

    
}

  
},

  
"
type
"
:
 
"
track
"
,

  
"
userId
"
:
 
"
u123
"
,

  
"
event
"
:
 
"
Audience Entered
"
,
 
// "Audience Exited" when a user exits an audience

  
"
properties
"
:
 
{

     
"
audience_key
"
:
 
"
first_time_shopper
"
,

     
"
first_time_shopper
"
:
 
true
 
// false when a user exits the audience

  
}


}



Journeys generated events

The data type you send to a destination depends on whether the destination is an event destination or a list destination. For more information, read the 
Journeys documentation
 on how Journeys Identity and Track event payloads get formatted when sending to Event destinations.

See 
this doc
 for more information on Journeys events.


Track
 events generated by a journey have a key for the journey name “audience_key”, and a key for the journey value:

{

  
"
context
"
:
 
{

    
"
personas
"
:
 
{

      
"
computation_class
"
:
 
"
audience
"
,
 
// the type of computation

      
"
computation_id
"
:
 
"
aud_###
"
,
 
// the audience's id found in the URL

      
"
computation_key
"
:
 
"
j_o_###
"
,
 
// the configured journey key that appears on user profile

      
"
namespace
"
:
 
"
spa_###
"
,
 
// the Engage Space's ID

      
"
space_id
"
:
 
"
spa_###
"
 
// the Engage Space's ID

    
}

  
},

  
"
type
"
:
 
"
track
"
,

  
"
userId
"
:
 
"
u123
"
,

  
"
event
"
:
 
"
Audience Entered
"
,
 
// "Audience Exited" when a user exits an audience

  
"
properties
"
:
 
{

     
"
audience_key
"
:
 
"
j_o_###
"
,

     
"
recent_buyer
"
:
 
true
 
// false when a user exits the journey

  
}


}



Identify
 events generated by a Journey have the Journey key set to 
true
 or 
false
 based on whether the user is entering or exiting the Journey:

{

  
"
context
"
:
 
{

    
"
personas
"
:
 
{

      
"
computation_class
"
:
 
"
audience
"
,
 
// the type of computation

      
"
computation_id
"
:
 
"
aud_###
"
,
 
// the audience's id found in the URL

      
"
computation_key
"
:
 
"
j_o_###
"
,
 
// the configured journey key that appears on user profile

      
"
namespace
"
:
 
"
spa_###
"
,
 
// the Engage Space's ID

      
"
space_id
"
:
 
"
spa_###
"
 
// the Engage Space's ID

    
}

  
},

  
"
type
"
:
 
"
identify
"
,

  
"
userId
"
:
 
"
u123
"
,

  
"
traits
"
:
 
{

     
"
recent_buyer
"
:
 
true
 
// false when a user exits the journey

  
}


}



Additional identifiers

Engage has a flexible identity resolution layer that allows you to build user profiles based on multiple identifiers like 
user_id
, 
email
, or 
mobile advertisingId
. However, different destinations may require different keys, so they can do their own matching and identification. For example, Zendesk requires that you include the 
name
 property.
Engage includes logic to automatically enrich payloads going to these destinations with the required keys.

If you send events to a destination that requires specific enrichment Segment doesn’t already include, you can use 
ID Sync
 or 
Trait Enrichment
 to send additional data points to the destination.

Profiles with multiple identifiers (for example, 
user_id
 and 
email
) will trigger one API call per identifier when the audience or computed trait is first synced to a destination.

Email as an identifier is set in 
traits.email
 for Identify calls, and 
context.traits.email
 for Track calls.

Multiple identifiers of the same type

You might also see that profiles that have multiple values for the same 
external_id
 type, for example a profile might have multiple email addresses. When this happens, Engage sends one event per email for each audience or computed trait event. This ensures that all downstream email-based profiles receive the complete audience or computed trait.

In some situations, this behavior might cause an unexpected volume of API calls. You can use 
ID Sync
 to establish a strategy and control the number of events sent.

New external identifiers added to a profile

There are two situations when Engage sends an audience or computed trait to a destination.

The first is when the value of the trait or audience changes.

The second, less common case is that Engage re-syncs an audience or computed trait when a new 
external_id
 is added to a profile. For example, an ecommerce company has an anonymous visitor with a computed trait called 
last_viewed_category = 'Shoes'
. That visitor then creates an account and an email address is added to that profile, even though the computed trait value hasn’t changed. When that email address is added to the profile, Engage re-syncs the computed trait that includes an email to downstream tools. This allows the ecommerce company to start personalizing the user’s experience from a more complete profile.

For more granular control that lets you specify which external IDs Segment sends to a destination, see the 
ID Sync documentation
.

Rate limits on Engage Event Destinations

Many Destinations have strict rate limits that prevent Segment (and other partners) from sending too much data to a Destination at one time. Engage caps the number of requests per second to certain Destinations to avoid triggering rate limits that would cause data to be dropped. The most common scenario when customers run into rate-limits is when Engage first tries to sync a large set of historical users. Once this initial sync is done, we rarely run into rate-limit issues.

For additional information on Destination-specific rate limits, check the documentation for that Destination. If you need a higher rate limit, 
let Segment know
 which Destination you need it for and why.

Syncing data to a new Destination for the first time

When you create a new Computed Trait or Audience in Engage, you can choose to calculate it either using all the available historical data from your Segment implementation, or only using data that arrives after you set up the trait or audience. By default, Segment opts to include historical data. Afterwards, Segment only sends updates to that destination.

Why would I disable historical data?
 You might want to disable historical data if you’re sending a triggered campaign. For example, if you want to send an email confirming a purchase, you 
probably
 don’t want to email users who bought something months ago, but you 
do
 want to target current users as they make purchases (and thus enter the audience).

The Facebook Custom Audiences Website destination does not accept historical data, and so only uses data from after the moment you configure it.

Use the 
Engage settings
 to add a destination to your Engage space.

Engage compatible Destinations: Event type

Connect any Cloud-mode destination that supports Identify or Track calls to Engage as an event type destination.

Engage compatible Destinations: List type

This page was last modified: 14 Nov 2024

Need support?

Questions? Problems? Need more info? Contact Segment Support for assistance!

Help improve these docs!

Was this page helpful?

Thanks for your feedback!

Can we improve this doc? 
Send us feedback!

Get started with Segment

On this page

Was this page helpful?

Thanks for your feedback!

Can we improve
 this doc? 
Send us feedback!

Product

For Developers

Company

Support

© 2025 Segment.io, Inc.

=== Content from https://segment.com/docs/unify/product-limits/ ===

Unify Limits
        

Unify Plus requires a business tier account and is included with Engage


See the 
available plans
, or 
contact Support
.

On this page

Beginning November 6, 2024, new Unify Plus and Engage users can refer to this page for Segment’s product limits. Existing users prior to this date can continue to refer to the Engage product limits in the 
Engage Default Limits
 documentation.

To provide consistent performance and reliability at scale, Segment enforces default use and rate limits within Unify. Most customers do not exceed these limits.

To learn more about custom limits and upgrades, contact your dedicated Customer Success Manager or 
friends@segment.com
.

Unify Plus limits

Unify Plus customers receive the following based on their signup date:

Unify Plus limits vary based on your Engage plan:

Visit Segment’s 
pricing page
 to learn more.

Default limits

Audiences and Computed Traits

SQL Traits

Profile API

These limits are set per each Unify/Engage Space.

Identity

Unify ingestion limitations

Unify will silently drop events if:

This page was last modified: 05 Nov 2024

Need support?

Questions? Problems? Need more info? Contact Segment Support for assistance!

Help improve these docs!

Was this page helpful?

Thanks for your feedback!

Can we improve this doc? 
Send us feedback!

Get started with Segment

On this page

Was this page helpful?

Thanks for your feedback!

Can we improve
 this doc? 
Send us feedback!

Product

For Developers

Company

Support

© 2025 Segment.io, Inc.

=== Content from https://segment.com/docs/connections/spec/ ===

Spec Overview
        

The Segment Spec provides guidance on meaningful data to capture, and the best format for it, across all of Segment’s libraries and APIs. If you implement Segment using these formats, it’s simple to translate your data to downstream tools.

Check out our high-level overview of these APIs in Segment University. (Must be logged in to access.)

Event and Product Limits

Events ingested by Segment are subject to defined 
Product Limits
.

The Segment Spec has three components.

First, it 
outlines the semantic definition of the customer data Segment captures across all libraries and APIs
.  There are six API calls in the Spec. They each represent a distinct type of semantic information about a customer. Every call shares the same 
common fields
.

Second, it 
details the event data Segment captures across some cloud sources and destinations
.

Third, it 
shares the events Segment recommends you track for a particular industry based on experience working with thousands of customers
. When you respect these specs, Segment maps these events to particular features within end destinations like Google Analytics and Facebook Ads.

This page was last modified: 18 Apr 2024

Need support?

Questions? Problems? Need more info? Contact Segment Support for assistance!

Help improve these docs!

Was this page helpful?

Thanks for your feedback!

Can we improve this doc? 
Send us feedback!

Get started with Segment

Was this page helpful?

Thanks for your feedback!

Can we improve
 this doc? 
Send us feedback!

Product

For Developers

Company

Support

© 2025 Segment.io, Inc.

=== Content from https://segment.com/docs/connections/storage/warehouses/schema/ ===

Warehouse Schemas
        

On this page

A 
schema
 describes the way that the data in a warehouse is organized. Segment stores data in relational schemas, which organize data into the following template:

<source>.<collection>.<property>
, for example 
segment_engineering.tracks.user_id
, where source refers to the source or project name (segment_engineering), collection refers to the event (tracks), and the property refers to the data being collected (user_id). All schemas convert collection and property names from 
CamelCase
 to 
snake_case
 using the 
go-snakecase
 package.

Warehouse column creation

Segment creates tables for each of your custom events in your warehouse, with columns for each event’s custom properties. Segment does not allow unbounded 
event
 or 
property
 spaces in your data. Instead of recording events like “Ordered Product 15”, use a single property of “Product Number” or similar.

Segment creates and populates a column only when it receives a non-null value from the source.

How warehouse tables handle nested objects and arrays

Segment’s libraries pass nested objects and arrays into tracking calls as 
properties
, 
traits
, and 
tracking calls
. To preserve the quality of your events data, Segment uses the following methods to store properties and traits in database tables:

context:
 
{

  
app:
 
{

    
version:
 
"1.0.0"

  
}


}



traits:
 
{

  
address:
 
{

    
street:
 
"6th Street"

  
}


}



properties:
 
{

  
product_id:
 
{

    
sku:
 
"G-32"

  
}


}



products:
 
{

  
product_id:
 
[

    
"507f1"
,
 
"505bd"

  
]


}



Warehouse tables

The table below describes the schema in Segment Warehouses:

Identifies table

The 
identifies
 table stores the 
.identify()
 method calls. Query it to find out user-level information. It has the following columns:

Querying the Identifies table

To see a list of the columns in the 
identifies
 table for your 
<source>
, run the following:

SELECT
 
column_name
 
AS
 
Columns


FROM
 
columns


WHERE
 
schema_name
 
=
 
'<source>'


AND
 
table_name
 
=
 
'identifies'


ORDER
 
by
 
column_name



The 
identifies
 table is where you can query information about your users and their traits. For example, this query returns unique users you’ve seen on your site each day:

SELECT
 
DATE
(
sent_at
)
 
AS
 
Day
,
 
COUNT
(
DISTINCT
(
user_id
))
 
AS
 
Users


FROM
 
<
source
>
.
identifies


GROUP
 
BY
 
day


ORDER
 
BY
 
day



Groups table

The  
groups
 table stores the 
group
 method calls. Query it to find out group-level information. It has the following columns:

Querying the Groups table

To see a list of the columns in the 
groups
 table for your 
<source>
, run the following:

SELECT
 
column_name
 
AS
 
Columns


FROM
 
columns


WHERE
 
schema_name
 
=
 
'<source>'


AND
 
table_name
 
=
 
'groups'


ORDER
 
by
 
column_name



To see a list of the groups using your product, run the following:

SELECT
 
name
 
AS
 
Company


FROM
 
<
source
>
.
groups


GROUP
 
BY
 
name



Pages and Screens tables

The 
pages
 and 
screens
 tables store the 
page
 and 
screen
 method calls. Query it to find out information about page views or screen views. It has the following columns:

Querying the Pages and Screens tables

To see a list of the columns in the 
pages
 table for your 
<source>
, run the following:

SELECT
 
column_name
 
AS
 
Columns


FROM
 
columns


WHERE
 
schema_name
 
=
 
'<source>'


AND
 
table_name
 
=
 
'pages'


ORDER
 
by
 
column_name



The pages table can give you interesting information about page views that happen on your site. The following query, for example, shows page views grouped by day:

SELECT
 
DATE
(
sent_at
)
 
AS
 
Day
,
 
COUNT
(
*
)
 
AS
 
Views


FROM
 
<
source
>
.
pages


GROUP
 
BY
 
day


ORDER
 
BY
 
day



Tracks table

The 
tracks
 table stores the 
track
 method calls. Query it to find out information about the events your users have triggered. It has the following columns:

Querying the Tracks table

Your 
tracks
 table is a rollup of the different event-specific tables, for quick querying of just a single type. For example, you could see the number of unique users signed up each day:

SELECT
 
DATE
(
sent_at
)
 
AS
 
Day
,
 
COUNT
(
DISTINCT
(
user_id
))
 
AS
 
Users


FROM
 
segment
.
tracks


WHERE
 
event
 
=
 
'signed_up'


GROUP
 
BY
 
day


ORDER
 
BY
 
day



Event Tables

Your event tables are a series of table for each custom event you record to Segment. We break them out into their own tables because the properties, and, as a result, the columns, differ for each event. Query these tables to find out information about specific properties of your custom events. They have the following columns:

Querying the Events tables

To see a list of the event tables for a given 
<source>
, run the following:

SELECT
 
schema
 
as
 
source
,
 
"table"
 
as
 
Event


FROM
 
disk


WHERE
 
schema
 
=
 
'<source>'

  
AND
 
"table"
 
!=
 
'aliases'

  
AND
 
"table"
 
!=
 
'groups'

  
AND
 
"table"
 
!=
 
'identifies'

  
AND
 
"table"
 
!=
 
'pages'

  
AND
 
"table"
 
!=
 
'screens'

  
AND
 
"table"
 
!=
 
'tracks'


ORDER
 
BY
 
"table"



To see a list of the columns in one of your event tables, run the following:

SELECT
 
column_name
 
AS
 
Columns


FROM
 
columns


WHERE
 
schema_name
 
=
 
'<source>'


AND
 
table_name
 
=
 
'<event>'


ORDER
 
by
 
column_name



Tracks vs. Events Tables

To see the tables for your organization, you can run this query:

SELECT
 
schema
 
||
 
'.'
 
||
 
"table"
 
AS
 
table
,
 
rows


FROM
 
disk


ORDER
 
BY
 
1



The 
source.event
 tables have the same columns as the 
source.track
 tables, but they also include columns specific to the properties of each event.

If you’re recording an event like:

analytics
.
track
(
'
Register
'
,
 
{

  
plan
:
 
'
Pro Annual
'
,

  
accountType
:
 
'
Facebook
'


});



Then you can expect to see columns named 
plan
 and 
account_type
 as well as the default 
event
, 
id
, and so on. That way, you can write queries against any of the custom data sent in track calls.

Note

Because Segment adds 
properties
 and 
traits
 as un-prefixed columns to your tables, there is a chance the names can collide with the reserved column names. For this reason, Segment discards properties with the same name as the reserved column name (for example, 
user_id
).

Your event tables are one of the more powerful datasets in Segment SQL. They allow you to see which actions users perform when interacting with your product.

Because every source has different events, what you can do with them will vary. Here’s an example where you can see the number of “Enterprise” users signed up for each day:

SELECT
 
DATE
(
sent_at
)
 
AS
 
Day
,
 
COUNT
(
DISTINCT
(
user_id
))
 
AS
 
Users


FROM
 
<
source
>
.
signed_up


WHERE
 
account_type
 
=
 
'Enterprise'


GROUP
 
BY
 
day


ORDER
 
BY
 
day



Here’s an example that queries the daily revenue for an ecommerce store:

SELECT
 
DATE
(
sent_at
)
 
AS
 
Day
,
 
SUM
(
total
)
 
AS
 
Revenue


FROM
 
<
source
>
.
completed_order


GROUP
 
BY
 
day


ORDER
 
BY
 
day



Schema Evolution and Compatibility

New Columns

New event properties and traits create columns. Segment processes the incoming data in batches, based on either data size or an interval of time. If the table doesn’t exist we lock and create the table. If the table exists but new columns need to be created, we perform a diff and alter the table to append new columns.

When Segment process a new batch and discover a new column to add, we take the most recent occurrence of a column and choose its datatype.

Data Types

The data types that Segment currently supports include:

Data types are set up in your warehouse based on the first value that comes in from a source. For example, if the first value that came in from a source was a string, Segment would set the data type in the warehouse to 
string
.

In cases where a data type is determined incorrectly, the support team can help you update the data type. As an example, if a field can include float values as well as integers, but the first value we received was an integer, we will set the data type of the field to integer, resulting in a loss of precision.

To update the data type, reach out to the Segment support team. They will update the internal schema that Segment uses to infer your warehouse schema. Once the change is made, Segment will start syncing the data with the correct data type. However, if you want to backfill the historical data , you must drop the impacted tables on your end so that Segment can recreate them and backfill those tables.

To request data types changes, please reach out to 
Segment Support
 for assistance, and provide with these details for the affected columns in the following format:

<schema_name>.<table_name>.<column_name>.<current_datatype>.<new_datatype>

Column Sizing

After analyzing the data from dozens of customers, we set the string column length limit at 512 characters. Longer strings are truncated. We found this was the sweet spot for good performance and ignoring non-useful data.

Segment uses special-case compression for some known columns, like event names and timestamps. The others default to LZO. Segment may add look-ahead sampling down the road, but from inspecting the datasets today this would be unnecessarily complex.

Timestamps

The Segment API associates four timestamps with every call: 
timestamp
, 
original_timestamp
, 
sent_at
 and 
received_at
.

All four timestamps pass through to your Warehouse for every ETL’d event. In most cases the timestamps are close together, but they have different meanings which are important.

timestamp
 is the UTC-converted timestamp which is set by the Segment library. If you are importing historical events using a server-side library, this is the timestamp you’ll want to reference in your queries.

original_timestamp
 is the original timestamp set by the Segment library at the time the event is created.  Keep in mind, this timestamp can be affected by device clock skew. You can override this value by manually passing in a value for 
timestamp
 which will then be relabeled as 
original_timestamp
. Generally, this timestamp should be ignored in favor of the 
timestamp
 column.

sent_at
 is the UTC timestamp set by library when the Segment API call was sent.  This timestamp can also be affected by device clock skew.

received_at
 is UTC timestamp set by the Segment API when the API receives the payload from client or server. All tables use 
received_at
 for the sort key.

Segment recommends using the 
received_at
 timestamp for all queries based on time. The reason for this is two-fold. First, the 
sent_at
 timestamp relies on a client’s device clock being accurate, which is generally unreliable. Secondly, Segment sets 
received_at
 as the sort key in Redshift schemas, which means queries will execute much faster when using 
received_at
. You can continue to use 
timestamp
 or 
sent_at
 timestamps in queries if 
received_at
 doesn’t work for your analysis, but the queries will take longer to complete.

For Business Tier customers, Segment suggests enabling 
received_at
 in the Selective Sync settings to ensure syncs and backfills complete successfully.

received_at
 does not ensure chronology of events.  For queries based on event chronology, 
timestamp
 should be used.

ISO-8601 date strings with timezones included are required when using timestamps with 
Engage
. Sending custom traits without a timezone included in the timestamp will result in the value not being saved.

To learn more about timestamps in Segment, 
read our timestamps overview
 in the Segment Spec.

id

Each row in your database will have an 
id
 which is equivalent to the messageId which is passed through in the raw JSON events. The 
id
 is a unique message id associated with the row.

uuid, uuid_ts, and loaded_at

The 
uuid
 column is used to prevent duplicates. You can ignore this column.

The 
uuid_ts
 column is used to keep track of when the specific event was last processed by our connector, specifically for deduping and debugging purposes. You can generally ignore this column.

The 
loaded_at
 column contains the UTC timestamp reflecting when the data was staged by the processor. This column is created only in BigQuery warehouse.

Sort Key

All tables use 
received_at
 for the sort key. Amazon Redshift stores your data on disk in sorted order according to the sort key. The Redshift query optimizer uses sort order when it determines optimal query plans.

More Help

How do I send custom data to my warehouse?

How do I give users permissions to my warehouse?

How frequently does data sync to my warehouse?

Check out our 
Frequently Asked Questions about Warehouses
 and 
a list of helpful Redshift queries to get you started
.

This page was last modified: 07 Feb 2025

Need support?

Questions? Problems? Need more info? Contact Segment Support for assistance!

Help improve these docs!

Was this page helpful?

Thanks for your feedback!

Can we improve this doc? 
Send us feedback!

Get started with Segment

On this page

Was this page helpful?

Thanks for your feedback!

Can we improve
 this doc? 
Send us feedback!

Product

For Developers

Company

Support

© 2025 Segment.io, Inc.

=== Content from https://segment.com/docs/connections/find-writekey/ ===

Locate your Write Key
        

On this page

The write key is a unique identifier for each source. It lets Segment know which source is sending the data and which destinations should receive that data.

Find the write key for a source

To find a write key, you first need to create an 
event streams source
 like a website, server, or mobile source. (
Cloud-sources
 do not have write keys, as they use a token or key from your account with that service.)

Then, in the Source, go to 
Settings
 and select 
API Keys
.



Now you can add the source’s write key to your app and begin sending data to Segment.

Locate a source using your write key

To find the source given a write key within your workspace, open your workspace and select the search icon. Enter your write key into the search bar. If the write key exists in the workspace and is connected to a source, the source shows up in the list of results.



This method is only available to locate event streams sources

This method cannot be used to find a destination or cloud event source.

This page was last modified: 20 Nov 2023

Need support?

Questions? Problems? Need more info? Contact Segment Support for assistance!

Help improve these docs!

Was this page helpful?

Thanks for your feedback!

Can we improve this doc? 
Send us feedback!

Get started with Segment

On this page

Was this page helpful?

Thanks for your feedback!

Can we improve
 this doc? 
Send us feedback!

Product

For Developers

Company

Support

© 2025 Segment.io, Inc.

=== Content from https://segment.com/docs/engage/profiles/csv-upload/ ===

Add or Update Profiles and Traits with a CSV
        

Engage Foundations requires a Business tier account and includes Unify.


See the 
available plans
, or 
contact Support
.

On this page

You can use the Profiles CSV Uploader to add or update user profiles and traits. This page contains guidelines for your CSV upload and explains how to upload a CSV file to Engage.

When you upload a CSV file, Engage generates internal Identify calls using Segment’s Tracking API and sends them into the 
Engage output source
.

CSV file upload guidelines

Keep the following guidelines in mind as you upload CSV files to Twilio Engage:

Upload a CSV file

Use the 
Upload CSV
 page to upload a CSV file in your Segment space:

1. Download your CSV template

Click 
Download Template
 to download a CSV template with identifier columns from your identity resolution configuration.

2. Fill out your CSV file

Enter values for the identifiers in your CSV file.

3. Upload your CSV file

Upload a CSV file to Twilio Engage in two ways:

Work with the CSV template

Keep the following in mind as you fill out your CSV template.

Allowed CSV file characters

You can use these characters in your CSV file:

àáâäǎæãåāçćčċďðḍèéêëěẽēėęğġgg͟hħḥh̤ìíîïǐĩīıįķk͟hłļľl̥ṁm̐òóôöǒœøõōřṛr̥ɽßşșśšṣs̤s̱sțťþṭt̤ʈùúûüǔũūűůŵýŷÿźžżẓz̤ÀÁ
ÄǍÆÃÅĀÇĆČĊĎÐḌÈÉÊËĚẼĒĖĘĞĠGG͟HĦḤH̤ÌÍÎÏǏĨĪIĮĶK͟HŁĻĽL̥ṀM̐ÒÓÔÖǑŒØÕŌŘṚR̥ɌSẞŚŠŞȘṢS̤S̱ȚŤÞṬT̤ƮÙÚÛÜǓŨŪŰŮŴÝŶŸŹŽŻẒZ

View Update History

Use the Update History page to view CSV file uploads in your workspace over the last 30 days.

To view the Update History page:

Validation errors

The following table lists validation errors you may run into with your profiles and traits CSV upload:

This page was last modified: 03 Oct 2024

Need support?

Questions? Problems? Need more info? Contact Segment Support for assistance!

Help improve these docs!

Was this page helpful?

Thanks for your feedback!

Can we improve this doc? 
Send us feedback!

Get started with Segment

On this page

Was this page helpful?

Thanks for your feedback!

Can we improve
 this doc? 
Send us feedback!

Product

For Developers

Company

Support

© 2025 Segment.io, Inc.

=== Content from https://segment.com/docs/unify/traits/predictions/predictions-nutrition-facts/ ===

Predictions Nutrition Facts Label
        

Twilio’s 
AI Nutrition Facts
 provide an overview of the AI feature you’re using, so you can better understand how the AI is working with your data. Predictions’s AI qualities are outlined in the following Nutrition Facts label. For more information, including the glossary regarding the AI Nutrition Facts label, refer to the 
AI Nutrition Facts
 page.





AI Nutrition Facts






Predictions

Description






Predictions creates propensity models that predict if a customer will purchase, churn, or perform any other conversion event

Privacy Ladder Level 


2



Feature is Optional


Yes

Model Type 


Predictive

Base Model 


Gradient Boosted Trees

Trust Ingredients

Base Model Trained with Customer Data


N/A






Customer Data is used to develop a model created specifically for each customer and is never reused for other customers 

Customer Data Shared with Model Vendor


No






Customer Data is used to build the model, but it is built by Twilio Segment for the customer and not by a third party vendor 

Training Data Anonymized   


 No

Data Deletion


Yes

Human in the Loop 


N/A

Data Retention  


30 days

Input/Output Consistency


N/A

Other Resources






Learn more at: 
https://twilioalpha.com/
 

This page was last modified: 26 Feb 2024

Need support?

Questions? Problems? Need more info? Contact Segment Support for assistance!

Help improve these docs!

Was this page helpful?

Thanks for your feedback!

Can we improve this doc? 
Send us feedback!

Get started with Segment

Was this page helpful?

Thanks for your feedback!

Can we improve
 this doc? 
Send us feedback!

Product

For Developers

Company

Support

© 2025 Segment.io, Inc.

=== Content from https://segment.com/docs/unify/identity-resolution/ecommerce-example/ ===

Identity Resolution eCommerce Example
        

Unify requires a Business tier account and is included with Engage.


See the 
available plans
, or 
contact Support
.

On this page

Identity Resolution helps to create a unified view of the user across devices, apps, and unique identifiers.

Take the example of a sneaker company called SegmentKicks which has an eCommerce app called SegKicks as well as a running app called SegRuns. This example follows Jane Doe through her customer journey from an anonymous user to a registered buyer on one app, SegKicks, to her use of the same app on a different device, and finally to her use of a different app belonging to the same company, SegRuns.

Anonymous to known identification

Identity Resolution can connect a user’s anonymous behaviors to a user’s post-account registration activity.

Take this example using the eCommerce app, SegKicks:

{

  
"
anonymousId
"
:
 
"
anon_123
"
,

  
"
context
"
:
 
{

 
"
app
"
:
 
"
SegKicks
"
,

 
"
device
"
:
 
{

   
"
id
"
:
 
"
ios_abc123
"
,

   
"
type
"
:
 
"
ios
"

 
},

  
},

  
"
event
"
:
 
"
App Opened
"
,

  
"
type
"
:
 
"
track
"


}



{

  
"
anonymousId
"
:
 
"
anon_123
"
,

  
"
context
"
:
 
{

 
"
app
"
:
 
"
SegKicks
"
,

 
"
device
"
:
 
{

   
"
id
"
:
 
"
ios_abc123
"
,

   
"
type
"
:
 
"
ios
"

 
},

  
},

  
"
event
"
:
 
"
ShoeA Clicked
"
,

  
"
type
"
:
 
"
track
"


}



{

  
"
anonymousId
"
:
 
"
anon_123
"
,

  
"
context
"
:
 
{

    
"
app
"
:
 
"
SegKicks
"
,

    
"
device
"
:
 
{

      
"
id
"
:
 
"
ios_abc123
"
,

      
"
type
"
:
 
"
ios
"

    
},

  
},

  
"
userId
"
:
 
"
abc123def
"
,

  
"
type
"
:
 
"
identify
"


}



By linking the original anonymous events to Jane’s logged-in activity, the app’s marketing team can now begin to map out her customer journey on a single app, understand her preferences, and re-target her with highly personalized emails about the shoes she didn’t complete purchasing.

Her identifiers will now contain the original anonymous_id, her email, and her user_id:


Cross-device identification

Users can have multiple touch points with an app ecosystem through more than one device. For example, users might interact with an eCommerce app through both a native app, a mobile browser, and a web browser.

Continuing with the example of Jane Doe, she now views the same mobile app SegKicks on her Android phone.

Jane logs into the Android phone with the same email 
janedoe@example.com
.

{

  
"
anonymousId
"
:
 
"
anon_456
"
,

  
"
context
"
:
 
{

    
"
app
"
:
 
"
SegKicks
"
,

    
"
device
"
:
 
{

      
"
id
"
:
 
"
and_1a2b3c4d
"
,

      
"
type
"
:
 
"
android
"

    
},

  
},

  
"
type
"
:
 
"
identify
"
,

  
"
userId
"
:
 
"
abc123def
"


}



Her new User Profile identities will now contains an 
android.id
:


Cross-app identification

A company’s product ecosystem may also spread out across multiple apps. For example, SegmentKicks also has a running app SegRuns.

When Jane downloads the Android app SegRuns and views a workout:

{

  
"
anonymousId
"
:
 
"
anon_789
"
,

  
"
context
"
:
 
{

    
"
app
"
:
 
"
SegRuns
"
,

    
"
device
"
:
 
{

      
"
id
"
:
 
"
and_1a2b3c4d
"
,

      
"
type
"
:
 
"
android
"

    
},

  
},

  
"
type
"
:
 
"
identify
"
,

  
"
userId
"
:
 
"
abc123def
"


}



Her final identifiers now have a new 
anonymous_id
 from the SegRuns app:


Conclusion

By combining the events throughout Jane’s entire customer journey from anonymous to known user, cross-device, and cross-app identification, SegKicks and SegRuns can now work together to understand how to give Jane the best customer experience possible while increasing her LTV across the entire SegmentKicks ecosystem.

For example, if Jane looked at ShoeC on her iPhone and completed checkout for ShoeC on her Android, SegKicks will now know to exclude her from a cart abandonment email for ShoeC. This wouldn’t be possible if SegKicks had only looked at her activity on the iPhone.

Additionally, most shoes need to be replaced every 300 to 400 miles. By understanding her activity on SegRuns, SegKicks will now be able to more effectively remind Jane to repurchase ShoeC or ShoeD once she’s reached that mileage.

This page was last modified: 28 Mar 2023

Need support?

Questions? Problems? Need more info? Contact Segment Support for assistance!

Help improve these docs!

Was this page helpful?

Thanks for your feedback!

Can we improve this doc? 
Send us feedback!

Get started with Segment

On this page

Was this page helpful?

Thanks for your feedback!

Can we improve
 this doc? 
Send us feedback!

Product

For Developers

Company

Support

© 2025 Segment.io, Inc.

=== Content from https://segment.com/docs/segment-app/set-up-envs/ ===

Prod and Testing Environments in Segment
        

We recommend that instead of setting up separate workspaces for different environments (local/development/prod), you set up one workspace and make each of these environments a 
different source
. 

We bill per workspace, and on our Team or Business plans you can create as many sources as you need. Each Segment source will have its own Write Key, so you can easily keep things separate.

For each source, you also get to choose which integrations you want it to send data to.

This page was last modified: 22 Jan 2020

Need support?

Questions? Problems? Need more info? Contact Segment Support for assistance!

Help improve these docs!

Was this page helpful?

Thanks for your feedback!

Can we improve this doc? 
Send us feedback!

Get started with Segment

Was this page helpful?

Thanks for your feedback!

Can we improve
 this doc? 
Send us feedback!

Product

For Developers

Company

Support

© 2025 Segment.io, Inc.

=== Content from https://segment.com/docs/engage/audiences/send-audience-data/ ===

Send Audience Data to Destinations
        

Engage Foundations requires a Business tier account and includes Unify.


See the 
available plans
, or 
contact Support
.

On this page

With the help of sources and destinations in Segment’s catalog, you can create and send audiences and computed traits to third-party services.

Segment’s Connections pipeline first collects and sends events from your source to your destination. Built on top of Connections, Engage then uses the same source events to let you create audiences and computed traits within Segment. You can then send the audience or computed trait you’ve built to your destination(s).

Because Engage only sends audiences and computed traits to destinations, it doesn’t replace a standard event pipeline. Connect a source directly to a destination if you want the destination to receive all events that Segment gathers.

Connect your audience to a destination

Once you’ve previewed your audience, you can choose to connect it to a destination or keep the audience in Segment and export it as a CSV file download.

When you create an audience, Segment starts syncing your audience to the destinations you selected. Audiences are either sent to destinations as a boolean user-property or a user-list, depending on what the destination supports. Read more about 
supported destinations
 in the Engage documentation.

For account-level audiences, you can send either a 
Group
 call and/or 
Identify
 call. Group calls send one event per account, whereas Identify calls send an Identify call for each user in the account. This means that even if a user hasn’t performed an event, Segment will still set the account-level computed trait on that user.

Because most marketing tools are still based at the user level, you’ll usually want to map this account-level trait onto each user within an account. See 
Account-level Audiences
 for more information.

When you connect a new destination to an existing audience, Engage will backfill historical data for that audience to the new destination.

Follow these steps to connect an audience to a destination:

View connected destinations

You can view a list of an audience’s connected destinations in the destination list table of the audience overview tab.



The Destinations table contains information about the destination’s matching mappings, status, and sync status.

Matching mappings

Actions destinations
 have mappings that can receive granular data from your audience. The 
Matching mappings
 column shows the number of mappings that match the data coming from the audience, as well as the number of enabled and disabled mappings. See 
Working with mappings
 for more information.

The Matching mappings column will show 
Not applicable
 for classic destinations.

Status columns

The 
Destination status
 column shows 
Connected
, 
Disconnected
, or 
Disabled
:

The 
Sync status
 column shows the current 
compute status
 between the audience and connected destination.

Working with mappings

You can add and access mappings within your audience’s connected destination by following these steps:

Segment then returns you to the audience’s destination side panel, which shows your new mapping(s).

Use Segment’s 
Duplicate mappings
 feature to create an exact copy of an existing mapping. The copied mapping has the same configurations and enrichments as your original mapping.

This page was last modified: 11 Sep 2024

Need support?

Questions? Problems? Need more info? Contact Segment Support for assistance!

Help improve these docs!

Was this page helpful?

Thanks for your feedback!

Can we improve this doc? 
Send us feedback!

Get started with Segment

On this page

Was this page helpful?

Thanks for your feedback!

Can we improve
 this doc? 
Send us feedback!

Product

For Developers

Company

Support

© 2025 Segment.io, Inc.

=== Content from https://segment.com/docs/connections/spec/video/ ===

Video Spec
        

On this page

Segment’s video spec helps you define how customers engage with your video and ad content. This document covers the naming syntax and conventions for how you should send events when tracking video analytics.

Note:
 not all destinations support video tracking, and you should always check with the individual destination documentation to confirm.

Getting started

Before you start implementing the Segment video spec, you should understand the overall structure and classification of events. The video spec will be organized into 
three
 distinct event categories:

Playback

You can think of playback events being related to the actual 
playback
 of the video content. This means that these events are meant to track information about the video player (such as pause, resume, or play). Thus, you can think of playback events to be at the session level. For example, when a customer presses play on your video, you would start by sending a 
Video Playback Started
 event with a unique 
session_id
. In particular, this event should fire after the last user action required for playback to begin.

Then, for the duration of that user’s session with that specific video player, all
subsequent events generated from this session/playback should be tied with the same aforementioned 
session_id
. So if you had a web page that had two video players, you would have two separate sessions and 
session_ids
 while contrastingly if you only had one video player on the page but the playback played two video contents in a row, you would only have one session but two contents tied to it.

Playback event object

All playback events share the same event properties that describe information about the current state of the player. Below is a table of the supported properties of this object.

Playback events

Below is the full list of Video Playback Events.

When a user presses Play; after the last user action required for playback to begin (for example, after user login/authentication).

{

    
"
action
"
:
 
"
track
"
,

    
"
event
"
:
 
"
Video Playback Started
"
,

    
"
userId
"
:
 
"
userId
"
,

    
"
properties
"
:
 
{

      
"
session_id
"
:
 
"
12345
"
,

      
"
content_asset_ids
"
:
 
[
"
0129370
"
],

      
"
content_pod_ids
"
:
 
[
"
segA
"
,
 
"
segB
"
],

      
"
ad_asset_id
"
:
 
[
"
ad123
"
,
 
"
ad097
"
],

      
"
ad_pod_id
"
:
 
[
"
adSegA
"
,
 
"
adSegB
"
],

      
"
ad_type
"
:
 
[
"
mid-roll
"
,
 
"
post-roll
"
],

      
"
position
"
:
 
0
,

      
"
total_length
"
:
 
392
,

      
"
bitrate
"
:
 
100
,

      
"
framerate
"
:
 
29.00
,

      
"
video_player
"
:
 
"
youtube
"
,

      
"
sound
"
:
 
88
,

      
"
full_screen
"
:
 
false
,

      
"
ad_enabled
"
:
 
true
,

      
"
quality
"
:
 
"
hd1080
"
,

      
"
livestream
"
:
 
false

    
}


}



When a user presses Pause.

{

    
"
action
"
:
 
"
track
"
,

    
"
event
"
:
 
"
Video Playback Paused
"
,

    
"
userId
"
:
 
"
userId
"
,

    
"
properties
"
:
 
{

      
"
session_id
"
:
 
"
12345
"
,

      
"
content_asset_id
"
:
 
"
0129370
"
,

      
"
content_pod_id
"
:
 
"
segA
"
,

      
"
position
"
:
 
278
,

      
"
total_length
"
:
 
392
,

      
"
bitrate
"
:
 
100
,

      
"
framerate
"
:
 
29.00
,

      
"
video_player
"
:
 
"
youtube
"
,

      
"
sound
"
:
 
55
,

      
"
full_screen
"
:
 
false
,

      
"
ad_enabled
"
:
 
false
,

      
"
quality
"
:
 
"
hd1080
"
,

      
"
livestream
"
:
 
false

    
}


}



When the playback stops unintentionally (such as from network loss, browser close/redirect, or app crash). With this event you can pass 
method
 as a property to denote the cause of the interruption.

{

    
"
action
"
:
 
"
track
"
,

    
"
event
"
:
 
"
Video Playback Interrupted
"
,

    
"
userId
"
:
 
"
userId
"
,

    
"
properties
"
:
 
{

      
"
session_id
"
:
 
"
12345
"
,

      
"
content_asset_id
"
:
 
"
0129370
"
,

      
"
content_pod_id
"
:
 
"
segA
"
,

      
"
position
"
:
 
278
,

      
"
total_length
"
:
 
392
,

      
"
bitrate
"
:
 
100
,

      
"
framerate
"
:
 
29.00
,

      
"
video_player
"
:
 
"
youtube
"
,

      
"
sound
"
:
 
55
,

      
"
full_screen
"
:
 
false
,

      
"
ad_enabled
"
:
 
false
,

      
"
quality
"
:
 
"
hd1080
"
,

      
"
livestream
"
:
 
false
,

      
"
method
"
:
 
"
network loss
"

    
}


}



When playback starts buffering content or an ad.

{

    
"
action
"
:
 
"
track
"
,

    
"
event
"
:
 
"
Video Playback Buffer Started
"
,

    
"
userId
"
:
 
"
userId
"
,

    
"
properties
"
:
 
{

      
"
session_id
"
:
 
"
12345
"
,

      
"
content_asset_id
"
:
 
"
0129370
"
,

      
"
content_pod_id
"
:
 
"
segA
"
,

      
"
position
"
:
 
278
,

      
"
total_length
"
:
 
392
,

      
"
bitrate
"
:
 
100
,

      
"
framerate
"
:
 
29.00
,

      
"
video_player
"
:
 
"
youtube
"
,

      
"
sound
"
:
 
55
,

      
"
full_screen
"
:
 
false
,

      
"
ad_enabled
"
:
 
false
,

      
"
quality
"
:
 
"
hd1080
"
,

      
"
livestream
"
:
 
false

    
}


}



When playback finishes buffering content or an ad.

{

    
"
action
"
:
 
"
track
"
,

    
"
event
"
:
 
"
Video Playback Buffer Completed
"
,

    
"
userId
"
:
 
"
userId
"
,

    
"
properties
"
:
 
{

      
"
session_id
"
:
 
"
12345
"
,

      
"
content_asset_id
"
:
 
"
0129370
"
,

      
"
content_pod_id
"
:
 
"
segA
"
,

      
"
position
"
:
 
278
,

      
"
total_length
"
:
 
392
,

      
"
bitrate
"
:
 
100
,

      
"
framerate
"
:
 
29.00
,

      
"
video_player
"
:
 
"
youtube
"
,

      
"
sound
"
:
 
55
,

      
"
full_screen
"
:
 
false
,

      
"
ad_enabled
"
:
 
false
,

      
"
quality
"
:
 
"
hd1080
"
,

      
"
livestream
"
:
 
false

    
}


}



When a user manually seeks a certain position of the content or ad in the playback. Pass in the 
seek_position
 to denote where the user is seeking to, and pass in the 
position
 property to denote where the user is seeking from.

{

    
"
action
"
:
 
"
track
"
,

    
"
event
"
:
 
"
Video Playback Seek Started
"
,

    
"
userId
"
:
 
"
userId
"
,

    
"
properties
"
:
 
{

      
"
session_id
"
:
 
"
12345
"
,

      
"
content_asset_id
"
:
 
"
0129370
"
,

      
"
content_pod_id
"
:
 
"
segA
"
,

      
"
position
"
:
 
278
,

      
"
seek_position
"
:
 
320
,

      
"
total_length
"
:
 
392
,

      
"
bitrate
"
:
 
100
,

      
"
framerate
"
:
 
29.00
,

      
"
video_player
"
:
 
"
youtube
"
,

      
"
sound
"
:
 
55
,

      
"
full_screen
"
:
 
false
,

      
"
ad_enabled
"
:
 
false
,

      
"
quality
"
:
 
"
hd1080
"
,

      
"
livestream
"
:
 
false

    
}


}



After a user manually seeks to a certain position of the content or ad in the playback. Pass in the 
position
 property to denote where the user desires to begin the playback from.

{

    
"
action
"
:
 
"
track
"
,

    
"
event
"
:
 
"
Video Playback Seek Completed
"
,

    
"
userId
"
:
 
"
userId
"
,

    
"
properties
"
:
 
{

      
"
session_id
"
:
 
"
12345
"
,

      
"
content_asset_id
"
:
 
"
0129370
"
,

      
"
content_pod_id
"
:
 
"
segA
"
,

      
"
position
"
:
 
320
,

      
"
total_length
"
:
 
392
,

      
"
bitrate
"
:
 
100
,

      
"
framerate
"
:
 
29.00
,

      
"
video_player
"
:
 
"
youtube
"
,

      
"
sound
"
:
 
55
,

      
"
full_screen
"
:
 
false
,

      
"
ad_enabled
"
:
 
false
,

      
"
quality
"
:
 
"
hd1080
"
,

      
"
livestream
"
:
 
false

    
}


}



When playback is resumed, by the user, after being paused.

{

    
"
action
"
:
 
"
track
"
,

    
"
event
"
:
 
"
Video Playback Resumed
"
,

    
"
userId
"
:
 
"
userId
"
,

    
"
properties
"
:
 
{

      
"
session_id
"
:
 
"
12345
"
,

      
"
content_asset_id
"
:
 
"
0129370
"
,

      
"
content_pod_id
"
:
 
"
segA
"
,

      
"
position
"
:
 
278
,

      
"
total_length
"
:
 
392
,

      
"
bitrate
"
:
 
100
,

      
"
framerate
"
:
 
29.00
,

      
"
video_player
"
:
 
"
youtube
"
,

      
"
sound
"
:
 
55
,

      
"
full_screen
"
:
 
false
,

      
"
ad_enabled
"
:
 
false
,

      
"
quality
"
:
 
"
hd1080
"
,

      
"
livestream
"
:
 
false

    
}


}



When playback is complete and only when the session is finished.

{

    
"
action
"
:
 
"
track
"
,

    
"
event
"
:
 
"
Video Playback Completed
"
,

    
"
userId
"
:
 
"
userId
"
,

    
"
properties
"
:
 
{

      
"
session_id
"
:
 
"
12345
"
,

      
"
content_asset_id
"
:
 
"
0129370
"
,

      
"
content_pod_id
"
:
 
"
segA
"
,

      
"
position
"
:
 
392
,

      
"
total_length
"
:
 
392
,

      
"
bitrate
"
:
 
100
,

      
"
framerate
"
:
 
29.00
,

      
"
video_player
"
:
 
"
youtube
"
,

      
"
sound
"
:
 
55
,

      
"
full_screen
"
:
 
false
,

      
"
ad_enabled
"
:
 
false
,

      
"
quality
"
:
 
"
hd1080
"
,

      
"
livestream
"
:
 
false

    
}


}



When user navigates away from a playback/stream.

{

    
"
action
"
:
 
"
track
"
,

    
"
event
"
:
 
"
Video Playback Exited
"
,

    
"
userId
"
:
 
"
userId
"
,

    
"
properties
"
:
 
{

      
"
session_id
"
:
 
"
12345
"
,

      
"
content_asset_id
"
:
 
"
0129370
"
,

      
"
content_pod_id
"
:
 
"
segA
"
,

      
"
position
"
:
 
392
,

      
"
total_length
"
:
 
392
,

      
"
bitrate
"
:
 
100
,

      
"
framerate
"
:
 
29.00
,

      
"
video_player
"
:
 
"
youtube
"
,

      
"
sound
"
:
 
55
,

      
"
full_screen
"
:
 
false
,

      
"
ad_enabled
"
:
 
false
,

      
"
quality
"
:
 
"
hd1080
"
,

      
"
livestream
"
:
 
false

    
}


}



Content

Underneath the playback level, we now have the 
pod
 level. A pod can be seen as a “group” or “segment” of either the content or advertisement.

Consider, for example, a playback session that might have some content and one mid-roll advertisement. This would mean that you would have two 
content
 pods (since the mid-roll ad split the content playback into two sections) while you might have one ad pod for the mid-roll ad. In this instance, you’d start and complete the first pod of content; you’d start and complete the ad; you’d start and complete the second pod of content. All of this would happen within one playback start.

Content event object

All content events share the same event properties that describe information about the current video content the user is interacting with. Below is a table of the supported properties of this object.

Content events

Below is the full list of Video Content Events.

When a video content segment starts playing within a playback.

{

    
"
action
"
:
 
"
track
"
,

    
"
event
"
:
 
"
Video Content Started
"
,

    
"
userId
"
:
 
"
userId
"
,

    
"
properties
"
:
 
{

      
"
session_id
"
:
 
"
12345
"
,

      
"
asset_id
"
:
 
"
0129370
"
,

      
"
pod_id
"
:
 
"
segA
"
,

      
"
program
"
:
 
"
Planet Earth
"
,

      
"
title
"
:
 
"
Seasonal Forests
"
,

      
"
description
"
:
 
"
David Attenborough reveals the greatest woodlands on earth.
"
,

      
"
season
"
:
 
"
1
"
,

      
"
position
"
:
 
0
,

      
"
total_length
"
:
 
3600
,

      
"
genre
"
:
 
"
Documentary
"
,

      
"
publisher
"
:
 
"
BBC
"
,

      
"
full_episode
"
:
 
true
,

      
"
keywords
"
:
 
[
"
nature
"
,
 
"
forests
"
,
 
"
earth
"
]

    
}


}



Heartbeats that you can fire every n seconds to track how far into the content the user is currently viewing as indicated by the 
position
.

{

    
"
action
"
:
 
"
track
"
,

    
"
event
"
:
 
"
Video Content Playing
"
,

    
"
userId
"
:
 
"
userId
"
,

    
"
properties
"
:
 
{

      
"
session_id
"
:
 
"
12345
"
,

      
"
asset_id
"
:
 
"
0129370
"
,

      
"
pod_id
"
:
 
"
segA
"
,

      
"
program
"
:
 
"
Planet Earth
"
,

      
"
title
"
:
 
"
Seasonal Forests
"
,

      
"
description
"
:
 
"
David Attenborough reveals the greatest woodlands on earth.
"
,

      
"
season
"
:
 
"
1
"
,

      
"
position
"
:
 
10
,

      
"
total_length
"
:
 
3600
,

      
"
genre
"
:
 
"
Documentary
"
,

      
"
publisher
"
:
 
"
BBC
"
,

      
"
full_episode
"
:
 
true
,

      
"
keywords
"
:
 
[
"
nature
"
,
 
"
forests
"
,
 
"
earth
"
]

    
}


}



When a video content segment completes playing within a playback. That is, 
position
 and 
total_length
 are equal.

{

    
"
action
"
:
 
"
track
"
,

    
"
event
"
:
 
"
Video Content Completed
"
,

    
"
userId
"
:
 
"
userId
"
,

    
"
properties
"
:
 
{

      
"
session_id
"
:
 
"
12345
"
,

      
"
asset_id
"
:
 
"
0129370
"
,

      
"
pod_id
"
:
 
"
segA
"
,

      
"
program
"
:
 
"
Planet Earth
"
,

      
"
title
"
:
 
"
Seasonal Forests
"
,

      
"
description
"
:
 
"
David Attenborough reveals the greatest woodlands on earth.
"
,

      
"
season
"
:
 
"
1
"
,

      
"
position
"
:
 
3600
,

      
"
total_length
"
:
 
3600
,

      
"
genre
"
:
 
"
Documentary
"
,

      
"
publisher
"
:
 
"
BBC
"
,

      
"
full_episode
"
:
 
true
,

      
"
keywords
"
:
 
[
"
nature
"
,
 
"
forests
"
,
 
"
earth
"
]

    
}


}



Ads

Just like Content events, Ad Events also live underneath the playback level and at the pod level. A given ad pod can have multiple ad assets (or just one) and a playback session might have multiple ad pods. For example, if your video playback has two pre-roll, one mid-roll, and one post-roll ads, you would have three ad pods:

Ad event object

All ad events share the same event properties that describe information about the current ad content the user is interacting with. Below is a table of the supported properties of this object.

Since some video destinations require sending Content metadata along with Ad metadata, you may need to send your content properties also in all your ad events under 
properties.content
 depending on the video destination you’re using.

Ad events

{

    
"
action
"
:
 
"
track
"
,

    
"
event
"
:
 
"
Video Ad Started
"
,

    
"
userId
"
:
 
"
userId
"
,

    
"
properties
"
:
 
{

      
"
session_id
"
:
 
"
12345
"
,

      
"
asset_id
"
:
 
"
0129370
"
,

      
"
pod_id
"
:
 
"
segA
"
,

      
"
type
"
:
 
"
pre-roll
"
,

      
"
title
"
:
 
"
The New New Thing!
"
,

      
"
position
"
:
 
0
,

      
"
total_length
"
:
 
30
,

      
"
publisher
"
:
 
"
Apple
"
,

      
"
load_type
"
:
 
"
dynamic
"

    
}


}



{

    
"
action
"
:
 
"
track
"
,

    
"
event
"
:
 
"
Video Ad Playing
"
,

    
"
userId
"
:
 
"
userId
"
,

    
"
properties
"
:
 
{

      
"
session_id
"
:
 
"
12345
"
,

      
"
asset_id
"
:
 
"
0129370
"
,

      
"
pod_id
"
:
 
"
segA
"
,

      
"
type
"
:
 
"
pre-roll
"
,

      
"
title
"
:
 
"
The New New Thing!
"
,

      
"
position
"
:
 
5
,

      
"
total_length
"
:
 
30
,

      
"
publisher
"
:
 
"
Apple
"
,

      
"
load_type
"
:
 
"
dynamic
"

    
}


}



{

  
"
action
"
:
 
"
track
"
,

  
"
event
"
:
 
"
Video Ad Completed
"
,

  
"
userId
"
:
 
"
userId
"
,

  
"
properties
"
:
 
{

    
"
session_id
"
:
 
"
12345
"
,

    
"
asset_id
"
:
 
"
0129370
"
,

    
"
pod_id
"
:
 
"
segA
"
,

    
"
type
"
:
 
"
pre-roll
"
,

    
"
title
"
:
 
"
The New New Thing!
"
,

    
"
position
"
:
 
30
,

    
"
total_length
"
:
 
30
,

    
"
publisher
"
:
 
"
Apple
"
,

    
"
load_type
"
:
 
"
dynamic
"

  
}


}



Resuming playback

When you fire a 
Video Playback Resumed
 event, you 
should
 immediately call a Segment heartbeat event (
Video Content Playing
 or 
Video Ad Playing
 depending on what the playback resumed to). This should effectively mean that you are also resuming your 10 second heartbeats (since they should’ve been paused after sending Video Playback Paused event).

Video quality event

It’s important to analyze the performance of your video content. To keep track of quality changes, you can track a 
Video Quality Updated
 event when there is a change in video quality with the following properties:

Example event lifecycle

Below is an example of how one might implement the video spec:

1) User presses play on a video:

analytics
.
track
(
'
Video Playback Started
'
,
 
{

  
session_id
:
 
'
12345
'
,

  
content_asset_ids
:
 
[
'
0129370
'
],

  
content_pod_ids
:
 
[
'
segA
'
,
 
'
segB
'
],

  
ad_asset_ids
:
 
[
 
'
ad123
'
,
 
'
ad097
'
 
],

  
ad_pod_ids
:
 
[
'
adSegA
'
,
 
'
adSegB
'
],

  
ad_types
:
 
[
'
mid-roll
'
,
 
'
post-roll
'
],

  
position
:
 
0
,

  
total_length
:
 
392
,

  
bitrate
:
 
100
,

  
video_player
:
 
'
youtube
'
,

  
sound
:
 
88
,

  
full_screen
:
 
false
,

  
ad_enabled
:
 
false
,

  
quality
:
 
'
hd1080
'


});



2) Playback starts to play content:

analytics
.
track
(
'
Video Content Started
'
,
 
{

  
session_id
:
 
'
12345
'
,

  
asset_id
:
 
'
0129370
'
,

  
pod_id
:
 
'
segA
'
,

  
title
:
 
'
Interview with Tony Robbins
'
,

  
description
:
 
'
short description
'
,

  
keywords
:
 
[
'
entrepreneurship
'
,
 
'
motivation
'
],

  
season
:
 
'
2
'
,

  
episode
:
 
'
177
'
,

  
genre
:
 
'
entrepreneurship
'
,

  
program
:
 
'
Tim Ferris Show
'
,

  
publisher
:
 
'
Tim Ferris
'
,

  
position
:
 
0
,

  
total_length
:
 
360
,

  
channel
:
 
'
espn
'
,

  
full_episode
:
 
true
,

  
livestream
:
 
false
,

  
airdate
:
 
'
1991-08-13
'


});



3) User views content for 20 seconds and we have 10 second heartbeats:

analytics
.
track
(
'
Video Content Playing
'
,
 
{

  
session_id
:
 
'
12345
'
,

  
asset_id
:
 
'
0129370
'
,

  
pod_id
:
 
'
segA
'
,

  
title
:
 
'
Interview with Tony Robbins
'
,

  
description
:
 
'
short description
'
,

  
keywords
:
 
[
'
entrepreneurship
'
,
 
'
motivation
'
],

  
season
:
 
'
2
'
,

  
episode
:
 
'
177
'
,

  
genre
:
 
'
entrepreneurship
'
,

  
program
:
 
'
Tim Ferris Show
'
,

  
publisher
:
 
'
Tim Ferris
'
,

  
position
:
 
10
,

  
total_length
:
 
360
,

  
channel
:
 
'
espn
'
,

  
full_episode
:
 
true
,

  
livestream
:
 
false
,

  
airdate
:
 
'
1991-08-13
'


});



analytics
.
track
(
'
Video Content Playing
'
,
 
{

  
session_id
:
 
'
12345
'
,

  
asset_id
:
 
'
0129370
'
,

  
pod_id
:
 
'
segA
'
,

  
title
:
 
'
Interview with Tony Robbins
'
,

  
description
:
 
'
short description
'
,

  
keywords
:
 
[
'
entrepreneurship
'
,
 
'
motivation
'
],

  
season
:
 
'
2
'
,

  
episode
:
 
'
177
'
,

  
genre
:
 
'
entrepreneurship
'
,

  
program
:
 
'
Tim Ferris Show
'
,

  
publisher
:
 
'
Tim Ferris
'
,

  
position
:
 
20
,

  
total_length
:
 
360
,

  
channel
:
 
'
espn
'
,

  
full_episode
:
 
true
,

  
livestream
:
 
false
,

  
airdate
:
 
'
1991-08-13
'


});



4) Playback is paused and resumed:

analytics
.
track
(
'
Video Playback Paused
'
,
 
{

  
session_id
:
 
'
12345
'
,

  
content_asset_id
:
 
'
0129370
'
,

  
content_pod_id
:
 
'
segA
'
,

  
ad_asset_id
:
 
null
,

  
ad_pod_id
:
 
null
,

  
ad_type
:
 
null
,

  
position
:
 
21
,

  
total_length
:
 
392
,

  
video_player
:
 
'
youtube
'
,

  
sound
:
 
88
,

  
bitrate
:
 
100
,

  
full_screen
:
 
false
,

  
ad_enabled
:
 
false
,

  
quality
:
 
'
hd1080
'


});



analytics
.
track
(
'
Video Playback Resumed
'
,
 
{

  
session_id
:
 
'
12345
'
,

  
content_asset_id
:
 
'
0129370
'
,

  
content_pod_id
:
 
'
segA
'
,

  
ad_asset_id
:
 
null
,

  
ad_pod_id
:
 
null
,

  
ad_type
:
 
null
,

  
position
:
 
21
,

  
total_length
:
 
392
,

  
sound
:
 
88
,

  
bitrate
:
 
100
,

  
full_screen
:
 
false
,

  
video_player
:
 
'
youtube
'
,

  
ad_enabled
:
 
false
,

  
quality
:
 
'
hd1080
'


});



5) Mid-roll ad starts playing:

analytics
.
track
(
'
Video Ad Started
'
,
 
{

  
session_id
:
 
'
12345
'
,

  
asset_id
:
 
'
ad123
'
,

  
pod_id
:
 
'
adSegA
'
,

  
type
:
 
'
mid-roll
'
,

  
title
:
 
'
Segment Connection Modes
'
,

  
publisher
:
 
'
Segment
'
,

  
position
:
 
0
,

  
total_length
:
 
21
,

  
load_type
:
 
'
dynamic
'


});



6) User watches the full 21 second ad and we have 10 second heartbeats:

analytics
.
track
(
'
Video Ad Playing
'
,
 
{

  
session_id
:
 
'
12345
'
,

  
asset_id
:
 
'
ad123
'
,

  
pod_id
:
 
'
adSegA
'
,

  
type
:
 
'
pre-roll
'
,

  
title
:
 
'
Segment Connection Modes
'
,

  
publisher
:
 
'
Segment
'
,

  
position
:
 
10
,

  
total_length
:
 
21
,

  
load_type
:
 
'
dynamic
'


});



analytics
.
track
(
'
Video Ad Playing
'
,
 
{

  
session_id
:
 
'
12345
'
,

  
asset_id
:
 
'
ad123
'
,

  
pod_id
:
 
'
adSegA
'
,

  
type
:
 
'
pre-roll
'
,

  
title
:
 
'
Segment Connection Modes
'
,

  
publisher
:
 
'
Segment
'
,

  
position
:
 
20
,

  
total_length
:
 
21
,

  
load_type
:
 
'
dynamic
'


});



analytics
.
track
(
'
Video Ad Completed
'
,
 
{

  
session_id
:
 
'
12345
'
,

  
asset_id
:
 
'
ad123
'
,

  
pod_id
:
 
'
adSegA
'
,

  
type
:
 
'
pre-roll
'
,

  
title
:
 
'
Segment Connection Modes
'
,

  
publisher
:
 
'
Segment
'
,

  
position
:
 
21
,

  
total_length
:
 
21
,

  
load_type
:
 
'
dynamic
'


});



7) Content resumes, user finishes the full content:

analytics
.
track
(
'
Video Content Playing
'
,
 
{

  
session_id
:
 
'
12345
'
,

  
asset_id
:
 
'
0129370
'
,

  
pod_id
:
 
'
segB
'
,

  
title
:
 
'
Interview with Tony Robbins
'
,

  
description
:
 
'
short description
'
,

  
keywords
:
 
[
'
entrepreneurship
'
,
 
'
motivation
'
],

  
season
:
 
'
2
'
,

  
episode
:
 
'
177
'
,

  
genre
:
 
'
entrepreneurship
'
,

  
program
:
 
'
Tim Ferris Show
'
,

  
publisher
:
 
'
Tim Ferris
'
,

  
position
:
 
31
,

  
total_length
:
 
360
,

  
channel
:
 
'
espn
'
,

  
full_episode
:
 
true
,

  
livestream
:
 
false
,

  
airdate
:
 
'
1991-08-13
'


});



(
Video Content Playing
 
hearbeats every 10 seconds
)

analytics
.
track
(
'
Video Content Completed
'
,
 
{

  
session_id
:
 
'
12345
'
,

  
asset_id
:
 
'
0129370
'
,

  
pod_id
:
 
'
segB
'
,

  
title
:
 
'
Interview with Tony Robbins
'
,

  
description
:
 
'
short description
'
,

  
keywords
:
 
[
'
entrepreneurship
'
,
 
'
motivation
'
],

  
season
:
 
'
2
'
,

  
episode
:
 
'
177
'
,

  
genre
:
 
'
entrepreneurship
'
,

  
program
:
 
'
Tim Ferris Show
'
,

  
publisher
:
 
'
Tim Ferris
'
,

  
position
:
 
360
,

  
total_length
:
 
360
,

  
channel
:
 
'
espn
'
,

  
full_episode
:
 
true
,

  
livestream
:
 
false
,

  
airdate
:
 
'
1991-08-13
'


});



8) 11 second post-roll ad plays after content finishes:

analytics
.
track
(
'
Video Ad Started
'
,
 
{

  
session_id
:
 
'
12345
'
,

  
asset_id
:
 
'
ad097
'
,

  
pod_id
:
 
'
adSegB
'
,

  
type
:
 
'
post-roll
'
,

  
title
:
 
'
Segment Cross Domain IDs
'
,

  
publisher
:
 
'
Segment
'
,

  
position
:
 
0
,

  
total_length
:
 
11
,

  
load_type
:
 
'
dynamic
'


});



analytics
.
track
(
'
Video Ad Playing
'
,
 
{

  
session_id
:
 
'
12345
'
,

  
asset_id
:
 
'
ad097
'
,

  
pod_id
:
 
'
adSegB
'
,

  
type
:
 
'
post-roll
'
,

  
title
:
 
'
Segment Cross Domain IDs
'
,

  
publisher
:
 
'
Segment
'
,

  
position
:
 
10
,

  
total_length
:
 
11
,

  
load_type
:
 
'
dynamic
'


});



analytics
.
track
(
'
Video Ad Completed
'
,
 
{

  
session_id
:
 
'
12345
'
,

  
asset_id
:
 
'
ad097
'
,

  
pod_id
:
 
'
adSegB
'
,

  
type
:
 
'
post-roll
'
,

  
title
:
 
'
Segment Cross Domain IDs
'
,

  
publisher
:
 
'
Segment
'
,

  
position
:
 
11
,

  
total_length
:
 
11
,

  
load_type
:
 
'
dynamic
'


});



9) Playback finishes:

analytics
.
track
(
'
Video Playback Completed
'
,
 
{

  
session_id
:
 
'
12345
'
,

  
content_asset_id
:
 
null
,

  
content_pod_id
:
 
null
,

  
ad_asset_id
:
 
'
ad907
'
,

  
ad_pod_id
:
 
'
adSegB
'
,

  
ad_type
:
 
null
,

  
position
:
 
392
,

  
total_length
:
 
392
,

  
sound
:
 
88
,

  
bitrate
:
 
100
,

  
full_screen
:
 
false
,

  
video_player
:
 
'
youtube
'
,

  
ad_enabled
:
 
false
,

  
quality
:
 
'
hd1080
'


});



Below is an example of how a playback that has three mid-roll ads interspersed within the content:
  



This page was last modified: 25 Oct 2023

Need support?

Questions? Problems? Need more info? Contact Segment Support for assistance!

Help improve these docs!

Was this page helpful?

Thanks for your feedback!

Can we improve this doc? 
Send us feedback!

Get started with Segment

On this page

Was this page helpful?

Thanks for your feedback!

Can we improve
 this doc? 
Send us feedback!

Product

For Developers

Company

Support

© 2025 Segment.io, Inc.

=== Content from https://segment.com/docs/connections/reverse-etl/reverse-etl-source-setup-guides/bigquery-setup/ ===

BigQuery Reverse ETL Setup
        

On this page

To set up your BigQuery source with Reverse ETL, you must 
construct a BigQuery role and service account
 and 
create a BigQuery source in the Segment app
.

BigQuery Reverse ETL sources support Segment's dbt extension

If you have an existing dbt account with a Git repository, you can use 
Segment’s dbt extension
 to centralize model management and versioning, reduce redundancies, and run CI checks to prevent breaking changes.

Constructing your own role or policy

You need to be an account admin to set up the Segment BigQuery connector as well as write permissions for the 
__segment_reverse_etl
 dataset.

There are two approaches you can take when granting Segment access to your BigQuery resources:

You can choose the approach that best suits your needs.

Grant Full Access

With this approach, use BigQuery predefined roles to create a service account for Segment to assume.

Enter your 
Service account name
 and a description of what the service account will do.

Grant Limited Access

With this approach, you can set up a custom role with the following permissions:

CREATE
 
SCHEMA
 
IF
 
NOT
 
EXISTS
 
`__segment_reverse_etl`
;



GRANT
 
`roles/bigquery.dataEditor`
 
ON
 
SCHEMA
 
`__segment_reverse_etl`
 
TO
 
"serviceAccount:<YOUR SERVICE ACCOUNT EMAIL>"
;



BigQuery resource location

When connecting your BigQuery warehouse to Segment, you’ll need to know the location of your resources.

You can find the location of your BigQuery resources using the following method:

Set up BigQuery as your Reverse ETL source

After you’ve added BigQuery as a source, you can 
add a model
 and follow the rest of the steps in the Reverse ETL setup guide.

This page was last modified: 22 Oct 2024

Need support?

Questions? Problems? Need more info? Contact Segment Support for assistance!

Help improve these docs!

Was this page helpful?

Thanks for your feedback!

Can we improve this doc? 
Send us feedback!

Get started with Segment

On this page

Was this page helpful?

Thanks for your feedback!

Can we improve
 this doc? 
Send us feedback!

Product

For Developers

Company

Support

© 2025 Segment.io, Inc.

=== Content from https://segment.com/docs/protocols/validate/connect-sources/ ===

Connect a Tracking Plan
        

Protocols is available as an add-on for Business plans only.


See the 
available plans
, or 
contact Support
.

On this page

With your Tracking Plan complete, it’s time to apply the Tracking Plan to one or more Sources. Select 
Connect Source
 from the right hand menu for your specific Tracking Plan.



From this menu, you will be redirected to a workflow to select a Source from your workspace. Note that a Source can only have one tracking plan applied to it. You 
can’t
 select a Source that already has a Tracking Plan connected to it, but you 
can
 apply a Tracking Plan to multiple sources.

After selecting a Source, you will be shown the consequences of connecting your Tracking Plan.

IMPORTANT: Make sure to read through the consequences of connecting a source!



Disconnect Source from Tracking Plan

To disconnect the Source from the Tracking Plan, go to the Tracking Plan overview page, locate the column for the tracking plan you want to disconnect, then click the icon under the 
Connected Sources
. In the settings that appear, click 
Disconnect
 next to the Source you want to disconnect.



This page was last modified: 03 Aug 2023

Need support?

Questions? Problems? Need more info? Contact Segment Support for assistance!

Help improve these docs!

Was this page helpful?

Thanks for your feedback!

Can we improve this doc? 
Send us feedback!

Get started with Segment

On this page

Was this page helpful?

Thanks for your feedback!

Can we improve
 this doc? 
Send us feedback!

Product

For Developers

Company

Support

© 2025 Segment.io, Inc.

=== Content from https://segment.com/docs/segment-app/iam/membership/ ===

Manage Workspace Access
        

Advanced Access Management is available for all Business plans.


See the 
available plans
, or 
contact Support
.

On this page

This page explains how to add 
Team Members
 and 
User Groups
 to your team’s workspace, how to assign them 
roles
, and how to remove them.

Note
: Only Workspace Owners can change a workspace’s Access Management settings.

Invite a new team member

Note
: Workspaces that are on Free or Team plans can only grant the 
workspace owner
 and 
source admin
 roles.

Change a team member’s access

Tip
: You can also grant the role for all current and future resources. For admin roles, this includes the ability to create new resources.

Create a new User Group

Workspace Owners can manage permissions for groups of team members who should have similar access using “user groups”.

Add a team member to a User Group

You can make changes to group membership from two places in the Segment App: From the Edit Team Member page (the user’s individual access page), and from the Edit User Group page, where you can see all members of the group.



To add a team member from the 
Edit Team Member
 page:

Tip
: This method is best when adding a 
single
 team member to one or more user groups.



To add a team member from the 
Edit User Group
 page:

Tip
: This method is best when adding more than one user to a single user group at the same time

Remove a team member from a User Group

Remove a team member from your workspace

Open the member details and click 
Remove Team Member
 at the top.

Team Management with Single Sign On

If you are on a Business plan and choose to use Single Sign On (SSO), you grant implicit access to your workspace by assigning team members access to Segment from your identity provider. By default, you grant these users 
minimal workspace access
.

Segment supports “Just-In-Time” user provisioning using SSO. Any users with access to the application as defined in your IDP can seamlessly create a Segment account when they first log in. By default, all automatically-provisioned users created this way are created as Workspace Members with Minimal Workspace Access.

Once they have been created, Workspace Owners can update these users’ access from the 
Access Management
 page in the Segment App.

Segment does not support programmatic de-provisioning at this time. However, if your workspace uses SSO, a user that cannot authenticate to your IDP cannot view or edit any of your workspaces or their contents.

Request Access

If you are a 
workspace member
, you might encounter a section of the Segment App that you do not have access to view. If you need expanded permissions, you can request access directly in the Segment App. Once submitted, Access requests are sent to all workspace owners by email.

To review an access request, workspace owners click the link in the access request email to go to their workspace’s Access Management Settings. The requestor’s access request message appears on the Segment Access Management page, and the workspace owner can adjust the user’s permissions. The access request message disappears after the permissions are updated.

Remove invalid or expired Invite

When you send an invitation to an incorrect email address or the token included in the email invite link expires, the invite might still show up as “Invite Pending” in the Segment App. In these cases, you can revoke the invite to remove it and resend the invite if needed. The invitation will expire within a few days. Therefore, if a user accepts an invite with an expired link, Segment does not grant them access.

To revoke invite:

This page was last modified: 03 Aug 2023

Need support?

Questions? Problems? Need more info? Contact Segment Support for assistance!

Help improve these docs!

Was this page helpful?

Thanks for your feedback!

Can we improve this doc? 
Send us feedback!

Get started with Segment

On this page

Was this page helpful?

Thanks for your feedback!

Can we improve
 this doc? 
Send us feedback!

Product

For Developers

Company

Support

© 2025 Segment.io, Inc.

=== Content from https://segment.com/docs/guides/how-to-guides/create-push-notification/ ===

Creating a Push Notification
        

Like emails, push notifications are an extremely powerful way to re-engage customers on mobile apps. Push notifications are personal, so targeting them precisely using customer behavioral data (from Segment) is crucial.

For example, 
Wanelo
 accepts direct product feeds from retailers. For any of these retailers, when a product goes on sale, they can send a push notification to the people who have saved that product in their profile.

Push messaging focuses around three key features:

Content
: Diversify your messaging just as you would with an investment portfolio. you want to target your consumers with right content and avoid opt out for push. For example, Netflix uses push notifications to let users know when their favorite shows are available. Rather than sending every user a notification every time any new show or season is released.

Frequency
: Consider your App Store Category. News/Sports apps send push notifications daily or multiple times a day if it’s “game day”. So do Social Networking/Messaging apps. However, apps that are utilitarian, for example, food and drink, health and fitness, or productivity only message when necessary.

Timing
: Always send push notifications to users in their local timezone. In general, mobile usage peaks between 6pm - 10pm.

Choose a destination

Self evaluate when trying to choose a destination that suits your needs.

You will find many alternatives, but choosing the right one for your app is important!

Key metrics for a successful push

Ask users to opt in to push notifications upon app install or after the first time they use an app, so it’s easier to be transparent about how users can opt out later.

Let your customers decide what notifications they want to receive. It may help to break up your notifications into categories so you can empower your customers with this decision.

Creating lists of your app users based on characteristics or events that align to specific campaigns will help you better target your mobile marketing efforts.

Make sure to use deep linking to guide users to the specific screen relevant to that offer.

Pay attention to user time zones and customize messages based on time of year (holidays) to make brand personable.

The ideal frequency depends on the type of app you have.

Test different action words, phrases, message lengths, and more.

To “auto-enroll” new users into existing campaigns.

Don’t silo the success of your campaign to just app opens.

This page was last modified: 10 Oct 2023

Need support?

Questions? Problems? Need more info? Contact Segment Support for assistance!

Help improve these docs!

Was this page helpful?

Thanks for your feedback!

Can we improve this doc? 
Send us feedback!

Get started with Segment

Was this page helpful?

Thanks for your feedback!

Can we improve
 this doc? 
Send us feedback!

Product

For Developers

Company

Support

© 2025 Segment.io, Inc.

=== Content from https://segment.com/docs/connections/functions/ ===

Functions Overview
        

On this page

Functions let you create your own sources and destinations directly within your workspace to bring new types of data into Segment and send data to new tools with just a few lines of JavaScript - no additional infrastructure required.

Functions is available to all customer plan types with a free allotment of usage hours. Read more about 
Functions usage limits
, or see 
your workspace’s Functions usage stats
.



What can you do with Functions?

Functions can help you bring external data into Segment (
Source functions
) and send data in Segment out to external destinations (
Destination functions
). Use 
Insert functions
 to transform data before it reaches your downstream destinations. Functions are scoped to your specific workspace. If you’re a technology partner and want to build a new integration and publish it in Segment’s catalog, see the 
Developer Center documentation
.

Source functions receive external data from a webhook and can create Segment events, objects, or both. Source functions have access to the full power of JavaScript so you can validate and transform the incoming data and even make external API requests to annotate your data.

Use cases:

Learn more about 
source functions
.

Destination functions can take events from a Segment source, transform the events, and deliver them to external APIs. Destination functions can make arbitrary requests to annotate data, as well.

Use cases:

Learn more about 
destination functions
.

Destination insert functions help you enrich your data with code before you send it to downstream destinations.

Use cases:

To learn more, visit 
destination insert functions
.

With Functions Copilot, you can instrument custom integrations, enrich and transform data, and even secure sensitive data nearly instantaneously without writing a line of code.

To learn more, visit the 
Functions Copilot documentation
.

IP Allowlisting uses a NAT gateway to route outbound Functions traffic from Segment’s servers to your destinations through a limited range of IP addresses, which can prevent malicious actors from establishing TCP and UDP connections with your integrations.

IP Allowlisting is available for customers on Business Tier plans.

To learn more, visit 
Segment’s IP Allowlisting documentation
.

This page was last modified: 04 Dec 2024

Need support?

Questions? Problems? Need more info? Contact Segment Support for assistance!

Help improve these docs!

Was this page helpful?

Thanks for your feedback!

Can we improve this doc? 
Send us feedback!

Get started with Segment

On this page

Was this page helpful?

Thanks for your feedback!

Can we improve
 this doc? 
Send us feedback!

Product

For Developers

Company

Support

© 2025 Segment.io, Inc.

=== Content from https://segment.com/docs/engage/content/sms/template/ ===

SMS Template
        

Engage Premier requires a Business tier account and includes Engage Foundations and Unify.


See the 
available plans
, or 
contact Support
.

On this page

Engage Premier entered an End of Sale (EOS) period effective  June 10, 2024. Existing Segment customers will continue to have access and support to Engage Premier until an end-of-life (EOL) date is announced. We recommend exploring the following pages in preparation of a migration or future MCM needs:

Twilio Marketing Campaigns

Preferred ISV Partners:

Airship Blog
 


Bloomreach Blog
 


Braze Blog
 


Insider Blog
 


Klaviyo Blog
 


Twilio Engage Foundations Documentation
 

Use Twilio Engage to build SMS message templates to include throughout your marketing campaigns.

You can build an SMS template and include personalized content in messages based on user profile traits. Once you build the SMS, Twilio Engage saves the template for you to preview, maintain, and reuse.

Use personalized SMS messages to connect with users in real-time, as they reach a specific step in a journey.

SMS template types

You can choose between two SMS template types:

Build an SMS message template

You must first configure your SMS service with Twilio to build an SMS template in Engage. Visit the 
onboarding steps
 for more on how to connect a Twilio account.

Follow these steps to build an SMS template:

Use the SMS Templates screen to preview and update existing SMS message templates.

Engage content validation

For all content editors in Engage, you’ll see alerts for any issues in your template, such as invalid profile traits or incorrect 
liquid syntax
. Engage both flags template issue(s), and displays recommended next steps. While you can save these templates, you must fix any issues before using them in Engage campaigns.

Test your SMS template

Send a test SMS message before you include it as a step in your Journey.

You can also test SMS templates 
directly within Journeys
 before you send them.

Personalize with merge tags

Personalize SMS content in Engage using profile traits as merge tags in your messages.

To personalize an SMS, click 
Merge Tags
 in the SMS builder and select the profile traits to include in your message.

Engage inserts the selected traits inside merge tags based on cursor placement in the message. This allows you to personalize each SMS you send to recipients. You can also use 
liquid templating
 to create dynamic content in the SMS editor.

To learn more about profile traits, visit Segment’s 
Computed Traits
 and 
SQL Traits
 documentation.

Configure Link Shortening

Use Link Shortening to send shorter, more manageable link URLs in your Engage SMS campaigns.

Configure Link Shortening in your 
Twilio Console
 in six steps:

Once you’ve configured Link Shortening, Twilio automatically shortens the link URLs for recipients of your SMS messages. Link shortening occurs during the message sending process, so shortened links don’t appear in the message editor.

Link Shortening is only available for SMS messages.

Working with SMS message templates

You can edit, duplicate, and delete SMS templates within your Engage workspace.

Edit an SMS message template

To edit an SMS template:

Duplicate an SMS message template

To duplicate an SMS template:

After you duplicate a template, you can edit it from the Templates page.

Delete an SMS message template

To delete an SMS template:

SMS best practices and limitations

Include an SMS opt-out message

When you build an SMS, include an opt-out message in the body of your text that informs recipients they can unsubscribe from a message channel.

When an SMS recipient replies “Stop” to an SMS, they’ll receive an opt-out confirmation, and Engage updates their phone number subscription status. Visit the 
User Subscription States
 documentation to learn more about user subscriptions in Engage.

SMS character limit

Note that there’s a 1,600 character count limit for SMS messages.
Visit Twilio’s 
SMS Character Limit
 documentation for more information.

Next steps

Use the Templates screen in Twilio Engage to 
build personalized email templates
.

This page was last modified: 15 Jul 2024

Need support?

Questions? Problems? Need more info? Contact Segment Support for assistance!

Help improve these docs!

Was this page helpful?

Thanks for your feedback!

Can we improve this doc? 
Send us feedback!

Get started with Segment

On this page

Was this page helpful?

Thanks for your feedback!

Can we improve
 this doc? 
Send us feedback!

Product

For Developers

Company

Support

© 2025 Segment.io, Inc.

=== Content from https://segment.com/docs/getting-started/04-full-install/ ===

A Full Segment Implementation
        

On this page

Related content

Before you start implementing from your tracking plan, let’s review the Segment methods, what they do, and when you should use each.

Segment methods in detail

Segment’s libraries generate and send messages to our tracking API in JSON format, and provide a standard structure for the basic API calls. We also provide recommended JSON structure (also known as a schema, or ‘Spec’) that helps keep the most important parts of your data consistent, while allowing great flexibility in what other information you collect and where.

There are six calls in the basic tracking API, which answer specific questions:

Among these calls, you can think of Identify, Group, and Alias as similar types of calls, all to do with updating our understanding of the user who is triggering Segment messages. You can think of these calls as adding information to, or updating an object record in a database. 
Objects
 are described using “traits”, which you can collect as part of your calls.

The other three, Track, Page, and Screen, can be considered as increasingly specific types of events. 
Events
 can occur multiple times, but generate separate records which append to a list, instead of being updated over time.

A Track call is the most basic type of call, and can represent any type of event. Page and Screen are similar and are triggered by a user viewing a page or screen, however Page calls can come from both web and mobile-web views, while Screen calls 
only
 occur on mobile devices. Because of the difference in platform, the context information collected is very different between the two types of calls.

Segment recommends that you always use the Page and Screen calls when recording a page-view, rather than creating a “Page Viewed” Track event, because the Page/Screen calls automatically collect more contextual information.

Anatomy of a Segment message

The most basic Segment message requires only a 
userID
 or 
anonymousID
; all other fields are optional to allow for maximum flexibility. However, a normal Segment message has three main parts: the 
common fields
, the 
“context” object
, and the properties (if it’s an event) or traits (if it’s an object).

The common fields include information specific to how the call was generated, like the timestamp and library name and version. The fields in the context object are usually generated by the library, and include information about the environment in which the call was generated: page path, user agent, OS, locale settings, etc. The properties and traits are optional and are where you customize the information you want to collect for your implementation.

Another common part of a Segment message is 
the 
integrations
 object
, which you can use to explicitly filter which destinations the call is forwarded to. However this object is optional, and is often omitted in favor of non-code based filtering options.

Identify calls

analytics
.
identify 
(
user_id
:
 
"
12345abcde
"
,

  
traits
:
 
{

    
email
:
 
'
michael.phillips@segment.com
'
,

    
name
:
 
'
Michael Phillips
'
,

    
city
:
 
'
New York
'
,

    
state
:
 
'
NY
'
,

    
internal
:
 
True
 
})



The Identify call allows Segment to know 
who
 is triggering an event.

When to call Identify

Call Identify when the user first provides identifying information about themselves (usually during log in), or when they update their profile information.

When called as part of the login experience, you should call Identify as soon as possible after the user logs in. When possible, follow the Identify call with a Track event that records what caused the user to be identified.

When you make an Identify call as part of a profile update, you only need to send the changed information to Segment. You can send all profile info on every Identify call if that makes implementation easier, but this is optional.

Learn More

Best Practices for Identifying Users

Traits in Identify calls

These are called 
traits
 for Identify calls, and 
properties
 for all other methods.

The most important trait to pass as part of the Identify call is userId
, which uniquely identifies a user across all applications.

You should use a hash value to ensure uniqueness, although other values are acceptable; for example, email address isn’t the best thing to use as a userid, but is usually acceptable since it will be unique, and doesn’t change often.

Beyond that, the Identify call is your opportunity to provide information about the user that can be used for future reporting, so you should try to send any fields that you might want to report on later.

Consider using Identify and traits when:

How to Call Identify

You can call Identify from any of Segment’s device-based or server-based libraries, including 
Javascript
, 
iOS
, 
Android
, 
Ruby
, and 
Python
.

Here are two examples of calling Identify from two different libraries:

analytics
.
identify
(
"
12345abcde
"
,
 
{

  
"
email
"
:
 
"
michael.phillips@segment.com
"
,

  
"
name
"
:
 
"
Michael Phillips
"
,

  
"
city
"
:
 
"
New York
"
,

  
"
state
"
:
 
"
NY
"
,

  
"
internal
"
:
 
True


});



analytics
.
identify
(
 
user_id: 
"12345abcde"
,

  
traits: 
{

     
email: 
'michael.phillips@segment.com'
,

     
name: 
'Michael Phillips'
,

     
city: 
'New York'
,

     
state: 
'NY'
,

     
internal: 
True
 
})



Using analytics.reset()

When a user explicitly signs out of one of your applications, you can call 
analytics.reset()
 to stop logging further event activity to that user, and create a new 
anonymousId
 for subsequent activity (until the user logins in again and is subsequently identify-ed). 
This call is most relevant for client-side Segment libraries
, as it clears cookies in the user’s browser.

Make a 
reset()
 call as soon as possible after sign-out occurs, and only after it succeeds (not immediately when the user clicks sign out). For more info on this call, 
see the JavaScript source documentation
.

Page and Screen

The Page and Screen calls tell Segment what web page or mobile screen the user is on. This call automatically captures important context traits, so you don’t have to manually implement and send this data.

Page and Screen call properties

You can always 
override the auto-collected Page/Screen properties
 with your own, and set additional custom page or screen properties.

Some downstream tools (like 
Marketo
) require that you attach specific properties (like email address) to every Page call.

This is considered a destination-specific implementation nuance, and you should check the documentation for each destination you plan to use and make a list of these nuances before you start implementation.

Named Page & Screen Calls

You can specify a page “Name” at the start of the page or Screen call, which is especially useful to make list of page names into something more succinct for analytics. For example, on an ecommerce site you might want to call 
analytics.page( "Product" )
 and then provide properties for that product:

analytics
.
page
(
"
Product
"
,
 
{

  
"
category
"
:
 
"
Smartwatches
"
,

  
"
sku
"
:
 
"
13d31
"


});



[[
SEGAnalytics
 
sharedAnalytics
]
 
screen
:
@"Product"


properties:
@{
 
@"category"
:
 
@"Smartwatches"
,
 
@"sku"
:
 
@"13d31"
 
}];

 
}
];



When to Call Page

Segment automatically calls a Page event whenever a web page loads. This might be enough for most of your needs, but if you change the URL path without reloading the page, for example in single page web apps, you must call Page manually .

If the presentation of user interface components don’t substantially change the user’s context (for example, if a menu is displayed, search results are sorted/filtered, or an information panel is displayed on the exiting UI) 
measure the event with a Track call, not a Page call.

When you manually trigger a Page call, make sure the call happens 
after
 the UI element is successfully displayed, not when it is called. It shouldn’t be called as part of the click event that initiates it.

For more info on Page calls, review 
Page spec
 and 
Analytics.js docs
.

When to call Screen

Segment Screen calls are essentially the Page method, except for mobile apps. Mobile Screen calls are treated similarly to standard Page tracking, only they contain more context traits about the device. The goal is to have as much consistency between web and mobile as is feasible.

Track calls

The Track call allows Segment to know 
what
 the user is doing.

When to call Track

The Track call is used to track user and system events, such as:

Events and Properties

Your Track calls should include both events and properties. 
Events are the actions you want to track
, and 
properties are the data 
about
 the event that are sent with each event
.

Properties
 are powerful. They enable you to capture as much context about the event as you’d like, and then cross-tabulate or filter your downstream tools. For example, let’s say an eLearning website is tracking whenever a user bookmarks an educational article on a page. Here’s what a robust analytics.js Track call could look like:

analytics
.
track
(
'
Article Bookmarked
'
,
 
{

  
"
title
"
:
 
'
How to Create a Tracking Plan
'
,

  
"
course
"
:
 
'
Intro to Data Strategy
'
,

  
"
author
"
:
 
'
Dr. Anna Lytics
'
,

  
"
publish_year
"
:
 
'
2019
'
,

  
"
publish_month
"
:
 
'
03
'
,

  
"
length
"
:
 
'
Medium - 1000-2000 words
'
,

  
"
assets
"
:
 
{
'
Infographics
'
,
'
Interactive Charts
'
},

  
"
topics
"
:
 
{
'
Data Planning
'
,
'
Segment
'
,
'
Data Flow
'
},

  
"
button_location
"
:
 
'
Subheader - 3rd Column
'


});



With this Track call, we can analyze which authors had the most popular articles, which months and years led to the greatest volume of bookmarking overall, which button locations drive the most bookmark clicks, or which users gravitate towards infographics related to Data Planning.

Event Naming Best Practices

Each event you track must have a name that describes the event, like ‘Article Bookmarked’ above. That name is passed in at the beginning of the Track call, and should be standardized across all your properties so you can compare the same actions on different properties.

Segment’s best practice is to use an “Object Action” (Noun<>Verb) naming convention for all 
Track
 events, for example, ‘Article Bookmarked’.

Segment maintains a set of 
Business Specs
 which follow this naming convention around different use cases such as eCommerce, B2B SaaS, and Mobile.

Let’s dive deeper into the Object Action syntax that all Segment Track events should use.

Objects are Nouns

Nouns are the entities or objects that the user or the system acts upon.

It’s important to be thoughtful when naming objects so that they are referred to consistently within an application, and so that you refer to the same objects that might exist in multiple applications or sites by the same name.

Use the following list of objects to see if there is a logical match with your application. If you have objects that aren’t in this list, name it in a way that makes sense if it were to appear in other applications, and/or run it by Product Analytics.

Actions are Verbs

Verbs indicate the action taken by either a user on your site. When you name a new Track event, consider if you can describe the current interaction using a verb from the list below.

If you can’t, choose a verb that describes what the user is trying to do in your specific case, but try to be flexible enough so that you could use it in other scenarios.

Property naming best practices

Segment recommends that you record property names using 
snake case
 (for example 
property_name
), and that you format property values to match how they are captured. For example, a 
username
 value would be captured in whatever case it the user typed it in as.

Ultimately, you can decide to use a casing different from our recommendations; however, 
the single most important aspect is that you’re consistent across your entire tracking with one casing method
.

You can read more about 
best practices for Track calls
, .

All of the basic 
Segment methods
 have a common structure and common fields which are automatically collected on every call. You can see these in the 
common fields documentation
.

Common properties to send with a Track call

The following properties should be sent with every Track call:

How to call Track

You can make a Track call from any of Segment’s client-side or server-side libraries, including 
JavaScript
, 
iOS
, 
Android
, 
Ruby
, and 
Python
. Here are two examples of calling Track from two different libraries:

analytics
.
track
(
'
Article Bookmarked
'
,
 
{

  
"
title
"
:
 
'
How to Create a Tracking Plan
'
,

  
"
course
"
:
 
'
Intro to Data Strategy
'
,

  
"
author
"
:
 
'
Dr. Anna Lytics
'
,


});



analytics
.
track
(
 
user_id: 
'12345abcde'
,

  
event: 
'Article Bookmarked'
,

  
properties: 
{

     
title: 
'How to Create a Tracking Plan'
,

     
course: 
'Intro to Data Strategy'
,

     
author: 
'Dr. Anna Lytics'
})



Think through your goals, plan your calls, and set yourself up for success.

Unlock the power of Segment with Destinations.

This page was last modified: 07 Feb 2025

Further reading

Need support?

Questions? Problems? Need more info? Contact Segment Support for assistance!

Help improve these docs!

Was this page helpful?

Thanks for your feedback!

Can we improve this doc? 
Send us feedback!

Get started with Segment

On this page

Related content

Was this page helpful?

Thanks for your feedback!

Can we improve
 this doc? 
Send us feedback!

Product

For Developers

Company

Support

© 2025 Segment.io, Inc.

=== Content from https://segment.com/docs/connections/spec/best-practices-event/ ===

Best Practices for Event Calls
        

On this page

How do you to determine which type of call you should use? When should you make a Page call instead of a just using a Track call? In theory, you 
could
 put together a full implementation using only track events, but this is a bad idea - this page explains some of the things you should consider when deciding which call to use.

Segment 
strongly
 encourages you to follow 
the Spec
 for clarity and simplicity’s sake, however we also give you the flexibility to make only the calls that fit your needs. In the end, it is up to you.

What is the Spec?

Segment recommends that you follow 
the Spec
, which gives general guidance about which methods to use when. You might read about “semantic spec”, which simply means Page calls should be about the page you’re viewing, and Track calls should be about events or activities you want to track.

The Spec outlines the specific data you should collect with each type of call. Each call type represents and is intended to collect specific information about a user or their activities. This means that your choice of method can imply things about the data you intend to collect.

For example, the properties for 
page()
 and 
screen()
 calls are intended to describe the page, not the user or their actions. Similarly, the data automatically included in a 
page()
 in particularly is important for UTM campaign capture. 

Simplifying implementation

As we mentioned above, you 
could
 build a full Segment implementation using only Track events, and this is probably a bad idea. To do this, you would need to include page-related data in every Track call, which means adding all of the information that Page calls automatically include, except now manually as event properties. As you might imagine, this gets unwieldy fast!

It’s better to pair a Page and a Track call together (making one of each call), especially if you have a complex tracking implementation. When you use the semantic methods you reduce the amount of information and other properties required in a single call.

Ensuring destination compatibility

The 
track()
 call, and 
page()
 or 
screen()
 calls are handled very differently by your downstream tools, and how you can use that data is different. When you use the Spec, Segment uses the call type to help translate the data into destination’s tracking format. This ensures the highest level of compatibility with the end tools.

Some destinations were built around a specific call type and Segment maps to those specific methods. Some downstream tools do not accept 
page()
 and 
screen()
 calls at all. Many of the destinations that 
do
 accept these calls, also expect a 
limited
 range of data in a 
page()
 call, and may not properly receive or handle data that would be expected in 
track()
 calls.

To help you with this, the Segment documentation includes 
a list of all of the supported destinations and the calls they accept
.

Filtering data by purpose

Finally, when you use the different methods correctly, it can help you separate out “types” of information in your downstream tools and warehouses, so you can use them for different purposes.

This page was last modified: 21 Nov 2023

Need support?

Questions? Problems? Need more info? Contact Segment Support for assistance!

Help improve these docs!

Was this page helpful?

Thanks for your feedback!

Can we improve this doc? 
Send us feedback!

Get started with Segment

On this page

Was this page helpful?

Thanks for your feedback!

Can we improve
 this doc? 
Send us feedback!

Product

For Developers

Company

Support

© 2025 Segment.io, Inc.

=== Content from https://segment.com/docs/connections/functions/insert-functions/ ===

Destination Insert Functions
        

On this page

Use Destination Insert Functions to enrich, transform, or filter your data before it reaches downstream destinations.

Implement advanced data computation
: Write custom computation, operations, and business logic on streaming data that you send to downstream destinations.

Enrich your data
: Use destination insert functions with Segment’s Profile API or third party sources to add additional context to your data and create personalized customer experiences.

Support data compliance
: Use destination insert functions to support data masking, encryption, decryption, improved PII data handling, and tokenization.

Customize filtration for your destinations
: Create custom logic with nested if-else statements, regex, custom business rules, and more to filter event data.

Destination Insert Functions are not compatible with IP Allowlisting

For more information, see the 
IP Allowlisting
 documentation.

Create destination insert functions

There are two ways you can access destination insert functions from your Segment space:

Using the catalog

To create an insert function from Segment’s catalog:

For data to flow to your downstream destinations, you’ll need to connect your insert function to a destination:

Storage destinations are not compatible with Destination Insert Functions

You cannot connect an Insert Function to a storage destination at this time.

Using the Destinations tab

To access insert functions through the Destinations tab:

Use this page to edit and manage insert functions in your workspace.

You can also use this page to 
enable destination insert functions
 in your workspace.

Code the destination insert function

To prevent “Unsupported Event Type” errors, ensure your insert function handles all event types (page, track, identify, alias, group) that are expected to be sent to the destination. It is highly recommended to 
test the function
 with each event type to confirm they are being handled as expected.

Segment invokes a separate part of the function (called a “handler”) for each event type that you send to your destination insert function.

If you’ve configured a 
destination filter
 and the event doesn’t pass the filter, then your function isn’t invoked for that event as Segment applies destination filters before insert functions. The same is true for the 
integrations object
). If an event is configured with the integrations object not to go to a particular destination, then the insert function connected to that destination won’t be invoked.

The default source code template includes handlers for all event types. You don’t need to implement all of them - just use the ones you need, and skip the ones you don’t. For event types that you want to send through the destination, return the event in the respective event handlers.

Removing the handler for a specific event type results in blocking the events of that type from arriving at their destination. To keep an event type as is but still send it downstream, add a 
return event
 inside the event type handler statement.

Insert functions can define handlers for each message type in the 
Segment spec
:

Each of the functions above accepts two arguments:

The example below shows a function that listens for “Track” events, and sends some details about them to an external service.

async
 
function
 
onTrack
(
event
)
 
{

  
await
 
fetch
(
'
https://example-service.com/api
'
,
 
{

    
method
:
 
'
POST
'
,

    
headers
:
 
{

      
'
Content-Type
'
:
 
'
application/json
'

    
},

    
body
:
 
JSON
.
stringify
({

      
event_name
:
 
event
.
event
,

      
event_properties
:
 
event
.
properties
,

      
timestamp
:
 
event
.
timestamp

    
})

  
})


  
return
 
event
;

  

}



To change which event type the handler listens to, you can rename it to the name of the message type. For example, if you rename this function 
onIdentify
, it listens for “Identify” events instead.

To ensure the Destination processes an event payload modified by the function, return the 
event
 object at the handler’s end.

Functions’ runtime includes a 
fetch()
 polyfill using a 
node-fetch
 package. Check out the 
node-fetch documentation
 for usage examples.

Errors and error handling

Segment considers a function’s execution successful if it finishes without error. You can 
throw
 an error to create a failure on purpose. Use these errors to validate event data before processing it to ensure the function works as expected.

You can 
throw
 the following pre-defined error types to indicate that the function ran as expected, but the data was not deliverable:

The examples show basic uses of these error types.

async
 
function
 
onGroup
(
event
)
 
{

  
if 
(
!
event
.
traits
.
company
)
 
{

    
throw
 
new
 
InvalidEventPayload
(
'
Company name is required
'
)

  
}


}



async
 
function
 
onPage
(
event
)
 
{

  
if 
(
!
event
.
properties
.
pageName
)
 
{

    
throw
 
new
 
ValidationError
(
'
Page name is required
'
)

  
}


}



async
 
function
 
onAlias
(
event
)
 
{

  
throw
 
new
 
EventNotSupported
(
'
Alias event is not supported
'
)


}



async
 
function
 
onTrack
(
event
)
 
{

  
let
 
res

  
try
 
{

    
res
 
=
 
await
 
fetch
(
'
http://example-service.com/api
'
,
 
{

      
method
:
 
'
POST
'
,

      
headers
:
 
{

        
'
Content-Type
'
:
 
'
application/json
'

      
},

      
body
:
 
JSON
.
stringify
({
 
event
 
})

    
})


    
return
 
event
;

    
  
}
 
catch 
(
err
)
 
{

    
// Retry on connection error

    
throw
 
new
 
RetryError
(
err
.
message
)

  
}

  
if 
(
res
.
status
 
>=
 
500
 
||
 
res
.
status
 
===
 
429
)
 
{

    
// Retry on 5xx and 429s (ratelimits)

    
throw
 
new
 
RetryError
(
`HTTP Status 
${
res
.
status
}
`
)

  
}


}



async
 
function
 
onIdentify
(
event
)
 
{

  
if 
(
event
.
traits
.
companyName
)
 
{

    
// Drop Event | Do NOT forward event to destination

    
throw
 
new
 
DropEvent
(
'
Company name is required
'
)

  
}

  
return
 
event
;


}




If you don’t supply a function for an event type, Segment throws an 
EventNotSupported
 error by default.

You can read more about 
error handling
 below.

Runtime and dependencies

On March 26, 2024, Segment is upgrading the Functions runtime environment to Node.js v18, which is the current long-term support (LTS) release.

This upgrade keeps your runtime current with industry standards. Based on the 
AWS Lambda
 and 
Node.js
 support schedule, Node.js v16 is no longer in 
Maintenance LTS
. Production applications should only use releases of Node.js that are in 
Active LTS
 or 
Maintenance LTS
.

All new functions will use Node.js v18 starting March 26, 2024.

For existing functions, this change automatically occurs as you update and deploy an existing function. Segment recommends that you check your function post-deployment to ensure everything’s working. Your function may face issues due to the change in sytax between different Node.js versions and dependency compatibility.

Limited time opt-out option 


If you need more time to prepare, you can opt out of the update before March 19, 2024. 
 Note that if you opt out: 

- The existing functions will continue working on Node.js v16. 

- You won’t be able to create new functions after July 15, 2024. 

- You won’t be able to update existing functions after August 15, 2024. 

- You won’t receive future bug fixes, enhancements, and dependency updates to the functions runtime. 


Contact Segment
 to opt-out or with any questions. 

Node.js 18 

Segment strongly recommends updating to Node.js v18 to benefit from future runtime updates, the latest security, and performance improvements.

Functions do not currently support importing dependencies, but you can 
contact Segment Support
 to request that one be added.

The following dependencies are installed in the function environment by default.

zlib v1.0.5
 exposed as 
zlib.zlib

 
uuidv5
 is exposed as an object. Use 
uuidv5.uuidv5
 to access its functions. For example:

  
async
 
function
 
onRequest
(
request
,
 
settings
)
 
{

       
uuidv5
 
=
 
uuidv5
.
uuidv5
;

       
console
.
log
(
typeof
 
uuidv5
);


        
//Generate a UUID in the default URL namespace

        
var
 
urlUUID
 
=
 
uuidv5
(
'
url
'
,
 
'
http://google/com/page
'
);

        
console
.
log
(
urlUUID
);


        
//Default DNS namespace

        
var
 
dnsUUID
 
=
 
uuidv5
(
'
dns
'
,
 
'
google.com
'
);

        
console
.
log
(
dnsUUID
);

    
}



zlib
’s asynchronous methods 
inflate
 and 
deflate
 must be used with 
async
 or 
await
. For example:

zlib
 
=
 
zlib
.
zlib
;
  
// Required to access zlib objects and associated functions


async
 
function
 
onRequest
(
request
,
 
settings
)
 
{

  
const
 
body
 
=
 
request
.
json
();


  
const
 
input
 
=
 
'
something
'
;


  
// Calling inflateSync method

  
var
 
deflated
 
=
 
zlib
.
deflateSync
(
input
);


  
console
.
log
(
deflated
.
toString
(
'
base64
'
));


  
// Calling inflateSync method

  
var
 
inflated
 
=
 
zlib
.
inflateSync
(
new
 
Buffer
.
from
(
deflated
)).
toString
();


  
console
.
log
(
inflated
);


  
console
.
log
(
'
Done
'
);

  
}



The following Node.js modules are available:

Other built-in Node.js modules
 aren’t available.

For more information on using the 
aws-sdk
 module, see how to 
set up functions for calling AWS APIs
.

Caching

Basic cache storage is available through the 
cache
 object, which has the following methods defined:

Some important notes about the cache:

const
 
ttl
 
=
 
5
 
*
 
60
 
*
 
1000
 
// 5 minutes


const
 
val
 
=
 
await
 
cache
.
load
(
"
mycachekey
"
,
 
ttl
,
 
async 
()
 
=>
 
{

    
const
 
res
 
=
 
await
 
fetch
(
"
http://echo.jsontest.com/key/value/one/two
"
)

    
const
 
data
 
=
 
await
 
res
.
json
()

    
return
 
data


})



Insert Functions and Actions destinations

A payload must come into the pipeline with the attributes that allow it to match your mapping triggers. You can’t use an Insert Function to change the event to match your mapping triggers. If an event comes into an Actions destination and already matches a mapping trigger, that mapping subscription will fire. If a payload doesn’t come to the Actions destination matching a mapping trigger, even if an Insert Function is meant to alter the event to allow it to match a trigger, it won’t fire that mapping subscription. Segment sees the mapping trigger first in the pipeline, so a payload won’t make it to the Insert Function at all if it doesn’t come into the pipeline matching a mapping trigger.

Unlike Source Functions and Destination Functions, which return multiple events, an Insert Function only returns one event. When the Insert Function receives an event, it sends the event to be handled by its configured mappings.

If you would like 
multiple mappings triggered by the same event
:

You can also configure the Insert Function to add additional data to the event’s payload before it’s handled by the mappings and configure the mapping’s available fields to reference the payload’s available fields.

You may want to consider the 
context object’s
 available fields when adding new data to the event’s payload.

Create settings and secrets

Settings allow you to pass configurable variables to your function, which is the best way to pass sensitive information such as security tokens. For example, you might use 
settings
 as placeholders to use information such as an API endpoint and API key. This way, you can use the same code with different settings for different purposes. When you deploy a function in your workspace, you are prompted to fill out these settings to configure the function.

First, add a setting in 
Settings
 tab in the code editor:



Click 
Add Setting
 to add your new setting.



You can configure the details about this setting, which change how it’s displayed to anyone using your function:

As you change the values, a preview to the right updates to show how your setting will look and work.

Click 
Add Setting
 to save the new setting.

Once you save a setting, it appears in the 
Settings
 tab for the function. You can edit or delete settings from this tab.



Next, fill out this setting’s value in the 
Test
 tab, so you can run the function and verify that the correct setting value is passed. (This value is only for testing your function.)

Now that you’ve configured a setting and entered a test value, you can add code to read its value and run the function, as in the example below:

async
 
function
 
onTrack
(
request
,
 
settings
)
 
{

  
const
 
apiKey
 
=
 
settings
.
apiKey

  
//=> "super_secret_string"


}



When you deploy your destination insert function in your workspace, you fill out the settings on the destination configuration page, similar to how you would configure a normal destination.

Test the destination insert function

You can manually test your code from the functions editor:

The Event Tester and Mapping Tester don’t support Insert Functions. They won’t apply an Insert Function, show its impact on your data, or send data through the Insert Function pipeline. Use the Function Tester instead to evaluate how your Insert Function affects your data.

Save and deploy the destination insert function

Once you finish building your insert function, click 
Next: Configure & Create
 to name it, then click 
Create Function
 to save it.

Once you do that, you’ll see the insert function from the Functions page in your catalog.

If you’re editing an existing function, you can save changes without updating the instances of the function that are already deployed and running.

You can also choose to 
Save & Deploy
 to save the changes, then choose which already-deployed functions to update with your changes.

You may need additional permissions to update existing functions.

Enable the destination insert function

You need to enable your insert function for it to process your data.

To enable your insert function:

To prevent your insert function from processing data, toggle Enable Function off.

Batching the destination insert function

Batch handlers are an extension of insert functions. When you define an 
onBatch
 handler alongside the handler functions for single events (for example, 
onTrack
 or 
onIdentity
), you’re telling Segment that the insert function can accept and handle batches of events.

Batching is available for destination and destination insert functions only.

When to use batching

Consider creating a batch handler if:

You have a high-throughput function and want to reduce cost.
 When you define a batch handler, Segment invokes the function once per 
batch
, rather than once per event. As long as the function’s execution time isn’t adversely affected, the reduction in invocations should lead to a reduction in cost.

Your destination supports batching
. When your downstream destination supports sending data downstream in batches you can define a batch handler to avoid throttling. Batching for functions is independent of batch size supported by the destination. Segment automatically handles batch formation for destinations.

If a batched function receives too low a volume of events (under one event per second) to be worth batching, Segment may not invoke the batch handler.

Define the batch handler

Segment collects the events over a short period of time and combines them into a batch. The system flushes them when the batch reaches a certain number of events, or when the batch has been waiting for a specified wait time.

To create a batch handler, define an 
onBatch
 function within your destination insert function. You can also use the “Default Batch” template found in the Functions editor to get started quickly.

async
 
function
 
onBatch
(
events
,
 
settings
){

  
// handle the batch of events

  
return
 
events


}



The 
onBatch
 handler is an optional extension. Destination insert functions must still contain single event handlers as a fallback, in cases where Segment doesn’t receive enough events to execute the batch.

The handler function receives an array of events. The events can be of any supported type and a single batch may contain more than one event type. Handler functions can also receive function settings. Here is an example of what a batch can look like:

[

    
{

      
"type"
:
 
"identify"
,

      
"userId"
:
 
"019mr8mf4r"
,

      
"traits"
:
 
{

        
"email"
:
 
"jake@yahoo.com"
,

        
"name"
:
 
"Jake Peterson"
,

        
"age"
:
 
26

      
}

    
},

    
{

      
"type"
:
 
"track"
,

      
"userId"
:
 
"019mr8mf4r"
,

      
"event"
:
 
"Song Played"
,

      
"properties"
:
 
{

        
"name"
:
 
"Fallin for You"
,

        
"artist"
:
 
"Dierks Bentley"

      
}

    
},

    
{

      
"type"
:
 
"track"
,

      
"userId"
:
 
"971mj8mk7p"
,

      
"event"
:
 
"Song Played"
,

      
"properties"
:
 
{

        
"name"
:
 
"Get Right"
,

        
"artist"
:
 
"Jennifer Lopez"

      
}

    
}


]



Configure the event types within a batch

Segment batches together any event 
of any type
 that it sees over a short period of time to increase batching efficiency and give you the flexibility to decide how batches are created. If you want to split batches by event type, you can implement this in your functions code by writing a handler.

async
 
function
 
onBatch
(
events
,
 
settings
)
 
{

  
// group events by type

  
const
 
eventsByType
 
=
 
{}

  
for 
(
const
 
event
 
of
 
events
)
 
{

    
if 
(
!
(
event
.
type
 
in
 
eventsByType
))
 
{

      
eventsByType
[
event
.
type
]
 
=
 
[]

    
}

    
eventsByType
[
event
.
type
].
push
(
event
)

  
}


  
// concurrently process sub-batches of a specific event type

  
const
 
promises
 
=
 
Object
.
entries
(
eventsByType
).
map
(([
type
,
 
events
])
 
=>
 
{

    
switch 
(
type
)
 
{

    
case
 
'
track
'
:

      
return
 
onTrackBatch
(
events
,
 
settings
)

    
case
 
'
identify
'
:

      
return
 
onIdentifyBatch
(
events
,
 
settings
)

    
// ...handle other event types here...

    
}

  
})

  
try
 
{

    
const
 
results
 
=
 
await
 
Promise
.
all
(
promises
);

    
const
 
batchResult
 
=
 
[].
concat
(...
results
);
 
// Combine arrays into a single array

    
return
 
batchResult
;

  
}
 
catch 
(
error
)
 
{

    
throw
 
new
 
RetryError
(
error
.
message
);

  
}


}



async
 
function
 
onTrackBatch
(
events
,
 
settings
)
 
{

  
// handle a batch of track events

  
return
 
events


}



async
 
function
 
onIdentifyBatch
(
events
,
 
settings
)
 
{

  
// handle a batch of identify events

  
return
 
events


}



Configure your batch parameters

By default, Functions waits up to 10 seconds to form a batch of 20 events. You can increase the number of events included in each batch (up to 400 events per batch) by contacting 
Segment support
. Segment recommends users who wish to include fewer than 20 events per batch use destination insert functions without the 
onBatch
 handler.

Test the batch handler

The 
Functions editing environment
 supports testing batch handlers.

To test the batch handler:

The Sample Event option tests single events only. You must use Manual Mode to add more than one event so you can test batch handlers.

The editor displays logs and request traces from the batch handler.

The 
Public API
 Functions/Preview endpoint also supports testing batch handlers. The payload must be a batch of events as a JSON array.

Handling filtering in a batch

Events in a batch can be filtered out using custom logic. The filtered events will be surfaced in the 
Event Delivery
 page with reason as 
Filtered at insert function

async
 
function
 
onBatch
(
events
,
 
settings
)
 
{

  
let
 
response
 
=
 
[];

  
try
 
{

    
for 
(
const
 
event
 
of
 
events
)
 
{

      
// some business logic to filter event. Here filtering out all the events with name `drop`

      
if 
(
event
.
properties
.
name
 
===
 
'
drop
'
)
 
{

        
continue
;

      
}


      
// some enrichments if needed

      
event
.
properties
.
message
 
=
 
"
Enriched from insert function
"
;


      
// Enriched events are pushed to response

      
response
.
push
(
event
);

    
}

  
}
 
catch 
(
error
)
 
{

    
console
.
log
(
error
)

    
throw
 
new
 
RetryError
(
'
Failed function
'
,
 
error
);

  
}


  
// return a subset of transformed event

  
return
 
response
;


}



Handling batching errors

Standard 
function error types
 apply to batch handlers. Segment attempts to retry the batch in the case of Timeout or Retry errors. For all other error types, Segment discards the batch.

Destination insert functions error types

Segment only attempts to send the event to your destination insert function again if a 
Retry
 error occurs.

You can view Segment’s list of 
Integration Error Codes
 for more information about what might cause an error.

Destination insert functions logs

If your function throws an error, execution halts immediately. Segment captures the event, any outgoing requests/responses, any logs the function might have printed, as well as the error itself.

Segment then displays the captured error information in the 
Event Delivery
 page for your destination. You can use this information to find and fix unexpected errors.

You can throw 
an error or a custom error
 and you can also add helpful context in logs using the 
console
 API
. For example:

async
 
function
 
onTrack
(
event
,
 
settings
)
 
{

  
const
 
userId
 
=
 
event
.
userId


  
console
.
log
(
'
User ID is
'
,
 
userId
)


  
if 
(
typeof
 
userId
 
!==
 
'
string
'
 
||
 
userId
.
length
 
<
 
8
)
 
{

    
throw
 
new
 
ValidationError
(
'
User ID is invalid
'
)

  
}


  
console
.
log
(
'
User ID is valid
'
)


}



Don’t log sensitive data, such as personally-identifying information (PII), authentication tokens, or other secrets. Avoid logging entire request/response payloads. The 
Function Logs
 tab may be visible to other workspace members if they have the necessary permissions.

Caching in destination insert functions

Functions execute only in response to incoming data, but the environments that functions run in are generally long-running. Because of this, you can use global variables to cache small amounts of information between invocations. For example, you can reduce the number of access tokens you generate by caching a token, and regenerating it only after it expires. Segment cannot make any guarantees about the longevity of environments, but by using this strategy, you can improve the performance and reliability of your Functions by reducing the need for redundant API requests.

This example code fetches an access token from an external API and refreshes it every hour:

const
 
TOKEN_EXPIRE_MS
 
=
 
60
 
*
 
60
 
*
 
1000
 
// 1 hour


let
 
token
 
=
 
null


async
 
function
 
getAccessToken
 
()
 
{

  
const
 
now
 
=
 
new
 
Date
().
getTime
()

  
if 
(
!
token
 
||
 
now
 
-
 
token
.
ts
 
>
 
TOKEN_EXPIRE_MS
)
 
{

    
const
 
resp
 
=
 
await
 
fetch
(
'
https://example.com/tokens
'
,
 
{

      
method
:
 
'
POST
'

    
}).
then
(
resp
 
=>
 
resp
.
json
())

    
token
 
=
 
{

      
ts
:
 
now
,

      
value
:
 
resp
.
token

    
}

  
}

  
return
 
token
.
value


}



Managing destination insert functions

Functions permissions

Functions have specific roles which can be used for 
access management
 in your Segment workspace.

Access to functions is controlled by two permissions 
roles
:

You also need additional 
Source Admin
 permissions to enable source functions, connect destination functions to a source, or to deploy changes to existing functions.

Editing and deleting functions

If you are a 
Workspace Owner
 or 
Functions Admin
, you can manage your function from the 
Functions
 page.

Destination insert functions FAQs

Yes, Functions access is logged in the 
Audit Trail
, so user activity related to functions appears in the logs.

Yes, Segment retries invocations that throw RetryError or Timeout errors (temporary errors only). Segment’s internal system retries failed functions API calls for four hours with a randomized exponential backoff after each attempt. This substantially improves delivery rates.

Retries
 work the same for both functions and cloud-mode destinations in Segment.

No, Segment can’t guarantee the order in which the events are delivered to an endpoint.

No, specifying an endpoint is not always required for insert functions. If your function is designed to transform or filter data internally—such as adding new properties to events or filtering based on existing properties—you won’t need to specify an external endpoint.

However, if your function aims to enrich event data by fetching additional information from an external service, then you must specify the endpoint. This would be the URL of the external service’s API where the enriched or captured data is sent.

No, Destination Insert Functions are currently available for use with Cloud Mode (server-side) destinations only. Segment is in the early phases of exploration and discovery for supporting customer web plugins for custom Device Mode destinations and other use cases, but this is unsupported today.

Insert Functions are only supported by Cloud Mode (server-side) destinations and aren’t compatible with Storage destinations.

Yes, you can connect an insert function to multiple destinations.

No, you can only connect one insert function to a destination.

Yes, you can have both destination filters and destination insert functions in the same connection.

Segment’s data pipeline applies Destination Filters before invoking Insert Functions.

There is an 120-Character limit for the insert function display name.

This error occurs because your insert function code might not be handling all event types (Page, Track, Identify, Alias, Group) that your destination supports. When these unlisted events pass through the function, they are rejected with the “Unsupported Event Type” error.

To resolve this, verify your insert function includes handlers for all expected event types and returns the event object for each. Here’s an example of how you can structure your insert function to handle all event types:

async function onTrack(event, settings) {
    //Return event to handle page event OR Your existing code for track event
    return event;
}

async function onPage(event, settings) {
    //Return event to handle page event OR Your existing code for track event
    return event;
}

async function onIdentify(event, settings) {
    //Return event to handle page event OR Your existing code for track event
    return event;
}

async function onAlias(event, settings) {
    //Return event to handle page event OR Your existing code for track event
    return event;
}

async function onGroup(event, settings) {
  //Return event to handle page event OR Your existing code for track event
    return event;
}

// Ensure that all expected event types are included in your function


By including handlers for all the major event types, you ensure that all supported events are processed correctly, preventing the “Unsupported Event Type” error. Always test your updated code before implementing it in production.

The test function interface has a 4KB console logging limit. Outputs surpassing this limit won’t be visible in the user interface.

This page was last modified: 23 Jan 2025

Need support?

Questions? Problems? Need more info? Contact Segment Support for assistance!

Help improve these docs!

Was this page helpful?

Thanks for your feedback!

Can we improve this doc? 
Send us feedback!

Get started with Segment

On this page

Was this page helpful?

Thanks for your feedback!

Can we improve
 this doc? 
Send us feedback!

Product

For Developers

Company

Support

© 2025 Segment.io, Inc.

=== Content from https://segment.com/docs/guides/what-is-replay/ ===

Replay
        

Replay is available to all Business plans.


See the 
available plans
, or 
contact Support
.

On this page

Replay takes an archived copy of your Segment data, and re-sends it to new or existing tools providing huge benefits to mature data systems. By archiving and replaying data, you can avoid vendor lock-in, and protect your system against data loss.

Replays for tooling changes

With Replays, you can send your existing data to new tools.
This means you can send a limited sample of your data to a new tool to test it out, and run similar tools in parallel to verify the data format or accuracy of the output. Finally, when you’re ready to switch to a new tool, you can replay a full set of your data to the new tool to backfill it with data that extends before you set up the tool - no warm-up time or operational gap to disrupt your work.

Note

Any destinations which accept cloud-mode data (meaning data from Segment, and not directly from users’ devices) can use replay, however they must also process timestamps on the data for replay to be useful.

Replays for resilience

With Replays, you’re protected from outages and errors. If a destination which you rely on experiences an outage, or is temporarily unable to accept incoming data, you can use Replays to re-send data to that tool once the service recovers. You can also use Replays to recover from errors caused by misconfigurations in your Segment systems. For example, if you send data in the wrong format, or want to apply 
destination filters
. In this case, you can change your mapping using a destination filter, clear out the bad data, and replay it to that destination. You can also use this to update the schema in your data warehouse when it changes.

For more information, 
Contact us
 and our Success Engineers will walk you through the process.

Replays considerations

Replays are currently only available for Business Tier customers, and due to their complex nature are not self-serve. 
Contact us
 to learn more, or to request a replay for your workspace. When requesting a replay, include the workspace, the source to replay from, the destination tool or tools, and the time period.

Replays can process unlimited data, but they’re rate limited to respect limitations in downstream partner tools. If you’re also sending data to the destination being replayed to in real time, then, when determining your replay’s limit, you’ll want to take into account the rate limit being used by real-time events. You should also account for a small margin of your rate limit to allow events to be retried.

Replay time depends both on the tool Segment replays to and the amount of data included in the replay.

Replays do not affect your 
MTU count
, unless you are using a 
Repeater destination
. Notify your team before initiating a Replay if you’re using a Repeater destination.

Once a replay starts, you will not see replayed events in the Event Delivery tab.

You can initiate replays for some or all events, but you can’t apply conditional filters that exclude certain rows of data from being replayed. You can set up 
destination filters
 to conditionally filter replayed events.

The destination is not required to be enabled in order for a replay to be successful, including Destination Functions.

Replay-eligible destinations

Replays are available for any destinations which support cloud-mode data (meaning data routed through Segment) and which also process timestamps. Destinations that are only available in device-mode (meaning where data is sent directly from the users’ devices to the destination tool) cannot receive Replays.

Not all destinations support data deduplication, so you may need to delete, archive, or remove any older versions of the data before initiating a replay. 
contact Segment support
 if you have questions or want help.

Replays & Destination Filters

Replays are subject to the 
Destination Filters
 you’ve configured on that destination. For example, if you request that Identify calls be included in the replay, but your destination has a Destination Filter that blocks Identify events, the filter then blocks all Identify events from making it to the destination. In this case, Segment recommends that you avoid including Identify events in the replay if you know they’ll be blocked by the destination filter.

When you request a replay, Segment asks you to provide a list of the events (type and/or name) that you want included in the replay. If you specify a list of events, then Segment only includes those specified events in the replay. If you need to exclude events in your replay, 
contact Segment support
. The Segment team can help you handle filtering you’re unable to do in the replay.

Replays & Engage

There are two types of replays with Engage.

Replay a Profile Source’s data into Engage Space, (sending a standard source’s data into an Engage Space), which can be configured to send over a specified timeframe as well as the ability to specify all or only a specific subset of events by type or name.

Replay from an Engage Space to its connected destination, (sending data from an Engage Output Source to its connected destination), which includes all the computational data (Audiences, Computed Traits, Journeys) that destination is currently configured to receive, which can be configured to send over a specified timeframe as well as the ability to specify all or only a specific subset of events by type or name.

1. Replay a Profile Source’s data into Engage Space

2. Replay from an Engage Space to its connected destination

This page was last modified: 05 Jun 2024

Need support?

Questions? Problems? Need more info? Contact Segment Support for assistance!

Help improve these docs!

Was this page helpful?

Thanks for your feedback!

Can we improve this doc? 
Send us feedback!

Get started with Segment

On this page

Was this page helpful?

Thanks for your feedback!

Can we improve
 this doc? 
Send us feedback!

Product

For Developers

Company

Support

© 2025 Segment.io, Inc.

=== Content from https://segment.com/docs/getting-started/05-data-to-destinations/ ===

Sending data to destinations
        

On this page

Once you’ve got data flowing 
into
 Segment, what do you do with it? The Segment Destination catalog lists all of the places we can send your data.

Routing data to destinations

When you enable a destination in the Segment App, you link it to a specific source (or sources). By default, Segment first processes the data from the selected source(s), then translates it and routes it from the Segment servers to the API endpoint for that destination.

This means that if you previously had loaded code or a snippet for that tool on your website or app, you should remove it once you have Segment implemented so you don’t send duplicate data.

You might also want to enable tools that need to be loaded on the user’s device (either a computer or mobile device) in order to function properly. For our Analytics.js library, you can make these changes from the Segment App, and the Segment systems then update the bundle of code served when users request the page to include code required by the destination. You can read more about this in our 
documentation on Connection Modes
.

Adding new destinations

Adding a destination is quick and easy from the Segment App. You’ll need a token or API key for the tool, or some way to confirm your account in the tool.

Recommended destinations

If you’re just starting out, we know the 
catalog
 can be really overwhelming. How do you choose from all of the available destinations?

We’ve written a lot about 
how to choose your tools
, but as a start, we recommend that you have one tool from each of the following categories:

If you’re adding more destinations after you’ve done your Segment instrumentation, you might want to check that the destinations you choose 
can accept the methods
 you’re already using, and that they can 
use the Connection Modes
 you’re already using.

We also feel that it’s really important to have a data warehouse, so you can get a 
clearer view of all of your data
 for analytics purposes. More on that just below.

Adding a warehouse

Warehouses are a special type of destination which receive streaming data from your Segment sources, and store it in a table 
schema based on your Segment calls
. This allows you to do a lot of interesting analytics work to answer your own questions about what your users are doing and why.

All customers can connect a data warehouse to Segment. Free and Team customers can connect one warehouse, while Business customers can connect as many as needed.

You should spend a bit of time 
considering the benefits and tradeoffs of the warehouse options
, and then choose one from Segment’s 
warehouse catalog
.

When you choose a warehouse, you can then use the steps in the documentation to connect it. This may require that you create a new dedicated user (or “service user”) to allow Segment to access the database.

Once your warehouse is configured and running, you can connect to it using a Business Intelligence (BI) tool (such as Looker, Mode, Tableau, or others) to analyze your data in-depth.

There are also a number of Business tier features you can then use with your warehouse, including 
selective sync
 and 
Replay
.

Check out our course on warehouses in Segment University. (Must be logged in to access.)

Take your plans, and make them real.

Test your implementation and see where your data is and isn't arriving.

This page was last modified: 07 Feb 2025

Need support?

Questions? Problems? Need more info? Contact Segment Support for assistance!

Help improve these docs!

Was this page helpful?

Thanks for your feedback!

Can we improve this doc? 
Send us feedback!

Get started with Segment

On this page

Was this page helpful?

Thanks for your feedback!

Can we improve
 this doc? 
Send us feedback!

Product

For Developers

Company

Support

© 2025 Segment.io, Inc.

=== Content from https://segment.com/docs/guides/ ===

An introduction to Segment
        

On this page

Welcome! This page is a high-level introduction to the Segment Platform, including what it does and how. (If you’re looking for detailed information about architecture, setup, or maintenance, you can 
skip ahead
.
)

What is Segment?

Segment is a Customer Data Platform (CDP), which means that it provide a service that simplifies collecting and using data from the users of your digital properties (websites, apps, etc). With Segment, you can collect, transform, send, and archive your 
first-party customer data
. Segment simplifies the process of collecting data and hooking up new tools, allowing you to spend more time using your data, and less time trying to collect it.

You can also enrich the customer data you collect by connecting data from your other tools, and then aggregate it to monitor performance, inform decision-making processes, and create uniquely customized user experiences. You can also use Unify, Segment’s identity resolution tool, to unify data from individual users to gain a wholistic understanding of their actions.

Check out how to get started with Segment in Segment University! (Must be logged in to access.)

What does it do?

In its very simplest form, Segment generates messages about what’s happening in your site or app, then translates the content of those messages into different formats for use by other tools (called ‘
Destinations
’), and transmits messages to those tools. The Segment servers also archive a copy of the data, and can 
send data to your storage systems
 (such as databases, warehouses, or bulk-storage buckets).

How does Segment work?

Segment’s libraries generate and send messages to the tracking API in JSON format. Segment provides a standard structure for the basic API calls, along with a recommended JSON structure (also known as the ‘Spec’, a type of schema) that helps keep the most important parts of your data consistent, while allowing great flexibility in what other information you collect and where.

Segment Messages

When you implement Segment, you add the Segment code to your website, app, or server, which generates messages based on specific triggers you define. At its very simplest, this code can be a snippet that you copy and paste into the HTML of a website to track page views. It can also be as complex as Segment calls embedded in a React mobile app to send messages when the app is opened or closed, when the user performs different actions, or when time based conditions are met (for example “ticket reservation expired” or “cart abandoned after 2 hours”).

Segment has 
Sources
 and 
Destinations
. Sources send messages 
into
 Segment (and other tools), while Destinations receive messages 
from
 Segment.

Anatomy of a Segment message

The most basic Segment message requires only a 
userID
 or 
anonymousID
; all other fields are optional to allow for maximum flexibility. However, a normal Segment message has three main parts: the 
common fields
, the 
“context” object
, and the properties (if it’s an event) or traits (if it’s an object).

The common fields include information specific to how the call was generated, like the timestamp and library name and version. The fields in the context object are usually generated by the library, and include information about the environment in which the call was generated: page path, user agent, OS, locale settings, etc. The properties and traits are optional and are where you customize the information you want to collect for your implementation.

Another common part of a Segment message is 
the 
integrations
 object
, which you can use to explicitly filter which destinations the call is forwarded to. However this object is optional, and is often omitted in favor of non-code based filtering options.

Segment Sources

Segment provides several types of Sources which you can use to collect your data, and which you can choose among based on the needs of your app or site. For websites, you can embed a library which loads on the page to create the Segment messages. If you have a mobile app, you can embed one of Segment’s Mobile libraries, and if you’d like to create messages directly on a server (if you have, for example a dedicated .NET server that processes payments), there are several server-based libraries that you can embed directly into your backend code. (You can also use 
cloud-sources
 to import data about your app or site from other tools like Zendesk or Salesforce, to enrich the data sent through Segment.)

Destinations

Once Segment generates the messages, it can send them directly to the Segment servers for translation and forwarding on to the Destinations you’re using, or it can make calls directly from the app or site to the APIs of your Destination tools. Which of these methods you choose depends on which Destinations you’re using and other factors. You can read more about these considerations in our 
Connection Modes documentation

What happens next?

Messages sent to the Segment servers using the tracking API can then be translated and forwarded on to Destination tools, inspected to make sure that they’re in the correct format or schema, inspected to make sure they don’t contain any Personally Identifying Information (PII), aggregated to illustrate overall performance or metrics, and archived for later analysis and reuse.

What are the other parts of the Segment platform?

In addition to 
Connections
 (our core message routing product) Segment offers additional features to help your organization do more with its data, and keep data clean, consistent, and respectful of end-user privacy. The following products are available:

Where can I learn more?


I’m a Segment Developer



I’m a Segment Data user



I’m a Segment Workspace administrator


What’s a Workspace?

A workspace is a group of sources that can be administered and billed together. Workspaces help companies manage access for multiple users and data sources. Workspaces let you collaborate with team members, add permissions, and share sources across your whole team using a shared billing account.

When you first log in to your Segment account, you can create a new workspace, or choose to log into an existing workspace if your account is part of an existing organization.

What’s a Source?

In Segment, you create a source (or more than one!) for each website or app you want to track. We 
highly recommend
 creating a Source for each unique source of data (each site, app, or server), though this isn’t required.

Sources belong to a workspace, and the URL for a source looks something like this:

https://segment.com/<my-workspace>/sources/<my-source-name>/

You can create new sources using the button in the workspace view. Each source you create has a write key, which is used to send data to that source. For example, to load 
analytics.js
, the Segment JavaScript library
 on your page, the snippet on the 
Quickstart Guide
 includes:

analytics
.
load
(
"
YOUR_WRITE_KEY
"
);



What’s a Destination?

Destinations are business tools or apps that you can connect to the data flowing through Segment. Some of Segment’s most popular destinations are Google Analytics, Mixpanel, Kissmetrics, Customer.io, Intercom, and KeenIO.

All of these tools run on the same data: who are your customers and what are they doing? But each tool requires that you send that data in a slightly different format, which means that you’d have to write code to track all of this information, again and again, for each tool, on each page of your app or website.

Enter Segment. Do it once.

Segment eliminates this process by introducing an abstraction layer. You send your data to Segment, and Segment understands how to translate it so we can send it along to any destination. You enable destinations from the catalog in the Segment App, and user data immediately starts flowing into those tools. No extra code required!

Segment supports many categories of destinations, from advertising to marketing, email to customer support, CRM to user testing, and even data warehouses. You can view a complete list of available 
destinations
 or check out the 
destination page
 for a searchable list broken down by category.

What’s a Warehouse?

A warehouse is a central repository of data collected from one or more sources. This is what commonly comes to mind when you think about a relational database: structured data that fits neatly into rows and columns.

In Segment, a Warehouse is a special type of destination. Instead of streaming data to the destination all the time, we load data to them in bulk at regular intervals. When we load data, we insert and update events and objects, and automatically adjust their schema to fit the data you’ve sent to Segment.

This page was last modified: 28 Mar 2023

Need support?

Questions? Problems? Need more info? Contact Segment Support for assistance!

Help improve these docs!

Was this page helpful?

Thanks for your feedback!

Can we improve this doc? 
Send us feedback!

Get started with Segment

On this page

Was this page helpful?

Thanks for your feedback!

Can we improve
 this doc? 
Send us feedback!

Product

For Developers

Company

Support

© 2025 Segment.io, Inc.

=== Content from https://segment.com/docs/segment-app/iam/secure-password/ ===

Picking a secure password
        

Picking a strong password is one of the most important things you can do to protect your account.

Twilio Unified Login users can manage their password in their Twilio account

Twilio Unified Login users can manage their user settings, including name, email, password, and 2FA settings, directly in their Twilio account. To learn more about Twilio’s user and password policies, review Twilio’s 
Account Management
 documentation.

Under the Hood

When you first create a Segment account, or when you reset or change the password of an existing account, you’ll see some tools which Segment uses to help you choose a strong password. Segment uses 
zxcvbn
 to show your password strength, and 
Have I Been Pwned
 to notify you if your password has been found in any data breaches. Your password is never stored in plaintext, and is securely stored using the 
bcrypt
 password hashing function in Segment’s database.

General Guidance

Here are some general password guidelines:

Use a password manager like 
1Password
 or 
LastPass
 to generate and store passwords.

Passwords should be 8 or more characters. Consider using pass-phrases (for example,  
customer data infrastructure
), combinations of random characters (for example,  
9;ske%t!u9jdckd#s>
), or other strategies that are difficult to guess, such as 
icOnsTent CaPitaliZation
.

Use a different password for every website. If you use the same password on multiple websites and one is breached, your accounts on all of these websites may be affected.

Do not share your password with anyone, even your co-workers. Once shared, they may use that password on another site or share it with another co-worker without telling you. If one of them leaves the company they will still be able to take actions under your account.

Has my password been compromised?

If you see a message that says “This password is known to have been previously compromised in a data breach”, it means that the password you typed has been used before, and was in a database that was compromised and put on the internet. This does 
not
 mean that Segment has been compromised, or that someone has accessed your Segment account. Check out 
Have I Been Pwned
 for more information, and choose a different password.



This page was last modified: 04 Dec 2023

Need support?

Questions? Problems? Need more info? Contact Segment Support for assistance!

Help improve these docs!

Was this page helpful?

Thanks for your feedback!

Can we improve this doc? 
Send us feedback!

Get started with Segment

Was this page helpful?

Thanks for your feedback!

Can we improve
 this doc? 
Send us feedback!

Product

For Developers

Company

Support

© 2025 Segment.io, Inc.

=== Content from https://segment.com/docs/connections/spec/page/ ===

Spec: Page
        

On this page

The Page call lets you record whenever a user sees a page of your website, along with any optional properties about the page. Calling Page or 
Screen
 in a Segment 
source
 is one of the first steps to getting started with Segment.

Check out our high-level overview of the Page method in Segment University. (Must be logged in to access.)

Note: In 
analytics.js
 a Page call is included in the snippet by default
 just after 
analytics.load
. Many destinations require this page event to be fired at least once per page load for proper initialization. You may add an optional 
name
 or 
properties
 to the default call, or call it multiple times per page load if you have a single-page application.

Here’s the payload of a typical Page call with most 
common fields
 removed:

{

  
"type"
:
 
"page"
,

  
"name"
:
 
"Home"
,

  
"properties"
:
 
{

    
"title"
:
 
"Welcome | Initech"
,

    
"url"
:
 
"http://www.example.com"

  
}


}



And here’s the corresponding JavaScript event that would generate the above payload. If you’re using Segment’s JavaScript library, the page name and URL are automatically gathered and passed as properties into the event payload:

analytics
.
page
(
"
Retail Page
"
,
"
Home
"
);



Based on the library you use, the syntax in the examples might be different. You can find library-specific documentation on the 
Sources Overview
 page.

Beyond the common fields, the Page call takes the following fields:

Example

Here’s a complete example of a Page call:

{

  
"anonymousId"
:
 
"507f191e810c19729de860ea"
,

  
"channel"
:
 
"browser"
,

  
"context"
:
 
{

    
"ip"
:
 
"8.8.8.8"
,

    
"userAgent"
:
 
"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_5) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/40.0.2214.115 Safari/537.36"

  
},

  
"integrations"
:
 
{

    
"All"
:
 
true
,

    
"Mixpanel"
:
 
false
,

    
"Salesforce"
:
 
false

  
},

  
"messageId"
:
 
"022bb90c-bbac-11e4-8dfc-aa07a5b093db"
,

  
"name"
:
 
"Home"
,

  
"properties"
:
 
{

    
"title"
:
 
"Welcome | Initech"
,

    
"url"
:
 
"http://www.example.com"

  
},

  
"receivedAt"
:
 
"2015-02-23T22:28:55.387Z"
,

  
"sentAt"
:
 
"2015-02-23T22:28:55.111Z"
,

  
"timestamp"
:
 
"2015-02-23T22:28:55.111Z"
,

  
"type"
:
 
"page"
,

  
"userId"
:
 
"97980cfea0067"
,

  
"version"
:
 
"1.1"


}



Create your own Page call

Use the following interactive code pen to see what your Page calls would look like with user-provided information:





Identities

The User ID is a unique identifier for the user performing the actions. Check out the 
User ID docs
 for more detail.

The Anonymous ID can be any pseudo-unique identifier, for cases where you don’t know who the user is, but you still want to tie them to an event. Check out the 
Anonymous ID docs
 for more detail.

Note: In our browser and mobile libraries a User ID is automatically added
 from the state stored by a previous 
identify
 call, so you do not need to add it yourself. They will also automatically handle Anonymous IDs under the covers.

Properties

Properties are extra pieces of information that describe the page. They can be anything you want.

Segment handles properties with semantic meanings in unique ways. For example, Segment always expects 
path
 to be a page’s URL path, and 
referrer
 to be the previous page’s URL.

You should 
only use reserved properties for their intended meaning
.

Reserved properties Segment has standardized:

Note:
 In 
analytics.js
, Segment automatically sends the following properties: 
title
, 
path
, 
url
, 
referrer
, and 
search
.

This page was last modified: 21 Nov 2023

Need support?

Questions? Problems? Need more info? Contact Segment Support for assistance!

Help improve these docs!

Was this page helpful?

Thanks for your feedback!

Can we improve this doc? 
Send us feedback!

Get started with Segment

On this page

Was this page helpful?

Thanks for your feedback!

Can we improve
 this doc? 
Send us feedback!

Product

For Developers

Company

Support

© 2025 Segment.io, Inc.

=== Content from https://segment.com/docs/connections/spec/best-practices-identify/ ===

Best Practices for Identifying Users
        

On this page

The most important calls you make with Segment are the 
identify
 and 
track
 calls. When you use these calls together, you can attribute actions on your site or app to individuals, and gain a better understanding of their activities, identity, and use patterns over time. Tracking users with the identify and track calls reduces the number of 
Monthly Tracked Users
 you are billed for.

Identifying users

The Identify call specifies a customer identity that you can reference across the customer’s lifetime. There are instances where you want to record information about a user that isn’t already known to you. An example of this might be, a user that visits your site and doesn’t register, but they do give you their email address through a newsletter email sign-up form. In this instance, you would record that email address as a trait, and for the identifier (ID), you would use anonymous ID.

When you make an 
identify
 call using Segment’s Analytics.js library, Segment saves the 
userId
 to the browser cookie, and writes all the user traits in 
localStorage
. If you’re using one of the Segment mobile libraries, the 
userId
 and traits are stored in the device’s memory. This makes it possible to append the user’s data to all subsequent 
page calls
 or 
track calls
 for the user, so you can properly attribute those actions.

If a user returns to your site after the 
cookie expires
, Analytics.js looks for an old ID in the user’s 
localStorage
, and if one is found, sets it as the user’s ID again in a new cookie. If the user clears their cookies 
and
 
localStorage
, all of the IDs are removed and the user gets a completely new 
anonymousId
 when they next visit the page.

Whenever possible, follow the Identify call with a Track event that records what caused the user to be identified.

AnonymousId generation

If you’re using Segment’s browser or mobile libraries, the Segment SDK generates and sets a UUID as 
anonymousID
 at the user’s first visit to your site. That 
anonymousId
 is saved in the user’s cookie, as well as localStorage, and will stick with that user until the cache is cleared or a 
reset
 call is triggered.

You can use the 
anonymousId
 to link events performed by the user as they navigate around your website. When you track the 
anonymousId
, you can attribute activities over multiple days to the same user by collecting all of the activities with that ID. If a user chooses to register for your site, or log in to your app, you can Identify them, and still include their 
anonymousId
 in the event payload along with the new 
userId
.

If you use Segment’s server libraries, you must generate an 
anonymousId
 manually. It can be any pseudo-unique identifier, for example, you might use a 
sessionId
 from a backend server.

Best options for userIds

Segment recommends that you use a unique user identifier (UUID) that won’t change for your 
userId
. A 
userId
 should be a robust, static, unique identifier that you recognize a user by in your own database systems. Because these IDs are consistent across a customer’s lifetime, you should include a 
userId
 in Identify calls as often as you can. If you don’t have a userId, you need to include an anonymousId in your Identify call in order to record identifying information about your user.

Ideally, the 
userId
 could be a database ID. For example, if you’re using MongoDB it might be a row identifier and look something like 
507f191e810c19729de860ea
. These can also be 
UUID
s that you generate somewhere in your application. You can also use identifiers that you get from other tools - such as Shopify or Braze - however this approach can lead to extra complexity in your systems.

Segment does 
not
 recommend using simple email addresses or usernames as a User ID, as these can change over time. Segment recommends that you use static IDs instead, so the IDs 
never
 change. When you use a static ID, you can still recognize the user in your analytics tools, even if the user changes their email address. And even better, you can link your analytics data with your own internal database.

Tip!
 Even though Segment doesn’t recommend using an email address or a username as a User ID, you can still send that identifying information in your Identify call as 
traits
.

When to call Identify

You should make an Identify call in the following situations:

Soft User Registration

An anonymous user visits the site for the very first time. The home page has the analytics.js tracking snippet loaded in its header. When the page loads, this sets off the default Page call to Segment. The Segment SDK generates and sets 
anonymousId
.

analytics
.
page
({

  
path
:
 
'
/
'
,

  
title
:
 
'
Home Page
'
,

  
url
:
 
'
https://somesite.com/
'
,


})



You can see in this full page event, the 
anonymousId
 is populated, and the 
userId
 is null.

{

  
"
anonymousId
"
:
 
"
bd077b70-816b-448b-ae79-2f5f7d856513
"

  
"
context
"
:
 
{

    
"
ip
"
:
 
"
0.0.0.0
"
,

    
"
library
"
:
 
{

      
"
name
"
:
 
"
analytics.js
"
,

      
"
version
"
:
 
"
3.11.4
"

    
},

    
"
locale
"
:
 
"
en-US
"
,

    
"
page
"
:{

      
"
path
"
:
"
/
"

      
"
referrer
"
:
 
""
,

      
"
search
"
:
 
""
,

      
"
title
"
:
 
"
Home Page
"
,

      
"
url
"
:
 
"
https://somesite.com
"

      
},

    
"
userAgent
"
:
 
"
Mozilla/5.0
"

    
},

    
"
integrations
"
:
 
{},

    
"
messageId
"
:
 
"
ajs-84d32beb4273e661a2257bfef41c4964
"
,

    
"
originalTimestamp
"
:
 
"
2020-04-23T22:38:48.55Z
"
,

    
"
properties
"
:{

      
"
path
"
:
 
"
/
"
,

      
"
referrer
"
:
 
""
,

      
"
search
"
:
 
""
,

      
"
title
"
:
 
"
Home Page
"
,

      
"
url
"
:
 
"
https://somesite.com
"

    
},

  
"
receivedAt
"
:
 
"
2020-04-23T22:38:48.55Z
"
,

  
"
sentAt
"
:
 
"
2020-04-23T22:38:48.55Z
"
,

  
"
timestamp
"
:
 
"
2020-04-23T22:38:48.55Z
"
,

  
"
type
"
:
 
"
page
"
,

  
"
userId
"
:
 
null


}



The user signs up for an email newsletter and fills out the form giving you their first and last name, as well as their email address. At this point, you will fire off an Identify call. You won’t yet assign them a user ID in this example, but you can still grab these traits about them.

analytics
.
identify
({

  
firstName
:
 
'
Joe
'
,

  
lastName
:
 
'
Visitor
'
,

  
email
:
 
'
jvisitor@thissite.com
'


});



You’ll notice the Identify call contains no 
userId
. These traits will be associated to the 
anonymousId
 that is available in the user’s cookie and 
localStorage
.

{

  
"
anonymousId
"
:
 
"
bd077b70-816b-448b-ae79-2f5f7d856513
"

  
"
context
"
:
 
{

    
"
ip
"
:
 
"
0.0.0.0
"
,

    
"
library
"
:
 
{

      
"
name
"
:
 
"
analytics.js
"
,

      
"
version
"
:
 
"
3.11.4
"

    
},

    
"
locale
"
:
 
"
en-US
"
,

    
"
page
"
:{

      
"
path
"
:
"
/
"

      
"
referrer
"
:
 
""
,

      
"
search
"
:
 
""
,

      
"
title
"
:
 
"
Email Signup
"
,

      
"
url
"
:
 
"
https://somesite.email
"

      
},

    
"
userAgent
"
:
 
"
Mozilla/5.0
"

    
},

    
"
integrations
"
:
 
{},

    
"
messageId
"
:
 
"
ajs-84d32beb4273e661a2257bfef41c4964
"
,

    
"
originalTimestamp
"
:
 
"
2020-04-23T22:38:48.55Z
"
,

    
"
properties
"
:{

      
"
path
"
:
 
"
/
"
,

      
"
referrer
"
:
 
""
,

      
"
search
"
:
 
""
,

      
"
title
"
:
 
"
Home Page
"
,

      
"
url
"
:
 
"
https://somesite.com
"

    
},

  
"
receivedAt
"
:
 
"
2020-04-23T22:38:48.55Z
"
,

  
"
sentAt
"
:
 
"
2020-04-23T22:38:48.55Z
"
,

  
"
timestamp
"
:
 
"
2020-04-23T22:38:48.55Z
"
,

  
"
traits
"
{

    
"
email
"
:
 
"
jvisitor@thissite.com
"
,

    
"
first_name
"
:
 
"
Joe
"

    
"
last_name
"
:
 
"
Visitor
"

  
},

  
"
type
"
:
 
"
page
"
,

  
"
userId
"
:
 
null


}



Full User Registration

An anonymous visitor registers for an account and becomes a known user. The account creation process allows you to assign a 
userId
 from your production database and capture additional traits. For this example, the 
userId
 that is assigned is “123abc”. This is when you’ll want to fire an Identify call with this user’s newly assigned 
userId
 and additional traits.

analytics
.
identify
(
`123abc`
,{

  
phone
:
 
'
555-555-5555
'
,

  
address
:
 
{

    
street
:
 
'
6th Street
'
,

    
city
:
 
'
San Fransisco
'
,

    
state
:
 
'
CA
'
,

    
postalCode
:
 
'
94103
'
,

    
country
:
 
'
US
'
,

  
}


});



After you fire the Identify call with the 
userId
, you’ll notice that the payload now has both a 
userId
 
and
 an 
anonymousId
 attributed to the user.

{

  
"
anonymousId
"
:
 
"
bd077b70-816b-448b-ae79-2f5f7d856513
"

  
"
context
"
:
 
{

    
"
ip
"
:
 
"
0.0.0.0
"
,

    
"
library
"
:
 
{

      
"
name
"
:
 
"
analytics.js
"
,

      
"
version
"
:
 
"
3.11.4
"

    
},

    
"
locale
"
:
 
"
en-US
"
,

    
"
page
"
:{

      
"
path
"
:
"
/
"

      
"
referrer
"
:
 
""
,

      
"
search
"
:
 
""
,

      
"
title
"
:
 
"
Email Signup
"
,

      
"
url
"
:
 
"
https://somesite.email
"

      
},

    
"
userAgent
"
:
 
"
Mozilla/5.0
"

    
},

    
"
integrations
"
:
 
{},

    
"
messageId
"
:
 
"
ajs-84d32beb4273e661a2257bfef41c4964
"
,

    
"
originalTimestamp
"
:
 
"
2020-04-23T22:38:48.55Z
"
,

    
"
properties
"
:{

      
"
path
"
:
 
"
/
"
,

      
"
referrer
"
:
 
""
,

      
"
search
"
:
 
""
,

      
"
title
"
:
 
"
Home Page
"
,

      
"
url
"
:
 
"
https://somesite.com
"

    
},

  
"
receivedAt
"
:
 
"
2020-04-23T22:38:48.55Z
"
,

  
"
sentAt
"
:
 
"
2020-04-23T22:38:48.55Z
"
,

  
"
timestamp
"
:
 
"
2020-04-23T22:38:48.55Z
"
,

  
"
traits
"
{

    
"
phone
"
:
 
'
555-555-5555
'
,

    
"
address
"
:
 
{

    
"
street
"
:
 
'
6th Street
'
,

    
"
city
"
:
 
'
San Fransisco
'
,

    
"
state
"
:
 
'
CA
'
,

    
"
postalCode
"
:
 
'
94103
'
,

    
"
country
"
:
 
'
US
'
,

  
}
  
  
},

  
"
type
"
:
 
"
page
"
,

  
"
userId
"
:
 
"
123abc
"


}



Merging Identified and Anonymous user profiles

The illustration below shows a timeline with a user’s interactions on a website, including sample API calls above that show Segment calls, and the user’s 
anonymousId
 and 
userId
.



When the user first visits a page, Analytics.js automatically assigns the user an 
anonymousId
 and saves it to the user’s 
localStorage
. As the user interacts with the site, for example clicking around to different pages, Analytics.js includes this 
anonymousId
 and some 
contextual information
 with each Page and Track call. The contextual information might be the user’s 
IP address, browser, and more
.

When a user signs up to create an account on the website, the 
.identify("userId")
 and 
.track(“Signed Up”)
 events fire, in that order. You pull the 
userId
 unique to the user from your systems, and send it to the Segment library so you can label that user’s later events with their ID. The later Track call (“Signed Up”) contains both the 
userId
 
and
 the automatically-collected 
anonymousId
 for the user, and any other information you capture about them -  such as their first name, last name, and email address.

The example below shows an Identify call including user traits. It uses a database ID (
97980cfea0067
) as the 
userId
.

analytics
.
identify
(
"
97980cfea0067
"
,
 
{

  
name
:
 
"
Peter Gibbons
"
,
 
//user trait

  
email
:
 
"
peter@example.com
"
,
 
//user trait

  
plan
:
 
"
premium
"
 
//user trait


});



For a Track call, information about this event is stored either in the 
context
 field or in the event 
properties
. The example below shows a Track call including properties that tell you about the user.

analytics
.
track
(
"
Signed Up
"
,
 
{

  
userId
:
 
"
97980cfea0067
"
,
 
//event property

  
name
:
 
"
Peter Gibbons
"
,
 
//event property

  
email
:
 
"
peter@example.com
"
,
 
//event property

  
plan
:
 
"
premium
"
 
//event property


});



Additionally, Analytics.js adds a 
message_id
 and 
four timestamps
 to the call.

Now, as the user interacts with your site and different buttons or links that you track using Segment, their 
userId
 and 
anonymousId
 are sent with each subsequent tracking API call.

UserId merge examples

Let’s go through some more scenarios to explain how an 
anonymousId
 is assigned and how it might be merged with a 
userId
.

If a user clicks on an ad and is directed to a webpage, they are assigned an 
anonymousId
. While this user is anonymous, they navigate to different pages and click around on the website. Say they come back two days later from the same device, sign up, and are assigned a 
userId
 from your database.



For simplicity, we’re assuming that the user has 
not
 cleared their cookies or 
localStorage
, where the original 
anonymousId
 is stored. If they had, they’d be assigned a new 
anonymousId
 when they visited the website, and the 
userId
 they got when they register on the website would 
not
 be attached to the activities tracked with the old 
anonymousId
.

In this scenario, the person uses both a web browser, and a mobile application to interact with your site. In each case, they are assigned a different 
anonymousId
. In this scenario, the user signs up on the web browser, so Segment assigns their 
web
 session a 
userId
. However, because they do not log in on the mobile application, Segment cannot tie the mobile activity to this specific user. Their mobile application activity remains anonymous unless they log in on the mobile application.



Similar to the previous scenario, the user accessed both your website and mobile application, and also logged in on both. In this case, both sessions on the web and mobile app receive the user’s 
userId
, so Segment can tie the anonymous activity on both web and mobile to this user.



User profiles in warehouses

Your data warehouse has a schema for each of your Segment sources. User information is stored in two tables in your source schemas - the 
identifies
 and 
users
 table.

The 
identifies
 table contains all of your identify events, and the timestamps for these events. Every time you make an Identify call, Segment adds the 
userId
, 
anonymousId
, any updated or added user traits from the call, as well as the timestamp of when the call was made. Your 
identifies
 table is your first stop when you have questions about users and their traits.

The 
users
 table contains only unique Identify method calls, and is a collation of the 
identifies
 table. The 
users
 table is the single source of truth for a user’s most up-to-date traits.

These tables only contain information about a user 
once they have been identified.
 However, you can still find information about an 
anonymous
 user on the 
pages
, 
screens
, and 
tracks
 tables, as well as the individual track event tables.

ID expiration and overwriting

The Segment ID cookie is set with a one year expiration. However, there are some ways an ID can be reset or overwritten:

Remember, if a user has multiple devices, they can have different 
anonymousId
s on each different device.

Linking server and client generated Ids

If you’re tracking on the client and on the server, the 
anonymousId
 can be retrieved from 
localStorage
 on the client and passed to the server. You can access a user’s anonymousId using the following call:

analytics
.
user
().
anonymousId
()



If you’re identifying on the server, then you will want to pass the user ID from the server to the client using an Identify call with the 
anonymousId
. That will allow the 
userId
 to be aliased with the existing 
anonymousId
 and stored in the cookie in localStorage. With that, all previous anonymous activity and all subsequent activity is associated to the newly generated 
userId
, as well as existing 
anonymousId
s.

There are some advantages to sending details about your users directly from your server once the user registers. Server library 
Identify calls
 are invisible to the end user, making them more secure, and much more reliable. Or, if you want to send user data that is sensitive or which you don’t want to expose to the client, then you can make an Identify call from the server with all the traits you know about the user. More about 
collecting data on the client or server
 in Segment’s documentation.

Aliasing from a server library

If you plan to track anonymous visitors from the browser and only make 
Identify calls
 from your server libraries, Kissmetrics and Mixpanel might require that you make an 
Alias call
 to link the records. The Alias call links client-side anonymous visitors with server-identified users. This isn’t recommended, but if you do this, read the 
Kissmetrics
 and 
Mixpanel
 specific 
alias
 docs.

Common questions

There are a few things that might cause your numbers to be off.

The most common problem people run into when tracking new user signups client-side is that only a portion of their new users are showing up in reports.

This is usually caused by the page redirecting or reloading before the tracking calls get a chance to run. Segment recommends that you make those calls from a welcome page after the user registers, rather than trying to squeeze in the tracking calls on the sign-up page itself.

This is usually only an issue in 
Mixpanel
, since it’s the only destination that requires a call to 
alias
 in the browser to link anonymous browsing history to a new identified user.

Remember that for destinations that require aliasing, you must make the 
Alias call
 before you make the 
Identify call
 for that user. Even if you make an 
Identify call
 from a server library, it can’t happen before the client-side 
alias
.

Unfortunately, there is no way to change an existing 
userId
 within Segment. Historical data with an existing 
userId
 remains the same, and a new 
userId
 will not replace the existing 
userId
 in Segment event call logs. For downstream destinations, consult the corresponding docs about user profile behaviors when using a new 
userId
.

Changing a 
userId
 is incredibly hard to do, as that is a fundamental part of analytics. While some downstream analytics tools let you change a 
userId
 once set, others don’t and the process will be different for each tool.

This page was last modified: 15 Mar 2024

Need support?

Questions? Problems? Need more info? Contact Segment Support for assistance!

Help improve these docs!

Was this page helpful?

Thanks for your feedback!

Can we improve this doc? 
Send us feedback!

Get started with Segment

On this page

Was this page helpful?

Thanks for your feedback!

Can we improve
 this doc? 
Send us feedback!

Product

For Developers

Company

Support

© 2025 Segment.io, Inc.

=== Content from https://segment.com/docs/protocols/tracking-plan/best-practices/ ===

Data Collection Best Practices
        

On this page

Figuring out what events to track in Segment can feel overwhelming. Fortunately, Segment has helped thousands of customers through this process and has amassed a ton of resources to help you get started. Whether you’re a small team just getting your app off the ground or a highly complex enterprise with hundreds of stakeholders, these resources can help.

That being said, be prepared to invest time defining how you want to track data. Any investment in improving data quality will reap massive rewards, and compound over time by allowing your analytics teams to produce better insights, your marketing teams to run better campaigns and so much more.

Data tracking philosophy

Tracking is about learning and taking action. Think about what you want to know about your product or customers. Think about what assumptions need to be tested or invalidated. Think about the unknowns. Here are some helpful questions to get started:

Define business objectives

Segment recommends documenting your high-level business objectives. What measurable business outcomes do you want to achieve? Do you want to acquire new customers, activate new signups, drive incremental revenues among your current customer base? You can best answer this question by interviewing stakeholders who would consume the data in your organization.

With your business goals documented, you now need to map user actions to those business goals. For example, if one of your goals is to activate new signups, you want to think about which activities are related to a signup. Ask yourself what actions people take before signing up. Do specific actions predict user signups?

As an example, you may end up with a list like the following:

While these may only represent a portion of the total user actions you will track, focusing on business objectives helps make data collection more manageable.

Formalize your naming and collection standards

With your business objectives documented, it’s time to build a set of standards that you and your team will use when determining what to track. Segment’s most successful customers limit their tracking plan to a minimal number of core events with rich properties that provide context. While some customers find success with the “less is more” philosophy of tracking data, others take a more liberal “track more and analyze later” approach. Both options have pros and cons you should take into account when you consider your company’s needs.

Regardless of your approach, keep the following tips in mind:

Pick a casing convention.
 Segment recommends 
Title Case
 for event names and 
snake_case
 for property names. Make sure you pick a casing standard and enforce it across your events and properties.

Pick an event name structure.
 As you may have noticed from the 
Segment specs
, Segment uses the Object (
Blog Post
) + Action (
Read
) framework for event names. Pick a convention and stick to it.

Don’t create event names dynamically.
 Avoid creating events that pull a dynamic value into the event name (like 
User Signed Up (11-01-2019)
).

Don’t create events to track properties.
 Avoid adding values to event names that could be a property. Instead, add values a property (like 
"blog_post_title":"Best Tracking Plans Ever"
).

Don’t create property keys dynamically.
 Avoid creating property names like 
"feature_1":"true"
,
"feature_2":"false"
, as these are ambiguous and difficult to analyze.



Create a tracking plan

A 
tracking plan
 clarifies what events to track, where those events live in the code base, and why those events are necessary from a business perspective. Prior to Protocols, tracking plans typically lived in a spreadsheet. The tracking plan served as a project management tool to align an entire organization around data as the basis on which to make decisions. The tracking plan helps marketers, product managers, engineers, and analysts get on the same page.

The tracking plan has been so instrumental in helping organizations reclaim their own data efforts that Segment invested years of product development to create 
Protocols
. Whatever tool you choose to build your tracking plan, make sure that it represents a single source of truth for your data collection efforts.

Identify your users

The Identify call is important because it updates all records of the user with a set of traits. But how do you choose which traits to include?

Here is a sample Identify call (with 
analytics.js
) for Segment:

analytics
.
identify
({


 
 
name
:
 
'
Jane Doe
'
,


 
 
email
:
 
'
janedoe@iamawesome.com
'
,


 
 
login
:
 
'
janedoe
'
,


 
 
type
:
 
'
user
'
,


 
 
created
:
 
'
2016-11-07T16:40:52.238Z
'
,


});



The traits represent dimensions in your data that you can group or pivot on. For example, in the previous sample call, you can easily create cohorts of all types that are 
users
 or accounts created within a time window of your choosing.

Define your Track events

After you’ve documented your 
event naming and collection standards
, it’s time to add events to your tracking plan. Segment recommends starting with fewer events that are directly tied to one of your 
business objectives
. This focused effort helps avoid a situation where you become overwhelmed by endless possible actions to track. As you get more comfortable, you can add more events to your tracking plan that can answer peripheral questions.

Segment began by tracking these events:

Next, Segment added some of the following peripheral events that helped monitor performance:

For an ecommerce company, however, the main events might be something like:

Note that Segment has a set of “reserved” event names specifically for ecommerce, called the Segment 
ecommerce spec
. Check it out to see which events Segments covers and how they are used in our downstream destinations.

For a community, on the other hand, an entirely different set of actions indicate engagement, listed in the following pyramid. For example, a community like 
GrowthHackers
 may want to track actions like:

With this, they’re able to measure key metrics around engagement and understand how users are moving towards their ultimate conversion event: curation content for others. For more information, check out 
this article
 from GrowthHackers about the events they track and why.

Define your Track event properties

Each Track call can accept an optional dictionary of 
properties
, which can contain any key-value pair you want. These 
properties
 act as dimensions that allow your end tool to group, filter, and analyze the events. They give you additional detail on broader events.

As mentioned earlier, events should be generic and high level, whereas properties are specific and detailed. For example, at Segment, 
Business Tier Workspace Created
 works poorly as an event name. Instead, Segment used 
Workspace Created
 with a 
property
 of 
account_tier
 and value of 
business
:

analytics
.
track
(
'
Workspace Created
'
,
 
{


 
 
account_tier
:
 
'
business
'


})



Similar to the traits in the Identify call, the properties provide you a column that you can pivot against or filter on in your analytics tools or allow you to create a cohort of users in email tools.

Avoid dynamically generated 
key
’s in the 
properties
 dictionary, as each 
key
 will create a new column in your downstream tools. Dynamically generated 
key
’s will clutter your tools with data that will make it difficult and confusing to use later.

Here is Segment’s 
Lead Captured
 Track call:

analytics
.
track
(
userId
,
 
'
Lead Captured
'
,
 
{


 
 
email
:
 
'
email
'
,


 
 
location
:
 
'
header navbar
'


 
 
url
:
 
'
https://segment.com/
'


});



The high level event is 
Lead Captured
 and all of the details are tucked into the 
properties
 dictionary. In its downstream tools, Segment can easily look at how many leads were captured in different locations on the Segment website.

If you want to learn more about how properties are used by downstream tools, check out 
The Anatomy of a Track Call
.

Want a free consultation from our Customer Success Managers on how they simplify customer’s analytics? 
Request a demo of Segment
.

This page was last modified: 28 Feb 2024

Need support?

Questions? Problems? Need more info? Contact Segment Support for assistance!

Help improve these docs!

Was this page helpful?

Thanks for your feedback!

Can we improve this doc? 
Send us feedback!

Get started with Segment

On this page

Was this page helpful?

Thanks for your feedback!

Can we improve
 this doc? 
Send us feedback!

Product

For Developers

Company

Support

© 2025 Segment.io, Inc.

=== Content from https://segment.com/docs/connections/destination-data-control/ ===

Redirecting…

=== Content from https://segment.com/docs/guides/regional-segment/ ===

Regional Segment
        

Regional Segment is available to customers on the Business Tier plan.


See the 
available plans
, or 
contact Support
.

On this page

On July 10, 2023, the European Commission adopted the Adequacy Decision for the EU-US Data Privacy Framework (
DPF
). This concludes that EU personal data transferred to the United States under the DPF is adequately protected when compared to the protection in the EU. With this adequacy decision in place, personal data can safely flow from the EU to US companies participating in the DPF without additional safeguards in place.

Twilio is certified under the DPF and relies on the DPF as its primary personal data transfer mechanism for EU-US personal data transfer. Twilio will rely on the DPF for any Swiss-US personal data transfers as soon as a corresponding Swiss adequacy decision is made. Twilio understands that interpretations of data residency are multi-faceted and some customers might still want their data to reside in the EU. Twilio Segment therefore offers a data residency solution outside of the DPF.

Segment offers customers the option to lead on data residency by providing regional infrastructure in both Europe and the United States. The default region for all users is in Oregon, United States. You can configure workspaces to use the EU West Data Processing Region to ingest (for supported sources), process, filter, deduplicate, and archive data through Segment-managed archives hosted in AWS S3 buckets located in Dublin, Ireland. The regional infrastructure has the same 
rate limits and SLA
 as the default region.

Regional Data Ingestion

Regional Data Ingestion enables you to send data to Segment from both Device-mode and Cloud-mode sources through regionally hosted API ingest points. The regional infrastructure can fail-over across locations within a region, but never across regions.

Cloud-event sources

The following cloud sources are supported in EU workspaces:

Client-side sources

You can configure Segment’s client-side SDKs for JavaScript, iOS, Android, and React Native sources to send data to a regional host after you’ve updated the Data Ingestion Region in that source’s settings. Segment’s EU instance only supports data ingestion from Dublin, Ireland with the 
events.eu1.segmentapis.com/
 endpoint. If you are using the Segment EU endpoint with an Analytics-C# source, you must manually append 
v1
 to the URL. For instance, 
events.eu1.segmentapis.com/v1
.

For workspaces that use the EU West Data Processing region, the Dublin Ingestion region is preselected for all sources.

To set your Data Ingestion Region:

All regions are configured on a 
per-source
 basis. You’ll need to configure the region for each source separately if you don’t want to use the default region.

All Segment client-side SDKs read this setting and update themselves automatically to send data to new endpoints when the app reloads. You don’t need to change code when you switch regions.

Server-side and project sources

When you send data from a server-side or project source, you can use the 
host
 configuration parameter to send data to the desired region:

If you are using the Segment EU endpoint with an Analytics-C# source, you must manually append 
v1
 to the URL. For instance, 
events.eu1.segmentapis.com/v1
.

Here is an example of how to set the host:

Analytics.Initialize(
"<YOUR WRITEKEY HERE>"
,
 
new
 
Config().SetHost(
"https://events.eu1.segmentapis.com (https://events.eu1.segmentapis.com/)"
));



Create a new workspace with a different region

Use 
this form
 if you need to transition from your existing US-based workspace to an EU workspace.

To create a workspace with a different data processing region, reach out your Segment account executive, and they will assist you with enabling the feature. Once the feature has been enabled, you’ll be able to self-serve and create a new workspace in a different data processing region by following these steps:

Once you create a workspace with a specified data processing region, you can’t change the region. You must create a new workspace to change the region.

EU Storage Updates

Segment Data Lakes (AWS)

Regional Segment in the EU changes the way you 
configure the Segment Data Lakes (AWS) environment

Warehouse Public IP Range

Use Segment’s custom CIDR 
3.251.148.96/29
 while authorizing Segment to write in to your Redshift or Postgres port. 
BigQuery
 doesn’t require you to allow a custom IP address.

Known Limitations

Regional Segment is currently limited to the EU. Future expansion of Regional Segment beyond the EU is under evaluation by Segment Product and R&D.

Edge proxies are deprecated. Customers using Regional Endpoints may see US-based IP addresses in event payloads, Segment recommends using the US-based endpoint (
api.segment.io
) to preserve client IP addresses. For EU customers, Segment recommends using a Regionalized EU workspace.

Destination support and Regional endpoint availability

Don't see a regional endpoint for a tool you're using?

As more of the partner tools you use (Sources, Destinations, and Warehouses) start to support a regional endpoint, Segment will update this list. Your contact for that tool should have a timeline for when they’re hoping to support regional data ingestion. You can also visit Segment’s 
support page
 for any Segment-related questions.

The following integrations marked with a 
 (checkmark) support EU Regional endpoints.

Integrations available in EU workspaces do not guarantee data residency

Before you configure an integration, you should check directly with the integration partner to determine if they offer EU endpoints.



Source Regional support

Don't see regional support for a source you're using?

As more of the partner Sources start to support posting data to our regional endpoint, Segment will update this list. Your contact for that tool should have a timeline for when they’re hoping to support regional data ingestion. You can also visit Segment’s 
support page
 for any Segment-related questions.

The following Sources marked with a 
 (checkmark) are supported in EU workspaces.



This page was last modified: 28 Jan 2025

Need support?

Questions? Problems? Need more info? Contact Segment Support for assistance!

Help improve these docs!

Was this page helpful?

Thanks for your feedback!

Can we improve this doc? 
Send us feedback!

Get started with Segment

On this page

Was this page helpful?

Thanks for your feedback!

Can we improve
 this doc? 
Send us feedback!

Product

For Developers

Company

Support

© 2025 Segment.io, Inc.

=== Content from https://segment.com/docs/connections/reverse-etl/reverse-etl-catalog/ ===

Reverse ETL Catalog
        

On this page

Reverse ETL supports the Actions destinations listed in this catalog. Most destinations not listed here are supported through the 
Segment Connections
 destination.

Twilio Engage Premier Subscriptions users can use the 
Segment Profiles
 destination to enrich their warehouse data.

The following destinations natively support 
Reverse ETL
. If you don’t see your destination listed in the Reverse ETL catalog, use the 
Segment Connections destination
 to send data from your Reverse ETL warehouse to other destinations listed in the 
catalog
.

Beta

Beta

Beta

Beta

Beta

Beta

Beta

Beta

Beta

Beta

Beta

Beta

Beta

Beta

Beta

Beta

Beta

Beta

Beta

Beta

Beta

Segment Connections destination

If you don’t see your destination listed in the Reverse ETL catalog, use the 
Segment Connections destination
 to send data from your Reverse ETL warehouse to other destinations listed in the 
catalog
.

The Segment Connections destination enables you to mold data extracted from your warehouse in 
Segment Spec
 API calls that are then processed by 
Segment’s HTTP Tracking API
. The requests hit Segment’s servers, and then Segment routes your data to any destination you want. Get started with the 
Segment Connections destination
.

The Segment Connections destination sends data to Segment’s Tracking API, which has cost implications. New users count as new MTUs and each call counts as an API call. For information on how Segment calculates MTUs and API calls, please see 
MTUs, Throughput and Billing
.

Send data to Engage with Segment Profiles

Engage Premier Subscriptions users can use Reverse ETL to sync subscription data from warehouses to destinations.

To get started with using Reverse ETL for subscriptions:

This page was last modified: 10 Sep 2024

Need support?

Questions? Problems? Need more info? Contact Segment Support for assistance!

Help improve these docs!

Was this page helpful?

Thanks for your feedback!

Can we improve this doc? 
Send us feedback!

Get started with Segment

On this page

Was this page helpful?

Thanks for your feedback!

Can we improve
 this doc? 
Send us feedback!

Product

For Developers

Company

Support

© 2025 Segment.io, Inc.

=== Content from https://segment.com/docs/privacy/account-deletion/ ===

Account & Data Deletion
        

On this page

Segment allows you to delete specific data relating to an individual end user, all data from associated with a source, all data related to a Unify space, or all data in your entire workspace.

Delete individual user data

To delete the data for an individual user from you workspace, follow the instructions on the 
User Deletion and Suppression
 page.

Delete data from a source

To delete the data for an entire source, email the Customer Success team 
(friends@segment.com)
 to create a support ticket. In your email to Customer Success, include the following information:

*Due to the way Segment stores data internally, source-level deletions can only be scoped to one day in granularity. Deletion requests for smaller time frames are not supported.

Deleting source data

When Segment deletes your data for a particular source, the deletion is not forwarded to sources or data storage providers associated with your account: your data is only removed from Segment’s S3 archive buckets. To remove your data from external sources, reach out to the individual source about their deletion practices.

Delete the data from a Unify space

Workspace Owners can delete a Unify space and all of its profiles, computed traits, audiences, journeys, and other settings.

To delete a Unify space:

If you are unable to delete your Unify space, send an email to Segment’s Customer Success Team 
(friends@segment.com)
 with your workspace slug and the name of the Unify space you’d like to delete.

Segment does not begin a Unify space deletion until 5 calendar days after you initiate a deletion request. If you would like to reverse your space deletion request, you must cancel your request during the 5 calendar days after your initial request. Once Segment deletes a Unify space, it can’t be recovered.

Cancel a Unify space deletion request

If you want to cancel your Unify space deletion request:

Unify space deletion doesn’t delete data from connected Twilio Engage destinations. To remove your data from external destinations, reach out to the individual destination about their deletion practices.

Delete your workspace data

Workspace admins can delete all of the data associated with a workspace, including customer data.

To delete all data from one workspace:

Segment will irrevocably delete your workspace 5 days after you initiate your deletion request.

If you want to revoke the workspace deletion request during the 5 days after you initiated your request, open the 
Workspace Settings
 page, select the 
General Settings
 tab and click 
Revoke Workspace Deletion
.

To delete data from all workspaces in which you have workspace admin permissions:

After you delete your workspace or account, Segment removes all data associated with each workspace within 30 days in a process called a 
complete data purge
. For a data purge status update, email the Customer Success team 
(friends@segment.com)
.

If you don’t delete your workspace after you stop using Segment, 
your data remains in Segment’s internal servers until you submit a written deletion request
.

Purging data from workspaces deleted prior to March 31, 2022

If you deleted your workspace prior to March 31, 2022, and would like to have data associated with your workspace purged from Segment’s S3 archive buckets, email the Customer Success team 
(friends@segment.com)
 to create a support ticket. In your email to Customer Success, include either the slug or the ID of the workspace you’d like to have purged from internal Segment servers.

What is a complete data purge?

A complete data purge is the way Segment removes all workspace and customer data from internal servers across all product areas. To trigger a complete data purge, either 
delete your workspace
 or raise a support ticket with the Customer Success team by emailing 
(friends@segment.com)
. In your email to Customer Success, include either the slug or the ID of the workspace that you’d like to delete. Deletions related to data purges will 
not
 be forwarded to your connected third-party destinations or raw data destinations.

Segment waits for five calendar days before beginning a complete data purge to safeguard against malicious deletion requests. If you notice your workspace or account has been maliciously deleted, reach out to 
friends@segment.com
 to cancel the data purge. After the five-day grace period, the deletion will be irreversible.

This page was last modified: 07 Feb 2025

Need support?

Questions? Problems? Need more info? Contact Segment Support for assistance!

Help improve these docs!

Was this page helpful?

Thanks for your feedback!

Can we improve this doc? 
Send us feedback!

Get started with Segment

On this page

Was this page helpful?

Thanks for your feedback!

Can we improve
 this doc? 
Send us feedback!

Product

For Developers

Company

Support

© 2025 Segment.io, Inc.

=== Content from https://segment.com/docs/connections/reverse-etl/ ===

Reverse ETL
        

Reverse ETL (Extract, Transform, Load) extracts data from a warehouse using a query you provide and syncs this warehouse data to your third party destinations.

Use Reverse ETL when you want to:

Reverse ETL supports event and object data

Event and object data includes customer profile data, subscriptions, product tables, shopping cart tables, and more.

Get started with Reverse ETL

Set up the infrastructure you need to sync data from your warehouse to your downstream destinations.

View your sync history, reset your syncs, or subscribe to alerts.

Learn more

Learn more about the system that powers Reverse ETL, supported destinations, and frequently asked questions.

Reference material about system limits and how Segment detects data changes.

View the destinations you can connect to your Reverse ETL sources.

Frequently asked questions about Reverse ETL.

More Reverse ETL resources

In this blog from Segment, learn how Reverse ETL helps businesses activate their data to drive better decision-making and greater operational efficiency.

Learn how MongoDB used Reverse ETL to connect the work of analytics teams to downstream marketing and sales tools to deliver just-in-time communications that increased customer satisfaction and engagement.

This page was last modified: 10 Sep 2024

Need support?

Questions? Problems? Need more info? Contact Segment Support for assistance!

Help improve these docs!

Was this page helpful?

Thanks for your feedback!

Can we improve this doc? 
Send us feedback!

Get started with Segment

Was this page helpful?

Thanks for your feedback!

Can we improve
 this doc? 
Send us feedback!

Product

For Developers

Company

Support

© 2025 Segment.io, Inc.

=== Content from https://segment.com/docs/engage/journeys/journeys-edits/ ===

Journeys Edits & Versioning
        

Engage Foundations requires a Business tier account and includes Unify.


See the 
available plans
, or 
contact Support
.

On this page

With Journeys edits and versioning, you can make changes to live journeys.

Before you begin

Keep the following in mind when you edit a journey:

Edit a journey

Follow these steps to edit a journey:

After you’ve published, users who meet the entry criteria can enter the new journey version.

You can return to the Journeys list page to view the new live journey and its previous versions, which are nested under the journey container.

Journey settings

A Journey’s settings can’t be edited once the Journey has been published, including 
entry and exit settings
. The only settings you can change after publishing a Journey are the Journey’s name and description.

Working with Journeys versioning

Exit settings and user flow between journeys

Exit settings determine how users flow between journey versions.

Suppose you have a journey with exit settings enabled. The following table lists the actions you can take with the journey, as well as the results:

Suppose you have a journey 
without
 enabled exit settings. If you pause or archive Version 1 when publishing Version 2 of that journey, then users can immediately enter Version 2 when they meet its entry criteria, even if they’re still in Version 1.

Version exclusion

To prevent users from a previous journey from ever entering a new journey version, enable version exclusion when you create the new journey version.

List destinations

Adding a list destination to a journey version creates a new record in Segment’s systems. This process can take up to ten hours. During this time, you’ll be unable to publish new versions of a journey.

For example, if you add a list destination to Version 1 of a journey, and users begin flowing into the version, then Segment will begin creating the new record. If you create a Version 2 draft from Version 1 of the journey while Segment is still creating the new record, you won’t be able to publish Version 2 until this process is completed.

If the version has a list destination but no users have flowed into the version, though, Segment won’t create a new record for that list destination, and you won’t have to wait to publish a new journey version.

This page was last modified: 03 Oct 2024

Need support?

Questions? Problems? Need more info? Contact Segment Support for assistance!

Help improve these docs!

Was this page helpful?

Thanks for your feedback!

Can we improve this doc? 
Send us feedback!

Get started with Segment

On this page

Was this page helpful?

Thanks for your feedback!

Can we improve
 this doc? 
Send us feedback!

Product

For Developers

Company

Support

© 2025 Segment.io, Inc.

=== Content from https://segment.com/docs/engage/audiences/generative-audiences/ ===

Generative Audiences
        

Engage Foundations requires a Business tier account and includes Unify.


See the 
available plans
, or 
contact Support
.

On this page

With Generative Audiences, part of Segment’s AI capabilities, you can use use generative AI to create Engage Audiences with natural language prompts.

Describe your desired audience based on events performed, profile traits, or existing audiences in your workspace. Based on your prompt, Segment builds the audience with generative AI.

For more details on AI usage and data, see 
Generative Audiences Nutrition Facts Label
.

In this article, you’ll learn how to use Generative Audiences along with some best practices.

Create an audience with Generative Audiences

To create an audience with Generative Audiences:

To help you write your prompt, view these 
example prompts
 and 
best practices
.

To use Generative Audiences, a workspace owner must first accept Segment’s Terms and Conditions.

Modify an audience description

Once Segment generates the audience conditions, the prompt box remains open for reference. You can close this box, or modify your audience description and click 
Build with AI
 again.

Modifying an audience description overwrites the existing conditions previously generated. You can also edit any conditions straight from the audience builder.

Example prompts

Use the following examples to help you get started with audience prompts.

To build an audience with customers who haven’t made a purchase in the last 30 days, enter: 
Customers who haven't purchased in the last 30 days.

To find all profiles that have recently opened an email, enter: 
Profiles that recently opened an email.

To build an audience with customers who spend over $50 on an order, enter: 
Customers who have orders greater than $50.

You’ll have more accurate results if you base your audience prompts on specific events and traits that are in your Segment space.

Using negative conditions

This section shows a few examples of how Generative Audiences configures audience conditions for negative prompts. Negative conditions might include, for example, building an audience of users without a certain profile trait, or who haven’t performed certain events.

Best practices

As you use Generative Audiences, keep the following best practices in mind:

View events and traits in your workspace

As you’re writing your prompt, you can view traits and events that are active in your workspace from the audience builder. After you add a condition in the builder, click the property field to view active and inactive traits or events in your workspace.

You can also use the Profile explorer (
Unify
 > 
Profile explorer
) to view specific events and traits associated with profiles in your Segment space.

Learn more about 
using existing events and traits
 to build audiences.

Due to a 
limited space schema
, Segment may not recognize some events or traits that are inactive in your workspace.

Error handling

Engage uses the following error messages with Generative Audiences:

Known limitations

Limited space schema

Segment’s generative AI service is handled by a third party that needs context about your Engage workspace and has limitations to how many contextual parameters Segment can send it.
Segment solves this limitation by including only the most recently used properties and values for events and traits within your Engage space. As a result, some event and traits within your workspace may not be recognized.

Language support

At this time, Segment only supports audience description prompts in the English language. Support in other languages is currently unavailable and might provide undesired results.

This page was last modified: 17 Nov 2024

Need support?

Questions? Problems? Need more info? Contact Segment Support for assistance!

Help improve these docs!

Was this page helpful?

Thanks for your feedback!

Can we improve this doc? 
Send us feedback!

Get started with Segment

On this page

Was this page helpful?

Thanks for your feedback!

Can we improve
 this doc? 
Send us feedback!

Product

For Developers

Company

Support

© 2025 Segment.io, Inc.

=== Content from https://segment.com/docs/engage/campaigns/ ===

Campaigns Overview
        

Engage Premier requires a Business tier account and includes Engage Foundations and Unify.


See the 
available plans
, or 
contact Support
.

On this page

Engage Premier entered an End of Sale (EOS) period effective  June 10, 2024. Existing Segment customers will continue to have access and support to Engage Premier until an end-of-life (EOL) date is announced. We recommend exploring the following pages in preparation of a migration or future MCM needs:

Twilio Marketing Campaigns

Preferred ISV Partners:

Airship Blog
 


Bloomreach Blog
 


Braze Blog
 


Insider Blog
 


Klaviyo Blog
 


Twilio Engage Foundations Documentation
 

With Engage, you can build email and SMS marketing campaigns within Journeys.

Use real-time data and unified customer profiles to send personalized messages to subscribed users. Build and send email and SMS campaigns for multi-channel customer engagement.

Visit Segment’s 
Journeys documentation
 for more information on how to build a Journey.

Send personalized campaigns

Use Engage to send 
email
 and 
SMS campaigns
 in Journeys:

Build email and SMS templates

With Engage, you can build 
email
 and 
SMS templates
 to use throughout your campaigns. Build an email template using a drag and drop or a visual HTML editor. Personalize templates with merge tags and test your messages before you send them in campaigns.

Engage saves the message templates for you to preview, maintain, and reuse throughout your campaigns.

This page was last modified: 15 Jul 2024

Need support?

Questions? Problems? Need more info? Contact Segment Support for assistance!

Help improve these docs!

Was this page helpful?

Thanks for your feedback!

Can we improve this doc? 
Send us feedback!

Get started with Segment

On this page

Was this page helpful?

Thanks for your feedback!

Can we improve
 this doc? 
Send us feedback!

Product

For Developers

Company

Support

© 2025 Segment.io, Inc.

=== Content from https://segment.com/docs/getting-started/02-simple-install/ ===

A Basic Segment Installation
        

On this page

When you implement Segment, you add Segment code to your website, app, or server. This code generates messages based on specific triggers you define.

In a basic implementation, the code can be a snippet of JavaScript that you copy and paste into the HTML of a website to track page views. It can also be as complex as Segment calls embedded in a React mobile app to send messages when the app is opened or closed, when the user performs different actions, or when time based conditions are met (for example “ticket reservation expired” or “cart abandoned after 2 hours”).

The best way to learn about how Segment works is to see it in action. This tutorial walks you through an installation using one of Segment’s libraries: JavaScript, PHP, or the iOS library.

Before you begin

Before you start your Segment implementation, you need:

Tip
! If you don’t have any of those things, consider creating a simple 
GitHub Pages website
.

Create separate dev and prod sources

When you develop and test sources, Segment recommends you to create and use separate sources for each of your environments (production, development, staging) to prevent testing and development activities from filling production systems with invalid data.

You can give each source an environment label when you create it, and Segment strongly suggests that you use these labels to sort your sources. When you create a source during the steps below, make sure you enter an environment label.

Double-check when you enter write keys for dev and production environments to make sure that you send the right data to the right place.

Create a Segment source

To create a Segment source:

Find your write key

The write key is a unique identifier for a source that tells Segment which source the data comes from, to which workspace the data belongs, and which destinations should receive the data.

To find your write key:

Make note of or write down your write key, as you’ll need it in the next steps.

Any time you change a library’s settings in the Segment App, the write key regenerates.



Cloud-sources
 do not have write keys, as they use a token or key from your account with that service. Cloud-sources have other considerations and aren’t part of this tutorial.

Installing Segment

Click a tab below to see the tutorial content for the specific library you chose.

Step 1: Copy the Snippet


Navigate to 
Connections > Sources > JavaScript
 in the Segment app, copy the snippet from the JavaScript Source overview page, and paste it into the 
<head>
 tag of your site.


That snippet loads Analytics.js onto the page 
asynchronously
, so it won’t affect your page load speed. Once the snippet runs on your site, you can turn on destinations from the destinations page in your workspace and data starts loading on your site automatically.


The Segment snippet version history available on 
GitHub
. Segment recommends that you use the latest snippet version whenever possible.



Step 2: Identify Users


The 
identify
 method is how you tell Segment who the current user is. It includes a unique User ID and any optional traits you know about them. You can read more about it in the 
identify method reference
.


You don't need to call `identify` for anonymous visitors to your site

Segment automatically assigns them an 
anonymousId
, so just calling 
page
 and 
track
 works just fine without 
identify
.


Here’s an example of what a basic call to 
identify
 might look like:

analytics
.
identify
(
'
f4ca124298
'
,
 
{

  
name
:
 
'
Michael Brown
'
,

  
email
:
 
'
mbrown@example.com
'


});




This identifies Michael by his unique User ID (in this case, 
f4ca124298
, which is what you know him by in your database) and labels him with 
name
 and 
email
 traits. When you put that code on your site, you need to replace those hard-coded trait values with the variables that represent the details of the currently logged-in user.


To do that, Segment recommends that you use a backend template to inject an 
identify
 call into the footer of 
every page
 of your site where the user is logged in. That way, no matter what page the user first lands on, they will always be identified. You don’t need to call 
identify
 if your unique identifier (
userId
) is not known.


Depending on your templating language, your actual identify call might look something like this:



analytics
.
identify
(
'
 {{user.id}} 
'
,
 
{

  
name
:
 
'
{{user.fullname}}
'
,

  
email
:
 
'
{{user.email}}
'


});





With that call in your page footer, you successfully identify every user that visits your site.


You’ve completed a basic CRM set up. Return to the Segment app to enable Salesforce, Intercom, or your CRM system of choice and Segment starts sending all of your user data to it.



Step 3: Track Actions


The 
track
 method is how you tell Segment about the actions your users are performing on your site. Every action triggers what Segment calls an “event”, which can also have associated properties. You can read more about 
track
 in the 
track method reference
.


Here’s an example of what a call to 
track
 might look like when a user signs up:

analytics
.
track
(
'
Signed Up
'
,
 
{

  
plan
:
 
'
Enterprise
'


});




This example shows that your user triggered the 
Signed Up
 event and chose your hypothetical 
'Enterprise'
 plan.


Properties can be anything you want to record, for example:

analytics
.
track
(
'
Article Bookmarked
'
,
 
{

  
title
:
 
'
Snow Fall
'
,

  
subtitle
:
 
'
The Avalanche at Tunnel Creek
'
,

  
author
:
 
'
John Branch
'


});




If you’re just getting started, some of the events you should track are events that indicate the success of your site, like 
Signed Up
, 
Item Purchased
 or 
Article Bookmarked
. Segment recommends that you track a few important events as you can always add more later.


Once you add a few 
track
 calls, you’re done with setting up Segment. You successfully installed Analytics.js tracking. Now you’re ready to turn on any destination you like from the Segment App.

Step 1: Install the SDK


To install Analytics-iOS, Segment recommends you to use 
CocoaPods
, because it allows you to create a build with specific bundled destinations, and because it makes it simple to install and upgrade.


1) Add the 
Analytics
 dependency to your 
Podfile
 by adding the following line:

pod
 
'Analytics'
,
 
'~> 3.0'




2) In your application delegate’s 
- application:didFinishLaunchingWithOptions:
 method, set up the SDK like so:

SEGAnalyticsConfiguration
 
*
configuration
 
=
 
[
SEGAnalyticsConfiguration
 
configurationWithWriteKey
:
@"YOUR_WRITE_KEY"
];


configuration
.
trackApplicationLifecycleEvents
 
=
 
YES
;
 
// Enable this to record certain application events automatically!


configuration
.
recordScreenViews
 
=
 
YES
;
 
// Enable this to record screen views automatically!


[
SEGAnalytics
 
setupWithConfiguration
:
configuration
];





You don’t 
need
 to use initialization config parameters to track lifecycle events (
Application Opened
, 
Application Installed
, 
Application Updated
) and screen views automatically, but Segment highly recommends that you do, so you can start off already tracking some important core events.


3) Import the SDK in the files that you use it by adding the following line:

#import <Analytics/SEGAnalytics.h>




To keep the Segment SDK lightweight, the 
Analytics
 pod only installs the Segment library. This means all of the data goes first to Segment’s servers, and is then forwarded to any destination tools which accept the data 
from Segment
.


Some destinations don’t accept data from the Segment servers, and instead require that you collect the data from the device. In these cases you must bundle some additional destination code with the Segment SDK. This document skips over this part, but you can see the instructions on 
how to bundle the destination tools
.


Now that the SDK is installed and set up, you’re ready to start making calls.


Step 2: Identify Users


The 
identify
 method is how you tell Segment who the current user is. It takes a unique User ID, and any optional traits you know about them. You can read more about it in the 
identify reference
.


Here’s an example of what a basic call to 
identify
 might look like:

[[
SEGAnalytics
 
sharedAnalytics
]
 
identify
:
@"f4ca124298"

                                  
traits:
@{
 
@"name"
:
 
@"Michael Brown"
,

                                    
@"email"
:
 
@"mbrown@example.com"
 
}];




This call identifies Michael by his unique User ID (
f4ca124298
, which is the one you know him by in your database) and labels him with 
name
 and 
email
 traits.


When you put the above code in your iOS app, you would replace those hard-coded trait values with variables that represent the details of the user that’s currently signed in.



Step 3: Track Actions


The 
track
 method is how you tell Segment about the actions your users are performing in your app. Every action triggers what we call an “event”, which can also have associated properties. You can read more about 
track
 in the 
track method reference
.

The Segment iOS SDK can automatically track a few important common events, such as 
Application Installed
, 
Application Updated
, and 
Application Opened
. You can enable this option during initialization by adding the following lines:

SEGAnalyticsConfiguration
 
*
configuration
 
=
 
[
SEGAnalyticsConfiguration
 
configurationWithWriteKey
:
@"YOUR_WRITE_KEY"
];


configuration
.
trackApplicationLifecycleEvents
 
=
 
YES
;


[
SEGAnalytics
 
setupWithConfiguration
:
configuration
];




You should also track events that indicate success in your mobile app, like 
Signed Up
, 
Item Purchased
, or 
Article Bookmarked
. Segment recommends that you track a few important events as you can always add more later.


Here’s what a 
track
 call might look like when a user signs up:

[[
SEGAnalytics
 
sharedAnalytics
]
 
track
:
@"Signed Up"

                           
properties:
@{
 
@"plan"
:
 
@"Enterprise"
 
}];




This tells us that your user triggered the 
Signed Up
 event, and chose your hypothetical 
'Enterprise'
 plan.


Properties can be anything you want to record, for example:

[[
SEGAnalytics
 
sharedAnalytics
]
 
track
:
@"Article Bookmarked"

                           
properties:
@{

                               
@"title"
:
 
@"Snow Fall"
,

                               
@"subtitle"
:
 
@"The Avalanche at Tunnel Creek"
,

                               
@"author"
:
 
@"John Branch"
 
}];




Once you’ve added a few 
track
 calls, you’re all set. You successfully instrumented your app, and can enable destinations from your Segment workspace.

Step 1: Download the library


To install the library:


1) Clone the repository from GitHub into your desired application directory. (If you’re a composer user, 
you can use this
.)

git clone https://github.com/segmentio/analytics-php /my/application/folders/



2) Add the following to your PHP script to load the Segment analytics library in your code:

require_once
(
"/path/to/analytics-php/lib/Segment.php"
);


use
 
Segment\Segment
;




3) In your initialization script, make the following call (In the example, Segment first renames this module to 
Analytics
 for convenience):

# Set up our Segment tracking and


# alias to Analytics for convenience


class_alias
(
'Segment'
,
 
'Analytics'
);


Segment
::
init
(
"YOUR_WRITE_KEY"
);




4) Replace 
YOUR_WRITE_KEY
 with the actual write key, which you can find in Segment under your project settings. Otherwise, all that data goes straight to 
/dev/null
.


You only need to call 
init
 once when your php file is requested. All of your files then have access to the same 
Analytics
 client.

Segment’s default PHP consumer is the 
libcurl consumer
. If this is not working well for you or if you have a high-volume project, you might try one of Segment’s other consumers like the 
fork-curl consumer
.



Step 2: Identify Users


The 
identify
 method is how you tell Segment who the current user is. It includes a unique User ID and any optional traits that you might know about them.


Here’s what a basic call to 
identify
 might look like:

Segment
::
identify
(
array
(

  
"userId"
 
=>
 
"f4ca124298"
,

  
"traits"
 
=>
 
array
(

    
"name"
 
=>
 
"Michael Brown"
,

    
"email"
 
=>
 
"mbrown@example.com"

  
)


));




This identifies Michael by his unique User ID (in this case, 
f4ca124298
, which is what you know him by in your database) and labels him with 
name
 and 
email
 traits.

When you actually put that code on your site, you need to replace those hard-coded trait values with the variables that represent the details of the currently logged-in user. The easiest way in PHP is to keep a 
$user
 variable in memory.

Segment
::
identify
(
array
(

  
"userId"
 
=>
 
$user
->
id
,

  
"traits"
 
=>
 
array
(

    
"name"
 
=>
 
$user
->
fullname
,

    
"email"
 
=>
 
$user
->
email

  
)


));




With that call on the page, you’re now identifying every user that visits your site.


If you only want to use a basic CRM set up, you can stop here. Just enable Salesforce, Intercom, or any other CRM system from your Segment workspace, and Segment starts sending all of your user data to it.



Step 3: Track Actions


The 
track
 method is how you tell Segment about the actions your users are performing on your site. Every action triggers what Segment calls an “event”, which can also have associated properties.


Here’s what a call to 
track
 might look like when a user signs up:

Segment
::
track
(
array
(

  
"userId"
 
=>
 
"f4ca124298"
,

  
"event"
 
=>
 
"Signed Up"
,

  
"properties"
 
=>
 
array
(

    
"plan"
 
=>
 
"Enterprise"

  
)


));




This tells us that the user triggered the 
Signed Up
 event, and chose your hypothetical 
Enterprise
 plan.


Properties can be anything you want to record, for example:

Segment
::
track
(
array
(

  
"userId"
 
=>
 
"f4ca124298"
,

  
"event"
 
=>
 
"Article Bookmarked"
,

  
"properties"
 
=>
 
array
(

    
"title"
 
=>
 
"Snow Fall"
,

    
"subtitle"
 
=>
 
"The Avalanche at Tunnel Creek"
,

    
"author"
 
=>
 
"John Branch"

  
)


));




If you’re just getting started, some of the events you should track are events that indicate the success of your site, like 
Signed Up
, 
Item Purchased
 or 
Article Bookmarked
.


To get started, Segment recommends you to track track a few important events as you can always add more later.



Step 4: Flush the data


Call the Segment 
flush()
 method. This manually sends all the queued call data to make sure it makes it to the Segment servers. This is normally done automatically by the runtime, but some PHP installations won’t do it for you, so it’s worth calling at the end of your script, just to be safe.


Segment
::
flush
();



You’ve successfully installed PHP tracking. Now you’re ready to turn on any destination from the Segment App.

Test that it’s working

Once you’ve set up your Segment library, and instrumented at least one call, you can look at the Debugger tab for the Source to check that it produces data as you expected.

The Source Debugger is a real-time tool that helps you confirm that API calls made from your website, mobile app, or servers arrive at your Segment Source, so you can quickly see how calls are received by your Segment source, so you can troubleshoot quickly without having to wait for data processing.



The Debugger is separate from your workspace’s data pipeline, and is not an exhaustive view of all the events ever sent to your Segment workspace. The Debugger only shows a sample of the events that the Source receives in real time, with a cap of 500 events. The Debugger is a great way to test specific parts of your implementation to validate that events are being fired successfully and arriving to your Source.

Tip
: To see a more complete view of all your events, you might consider setting up either a 
warehouse
 or an 
S3 destination
.

The Debugger shows a live stream of sampled events arriving at the Source, but you can also toggle from “Live” to “Pause” to stop the stream and prevent it from displaying new events. Events continue to arrive to your Source while you Pause the stream, they just are not displayed.

You can search on any information you know is available in an event payload to search in the Debugger and show only matching payloads. You can also use advanced search options to limit the results to a specific event.



Two views are available when viewing a payload:

Set up your first destination

Once you’re satisfied that data is arriving from your new source, it’s time to set up your first destination! As long as you have 
page
 or 
screen
 data coming from the source, you can quickly enable Google Analytics to look at the page view statistics.

If you don’t have a Google Analytics account, you can either set up a free account, or look at the Destination Catalog for a different destination to enable.

You’ll need a tracking ID for Google Analytics (either a “website” or “serverside” tracking ID), or another API key if you’re substituting another destination. Make a note of this ID or key as you’ll need it to connect your destination.

To set up your first destination:

Congratulations! Data is now flowing from the source you set up, to the first destination. Do some test browsing on your site or app, then log in to your downstream tool to see the data in place.

You can click around and load pages to see your Segment calls in action, watch them arrive in the Debugger, and see them arrive in the destination tool.

Note:
 When you’re done with this test source and destination, you can delete them. This prevents you from getting unplanned “demo” data in your production environment later.

The basics of the Segment platform and what you can do with it.

Think through your goals, plan your calls, and set yourself up for success.

This page was last modified: 20 Feb 2025

Need support?

Questions? Problems? Need more info? Contact Segment Support for assistance!

Help improve these docs!

Was this page helpful?

Thanks for your feedback!

Can we improve this doc? 
Send us feedback!

Get started with Segment

On this page

Was this page helpful?

Thanks for your feedback!

Can we improve
 this doc? 
Send us feedback!

Product

For Developers

Company

Support

© 2025 Segment.io, Inc.

=== Content from https://segment.com/docs/unify/identity-resolution/use-cases/ ===

Identity Resolution Use-Cases
        

Unify requires a Business tier account and is included with Engage.


See the 
available plans
, or 
contact Support
.

On this page

Identity Resolution helps create a unified view of the user across devices, apps, and unique identifiers. Identity resolution is critical to understanding the customer journey at multiple touch points, which allows brands to deliver personalized experiences to its customers at scale.

Anonymous to known identification

Identity Resolution allows a company to link a customer’s journey from pre-account creation to post-account activity. This is important to help a brand understand the behaviors that lead a user to convert from a window shopper in the discovery stage to a buyer with intent in the consideration and decision stage to the loyal return customer in the conversion and retention stage.

By linking any anonymous events a user had before creating an account to a user’s logged-in activity, a marketing team can now have a complete understanding of a user’s past interactions with your app.

This can lead to invaluable insights into the behaviors and triggers in an app that motivate a user to register for an account.

Cross-device identification

Users can have multiple touch points with an app ecosystem through more than one device. For example, users might view an eCommerce site through a mobile native app, a mobile web browser, or a desktop web browser.

By tracking a user’s activity across all platforms, brands will be able to more efficiently target campaigns to users as they’ll have the knowledge of funnels that complete across devices.

For example, a user who adds a product to a cart on the iPhone app but completes the checkout on the Android app shouldn’t be targeted with abandoned cart push notifications on the iPhone app.

Cross-app identification

A company’s product ecosystem may also spread out across multiple apps.

If a company needs to understand a user’s activity across all apps, Segment recommends connecting all sources to the same Space. This provides a comprehensive view of a user’s activity across the entire app ecosystem.

If, however, each app should maintain its own metrics and LTV analysis, regardless of the overlap of users between apps, Segment recommends creating a separate Space per app and only connecting sources related to each app to its space. This will give a siloed view of how users interact with each individual app.

Each workspace has two spaces by default. Contact your CSM to enable additional spaces.

To learn more, visit Segment’s 
eCommerce Example doc
.

Cross-Channel identification

A user can interact with a brand through multiple channels and departments. A user might have touch points with a sales team, a marketing team, and a customer support team throughout their customer journey. It’s important for companies to have insights into these cross-functional activities to ensure they understand the complete customer experience.

For example, if a user has logged a complaint with a customer support team, the marketing team should exclude this user from an automatic follow-up email asking for them to leave a public product review on their site.

This page was last modified: 28 Mar 2023

Need support?

Questions? Problems? Need more info? Contact Segment Support for assistance!

Help improve these docs!

Was this page helpful?

Thanks for your feedback!

Can we improve this doc? 
Send us feedback!

Get started with Segment

On this page

Was this page helpful?

Thanks for your feedback!

Can we improve
 this doc? 
Send us feedback!

Product

For Developers

Company

Support

© 2025 Segment.io, Inc.

=== Content from https://segment.com/docs/connections/storage/warehouses/ ===

Data Warehouses
        

Free and Team plan workspaces can have 1 warehouse. Business plans can have more than one, and include custom sync schedules and filtering.


See the 
available plans
, or 
contact Support
.

On this page

What’s a Warehouse?

A warehouse is a central repository of data collected from one or more sources. This is what commonly comes to mind when you think about a relational database: structured data that fits neatly into rows and columns.

In Segment, a Warehouse is a special type of destination. Instead of streaming data to the destination all the time, we load data to them in bulk at regular intervals. When we load data, we insert and update events and objects, and automatically adjust their schema to fit the data you’ve sent to Segment.

When selecting and building a data warehouse, consider three questions:

Relational databases are great when you know and predefine the information collected and how it will be linked. This is usually the type of database used in the world of user analytics. For instance, a users table might be populated with the columns 
name
, 
email_address
, or 
plan_name
.

Examples of data warehouses include Amazon Redshift, Google BigQuery, and Postgres.

When Segment loads data into your warehouse, each sync goes through two steps:

Looking for the Warehouse Schemas docs?

They’ve moved: 
Warehouse Schemas
.

When your existing analytics tools can't answer your questions, it's time to level-up and use SQL for analysis.

More Help

How do I send custom data to my warehouse?

How do I give users permissions to my warehouse?

Check out the 
Frequently Asked Questions about Warehouses
 page and 
a list of helpful SQL queries to get you started with Redshift 
.

FAQs

How do I decide between Redshift, Postgres, and BigQuery?

What do you recommend for Postgres: Amazon or Heroku?

How do I give users permissions?

What are the limitations of Redshift clusters and warehouses connectors?

Where do I find my source slug?

Setting up a warehouse

How do I create a user, grant usage on a schema and then grant the privileges that the user will need to interact with that schema?

Which IPs should I allowlist?

Will Segment sync my historical data?

Can I load in my own data into my warehouse?

Can I control what data is sent to my warehouse?

Managing a warehouse

How fresh is the data in my warehouse?

Can I add, tweak, or delete some of the tables?

Can I transform or clean up old data to new formats or specs?

What are common errors and how do I debug them?

How do I speed up my Redshift queries?

Analyzing with SQL

How do I forecast LTV with SQL and Excel for e-commerce businesses?

How do I measure the ROI of my Marketing Campaigns?

This page was last modified: 12 Aug 2024

Need support?

Questions? Problems? Need more info? Contact Segment Support for assistance!

Help improve these docs!

Was this page helpful?

Thanks for your feedback!

Can we improve this doc? 
Send us feedback!

Get started with Segment

On this page

Was this page helpful?

Thanks for your feedback!

Can we improve
 this doc? 
Send us feedback!

Product

For Developers

Company

Support

© 2025 Segment.io, Inc.

=== Content from https://segment.com/docs/connections/spec/identify/ ===

Spec: Identify
        

On this page

The Segment Identify call lets you tie a user to their actions and record traits about them.  It includes a unique User ID and any optional traits you know about the user, like their email, name, and more.

Check out our high-level overview of the Identify method in Segment University. (Must be logged in to access.)

Segment recommends that you make an Identify call:

The first three examples are pretty self-explanatory, but many might ask: why you would call Identify on every page load if you’re storing the 
userId
 in the cookie/local storage?

Calling Identify in one of Segment’s 
libraries
 is one of the first steps to getting started with Segment. Refer to library-specific documentation for more details.

Here’s the payload of a typical Identify call with most 
common fields
 removed:

{

  
"type"
:
 
"identify"
,

  
"traits"
:
 
{

    
"name"
:
 
"Peter Gibbons"
,

    
"email"
:
 
"peter@example.com"
,

    
"plan"
:
 
"premium"
,

    
"logins"
:
 
5

  
},

  
"userId"
:
 
"97980cfea0067"


}



And here’s the corresponding JavaScript event that would generate the above payload:

analytics
.
identify
(
"
97980cfea0067
"
,
 
{

  
name
:
 
"
Peter Gibbons
"
,

  
email
:
 
"
peter@example.com
"
,

  
plan
:
 
"
premium
"
,

  
logins
:
 
5


});



Based on the library you use, the syntax in the examples might be different. You can find library-specific documentation on the 
Sources Overview
 page.

Beyond the common fields, an Identify call has the following fields:

Note that these traits coming in from your source events are called 
custom traits
.

Example

Here’s a complete example of an Identify call:

{

  
"anonymousId"
:
 
"507f191e810c19729de860ea"
,

  
"channel"
:
 
"browser"
,

  
"context"
:
 
{

    
"ip"
:
 
"8.8.8.8"
,

    
"userAgent"
:
 
"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_5) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/40.0.2214.115 Safari/537.36"

  
},

  
"integrations"
:
 
{

    
"All"
:
 
false
,

    
"Mixpanel"
:
 
true
,

    
"Salesforce"
:
 
true

  
},

  
"messageId"
:
 
"022bb90c-bbac-11e4-8dfc-aa07a5b093db"
,

  
"receivedAt"
:
 
"2015-02-23T22:28:55.387Z"
,

  
"sentAt"
:
 
"2015-02-23T22:28:55.111Z"
,

  
"timestamp"
:
 
"2015-02-23T22:28:55.111Z"
,

  
"traits"
:
 
{

    
"name"
:
 
"Peter Gibbons"
,

    
"email"
:
 
"peter@example.com"
,

    
"plan"
:
 
"premium"
,

    
"logins"
:
 
5
,

    
"address"
:
 
{

      
"street"
:
 
"6th St"
,

      
"city"
:
 
"San Francisco"
,

      
"state"
:
 
"CA"
,

      
"postalCode"
:
 
"94103"
,

      
"country"
:
 
"USA"

    
}

  
},

  
"type"
:
 
"identify"
,

  
"userId"
:
 
"97980cfea0067"
,

  
"version"
:
 
"1.1"


}



Create your own Identify call

Use the following interactive code pen to see what your Identify calls would look like with user-provided information:

Sample output goes here!

Identities

The Identify call specifies a customer identity that you can reference across the customer’s whole lifetime. 
Every Identify call must have a 
User ID
 or an 
Anonymous ID
, depending on how much you know about the user in question.

Anonymous ID

There are certain cases where you don’t actually know who the user is according to your database, but you still want to be able to tie them to traits, events, or page views. For example, you may not know who a user is when tracking newsletter signups or anonymous page views.

In these cases, you should use an Anonymous ID.

The Anonymous ID can be any pseudo-unique identifier. For example, on your servers you can use a session id. If you don’t have any readily available identifier, you can always generate a new random one — Segment recommends 
UUIDv4 format
.

Segment’s 
browser and mobile libraries
 automatically use Anonymous IDs to keep track of users as they navigate around your website or app, so you don’t need to worry about them when using those libraries.

Here’s an example of a JavaScript event for an anonymous user:

analytics
.
identify
({

  
subscriptionStatus
:
 
'
inactive
'


});



User ID

User IDs are a more permanent and robust identifier, like a database ID. Since these IDs are consistent across a customer’s lifetime, Identify calls should include a User ID as often as possible.

A User ID is usually the unique identifier that you recognize a user by in your own database. For example, if you’re using MongoDB, User IDs might look something like this: 
507f191e810c19729de860ea
.

Segment recommends using database IDs, 
in 
uuidv4
 format
, instead of email addresses or usernames because database IDs 
never
 change. That guarantees that even if the user changes their email address, you can still recognize them as the same person in all of your analytics tools, and you’ll be able to correlate analytics data with your own internal database.

Instead of using an email address or a username as a User ID, send them along as 
custom traits
.

Custom traits

Custom traits
 are pieces of information you know about a user that are included in an Identify call. These could be demographics like 
age
 or 
gender
, account-specific like 
plan
, or even things like whether a user has seen a particular A/B test variation.

Segment has reserved some custom traits that have semantic meanings for users, and will handle them in special ways. For example, Segment always expects 
email
 to be a string of the user’s email address. Segment sends this on to destinations like 
Mailchimp
 that require an email address for their tracking.

Only use reserved traits for their intended meaning.

Reserved custom traits Segment has standardized:

You might be used to some destinations recognizing special traits by slightly different names. For example, Mixpanel recognizes a 
$created
 trait when the user’s account was first created, while Intercom recognizes the same trait as 
created_at
 instead. Segment attempts to handle all the destination-specific conversions for you automatically. If you need help understanding if a specific field will be converted to a destination, take a look at Segment’s 
open source integration code
, view the destination’s documentation, or 
contact Segment support
.

You can pass these reserved traits using camelCase or snake_case
, so in JavaScript you can match the rest of your camelCase code by sending 
firstName
, while in Ruby you can match your snake-case code by sending 
first_name
. That way the API never seems alien to your code base. Keep in mind that not all destinations support these reserved traits, so sending these traits in camelCase and snake_case can result in two sets of traits in other destinations.

This page was last modified: 23 Apr 2024

Need support?

Questions? Problems? Need more info? Contact Segment Support for assistance!

Help improve these docs!

Was this page helpful?

Thanks for your feedback!

Can we improve this doc? 
Send us feedback!

Get started with Segment

On this page

Was this page helpful?

Thanks for your feedback!

Can we improve
 this doc? 
Send us feedback!

Product

For Developers

Company

Support

© 2025 Segment.io, Inc.

=== Content from https://segment.com/docs/engage/user-subscriptions/subscription-states/ ===

User Subscription States
        

Engage Premier requires a Business tier account and includes Engage Foundations and Unify.


See the 
available plans
, or 
contact Support
.

On this page

Engage Premier entered an End of Sale (EOS) period effective  June 10, 2024. Existing Segment customers will continue to have access and support to Engage Premier until an end-of-life (EOL) date is announced. We recommend exploring the following pages in preparation of a migration or future MCM needs:

Twilio Marketing Campaigns

Preferred ISV Partners:

Airship Blog
 


Bloomreach Blog
 


Braze Blog
 


Insider Blog
 


Klaviyo Blog
 


Twilio Engage Foundations Documentation
 

Customer profiles in your Segment audiences contain 
contact vectors
. A contact vector is a piece of unique, specific contact information associated with a customer, like the customer’s email address or phone number.

Segment associates one of four 
user subscription states
 with each contact vector in a customer profile. These subscription states indicate the level of consent customers give you to send them your marketing materials.

A customer profile, then, may have contact vectors with different subscription states. For example, a customer may consent to receive email campaigns but not SMS campaigns. Subscription states, then, describe permissions at the contact vector level, not at the customer level.

Understanding the four user subscription states helps you improve campaign deliverability and comply with sending guidelines and legislation. This page explains the four subscription states and how each impacts your sending ability.

Subscription states overview

The following table displays the four subscription states:

Understanding subscription states

You can gain insight into your audience profiles by learning how and why each subscription state is associated with a user’s profile.  Below, you’ll find the four states described in detail, along with common scenarios that produce those states.

Subscribed

A 
subscribed
 user has, at some point, given you explicit permission to send them your marketing materials.

Subscribed users have intentionally requested to receive your marketing materials and have taken voluntary action to confirm that choice. You may have received this consent from a number of sources, including the following:

It’s your responsibility to ensure that Segment correctly reflects your users’ subscription choices. Failure to do so may put you in violation of legislation like 
CAN-SPAM
, 
TCPA
, or 
GDPR
.

Unsubscribed

An 
unsubscribed
 user has intentionally opted out of receiving your marketing materials. You cannot send Engage campaigns to unsubscribed users.

Users commonly unsubscribe in the following ways:

You must include an unsubscribe option in all Engage email and SMS campaigns.

Did not subscribe

Users with the 
did-not-subscribe
 state associated with their email address or phone number gave you their contact information without explicitly agreeing to receive your marketing materials.

The following scenarios often lead to an email or phone number with the 
did-not-subscribe
 subscription state:

Emails or phone numbers with a 
did-not-subscribe
 status won’t receive your marketing campaigns.

No subscription status

Profiles with 
no subscription status
, or a blank status, indicate that Segment has created a profile for the user, but that the user never actually provided their contact information. Some situations that lead to the 
no subscription status
 state include the following:

Some contacts within your Segment space may fall into the no subscription status category. 
Identity resolution
, for example, may result in a user profile created from connecting an email address with an anonymous ID. In this case, the profile would exist within your audience despite the fact that the user never had the option to subscribe or unsubscribe.

Setting user subscriptions

You can set subscription states by either CSV file upload or, programmatically, with the 
Public API
.

Uploading contacts with a CSV file works best for initial batches of contacts you’d like to bring into Engage. Syncing programmatically with the Public API is best suited for real-time and ongoing subscription maintenance, like when a user signs up for a form on your site or unsubscribes from your marketing campaigns within their notification center or account settings.

To learn more about both options, reference the Engage documentation on using the 
CSV uploader
 and setting user subscriptions.

Sync subscription statuses with SQL

Use SQL to import user subscription states from your data warehouse back to Engage. When you sync with SQL, you can query user subscription data at automated intervals. Pull subscription statuses for each contact vector and use your data warehouse as a single source of truth for subscription data.

This option is especially useful if you don’t have the ability to set subscription states with CSVs or Segment’s Public API.

View 
Subscriptions with SQL Traits
 for more information.

Troubleshooting subscription states

On occasion, a user’s subscription state may not be up-to-date. For example, a user may have unsuccessfully attempted to unsubscribe from your marketing campaigns.

The Public API will resolve most subscribe and unsubscribe requests in real time. In some circumstances, however, you’ll need to take action to update a user’s subscription state.  The following table lists some situations in which you may find a manual update useful:

Reach out to support
 with questions you may have about resolving a user’s subscription state.

This page was last modified: 15 Jul 2024

Need support?

Questions? Problems? Need more info? Contact Segment Support for assistance!

Help improve these docs!

Was this page helpful?

Thanks for your feedback!

Can we improve this doc? 
Send us feedback!

Get started with Segment

On this page

Was this page helpful?

Thanks for your feedback!

Can we improve
 this doc? 
Send us feedback!

Product

For Developers

Company

Support

© 2025 Segment.io, Inc.

=== Content from https://segment.com/docs/engage/journeys/event-triggered-journeys/ ===

Event-Triggered Journeys
        

Engage Foundations requires a Business tier account and includes Unify.


See the 
available plans
, or 
contact Support
.

On this page

With Event-Triggered Journeys, you can build real-time, event-based marketing workflows to automate and personalize customer journeys.

Unlike traditional audience-based journeys that rely on pre-defined user segments, event-triggered journeys start automatically when users perform specific actions on your website or app.

On this page, you’ll learn how to create an event-triggered journey, configure entry conditions, and work with published event-triggered journeys.

Public Beta

Event-Triggered Journeys is in public beta, and Segment is actively working on this feature. Some functionality may change before it becomes generally available. Event-Triggered Journeys is not currently HIPAA eligible.

Overview

Event-triggered journeys help you create a responsive approach for time-sensitive use cases, like cart abandonment campaigns and transactional messages.

Where 
audience-based journeys
 activate based on aggregated conditions, event-triggered journeys respond instantly to individual events, delivering personalized experiences based on the full context of each event.

Opt for an event-triggered journey in situations like these:

Build an event-triggered journey

Before you begin

Before you start building an event-triggered journey, make sure that you’ve enabled all 
destinations
 you plan to send data to and that the events you want to use as triggers are already available in your Segment workspace.

To set up an event-triggered journey:

Send data to downstream destinations

When a journey instance reaches a 
Send to Destination
 step, you can configure how data is sent to your desired destination. This step allows you to define where the data goes, what actions are performed, and how information is mapped, giving you control over the integration. Event-Triggered Journeys currently supports all 
Actions Destinations
.

For other destinations or more complex logic, you can use 
Destination Functions
.

Select a Destination

Choose the destination where you want to send data. Currently, only 
Actions Destinations
 and 
Destination Functions
 are supported.

Choose an Action

Specify the action to take within the selected destination. For example, you might update a user profile, trigger an email, or log an event.

Define the Event Name

 Add a descriptive event name to send to your destination.

When a journey reaches this step, the Segment prepares and sends the payload based on your configuration. The integration ensures compatibility with the selected destination’s API, allowing seamless data transfer and execution of the specified action.

Journey setup configuration options

Event-Triggered Journeys includes advanced options to help you tailor journey behavior and customize data delivery to downstream destinations.

Unique identifiers in event-triggered journeys help you manage multiple journey instances when a user triggers the same event more than once.

When you select 
Re-enter every time event occurs
 when you create an event-triggered journey, you can choose an event property as a unique identifier. Selecting this option does two things:

For example, in an abandonment journey, suppose a user starts two applications (like 
application_started
), each with a different 
application_id
. By setting 
application_id
 as the unique identifier, Segment can match follow-up events (like 
application_completed
) to the correct application journey. As a result, each journey instance only receives the completion event for its specific application.

Notes and limitations

Best practices

Follow the best practices in this table to optimize your event-triggered journeys:

Working with Event-Triggered Journeys

Segment built Event-Triggered Journeys to respond instantly to events, offering real-time capabilities with a few considerations in mind.

Use Cases

Event-Triggered Journeys can power a variety of real-time, personalized experiences. This section details some common scenarios to help you see how they might work in practice.

Real-time event forwarding

Suppose you want to instantly send a personalized message whenever a user completes a specific action on your site, like filling out a form or subscribing to a service. With Event-Triggered Journeys, you can configure the journey to trigger each time this entry event occurs. Segment will forward the event data, including all relevant details, to your connected destination in real-time.

Real-time abandonment Campaigns

Imagine you’re running an e-commerce site and want to follow up with users who start the checkout process but don’t complete it within a certain timeframe. You can create an event-triggered Journey to watch for abandonment cases like these.

Start by setting the 
checkout_started
 event as the trigger and specify a unique identifier like 
session_id
 to track each user’s journey instance. Then, configure the journey to check for the 
purchase_completed
 event within a defined window (for example, 1 hour). If the user doesn’t complete the purchase, the journey can automatically send a nudge to encourage them to finish their order.

Personalized follow-up Messages

Say you want to follow up with users after they engage with specific content, like downloading an e-book or watching a demo video. Event-Triggered Journeys can help you send timely, personalized messages based on these interactions.

To do this, set the entry event to 
content_downloaded
 or 
video_watched
 and configure the journey to send a follow-up email. You could even personalize the email with details from the triggering event, like the content title or timestamp, by configuring your destination payload to enrich the message with event-specific context.

This page was last modified: 19 Dec 2024

Need support?

Questions? Problems? Need more info? Contact Segment Support for assistance!

Help improve these docs!

Was this page helpful?

Thanks for your feedback!

Can we improve this doc? 
Send us feedback!

Get started with Segment

On this page

Was this page helpful?

Thanks for your feedback!

Can we improve
 this doc? 
Send us feedback!

Product

For Developers

Company

Support

© 2025 Segment.io, Inc.

=== Content from https://segment.com/docs/engage/journeys/faq-best-practices/ ===

Journeys Best Practices and FAQ
        

Engage Foundations requires a Business tier account and includes Unify.


See the 
available plans
, or 
contact Support
.

On this page

Best practices

Enforce exclusivity in multi-branch splits

When you create a multi-branch split, do not create overlapping conditions that might lead a user to qualify for more than one step at a time.

For example:

Add time windows whenever possible

Add time windows when defining conditions to enforce funnel constraints in a Journey, rather than using an unbounded event condition which operates on the entire history of the user profile. For example, to check if a user has completed an order since receiving an email triggered 7 days ago, use the condition “Order Completed at least 1 time within 7 days.”

Suppress targeting with journey lists

Unlike lists associated with Engage Audiences, users who are added to a journey list cannot be subsequently removed. Lists are typically associated with advertising campaigns, and you must take additional steps if you wish to ensure that users do not continue to be targeted with ads after they achieve some goal. A typical implementation pattern is:

Review your Journey in drafts first

Save your Journey in a draft state so that you can review before you publish it. Once you publish a Journey, you cannot edit select portions of a Journey and Journeys sends data to destinations.

Know how to incorporate historical data

Aside from the entry condition, all Journey step conditions are triggered by future events and existing trait memberships. Event-based conditions only evaluate events that occur 
after
 the Journey is published.

When you 
include historical data
 in a Journey’s entry condition, Unify identifies users who previously satisfied the entry condition and adds them to entry. For example, to evaluate if a user has ever used a discount code mid-Journey, create and configure a 
Computed Trait
 to select for 
discount_used = true
 to use in your Journey.

Including historical data doesn’t impact any additional Journey steps, however. To include historical data in post-entry conditions, use the following table to identify which conditions will automatically include historical data:

To include historical data based on custom traits or events that predate the Journey, first build an Audience that includes the targeted data by following these steps:

For example, to include 
custom trait = ABC
 in a Journey, create an Audience called 
ABC
 that includes that custom trait, then add the Journey condition 
Part of Audience
 
ABC
.

Using the 
Part of Audience
 condition, Journeys then populates the custom trait as if it were using historical data.

Use dev spaces and data warehouse destinations to test journeys

Follow these best practices to test your journeys:

FAQs

Journeys run in real-time, like real-time Audiences in Engage. This means that users will progress through Journeys as Segment receives new events.

Yes. Users must first exit a Journey, however, before entering it again. To learn more about Journey re-entry, read the 
Journey re-entry section
 of the 
Build a Journey
 page.

Journeys supports all Engage destinations, including Destination Functions. Read more in 
Send data to destinations
 .

When building a Journey, if you check 
Use historical data
, you can see the estimated number of users in the initial cohort.

Once published, Journeys displays the number of users are in each step of the Journey at any given time.

The data type you send to a destination depends on whether the destination is an Event Destination or a List Destination.

For Engage customers, users with either the Engage User or Engage Admin roles can create, edit, and delete journeys. Users with the Engage Read-only role are restricted to view-only access.

Journeys triggers audience or trait-related events for each email 
external_id
 on a profile. If a profile has two email addresses, you’ll see two Audience Entered and two Audience Exited events for each Journey step. Journeys sends both email addresses to downstream destinations.

It may take up to five minutes for a user profile to enter each step of a Journey, including the entry condition. For Journey steps that reference a batch audience or SQL trait, Journeys processes user profiles at the same rate as the audience or trait computation. Visit the Engage docs to 
learn more about compute times
.

When you publish a journey, the entry step begins evaluating users in real time while the historical data backfill runs separately. If a user’s events or traits span both real-time and historical data, they might qualify for the journey immediately, even if their full historical data would have disqualified them.

To prevent inconsistencies, you can manually create an audience that includes the same conditions as the journey’s entry step. This ensures that it evaluates both real-time and historical data. You can then use this pre-built audience as the journey’s entry condition. This approach guarantees that Segment evaluates users consistently across both data sources.

This page was last modified: 30 Jan 2025

Need support?

Questions? Problems? Need more info? Contact Segment Support for assistance!

Help improve these docs!

Was this page helpful?

Thanks for your feedback!

Can we improve this doc? 
Send us feedback!

Get started with Segment

On this page

Was this page helpful?

Thanks for your feedback!

Can we improve
 this doc? 
Send us feedback!

Product

For Developers

Company

Support

© 2025 Segment.io, Inc.

=== Content from https://segment.com/docs/segment-app/verify-email-address/ ===

Verifying Your Email Address
        

Before you can use your Segment account, you first need to verify your email address. We automatically send a verification message to the address you used to sign, but you can re-send the verification email if you didn’t receive it.

If a team member invited you to a Segment workspace, your email is automatically verified if you set up your account from the link in the invitation email.



Troubleshooting why you didn’t get the verification email

You should receive a verification email from Segment within 20 minutes of signing up for a Segment account.

If you don’t receive the verification email, check out some of the suggestions below:

Typos
: Check the spelling of your email address. If the spelling is incorrect, sign up again using the correct spelling of the address.

Spam or Junk Folder
: Check your Spam or Junk folder. Verification emails may be filtered directly into your email provider’s spam or junk mail folder. Your ISP or corporate domain may be configured to deliver commercial mail to this folder by default.

Web Browser Needs a Refresh
: Try requesting another verification email, and refresh the page of your email web browser.

Blocked or Bounced Address
: If you tried to verify a specific email address but did not receive the verification email, your ISP or corporate domain may have blocked the email. If you haven’t received 
any
 Segment emails, try verifying an alternative email address or get help.

Role, Group, or Alias Address
: Some role addresses can be verified, but many can’t, because more than one person may have access to a role email address (such as 
admin
 or 
dev-ops
 emails). You should use a personal email address for your contact and billing address.

This page was last modified: 21 Apr 2023

Need support?

Questions? Problems? Need more info? Contact Segment Support for assistance!

Help improve these docs!

Was this page helpful?

Thanks for your feedback!

Can we improve this doc? 
Send us feedback!

Get started with Segment

Was this page helpful?

Thanks for your feedback!

Can we improve
 this doc? 
Send us feedback!

Product

For Developers

Company

Support

© 2025 Segment.io, Inc.

=== Content from https://segment.com/docs/getting-started/01-what-is-segment/ ===

How Segment Works
        

On this page

In a nutshell, the Segment libraries (
Sources
) generate messages about what’s happening in your site or app, and send them to the Segment servers. Segment then translates the content of those messages into different formats for use by other tools (which Segment calls 
Destinations
), and sends the translated messages to those tools. The Segment servers also archive a copy of the data, and can 
send data to your storage systems
 (such as databases, warehouses, or bulk-storage buckets).

Overview



Segment Spec methods
 are how you collect interaction data from your interfaces, and the 
Sources
 are what you package with your interfaces to collect and route the data.

Once you’ve collected your interaction data, there are several different actions you can take:

Sources for collecting data

You can collect data by implementing Segment’s tracking libraries as your Sources:

Sources for unique cases

Segment also offers these other source libraries to cover less straightforward cases:

Cloud App Sources

Segment also offers 
Cloud App Sources
 to integrate data from your third-party tools:

How you can track data

Segment supports several ways to implement tracking. The two most common are to use 
device-based
 or 
server-based
 libraries. You can use Segment’s device-based libraries, such as JavaScript, iOS, and Android, to make calls on users’ browsers or mobile devices. You can also track data with Segment’s server-based libraries, such as Node, Python, or PHP, where the calls are triggered on your own servers and then sent to the Segment servers.

When you collect data using device-based libraries, you can choose between these two different connection modes:

Although there are some tradeoffs between the two approaches, neither is better than the other, and Segment recommends that you implement a mix of both. In general, more direct interaction data is available using a device-based library, but server-based collection is more secure, reliable, and can’t be blocked by ad blockers.

The Segment Methods

The Segment libraries generate messages about what happens on your interface, translate those messages into different formats for use by destinations, and transmit the messages to those tools.

There are several 
tracking API methods
, that you can call to generate messages. The four most important methods are:

Every call shares the same 
common fields
. When you use these methods 
as intended
, it allows Segment to detect a specific type of data and correctly translate it to send it on to downstream destinations.

Where you can send data

Segment maintains a catalog of destinations where you can send your data.



Walk through a disposable, demo implementation.

This page was last modified: 21 Apr 2023

Need support?

Questions? Problems? Need more info? Contact Segment Support for assistance!

Help improve these docs!

Was this page helpful?

Thanks for your feedback!

Can we improve this doc? 
Send us feedback!

Get started with Segment

On this page

Was this page helpful?

Thanks for your feedback!

Can we improve
 this doc? 
Send us feedback!

Product

For Developers

Company

Support

© 2025 Segment.io, Inc.

=== Content from https://segment.com/docs/connections/sources/catalog/libraries/website/javascript/ ===

Analytics.js Source
                

Flagship libraries offer the most up-to-date functionality on Segment’s most popular platforms. Segment actively maintains flagship libraries, which benefit from new feature releases and ongoing development and support.

On this page

Analytics.js enables you to send your data to hundreds of 
destination tools
 without having to learn, test, or use a new API every time.

Segment’s Analytics.js library is fully open-source and can be viewed on 
GitHub
.

Getting started

Use the 
Analytics.js QuickStart Guide
 to learn how to add Analytics.js to your site. Once you’ve installed the library, read on for the detailed API reference.

Benefits of Analytics.js

Analytics.js provides two key benefits over the previous version.

Performance

Analytics.js reduces page load time and improves site performance. Its package size is 
~70%
 smaller than its predecessor, the classic version of Analytics.js.

Many factors impact page load time, including network conditions, hosting locations, and page weight. Page weight for each customer integration varies based on the number of device-mode destinations that are enabled for each source. The more device-mode destinations that are enabled, the more data gets added to the library, which will impact the weight of the library.

Developer experience

Analytics.js improves developer experience by introducing new ways for developers to augment events throughout the event timeline. For example, developers can augment events either before or after an event occurs, or while the event is in-flight.

For example, you can use Analytics.js to build features that:

Basic tracking methods

The basic tracking methods below serve as the building blocks of your Segment tracking. They include 
Identify
, 
Track
, 
Page
, 
Group
, and 
Alias
.

These methods correspond with those used in the 
Segment Spec
. The documentation on this page explains how to use these methods in Analytics.js.

For any of the methods described in this page, you can replace the properties in the code samples with variables that represent the data collected.

Identify

Use the 
identify
 method to link your users and their actions, to a recognizable 
userId
 and 
traits
. You can see 
an 
identify
 example in the Quickstart guide
 or 
find details on the identify method payload
.

Identify calls and anonymous visitors

Segment recommends 
against
 using 
identify
 for anonymous visitors to your site. Analytics.js automatically retrieves an 
anonymousId
 from 
localStorage
 or assigns one for new visitors, and then attaches it to all 
page
 and 
track
 events both before and after an 
identify
.

The Identify method follows the format below:

analytics
.
identify
([
userId
],
 
[
traits
],
 
[
options
],
 
[
callback
]);



The Identify call has the following fields:

By default, Analytics.js caches traits in the browser’s 
localStorage
 and attaches them to each Identify call.

For example, you might call Identify when someone signs up for a newsletter but hasn’t yet created an account on your site. The example below shows an Identify call (using hard-coded traits) that you might send in this case.

analytics
.
identify
({

  
nickname
:
 
'
Amazing Grace
'
,

  
favoriteCompiler
:
 
'
A-0
'
,

  
industry
:
 
'
Computer Science
'


});



Then, when the user completes the sign up process, you might see the following:

analytics
.
identify
(
'
12091906-01011992
'
,
 
{

  
name
:
 
'
Grace Hopper
'
,

  
email
:
 
'
grace@usnavy.gov
'


});



The traits object for the second call also includes 
nickname
, 
favoriteCompiler
, and 
industry
.

You may omit both traits and options, and pass the callback as the second argument.

analytics
.
identify
(
'
12091906-01011992
'
,
 
function
(){

  
// Do something after the identify request has been sent

  
// Note: site-critical functionality should not depend on your analytics provider


});



Track

The Track method lets you record actions your users perform. You can 
see a track example in the Quickstart guide
 or find details on 
the track method payload
.

The Track method follows the format below:

analytics
.
track
(
event
,
 
[
properties
],
 
[
options
],
 
[
callback
]);



The 
track
 call has the following fields:

The only required argument in Analytics.js is an 
event name string
. You can read more about 
how Segment recommends you name events
.

Example Track call:

analytics
.
track
(
'
Article Completed
'
,
 
{

  
title
:
 
'
How to Create a Tracking Plan
'
,

  
course
:
 
'
Intro to Analytics
'
,


});



For more information about choosing which events to track, event naming, and more, check out 
Analytics Academy
.

The only required argument on Track calls in Analytics.js is an 
event
 name string. Read more about how Segment recommends 
naming your events
.

trackLink
 is a helper method that attaches the 
track
 call as a handler to a link.
With 
trackLink
, Analytics.js inserts a timeout of 300 ms to give the 
track
 call more time. This is useful when a page would redirect before the 
track
 method could complete all requests.

The 
trackLink
 method follows the format below.

analytics
.
trackLink
(
element
,
 
event
,
 
[
properties
])



Example:

var
 
link
 
=
 
document
.
getElementById
(
'
free-trial-link
'
);



analytics
.
trackLink
(
link
,
 
'
Clicked Free-Trial Link
'
,
 
{

  
plan
:
 
'
Enterprise
'


});



trackForm
 is a helper method that binds a 
track
 call to a form submission.
The 
trackForm
 method inserts a timeout of 300 ms to give the 
track
 call more time to complete. This is useful to prevent a page from redirecting before the 
track
 method could complete all requests.

The 
trackForm
 method follows the format below.

analytics
.
trackForm
(
form
,
 
event
,
 
[
properties
])



Example:

var
 
form
 
=
 
document
.
getElementById
(
'
signup-form
'
);



analytics
.
trackForm
(
form
,
 
'
Signed Up
'
,
 
{

  
plan
:
 
'
Premium
'
,

  
revenue
:
 
99.00


});



Page

The 
Page
 method lets you record page views on your website, along with optional extra information about the page viewed by the user.

Because some Destinations require a 
page
 call to instantiate their libraries, 
you must call 
page
 at least once per page load. You can call it more than once if needed, for example, on virtual page changes in a single page app.

See the implementation guide for more information about 
calling the Page method
.

Analytics.js includes a Page call by default as the final line in 
the Analytics.js snippet
. You can update this 
page
 call within the guidelines below.

The 
page
 method follows the format below.

analytics
.
page
([
category
],
 
[
name
],
 
[
properties
],
 
[
options
],
 
[
callback
]);



The 
page
 call has the following fields:

Analytics.js adds properties to each 
page
 call.

analytics
.
page
(
'
Pricing
'
);



Segment adds the following information:

analytics
.
page
(
'
Pricing
'
,
 
{

  
title
:
 
'
Segment Pricing
'
,

  
url
:
 
'
https://segment.com/pricing
'
,

  
path
:
 
'
/pricing
'
,

  
referrer
:
 
'
https://segment.com/warehouses
'


});



You can override these values by explicitly setting them in your calls. For example:

analytics
.
page
(
'
Pricing
'
,
 
{

  
title
:
 
'
My Overridden Title
'
,

  
path
:
 
'
/pricing/view
'


});



Translates to:

analytics
.
page
(
'
Pricing
'
,
 
{

  
title
:
 
'
My Overridden Title
'
,

  
url
:
 
'
https://segment.com/pricing
'
,

  
path
:
 
'
/pricing/view
'
,

  
referrer
:
 
'
https://segment.com/warehouses
'


});



Segment sets the 
path
 and 
url
 property to the value of the canonical element on your page. If a canonical element is not set, the values will be set from the browser.

Group

The Group method associates an 
identified user
 with a company, organization, project, workspace, team, tribe, platoon, assemblage, cluster, troop, gang, party, society or any other collective noun you come up with for the same concept.

This is useful for 
tools like 
Intercom
, 
Preact
, and 
Totango
, as it ties the user to a 
group
 of other users.

The Group method follows the format below.

analytics
.
group
(
groupId
,
 
[
traits
],
 
[
options
],
 
[
callback
]);



The Group call has the following fields:

Example 
group
 call:

analytics
.
group
(
'
UNIVAC Working Group
'
,
 
{

  
principles
:
 
[
'
Eckert
'
,
 
'
Mauchly
'
],

  
site
:
 
'
Eckert–Mauchly Computer Corporation
'
,

  
statedGoals
:
 
'
Develop the first commercial computer
'
,

  
industry
:
 
'
Technology
'


});



By default, Analytics.js caches group 
traits
 in the browser’s local storage and attaches them to each 
group
 call, similar to how the 
identify
 method works.

Find more details about 
group
, including the 
group
 payload, in 
the Group Spec
.

Alias

The Alias method combines two unassociated user identities. Segment usually handles aliasing automatically when you call 
identify
 on a user, however some tools require an explicit 
alias
 call.

This is an advanced method, but it’s required to manage user identities successfully in 
some
 Segment destinations like 
Kissmetrics
 and 
Mixpanel
. 

The Alias method follows the format below:

analytics
.
alias
(
userId
,
 
[
previousId
],
 
[
options
],
 
[
callback
]);



The Alias call has the following fields:

For more details about Alias, including the 
alias
 call payload
, check out the 
Segment Spec
.

Utility methods

The Analytics.js utility methods help you change how Segment loads on your page. They include:

Load

The 
load
 method is also available when you load analytics.js through the 
NPM package
.

You can load a buffered version of analytics.js that requires you to call 
load
 explicitly before analytics.js initiates any network activity. This is useful if you want to, for example, wait for user consent before you fetch tracking destinations or send buffered events to Segment.

Call 
load
 one time only.

export
 
const
 
analytics
 
=
 
new
 
AnalyticsBrowser
()



analytics
.
identify
(
"
hello world
"
)



if 
(
userConsentsToBeingTracked
)
 
{

    
analytics
.
load
({
 
writeKey
:
 
'
<YOUR_WRITE_KEY>
'
 
})
 
// destinations loaded, enqueued events are flushed


}



You can also use 
load
 if you fetch some settings asynchronously.

const
 
analytics
 
=
 
new
 
AnalyticsBrowser
()


fetchWriteKey
().
then
(
writeKey
 
=>
 
analytics
.
load
({
 
writeKey
 
}))



analytics
.
identify
(
"
hello world
"
)



Ready

The 
ready
 method lets you pass in a method that gets called after Analytics.js finishes initializing and after all enabled device-mode destinations load. It’s like 
jQuery’s 
ready
 method
, except for Destinations. Because it doesn’t fire until all enabled device-mode destinations are loaded, it can’t be used to change configuration options for downstream SDKs. That can only be done if the SDK is loaded natively.

The 
ready
 method isn’t invoked if any Destination throws an error (for example, for an expired API key, incorrect settings configuration, or when a Destination is blocked by the browser) during initialization. If you want to check when Analytics.js has loaded, you can look at the value of 
window.analytics.initialized
. When it’s true, the library has successfully initialized, even if some destinations are blocked.

Note
: 
window.analytics.initialized
 is a simple boolean, not an event or a pub/sub system. This means you can’t subscribe to changes in its value. If you need to detect when it changes from 
false
 to 
true
, you must set up a polling mechanism to monitor the value.

The code in the 
ready
 function only executes after 
ready
 is emitted.

If you want to access end-tool library methods that do not match any Analytics.js methods, like adding an extra setting to Mixpanel, you can use a 
ready
 callback so that you’re guaranteed to have access to the Mixpanel object, like so:

analytics
.
ready
(()
 
=>
 
{

  
window
.
mixpanel
.
set_config
({
 
verbose
:
 
true
 
});


});



The 
ready
 method uses the following format:

analytics
.
ready
(
callback
);



The 
ready
 method has the following fields:

Debug

Calling the 
debug
 method turns on debug mode, which logs helpful messages to the console. Subsequent Segment events generate messages in the developer console after you invoke 
debug
.

Enable:

analytics
.
debug
(
true
);



Disable:

analytics
.
debug
(
false
);



Emitter

The global 
analytics
 object emits events whenever you call 
alias
, 
group
, 
identify
, 
track
, or 
page
.

Use the 
on
 method to set listeners for these events and run your own custom code. This can be useful if you want to send data to a service for which Segment doesn’t have a destination.

analytics
.
on
(
method
,
 
callback
);



Example:

analytics
.
on
(
'
track
'
,
 
(
event
,
 
properties
,
 
options
)
 
=>
 
{


  
bigdataTool
.
push
([
'
recordEvent
'
,
 
event
]);



});



This method emits events 
before
 they are processed by the Segment integration, and may not include some of the normalization Segment performs on the client before sending the data to the Segment servers.

Page event properties are stored in the 
options
 object.

Extending timeout

The 
timeout
 method sets the length (in milliseconds) of callbacks and helper functions. This is useful if you have multiple scripts that need to fire in your callback or 
trackLink
, 
trackForm
 helper function.

The example below sets the timeout to 500 ms.

analytics
.
timeout
(
500
);



If you’re triggering ad network conversion pixels, Segment recommends extending timeout to 500 ms to account for slow load times.

Reset or log out

Calling 
reset
 resets the 
id
, including 
anonymousId
, and clears 
traits
 for the currently identified user and group.

analytics
.
reset
();



The 
reset
 method only clears the cookies and 
localStorage
 created by Segment. It doesn’t clear data from other integrated tools, as those native libraries might set their own cookies to manage user tracking, sessions, and manage state. To completely clear out the user session, see the documentation provided by those tools.

Segment doesn’t share 
localStorage
 across subdomains. If you use Segment tracking on multiple subdomains, you must call 
analytics.reset()
 for each subdomain to completely clear out the user session.

Managing data flow with the Integrations object

Tip
: You can change how your data flows in several different ways without having to change your code. See 
Filtering Data
 to learn more.

You can pass an 
integrations
 object in the 
options
 of Alias, Group, Identify, Page, and Track 
 methods to send data to only the selected destinations. By default, all Destinations are enabled.

The example below sends a message only to Intercom and Google Analytics.

analytics
.
identify
(
'
user_123
'
,
 
{

  
email
:
 
'
jane.kim@example.com
'
,

  
name
:
 
'
Jane Kim
'


},
 
{

  
integrations
:
 
{

    
'
All
'
:
 
false
,

    
'
Intercom
'
:
 
true
,

    
'
Google Analytics
'
:
 
true

  
}


});



'All': false
 tells Segment not to send data to 
any
 Destinations by default, unless they’re explicitly listed as 
true
 in the next lines.

As an opposite example, the snippet below sends a message to all integrations 
except
 Intercom and Google Analytics.

analytics
.
identify
(
'
user_123
'
,
 
{

  
email
:
 
'
jane.kim@example.com
'
,

  
name
:
 
'
Jane Kim
'


},
 
{

  
integrations
:
 
{

    
'
Intercom
'
:
 
false
,

    
'
Google Analytics
'
:
 
false

  
}


});



You don’t need to include 
'All': true
 in this call because it’s implied as the default behavior. Instead, only list the destinations that you want to exclude, with a 
false
 flag for each.

Destination flags are 
case sensitive
 and match 
the destination’s name in the docs
 (for example, “AdLearn Open Platform”, “awe.sm”, “Mailchimp”, etc). If a Destination has more than one acceptable name, this appears in the documentation for that destination.

Business tier customers can filter Track calls from the Source Schema page in the Segment UI. Segment recommends that you use the UI to simplify filter management and make updates without changing your site’s code.

Load options

Note:
 To use this feature, you must be on snippet version 4.1.0 or later. You can get the latest version of the snippet from the 
Analytics.js Quickstart
.

You can modify the 
.load
 method in Analytics.js (the second line of the snippet) to take a second argument. If you pass an object with an 
integrations
 dictionary, then Segment only loads the integrations in that dictionary that are marked as enabled with the boolean value 
true
.

You can only call 
.load
 on page load, or reload (refresh). If you modify the 
.load
 method between page loads, it doesn’t have any effect until the page is reloaded.

For example:

analytics
.
load
(
'
writekey
'
,
 
{
 
integrations
:
 
{
 
All
:
 
false
,
 
'
Google Analytics
'
:
 
true
,
 
'
Segment.io
'
:
 
true
 
}
 
})



This way, you can conditionally load integrations based on what customers opt into on your site. The example below shows how you might load only the tools that the user agreed to use.

onConsentDialogClosed
((
consentedTools
)
 
=>
 
{

  
analytics
.
load
(
'
writekey
'
,
 
{
 
integrations
:
 
consentedTools
 
})


})



You can also add an 
obfuscate
 property to the object in the second parameter, which obscures the URL from which your integrations and destination actions are loaded. This helps prevent words that are flagged by ad blockers to not be detected in your URL, enabling the integration to properly load.

For example:

analytics
.
load
(
'
writekey
'
,
 
{
 
obfuscate
:
 
true
 
})



The 
obfuscate
 value is 
false
 by default.

By default, the Analytics.js library will convert ISO8061 strings to a Date object before passing it to downstream device-mode integrations. If you would like to disable this functionality and send those strings as they are passed to the event, you can use the load method to pass in the 
disableAutoISOConversion
 option.

For example:

analytics
.
load
(
'
writekey
'
,
 
{
 
disableAutoISOConversion
:
 
true
 
})



Some 
userAgent
 strings are frozen and contain less information. If you would like to request more information when it’s available, you can pass an array of strings with fields you would like to request to the 
highEntropyValuesClientHints
 option. The example array below contains all possible values.

For example:

analytics
.
load
(
'
writekey
'
,
 
{
 
highEntropyValuesClientHints
:
 
[
'
architecture
'
,
 
'
bitness
'
,
 
'
model
'
,
 
'
platformVersion
'
,
 
'
uaFullVersion
'
,
 
'
fullVersionList
'
,
 
'
wow64
'
]
 
})



For testing or staging environments, it can be useful to disable your SDK to ensure no events send.

If 
disable: true
 is passed, all analytics method calls will be a no-op, and no network calls will be initiated.

analytics
.
load
(
'
writekey
'
,
 
{
 
disable
:
 
true
 
})



For wrapper/plugin authors: if you have a use case where you need special access to the CDN Settings (for example, consent management), you can also pass a function. This API waits for 
cdnSettings
 to be fetched. Keep in mind that 
cdnSettings
 is an 
unstable
 object.

analytics
.
load
(
'
writekey
'
,
 
{
 
disable
:
 
(
cdnSettings
)
 
=>
 
true
 
})



Retries

When enabled, Analytics.js automatically retries network and server errors. With persistent retries, Analytics.js can:

Analytics.js stores events in 
localStorage
 and falls back to in-memory storage when 
localStorage
 is unavailable. It retries up to 10 times with an incrementally increasing back-off time between each retry. Analytics.js queues up to 100 events at a time to avoid using too much of the device’s local storage. See the 
destination Retries documentation
 to learn more.

Delivery strategy configuration

The 
deliveryStrategy.config
 object lets you customize how data is delivered to Segment. This includes options like setting custom headers and enabling 
keepalive
 to capture events during hard redirects.

Adding custom headers

You can override default headers by providing custom headers in your configuration. Use the 
deliveryStrategy.config.headers
 option to specify the headers, like in the following example:

analytics
.
load
(
"
<YOUR_WRITE_KEY>
"
,
 
{

  
integrations
:
 
{

    
'
Segment.io
'
:
 
{

      
deliveryStrategy
:
 
{

        
config
:
 
{

          
headers
:
 
{
 
'
x-api-key
'
:
 
'
foo
'
 
}

        
}

      
}

    
}

  
}


});



##
 
Keepalive



You
 
can
 
use
 
the
 
`keepalive`
 
option
 
to
 
make
 
sure
 
that
 
Segment
 
captures
 
API
 
calls
 
triggered
 
during
 
a
 
hard
 
redirect
.
 
When
 
enabled
,
 
`keepalive`
 
will
 
try
 
to
 
fire
 
events
 
before
 
the
 
redirect
 
occurs
.



By
 
default
,
 
`keepalive`
 
is
 
set
 
to
 
false
,
 
because
 
all
 
fetch
 
requests
 
with
 
the
 
`keepalive`
 
flag
 
are
 
subject
 
to
 
a
 
64
kb
 
size
 
limit
.
 
Additionally
,
 
`keepalive`
 
requests
 
share
 
this
 
size
 
limit
 
with
 
all
 
other
 
in
-
flight
 
`keepalive`
 
requests
,
 
regardless
 
of
 
whether
 
they
'
re related to Segment. This competition for resources can lead to data loss in some scenarios.

Segment only uses `keepalive` by default if:
- The browser detects that the page is unloading (like if the user closes the tab or navigates away).
- You have batching enabled.

To enable `keepalive`, use the following configuration:

```ts
analytics.load("<YOUR_WRITE_KEY>", {
  integrations: {
    
'
Segment
.
io
'
: {
      deliveryStrategy: {
        config: {
          keepalive: true
        }
      }
    }
  }
});


Batching

Batching is the ability to group multiple requests or calls into one request or API call. All requests sent within the same batch have the same 
receivedAt
 time. With Analytics.js, you can send events to Segment in batches. Sending events in batches enables you to have:

Setup

You can start batching by changing the 
strategy
 to 
"batching"
 and the parameters for 
size
 and 
timeout
 within the 
load
 method in the analytics object. Batching requires both parameters.

analytics
.
load
(
"
<write_key>
"
,
 
{

    
integrations
:
 
{

      
"
Segment.io
"
:
 
{

        
deliveryStrategy
:
 
{

          
strategy
:
 
"
batching
"
,

          
config
:
 
{

            
size
:
 
10
,

            
timeout
:
 
5000

          
}

        
}

      
}

    
}

  
});



You can check to see if batching works by checking your source’s debugger in 
Sources > Debugger
. When you select an event and view the 
Raw
 code, the 
receivedAt
 time of all the events in the batch should be the same.

The batch size is the threshold that forces all batched events to be sent once it’s reached. For example, 
size: 10
  means that after triggering 10 events, Analytics.js sends those 10 events together as a batch to Segment.

Your total batched events can’t exceed the maximum payload size of 500 KB, with a limit of 32 KB for each event in the batch. If the 500 KB limit is reached, the batch will be split.

timeout
 is the number of milliseconds that forces all events queued for batching to be sent, regardless of the batch size, once it’s reached. For example, 
timeout: 5000
 sends every event in the batch to Segment once 5 seconds passes.

Batching FAQs

Analytics.js does its best to deliver the queued events before the browser closes, but the delivery isn’t guaranteed.

Upon receiving the 
beforeunload
 browser event, Analytics.js attempts to flush the queue using 
fetch
 requests with 
keepalive
 set to true. Since the max size of 
keepalive
 payloads is limited to 64 KB, if the queue size is bigger than 64 KB at the time the browser closes, then there is a chance of losing a subset of the queued events. Reducing the batch size or timeout will alleviate this issue, but that will be a trade-off decision.

No, this batching only impacts events sent to Segment. Once the batch reaches Segment, it’s split up and follows the normal path of an event.

No, batching won’t impact billing or throughput.

Partner integrations don’t support batching as all other partner integrations run one event at a time. Only Segment.io events support batched delivery.

Batching won’t work on Internet Explorer.

Batching delays retries, as events that are queued for batching aren’t retried until a batch delivery fails.

No, there is no change in behavior to Middlewares.

No, there is no impact to how events filter.

Plugins and source middleware

When you develop against Analytics 2.0, the plugins you write can augment functionality, enrich data, and control the flow and delivery of events. From modifying event payloads to changing analytics functionality, plugins and middleware help to speed up the process of getting things done.

Plugins and source middleware accomplish the same thing, but plugins are significantly more powerful (but more verbose to implement).

For basic use cases like adding event fields or dropping specific events, use 
source middleware
. If you need more granular control of the lifecycle, or want to be able to abort the Segment initialization, you should use 
plugins
.

Source Middleware

Source middleware
 runs before any other plugins. You can use this to enrich or drop an event.

Here are some examples of using 
addSourceMiddleware
 for enrichment and validation.

  
analytics
.
addSourceMiddleware
(({
 
payload
,
 
next
 
})
 
=>
 
{

     
const
 
{
 
event
 
}
 
=
 
payload
.
obj
.
context

     
if 
(
event
.
type
 
===
 
'
track
'
)
 
{

        
event
.
event
.
toLowerCase
()

     
}

     
next
(
payload
)

  
});



  
analytics
.
addSourceMiddleware
(({
 
payload
,
 
next
 
})
 
=>
 
{

    
const
 
{
 
event
 
}
 
=
 
payload
.
obj
.
context

    
if 
(
!
isValid
(
event
))
 
{

      
return
 
null
 
// event is dropped

    
}

    
next
(
payload
)

  
});



Advanced Plugin API

For advanced modification to the event pipeline.

Example plugins

Here’s an example of a plugin that converts all track event names to lowercase before the event goes through the rest of the pipeline:

export
 
const
 
lowercase
:
 
Plugin
 
=
 
{

  
name
:
 
'
Lowercase events
'
,

  
type
:
 
'
enrichment
'
,

  
version
:
 
'
1.0.0
'
,


  
isLoaded
:
 
()
 
=>
 
true
,

  
load
:
 
()
 
=>
 
Promise
.
resolve
(),


  
track
:
 
(
ctx
)
 
=>
 
{

    
ctx
.
updateEvent
(
'
event
'
,
 
ctx
.
event
.
event
.
toLowerCase
())

    
return
 
ctx

  
}


}



const
 
identityStitching
 
=
 
()
 
=>
 
{

  
let
 
user


  
const
 
identity
 
=
 
{

    
// Identifies your plugin in the Plugins stack.

    
// Access `window.analytics.queue.plugins` to see the full list of plugins

    
name
:
 
'
Identity Stitching
'
,

    
// Defines where in the event timeline a plugin should run

    
type
:
 
'
enrichment
'
,

    
version
:
 
'
0.1.0
'
,


    
// use the `load` hook to bootstrap your plugin

    
// The load hook will receive a context object as its first argument

    
// followed by a reference to the analytics.js instance from the page

    
load
:
 
async 
(
_ctx
,
 
ajs
)
 
=>
 
{

      
user
 
=
 
ajs
.
user
()

    
},


    
// Used to signal that a plugin has been property loaded

    
isLoaded
:
 
()
 
=>
 
user
 
!==
 
undefined
,


    
// Applies the plugin code to every `identify` call in Analytics.js

    
// You can override any of the existing types in the Segment Spec.

    
async
 
identify
(
ctx
)
 
{

      
// Request some extra info to enrich your `identify` events from

      
// an external API.

      
const
 
req
 
=
 
await
 
fetch
(

        
`https://jsonplaceholder.typicode.com/users/
${
ctx
.
event
.
userId
}
`

      
)

      
const
 
userReq
 
=
 
await
 
req
.
json
()


      
// ctx.updateEvent can be used to update deeply nested properties

      
// in your events. It's a safe way to change events as it'll

      
//  create any missing objects and properties you may require.

      
ctx
.
updateEvent
(
'
traits.custom
'
,
 
userReq
)

      
user
.
traits
(
userReq
)


      
// Every plugin must return a `ctx` object, so that the event

      
// timeline can continue processing.

      
return
 
ctx

    
},

  
}


  
return
 
identity


}



// Registers Segment's new plugin into Analytics.js


await
 
window
.
analytics
.
register
(
identityStitching
())



Here’s an example of a 
utility
 plugin that allows you to change the format of the anonymous_id cookie:



window
.
analytics
.
ready
(()
 
=>
 
{

      
window
.
analytics
.
register
({

        
name
:
 
'
Cookie Compatibility
'
,

        
version
:
 
'
0.1.0
'
,

        
type
:
 
'
utility
'
,

        
load
:
 
(
_ctx
,
 
ajs
)
 
=>
 
{

          
const
 
user
 
=
 
ajs
.
user
()

          
const
 
cookieJar
 
=
 
user
.
cookies

          
const
 
cookieSetter
 
=
 
cookieJar
.
set
.
bind
(
cookieJar
)


          
// blindly convert any values into JSON strings

          
cookieJar
.
set
 
=
 
(
key
,
 
value
,
 
opts
)
 
=>
 
cookieSetter
(
key
,
 
JSON
.
stringify
(
value
),
 
opts
)


          
// stringify any existing IDs

          
user
.
anonymousId
(
user
.
anonymousId
())

          
user
.
id
(
user
.
id
())

        
},

        
isLoaded
:
 
()
 
=>
 
true

      
})

    
})



You can view Segment’s 
existing plugins
 to see more examples.

Register a plugin

Registering plugins enable you to modify your analytics implementation to best fit your needs. You can register a plugin using this:

// A promise will resolve once the plugins have been successfully loaded into Analytics.js


// You can register multiple plugins at once by using the variable args interface in Analytics.js


await
 
window
.
analytics
.
register
(
pluginA
,
 
pluginB
,
 
pluginN
)



Video player plugins

Segment offers video player ‘plugins’ so you can quickly collect video events using Analytics.js. See the specific documentation below to learn more:

Cross-subdomain analytics

Analytics.js tracks across subdomains out of the box. All Segment destinations fully support this feature.

To track activity on your subdomains, include the Segment Analytics.js snippet on each subdomain. Segment sets users’ 
anonymousId
 on the top-level domain, so that users are tracked across any subdomain.

Because Segment tracks across subdomains, you can either use the same Segment source, or use separate sources for each subdomain. What you decide depends on your team’s goals for tracking each subdomain.

Segment doesn’t offer tracking across top-level domains out of the box. If you want to track across top-level domains, you can utilize Segment’s 
Querystring API
 to pass the anonymousId from Website A to Website B in the query string. When a user moves from Website A to Website B with the anonymousId in the query string, Analytics.js reads that value and sets the anonymousId to it, rather than generating a new one.

UTM Tracking

UTM parameters are only used when linking to your site from outside your domain. When a visitor arrives using a link containing UTM parameters, Segment’s analytics.js library will parse the URL query string and add the information to the event payload. For more information about UTM tracking, see the 
Tracking Customers Across Channels and Devices
 documentation.

UTM parameters contain three essential components (utm_source, utm_medium, utm_campaign) and two optional (utm_content, utm_term). For example, if you include the following three parameters in your URL: 
?utm_source=mysource&utm_medium=email&utm_campaign=mytestcampaign
, once a visitor arrives using a link containing the above, Segment automatically grabs the UTM parameters and subsequent events will contain these parameters within the ‘context’ object (visible in the raw view of your Source Debugger.)

So, for example, if somebody follows the link with above query string to your site, the subsequent ‘page’ call in your Debugger should contain the below and will be passed to any enabled destinations:

"
context
"
:
 
{

 
"
campaign
"
:
 
{

 
"
medium
"
:
 
"
email
"
,

 
"
name
"
:
 
"
mytestcampaign
"
,

 
"
source
"
:
 
"
mysource
"
,

 
},



Whenever the UTM parameters are no longer a part of the URL, Segment no longer includes them. For example, if the user goes to a new page within your website which does not contain these parameters, they will not be included in subsequent events. UTM parameters are non-persistent by default as they could potentially cause data accuracy problems. Here’s an example of why: Say a user clicks on an ad and lands on your site. He navigates around and bookmarks an internal page - or maybe shares a link with a friend, who shares it with another friend. All those links would then point back to the same test utm_source as the initial referrer for any purchase.

Segment doesn’t validate UTM parameter names. This design supports the flexibility to track both standard parameters (for example, utm_source, utm_medium) and custom parameters defined by users. As a result, all parameters present in the URL collected as is, and are added to the context field without checks for naming conventions or validity.

If you want to ensure that only standard UTM parameters (such as, utm_source, utm_medium, utm_campaign, utm_content, utm_term) are included in the context.campaign object, you can implement 
Source middleware
 in your Analytics.js setup.

For example:

window
.
analytics
.
addSourceMiddleware
(({
 
payload
,
 
next
 
})
 
=>
 
{

  
if 
(
payload
.
obj
.
context
?.
campaign
)
 
{

    
const
 
allowedFields
 
=
 
[
"
source
"
,
 
"
medium
"
,
 
"
term
"
,
 
"
campaign
"
,
 
"
content
"
];

    
const
 
campaign
 
=
 
payload
.
obj
.
context
.
campaign
;

    
Object
.
keys
(
campaign
).
forEach
(
key
 
=>
 
{

      
if 
(
!
allowedFields
.
includes
(
key
))
 
{

        
delete
 
campaign
[
key
];

      
}

    
});

  
}

  
next
(
payload
);


});



This middleware filters out any non-standard parameters from the 
context.campaign
 object before they’re sent to Segment or forwarded to your enabled destinations.

Analytics.js performance

The Analytics.js library and all Destination libraries are loaded with the 
HTML script 
async
 tag
. This also means that Segment fires methods asynchronously, so you should adjust your code accordingly if you require that events be sent from the browser in a specific order.

While many tools require access to the DOM or cookies, for the Zendesk, Salesforce, and Mailchimp destinations, Segment doesn’t need to load a native JavaScript library. Instead, Segment’s servers send data to the end-tools.

Segment loads the libraries required for your 
enabled
 Destinations. When you disable a destination, the custom version of Analytics.js loaded on your site stops requesting that library.

Using Analytics.js doesn’t offer a large performance benefit, but is more performant than installing each of the destinations individually. And as more destinations move to accept data directly from Segment, you’ll receive more performance benefits automatically.

One option, if you don’t want to use any bundled third-party tools, is to use the 
Analytics-Node
 package.

Analytics.js doesn’t set third-party cookies and only sets first-party cookies.

Bundle size

Segment’s Analytics.js JavaScript snippet increases the page size by about 1.1KB.

The snippet asynchronously requests and loads a customized JavaScript bundle (
analytics.min.js
), which contains the code and settings needed to load your 
device-mode destinations
. The size of this file changes depending on the number of and which destinations you enable.

Without any destinations enabled, the 
analytics.min.js
 file is about 62KB. Each time you enable a destination, the file’s size may increase slightly.

Cookies set by Analytics.js

Segment sets three cookies in general:

For Google Chrome, these cookies expire by default 
one year
 after the date created. Other 
supported browsers
 might have a different expiration time.

Some user/group traits are also stored in 
localStorage
:

Note that 
localStorage
 variables don’t expire because the browser defines that functionality.

Local storage cookies used by Analytics.js

Analytics.js uses 
localstorage
 cookies if you have retries enabled, to keep track of retry timing.

For more information, visit the 
Segment localstorage-retry library
.

You can set the 
debug
 cookie to 
analytics.js
 to log debug messages from Analytics.js to the console.

Tracking Blockers and Browser Privacy Settings

Segment does not endorse bypassing tracking blockers or browser privacy settings for client-side tracking. Your users have control over what gets loaded on their pages and can use plugins or browser settings to block third-party scripts, including Segment. To minimize client-side data loss, Segment recommends you choose from the following routes:

To minimize client-side data loss, Segment provides a few workarounds. However, it’s important to note that Segment cannot guarantee their effectiveness.

Use the 
bundle obfuscation
 feature. You can add an obfuscate property to the object in the second parameter, which obscures the URL from which your integrations and destination actions are loaded. This helps prevent words that are flagged by ad blockers to not be detected in your URL, enabling the integration to properly load.

Create a 
custom proxy
. This changes the URL that Segment loads from (cdn.segment.com) and the outgoing requests generated when events are triggered (api.segment.io).

Consider implementing the 
Segment Edge SDK
. The Segment Edge SDK leverages Cloudflare Workers to facilitate first-party data collection and real-time user profiling for app personalization. It integrates Segment’s library into web apps, manages user identity via HttpOnly cookies, and employs an internal router for efficient data processing and user experience customization. This innovative approach simplifies tracking and personalization for Segment customers. More information is available in the 
Edge SDK README
.

Consider using one of Segment’s 
server-side libraries
. Using a server-side library eliminates concerns about tracking blockers and privacy browsers that can prevent Segment from loading. This option may require additional code to track actions like a Page call, as you now need to manually pass contextual information that would have been automatically collected by Analytics.js, like 
url
, 
path
, and 
referrer
. Note that some destinations are device-mode only.

Installing the library under a custom global namespace

When you load Analytics.js through snippet code, by default, the SDK installs on 
window.analytics
 global variable. If this causes a conflict with another library on your page, you can change the global variable used by Analytics.js if you use snippet version 5.2.1 or later.

Change the global variable in the beginning of your snippet code as shown below. In this case, Analytics.js uses 
window.custom_key
 to load instead of 
window.analytics
.

  - !function(){var i="analytics", ...
  + !function(){var i="custom_key", ...


Add destinations from npm

Bundle the destinations you want loaded from 
npm
 instead of having them loaded from a remote CDN. This enables you to have fewer network requests when adding destinations.

To add actions-based destinations from npm:

import
 
vwo
 
from
 
'
@segment/analytics-browser-actions-vwo
'


import
 
braze
 
from
 
'
@segment/analytics-browser-actions-braze
'



const
 
analytics
 
=
 
AnalyticsBrowser
.
load
({

  
writeKey
:
 
'
<WRITE_KEY>
'
,

  
plugins
:
 
[
vwo
,
 
braze
],


})



Pass in the destination plugin to the added config option called 
plugins
.  A list of all action destination packages can be found on GitHub in the 
@segmentio/action-destinations
 repository.

To add classic destinations from npm:

import
 
{
 
AnalyticsBrowser
 
}
 
from
 
'
@segment/analytics-next
'


import
 
GoogleAnalyticsIntegration
 
from
 
'
@segment/analytics.js-integration-google-analytics
'



// The following example assumes configuration for Google Analytics will be available in the fetched settings


const
 
analytics
 
=
 
AnalyticsBrowser
.
load
({

  
writeKey
:
 
'
<WRITE_KEY>
'
,

  
classicIntegrations
:
 
[
 
GoogleAnalyticsIntegration
 
]


}),



Segment Inspector

The Segment Inspector is a Chrome web extension that enables you to debug your Segment integration on web applications instrumented with Analytics.js. Analytics.js sends data to the extension so that you can see how events change before they’re sent to your destinations and so that you can verify that the event details are correct. The Segment Inspector also lets you analyze and confirm that API calls made from your website arrive to your Analytics.js source.

For the Segment inspector to work, you must enable the Analytics.js source.

To add the Segment Inspector as a Chrome extension:

Once installed, use the Inspect Elements developer tool in Chrome to use the Segment Inspector. To access the Inspector, go to the top menu bar of Chrome and navigate to 
View > Developer > Developer Tools
 and go to the 
Segment
 tab. On the Segment tab, you can:

Components of the Segment Inspector

The Segment Inspector is composed of these three components:

Example uses

Here are some examples of using Analytics.js. Note that the examples assume Analytics.js is installed through 
npm
.

External dependencies

Analytics.js production dependencies are 
listed under the 
dependencies
 key
.

This page was last modified: 07 Feb 2025

Need support?

Questions? Problems? Need more info? Contact Segment Support for assistance!

Help improve these docs!

Was this page helpful?

Thanks for your feedback!

Can we improve this doc? 
Send us feedback!

Get started with Segment

On this page

Was this page helpful?

Thanks for your feedback!

Can we improve
 this doc? 
Send us feedback!

Product

For Developers

Company

Support

© 2025 Segment.io, Inc.

=== Content from https://segment.com/docs/connections/storage/warehouses/health/ ===

Warehouse Health Dashboard
        

On this page

The Warehouse Health dashboard helps you understand trends in data volume (specifically, rows) synced to your data warehouse over time.

You can use this feature to answer questions such as:

Warehouse Health is available for all Warehouse customers.

The Warehouse Health dashboards are available at both the 
warehouse level
, and at the 
warehouse-source connection level
, explained below.

Data in the dashboards updates in real-time, and covers the previous 30 days. The timezones displayed in the dashboards are converted to the viewer’s local time.

Warehouse dashboards

Go to the Segment App, to the Destinations list, and select the warehouse. On the warehouse’s information page, click the 
Health
 tab.

This dashboard displays aggregate trends from 
all
 sources that sync to the specific warehouse.



A warehouse level dashboard

Warehouse-Source dashboards

Go to the Segment App, to the Destinations list, and select the warehouse. On the warehouse’s Overview page, select the Source (schema) you want to see data for, then click the 
Health
 tab.

This dashboard displays trends for each separate source that syncs to a specific warehouse. It also displays aggregations of the collections within that source.



A warehouse-source level dashboard

Warehouse Health Dashboard FAQs

No. These dashboards exist to help you understand high-level trends, but not to provide exact numbers about the data synced to the warehouse. The numbers provided in these dashboards are rounded, and are not exact.

These dashboards will help you understand trends in the data, and use signals to do deeper investigation and QA, as needed.

The Warehouse Overview, Sync History and Health tabs provide different levels of granularity into warehouse syncs.

Data is refreshed on a real time basis.

The data available shows the last 30 days.

All dates and times found within Warehouse Health, Sync History and Warehouse overview pages are in the user’s local time.

This page was last modified: 07 Feb 2025

Need support?

Questions? Problems? Need more info? Contact Segment Support for assistance!

Help improve these docs!

Was this page helpful?

Thanks for your feedback!

Can we improve this doc? 
Send us feedback!

Get started with Segment

On this page

Was this page helpful?

Thanks for your feedback!

Can we improve
 this doc? 
Send us feedback!

Product

For Developers

Company

Support

© 2025 Segment.io, Inc.

=== Content from https://segment.com/docs/engage/campaigns/sms-campaigns/ ===

SMS Campaigns
        

Engage Premier requires a Business tier account and includes Engage Foundations and Unify.


See the 
available plans
, or 
contact Support
.

On this page

Engage Premier entered an End of Sale (EOS) period effective  June 10, 2024. Existing Segment customers will continue to have access and support to Engage Premier until an end-of-life (EOL) date is announced. We recommend exploring the following pages in preparation of a migration or future MCM needs:

Twilio Marketing Campaigns

Preferred ISV Partners:

Airship Blog
 


Bloomreach Blog
 


Braze Blog
 


Insider Blog
 


Klaviyo Blog
 


Twilio Engage Foundations Documentation
 

With Twilio Engage, you can send email and SMS campaigns to users who have opted in to receive your marketing materials. On this page, you’ll learn how to create and send an SMS campaign.

Some knowledge of the Journeys product will benefit you as you read through this guide. If you’re new to Journeys, the 
Journeys documentation
 will bring you up to speed.

How Engage campaigns work

Twilio Engage uses Journeys to send email and SMS campaigns.  With Journeys, you add conditions and steps that trigger actions like sending an email or an SMS.

You’ll build and then send your campaign in three stages:

Create a Journey

Because Engage campaigns exist within Journeys, begin by creating a Journey:

Add a Journey condition

With your Journey created, you’ll now create a 
condition
 that will trigger your SMS campaign:

With your entry condition added, you’re now ready to create your SMS campaign.

Create, test, and publish your SMS campaign

Follow these steps to create an SMS campaign:

Unsubscribe options are required by law.  Your SMS campaign must contain “Reply STOP to unsubscribe.”

Test your SMS campaign

At this point, you can send a test SMS before publishing your campaign. Testing the SMS confirms that your content and merge tags appear as expected.

As part of your test send, you can enter custom values to populate the profile traits in your SMS message.

Follow these steps to test your campaign:

Publish your SMS campaign

With your SMS created and tested, you’re now ready to save the campaign and publish your Journey with the following steps:

Your SMS campaign is now live. Users who trigger the SMS step’s parent Journey condition will receive your SMS campaign.

SMS campaign fields

The following table contains descriptions of the three fields in the Journeys Send SMS builder.  All SMS fields are required.

Next steps

Using Journeys, you can create multi-channel customer engagement with both email and SMS campaigns.  Having published an SMS, learn how 
Engage email campaigns
 can help you market to customers through email.

This page was last modified: 15 Jul 2024

Need support?

Questions? Problems? Need more info? Contact Segment Support for assistance!

Help improve these docs!

Was this page helpful?

Thanks for your feedback!

Can we improve this doc? 
Send us feedback!

Get started with Segment

On this page

Was this page helpful?

Thanks for your feedback!

Can we improve
 this doc? 
Send us feedback!

Product

For Developers

Company

Support

© 2025 Segment.io, Inc.

=== Content from https://segment.com/docs/api/public-api/query-language/ ===

Segment Query Language Reference
        

The Public API is available to customers on Team or Business plans.


See the 
available plans
, or 
contact Support
.

On this page

Segment’s query language lets you define audience segments and computed traits. With clear syntax and practical functionality, the language simplifies the process of defining conditions and computations, helping you extract valuable insights from customer data.

This reference provides a comprehensive overview of the Segment query language.

Segment's query language in private beta

Segment’s query language is in private beta, and Segment is actively working on this feature. Some functionality may change before it becomes generally available.

Overview

Audience definitions specify the criteria for identifying users or accounts as members of a particular audience, while computed trait definitions outline the logic for aggregating or calculating values stored as traits on user or account level profiles.

With Segment’s query language, you can create these definitions and use them with Segment APIs to generate audiences and computed traits.

Available functions and operators

This section outlines the functions and operators you can use with the query language.

Syntax

Follow these syntax rules when you create definitions:

Syntactic sugar

The language supports the following syntactic sugar adjustments:

Definition type

The definition type (
USERS
 or 
ACCOUNTS
) determines whether the computation operates at the user or account level. For account-level audiences, you can apply additional functions 
ANY
 (to verify that all underlying users meet the defined conditions) and 
ALL
 (to check if any of the underlying users meet the defined conditions).

These functions use the association between accounts and users to determine audience membership.

Functions

The following tables list the query languages’s available functions.

Extractors

Filters

Reducers

Comparisons

Junctions

Return Type

Examples

Audiences

Suppose you wanted to collect all users who performed the 
Shoes Bought
 event at least once within the last seven days, where the purchase price was greater than or equal to 100.

Another way to think of this scenario would be:

Here’s how you could do that in Segment’s query language:

event
(
'Shoes Bought'
).
where
(
 
property
(
'price'
)
 
>=
 
100
 
).
within
(
7
 
days
).
count
()
 
>=
 
1



This example collects:

event
(
'Shoes Bought'
).
where
(
 

property
(
'price'
)
 
>=
 
trait
(
'avg_spend'
)


AND
 

event
(
'Shoes Returned'
).
within
(
parent
:
 
5
 
days
).
count
()
 
>=
 
1
 

).
within
(
30
 
days
).
count
()
 
>=
 
1



This example collects all users who did not perform the 
Shoes Bought
 event at least once and don’t have a 
total_spend
 trait with a value greater than 
200
:

NOT
 
(
 
event
(
'Shoes Bought'
).
count
()
 
>=
 
1
 
AND
 
trait
(
'total_spend'
)
 
>
 
200
 
)



This example collects all accounts where all associated users performed the 
Shoes Bought
 event at least once and have a 
total_spend
 trait greater than 
200
:

ALL
 
(
 
event
(
'Shoes Bought'
).
count
()
 
>=
 
1
 
AND
 
trait
(
'total_spend'
)
 
>
 
200
 
)



This example collects all accounts where no associated users performed the 
Shoes Bought
 event at least once:

ALL
 
NOT
 
event
(
'Shoes Bought'
).
count
()
 
>=
 
1



This example collects all accounts where any associated users performed the 
Shoes Bought
 event at least once:

ANY
 
event
(
'Shoes Bought'
).
count
()
 
>=
 
1



Computed Traits

Suppose you wanted to calculate the average spend based on all 
Shoes Bought
 events performed within the last 30 days for each user.

Another way to think of this would be:

Here’s how you could do that in Segment’s query language:

event
(
'Shoes Bought'
).
within
(
30
 
days
).
avg
(
property
(
'spend'
))



This example calculates the minimum spend for each user, based on all 
Shoes Bought
 events, where the price was greater than 
100
 and the brand was 
My_Brand
:

event
(
'Shoes Bought'
).
where
(
 
property
(
'price'
)
 
>
 
100
 
AND
 
property
(
'brand'
)
 
=
 
'My Brand'
 
).
min
(
property
(
'spend'
))



This example calculates the first-seen spend value for each user, based on all 
Shoes Bought
 events performed within the last 30 days:

event
(
'Shoes Bought'
).
within
(
30
 
days
).
first
(
property
(
'spend'
))



This example calculates the most frequent spend value for each user, based on all 
Shoes Bought
 events performed within the last 30 days. It only considers spend values that have a minimum frequency of 
2
:

event
(
'Shoes Bought'
).
within
(
30
 
days
).
mode
(
property
(
'spend'
),
 
2
)



This page was last modified: 04 Jun 2024

Need support?

Questions? Problems? Need more info? Contact Segment Support for assistance!

Help improve these docs!

Was this page helpful?

Thanks for your feedback!

Can we improve this doc? 
Send us feedback!

Get started with Segment

On this page

Was this page helpful?

Thanks for your feedback!

Can we improve
 this doc? 
Send us feedback!

Product

For Developers

Company

Support

© 2025 Segment.io, Inc.

=== Content from https://segment.com/docs/api/config-api/authentication/ ===

Authentication
        

On this page

The Segment Public API is available

Segment’s 
Public API
 is available for Team and Business tier customers to use. You can use the Public API and Config APIs in parallel, but moving forward any API updates will come to the Public API exclusively. 

Please contact your account team or 
friends@segment.com
 with any questions.

You can access the Config API programmatically using access tokens. When you authenticate with an access token, you have access to any resource and permission assigned to the token.

Create an Access Token

As a workspace owner, you can create access tokens from the Access Management page in Admin settings. You can assign the same granularity of permissions as you can for a logged-in user. As best practice, tokens should be assigned the least permissions needed to perform a required API action. All tokens must have a description.

Secret Token

You can not retrieve the plain-text 
token
 later, so you should save it in a secret manager. If you lose the 
token
 you can generate a new one.

Use an Access Token

Now that you have an access token, you can use this token to access the Config API by setting it in the 
Authorization
 header of your requests, for example:

$ ACCESS_TOKEN
=
qiTgISif4zprgBb_5j4hXfp3qhDbxrntWwwOaHgAMr8.gg9ok4Bk7sWlP67rFyXeH3ABBsXyWqNuoXbXZPv1y2g


$ 
curl 
\

  
-X
 GET 
\

  
-H
 
"Authorization: Bearer 
$ACCESS_TOKEN
"
 
\

  https://platform.segmentapis.com/v1beta/workspaces


Example response:

{

  
"workspaces"
:
 
[

    
{

      
"name"
:
 
"myworkspace"
,

      
"display_name"
:
 
"My Space"
,

      
"id"
:
 
"e5bdb0902b"
,

      
"create_time"
:
 
"2018-08-08T13:24:02.651Z"

    
}

  
]


}



Scopes

You cannot use access tokens created with the 
workspace:read
 scope to create or update resources. If you do so, you’ll get the following error:

{

  
"error"
:
 
"insufficient scope"
,

  
"code"
:
 
7


}



See 
Config API Errors
 for error codes.

This page was last modified: 27 Sep 2022

Need support?

Questions? Problems? Need more info? Contact Segment Support for assistance!

Help improve these docs!

Was this page helpful?

Thanks for your feedback!

Can we improve this doc? 
Send us feedback!

Get started with Segment

On this page

Was this page helpful?

Thanks for your feedback!

Can we improve
 this doc? 
Send us feedback!

Product

For Developers

Company

Support

© 2025 Segment.io, Inc.

=== Content from https://segment.com/docs/unify/data-graph/setup-guides/snowflake-setup// ===

Snowflake Data Graph Setup
        

Unify requires a Business tier account and is included with Engage.


See the 
available plans
, or 
contact Support
.

On this page

Data Graph, Reverse ETL, Profiles Sync require different warehouse permissions.

On this page, you’ll learn how to connect your Snowflake data warehouse to Segment for the 
Data Graph
.

Snowflake credentials

Segment assumes that you already have a warehouse that includes the datasets you’d like to use for the Data Graph. Log in to Snowflake with admin privileges to provide the Data Graph with the necessary permissions below.

Step 1: Create a user and internal database for Segment to store checkpoint tables

Segment recommends setting up a new Snowflake user and only giving this user permissions to access the required databases and schemas. Run the SQL code block below in your SQL worksheet in Snowflake to execute the following tasks:

Segment recommends creating a new database for the Data Graph.
If you choose to use an existing database that has also been used for 
Segment Reverse ETL
, you must follow the 
additional instructions
 to update user access for the Segment Reverse ETL schema.

-- ********** SET UP THE FOLLOWING WAREHOUSE PERMISSIONS **********



-- Update the following variables 


SET
 
segment_connection_username
 
=
 
'SEGMENT_LINKED_USER'
;


SET
 
segment_connection_password
 
=
 
'my-safe-password'
;


SET
 
segment_connection_warehouse
 
=
 
'SEGMENT_LINKED_WH'
;


SET
 
segment_connection_role
 
=
 
'SEGMENT_LINKED_ROLE'
;



-- The DB  used for Segment's internal bookkeeping.


-- Note: Use this DB in the connection settings on the Segment app. This is the only DB that Segment requires write access to.


SET
 
segment_connection_db
 
=
 
'SEGMENT_LINKED_PROFILES_DB'
;



-- ********** [OPTIONAL] UNCOMMENT THE CODE BELOW IF YOU NEED TO CREATE A NEW WAREHOUSE **********



-- CREATE WAREHOUSE IF NOT EXISTS identifier($segment_connection_warehouse)


-- WITH WAREHOUSE_SIZE = 'XSMALL'


--   WAREHOUSE_TYPE = 'STANDARD'


--   AUTO_SUSPEND = 600 -- 5 minutes


--   AUTO_RESUME = TRUE;



-- ********** RUN THE COMMANDS BELOW TO FINISH SETTING UP THE WAREHOUSE PERMISSIONS **********



-- Use admin role for setting grants


USE
 
ROLE
 
ACCOUNTADMIN
;



-- Create a role for the Data Graph


CREATE
 
ROLE
 
IF
 
NOT
 
EXISTS
 
identifier
(
$
segment_connection_role
)


COMMENT
 
=
 
'Used for Segment Data Graph'
;



-- Create a user for the Data Graph


CREATE
 
USER
 
IF
 
NOT
 
EXISTS
 
identifier
(
$
segment_connection_username
)


MUST_CHANGE_PASSWORD
 
=
 
FALSE


DEFAULT_ROLE
 
=
 
$
segment_connection_role


PASSWORD
 
=
 
$
segment_connection_password


COMMENT
 
=
 
'Segment Data Graph User'


TIMEZONE
 
=
 
'UTC'
;



-- Grant permission to the role to use the warehouse


GRANT
 
USAGE
 
ON
 
WAREHOUSE
 
identifier
(
$
segment_connection_warehouse
)
 
TO
 
ROLE
 
identifier
(
$
segment_connection_role
);



-- Grant role to the user


GRANT
 
ROLE
 
identifier
(
$
segment_connection_role
)
 
TO
 
USER
 
identifier
(
$
segment_connection_username
);



-- Create and Grant access to a Segment internal DB used for bookkeeping. This is the only DB that Segment requires write access to. This is also the DB you will use in the "Database Name" config while setting up the connection in the Segment app. 


CREATE
 
DATABASE
 
IF
 
NOT
 
EXISTS
 
identifier
(
$
segment_connection_db
);


GRANT
 
USAGE
 
ON
 
DATABASE
 
identifier
(
$
segment_connection_db
)
 
TO
 
ROLE
 
identifier
(
$
segment_connection_role
);


GRANT
 
USAGE
 
ON
 
ALL
 
SCHEMAS
 
IN
 
DATABASE
 
identifier
(
$
segment_connection_db
)
 
TO
 
ROLE
 
identifier
(
$
segment_connection_role
);


GRANT
 
CREATE
 
SCHEMA
 
ON
 
DATABASE
  
identifier
(
$
segment_connection_db
)
 
TO
 
ROLE
 
identifier
(
$
segment_connection_role
);




Step 2: Grant read-only access to additional databases for the Data Graph

Next, give the Segment role 
read-only
 access to additional databases you want to use for Data Graph including the Profiles Sync database. Repeat the following SQL query for 
each
 database you want to use for the Data Graph.



SET
 
segment_connection_role
 
=
 
'SEGMENT_LINKED_ROLE'
;



-- ********** REPEAT THE SQL QUERY BELOW FOR EACH DATABASE YOU WANT TO USE FOR THE DATA GRAPH **********


-- Change this for each DB you want to grant the Data Graph read-only access to


SET
 
linked_read_only_database
 
=
 
'MARKETING_DB'
;



GRANT
 
USAGE
 
ON
 
DATABASE
 
identifier
(
$
linked_read_only_database
)
 
TO
 
ROLE
 
identifier
(
$
segment_connection_role
);


GRANT
 
USAGE
 
ON
 
ALL
 
SCHEMAS
 
IN
 
DATABASE
 
identifier
(
$
linked_read_only_database
)
 
TO
 
ROLE
 
identifier
(
$
segment_connection_role
);


GRANT
 
SELECT
 
ON
 
ALL
 
TABLES
 
IN
 
DATABASE
 
identifier
(
$
linked_read_only_database
)
 
TO
 
ROLE
 
identifier
(
$
segment_connection_role
);


GRANT
 
SELECT
 
ON
 
FUTURE
 
TABLES
 
IN
 
DATABASE
 
identifier
(
$
linked_read_only_database
)
 
TO
 
ROLE
 
identifier
(
$
segment_connection_role
);


GRANT
 
SELECT
 
ON
 
ALL
 
VIEWS
 
IN
 
DATABASE
 
identifier
(
$
linked_read_only_database
)
 
TO
 
ROLE
 
identifier
(
$
segment_connection_role
);


GRANT
 
SELECT
 
ON
 
FUTURE
 
VIEWS
 
IN
 
DATABASE
 
identifier
(
$
linked_read_only_database
)
 
TO
 
ROLE
 
identifier
(
$
segment_connection_role
);


GRANT
 
SELECT
 
ON
 
ALL
 
EXTERNAL
 
TABLES
 
IN
 
DATABASE
 
identifier
(
$
linked_read_only_database
)
 
TO
 
ROLE
 
identifier
(
$
segment_connection_role
);


GRANT
 
SELECT
 
ON
 
FUTURE
 
EXTERNAL
 
TABLES
 
IN
 
DATABASE
 
identifier
(
$
linked_read_only_database
)
 
TO
 
ROLE
 
identifier
(
$
segment_connection_role
);


GRANT
 
SELECT
 
ON
 
ALL
 
MATERIALIZED
 
VIEWS
 
IN
 
DATABASE
 
identifier
(
$
linked_read_only_database
)
 
TO
 
ROLE
 
identifier
(
$
segment_connection_role
);


GRANT
 
SELECT
 
ON
 
FUTURE
 
MATERIALIZED
 
VIEWS
 
IN
 
DATABASE
 
identifier
(
$
linked_read_only_database
)
 
TO
 
ROLE
 
identifier
(
$
segment_connection_role
);




(Optional) Step 3: Restrict read-only access to schemas

If you want to restrict access to specific 
Snowflake schemas and tables
, then run the following commands:

-- [Optional] Further restrict access to only specific schemas and tables 


SET
 
db
 
=
 
'MY_DB'
;


SET
 
schema
 
=
 
'MY_DB.MY_SCHEMA_NAME'
;


SET
 
segment_connection_role
 
=
 
'SEGMENT_LINKED_ROLE'
;



-- View specific schemas in database


GRANT
 
USAGE
 
ON
 
DATABASE
 
identifier
(
$
db
)
 
TO
 
ROLE
 
identifier
(
$
segment_connection_role
);


GRANT
 
USAGE
 
ON
 
SCHEMA
 
identifier
(
$
schema
)
 
TO
 
ROLE
 
identifier
(
$
segment_connection_role
);


GRANT
 
SELECT
 
ON
 
ALL
 
TABLES
 
IN
 
SCHEMA
 
identifier
(
$
schema
)
 
TO
 
ROLE
 
identifier
(
$
segment_connection_role
);


GRANT
 
SELECT
 
ON
 
FUTURE
 
TABLES
 
IN
 
SCHEMA
 
identifier
(
$
schema
)
 
TO
 
ROLE
 
identifier
(
$
segment_connection_role
);


GRANT
 
SELECT
 
ON
 
ALL
 
VIEWS
 
IN
 
SCHEMA
 
identifier
(
$
linked_read_only_database
)
 
TO
 
ROLE
 
identifier
(
$
segment_connection_role
);


GRANT
 
SELECT
 
ON
 
FUTURE
 
VIEWS
 
IN
 
SCHEMA
 
identifier
(
$
linked_read_only_database
)
 
TO
 
ROLE
 
identifier
(
$
segment_connection_role
);


GRANT
 
SELECT
 
ON
 
ALL
 
EXTERNAL
 
TABLES
 
IN
 
SCHEMA
 
identifier
(
$
linked_read_only_database
)
 
TO
 
ROLE
 
identifier
(
$
segment_connection_role
);


GRANT
 
SELECT
 
ON
 
FUTURE
 
EXTERNAL
 
TABLES
 
IN
 
SCHEMA
 
identifier
(
$
linked_read_only_database
)
 
TO
 
ROLE
 
identifier
(
$
segment_connection_role
);


GRANT
 
SELECT
 
ON
 
ALL
 
MATERIALIZED
 
VIEWS
 
IN
 
SCHEMA
 
identifier
(
$
linked_read_only_database
)
 
TO
 
ROLE
 
identifier
(
$
segment_connection_role
);


GRANT
 
SELECT
 
ON
 
FUTURE
 
MATERIALIZED
 
VIEWS
 
IN
 
SCHEMA
 
identifier
(
$
linked_read_only_database
)
 
TO
 
ROLE
 
identifier
(
$
segment_connection_role
);




Step 4: Confirm permissions

To verify you have set up the right permissions for a specific table, log in with the username and password you created for 
SEGMENT_CONNECTION_USERNAME
 and run the following command to verify the role you created has the correct permissions. If this command succeeds, you should be able to view the respective table.

set
 
segment_connection_role
 
=
 
'SEGMENT_LINKED_ROLE'
;


set
 
linked_read_only_database
 
=
 
'YOUR_DB'
;


set
 
table_name
 
=
 
'YOUR_DB.SCHEMA.TABLE'
;



USE
 
ROLE
 
identifier
(
$
segment_connection_role
);


USE
 
DATABASE
 
identifier
(
$
linked_read_only_database
)
 
;


SHOW
 
SCHEMAS
;


SELECT
 
*
 
FROM
 
identifier
(
$
table_name
)
 
LIMIT
 
10
;




Step 5: Connect your warehouse to the Data Graph

To connect your warehouse to the Data Graph:

Update user access for Segment Reverse ETL schema

If Segment Reverse ETL has ever run in the database you are configuring as the Segment connection database, a Segment-managed schema is already created and you need to provide the new Segment user access to the existing schema. Run the following SQL if you run into an error on the Segment app indicating that the user doesn’t have sufficient privileges on an existing 
_segment_reverse_etl
 schema.

-- If you want to use an existing database that already has Segment Reverse ETL schemas, you’ll need to run some additional steps below to grant the role access to the existing schemas.



SET
 
retl_schema
 
=
 
concat
(
$
segment_connection_db
,
'.__segment_reverse_etl'
);


GRANT
 
USAGE
 
ON
 
SCHEMA
 
identifier
(
$
retl_schema
)
 
TO
 
ROLE
 
identifier
(
$
segment_connection_role
);


GRANT
 
CREATE
 
TABLE
 
ON
 
SCHEMA
 
identifier
(
$
retl_schema
)
 
TO
 
ROLE
 
identifier
(
$
segment_connection_role
);


GRANT
 
SELECT
,
INSERT
,
UPDATE
,
DELETE
 
ON
 
ALL
 
TABLES
 
IN
 
SCHEMA
 
identifier
(
$
retl_schema
)
 
TO
 
ROLE
 
identifier
(
$
segment_connection_role
);



This page was last modified: 28 Oct 2024

Need support?

Questions? Problems? Need more info? Contact Segment Support for assistance!

Help improve these docs!

Was this page helpful?

Thanks for your feedback!

Can we improve this doc? 
Send us feedback!

Get started with Segment

On this page

Was this page helpful?

Thanks for your feedback!

Can we improve
 this doc? 
Send us feedback!

Product

For Developers

Company

Support

© 2025 Segment.io, Inc.

=== Content from https://segment.com/docs/connections/storage/catalog/data-lakes/ ===

Set Up Segment Data Lakes
                

On this page

Segment Data Lakes provide a way to collect large quantities of data in a format that’s optimized for targeted data science and data analytics workflows. You can read 
more information about Data Lakes
 and learn 
how they differ from Warehouses
 in Segment’s Data Lakes documentation.
Segment supports two type of data-lakes:

You can also set up your Segment Data Lakes using 
Lake Formation
, a fully managed service built on top of the AWS Glue Data Catalog.

Set up Segment Data Lakes (AWS)

To set up Segment Data Lakes, create your AWS resources, enable the Segment Data Lakes destination in the Segment app, and verify that your Segment data is synced to S3 and Glue.

Prerequisites

Before you set up Segment Data Lakes, you need the following resources:

Step 1 - Set up AWS resources

You can use the 
open source Terraform module
 to automate much of the set up work to get Data Lakes up and running. If you’re familiar with Terraform, you can modify the module to meet your organization’s needs, however Segment guarantees support only for the template as provided. The Data Lakes set up uses Terraform v0.12+. To support more versions of Terraform, the AWS provider must use v4, which is included in the example 
main.tf
.

You can also use Segment’s 
manual setup instructions
 to configure these AWS resources, if you prefer.

The Terraform module and manual setup instructions both provide a base level of permissions to Segment (for example, the correct IAM role to allow Segment to create Glue databases on your behalf). If you want stricter permissions, or other custom configurations, you can customize these manually.

Step 2 - Enable Data Lakes destination

After you set up the necessary AWS resources, the next step is to set up the Data Lakes destination within Segment:

In the 
Segment App
, click 
Add Destination
, then search for and select 
Data Lakes
.

Click 
Configure Data Lakes
 and select the source to connect to the Data Lakes destination.
  
Warning
: You must add the Workspace ID to the external ID list in the IAM policy, or else the source data cannot be synced to S3.

You must individually connect each source to the Data Lakes destination. However, you can copy the settings from another source by clicking 
…
 (“more”) (next to the button for “Set up Guide”).

(Optional)
 
Glue Database Name
: Optional advanced setting to change the name of the Glue Database which is set to the source slug by default. Each source connected to Data Lakes must have a different Glue Database name otherwise data from different sources will collide in the same database.

Once the Data Lakes destination is enabled, the first sync will begin approximately 2 hours later.

Step 3 - Verify data is synced to S3 and Glue

You will see event data and 
sync reports
 populated in S3 and Glue after the first sync successfully completes. However if an 
insufficient permission
 or 
invalid setting
 is provided during set up, the first data lake sync will fail.

To receive sync failure alerts by email, subscribe to the 
Storage Destination Sync Failed
 activity email notification within the 
App Settings > User Preferences > 
Notification Settings
.

Sync Failed
 emails are sent on the 1st, 5th, and 20th sync failure. Learn more about the types of errors which can cause sync failures in Segment’s 
Sync errors
 docs.

(Optional) Step 4 - Replay historical data

If you want to add historical data to your data set using a 
replay of historical data
 into Data Lakes, 
contact the Segment Support team
 to request one.

Replay processing time can vary depending on the volume of data and number of events in each source. If you decide to run a Replay, Segment recommends that you start with data from the last six months to get started, and then replay additional data if you find you need more.

Segment creates a separate EMR cluster to run replays, then destroys it when the replay finishes. This ensures that regular Data Lakes syncs are not interrupted, and helps the replay finish faster.

Set up Segment Data Lakes (Azure)

To set up Segment Data Lakes (Azure), create your Azure resources and then enable the Data Lakes destination in the Segment app.

Prerequisites

Before you can configure your Azure resources, you must complete the following prerequisites:

Step 1 - Create an ALDS-enabled storage account

Before continuing, note the Location, Storage account name, and the Azure storage container name: you’ll need this information when configuring the Segment Data Lakes (Azure) destination in the Segment app.

Step 2 - Set up Key Vault

Step 3 - Set up Azure MySQL database

  mysql --host=/[HOSTNAME] --port=3306 --user=[USERNAME] --password=[PASSWORD]


  
CREATE
 
DATABASE
 
<
name
>
;



Before continuing, note the MySQL server URL, username and password for the admin account, and your database name: you’ll need this information when configuring the Segment Data Lakes (Azure) destination in the Segment app.

Step 4 - Set up Databricks

Databricks pricing tier

If you create a Databricks instance only for Segment Data Lakes (Azure) usage, only the standard pricing tier is required. However, if you use your Databricks instance for other applications, you may require premium pricing.

Before continuing, note the Cluster ID, Workspace name, Workspace URL, and the Azure Resource Group for your Databricks Workspace: you’ll need this information when configuring the Segment Data Lakes (Azure) destination in the Segment app.

Step 5 - Set up a Service Principal

az
 
login


az
 
ad
 
sp
 
create-for-rbac
 
--name
 
<
ServicePrincipalName
>



curl -X POST 'https://<per-workspace-url>/api/2.0/preview/scim/v2/ServicePrincipals' \
  --header 'Content-Type: application/scim+json' \
  --header 'Authorization: Bearer <personal-access-token>' \
  --data-raw '{
"schemas":[
  "urn:ietf:params:scim:schemas:core:2.0:ServicePrincipal"
],
"applicationId":"<application-id>",
"displayName": "test-sp",
"entitlements":[
  {
    "value":"allow-cluster-create"
  }
]
  }'


Before continuing, note the Client ID and Client Secret for your Service Principal: you’ll need this information when configuring the Segment Data Lakes (Azure) destination in the Segment app.

Step 6 - Configure Databricks Cluster

Optional configuration settings for log4j vulnerability

While Databricks released a statement that clusters are likely unaffected by the log4j vulnerability, out of an abundance of caution, Databricks recommends updating to log4j 2.15+ or adding the following options to the Spark configuration: 
 
spark.driver.extraJavaOptions "-Dlog4j2.formatMsgNoLookups=true"
spark.executor.extraJavaOptions "-Dlog4j2.formatMsgNoLookups=true"

## Configs so we can read from the storage account

spark
.
hadoop
.
fs
.
azure
.
account
.
oauth
.
provider
.
type
.
<
storage_account_name
>
.
dfs
.
core
.
windows
.
net
 
org
.
apache
.
hadoop
.
fs
.
azurebfs
.
oauth2
.
ClientCredsTokenProvider


spark
.
hadoop
.
fs
.
azure
.
account
.
oauth2
.
client
.
endpoint
.
<
storage_account_name
>
.
dfs
.
core
.
windows
.
net
 
https
:
//
login
.
microsoftonline
.
com
/<
azure
-
tenant
-
id
>/
oauth2
/
token


spark
.
hadoop
.
fs
.
azure
.
account
.
oauth2
.
client
.
secret
.
<
storage_account_name
>
.
dfs
.
core
.
windows
.
net
 
<
service
-
principal
-
secret
>


spark
.
hadoop
.
fs
.
azure
.
account
.
auth
.
type
.
<
storage_account_name
>
.
dfs
.
core
.
windows
.
net
 
OAuth


spark
.
hadoop
.
fs
.
azure
.
account
.
oauth2
.
client
.
id
.
<
storage_account_name
>
.
dfs
.
core
.
windows
.
net
 
<
service_principal_client_id
>


##
##

spark
.
hadoop
.
javax
.
jdo
.
option
.
ConnectionDriverName
 
org
.
mariadb
.
jdbc
.
Driver


spark
.
hadoop
.
javax
.
jdo
.
option
.
ConnectionURL
 
jdbc
:
mysql
:
//<
db
-
host
>
:
<
port
>/<
database
-
name
>
?
useSSL
=
true
&
requireSSL
=
true
&
enabledSslProtocolSuites
=
TLSv1
.
2


spark
.
hadoop
.
javax
.
jdo
.
option
.
ConnectionUserName
 
<
database_user
>


spark
.
hadoop
.
javax
.
jdo
.
option
.
ConnectionPassword
 
<
database_password
>


##
##
##

spark
.
hive
.
mapred
.
supports
.
subdirectories
 
true


spark
.
sql
.
storeAssignmentPolicy
 
Legacy


mapreduce
.
input
.
fileinputformat
.
input
.
dir
.
recursive
 
true


spark
.
sql
.
hive
.
convertMetastoreParquet
 
false


##

datanucleus
.
autoCreateSchema
 
true


datanucleus
.
autoCreateTables
 
true


spark
.
sql
.
hive
.
metastore
.
schema
.
verification
 
false


datanucleus
.
fixedDatastore
 
false


##

spark
.
sql
.
hive
.
metastore
.
version
 
2.3
.
7


spark
.
sql
.
hive
.
metastore
.
jars
 
builtin



mysql --host=[HOSTNAME] --port=3306 --user=[USERNAME] --password=[PASSWORD]


USE
 
<
db
-
name
>


INSERT
 
INTO
 
VERSION
 
(
VER_ID
,
 
SCHEMA_VERSION
)
 
VALUES
 
(
0
,
 
'2.3.7'
);



CREATE
 
TABLE
 
test
 
(
id
 
string
);



datanucleus
.
autoCreateSchema
 
false


datanucleus
.
autoCreateTables
 
false


spark
.
sql
.
hive
.
metastore
.
schema
.
verification
 
true


datanucleus
.
fixedDatastore
 
true



Step 7 - Enable the Data Lakes destination in the Segment app

After you set up the necessary resources in Azure, the next step is to set up the Data Lakes destination in Segment:

(Optional) Set up your Segment Data Lake (Azure) using Terraform

Instead of manually configuring your Data Lake, you can create it using the script in the 
terraform-segment-data-lakes
 GitHub repository.

This script requires Terraform versions 0.12+.

Before you can run the Terraform script, create a Databricks workspace in the Azure UI using the instructions in 
Step 4 - Set up Databricks
. Note the 
Workspace URL
, as you will need it to run the script.

In the setup file, set the following local variables:



locals
 
{


region
         
=
 
"
<segment-datlakes-region>
"


resource_group
 
=
 
"
<segment-datlakes-regource-group>
"


storage_account
 
=
 
"
<segment-datalake-storage-account
"


container_name
  
=
 
"
<segment-datlakes-container>
"


key_vault_name
 
=
 
"
<segment-datlakes-key vault>
"


server_name
 
=
 
"
<segment-datlakes-server>
"


db_name
     
=
 
"
<segment-datlakes-db-name>
"


db_password
 
=
 
"
<segment-datlakes-db-password>
"


db_admin
    
=
 
"
<segment-datlakes-db-admin>
"


databricks_workspace_url
 
=
 
"
<segment-datlakes-db-worspace-url>
"


cluster_name
   
=
 
"
<segment-datlakes-db-cluster>
"


tenant_id
      
=
 
"
<tenant-id>
"


}



After you’ve configured your local variables, run the following commands:

terraform
 
init


terraform
 
plan


terraform
 
apply



Running the 
plan
 command gives you an output that creates 19 new objects, unless you are reusing objects in other Azure applications. Running the 
apply
 command creates the resources and produces a service principal password you can use to set up the destination.

FAQ

Segment Data Lakes

No, Data Lakes automatically creates one Glue database per source. This database uses the source slug as its name.

Four roles are created when you set up Data Lakes using Terraform. You add the 
arn:aws:iam::$ACCOUNT_ID:role/segment-data-lake-iam-role
 role to the Data Lakes Settings page in the Segment web app.

The roles which Data Lakes assigns during set up are:

segment_emr_service_role
 - Restricted role that can only be assumed by the EMR service. This is set up based on 
AWS best practices
.

The module doesn’t create a new S3 bucket so you can re-use an existing bucket for your Data Lakes.

Yes, the S3 bucket and the EMR cluster must be in the same region.

To connect a new source to Data Lakes:

Yes, you can configure multiple sources to use the same EMR cluster. Segment recommends that the EMR cluster only be used for Data Lakes to ensure there aren’t interruptions from non-Data Lakes job.

If you don’t see data after enabling a source, check the following:

If all of these look correct and you’re still not seeing any data, please 
contact the Support team
.

The 
output
 tables are temporary tables Segment creates when loading data. They are deleted after each sync.

Yes, you can create new directories in S3 without interfering with Segment data.
Do not modify, or create additional directories with the following names:

Partitioned
 just means that the table has partition columns (day and hour). All tables are partitioned, so you should see this on all table names.

You can use the following command to create external tables in Spectrum to access tables in Glue and join the data with Redshift:

Run the 
CREATE EXTERNAL SCHEMA
 command:

create
 
external
 
schema
 
[
spectrum_schema_name
]


from
 
data
 
catalog


database
 
[
glue_db_name
]


iam_role
 
arn
:
aws
:
iam
::[
account_id
]:
role
/
MySpectrumRole


create
 
external
 
database
 
if
 
not
 
exists
;



Replace:

Segment Data Lakes (Azure)

Yes, your storage account and Databricks instance should be in the same region.

Segment Data Lakes (Azure) supports the following post-processing tools:

If you encounter errors related to your Databricks database, try adding the following line to the config: 

spark
.
sql
.
hive
.
metastore
.
schema
.
verification
.
record
.
version
 
false



After you’ve added to your config, restart your cluster so that your changes can take effect. If you continue to encounter errors, 
contact Segment Support
.

Check your Spark configs to ensure that the information you entered about the database is correct, then restart the cluster. The Databricks cluster automatically initializes the Hive Metastore, so an issue with your config file will stop the table from being created.  If you continue to encounter errors, 
contact Segment Support
.

This page was last modified: 07 Feb 2025

Need support?

Questions? Problems? Need more info? Contact Segment Support for assistance!

Help improve these docs!

Was this page helpful?

Thanks for your feedback!

Can we improve this doc? 
Send us feedback!

Get started with Segment

On this page

Was this page helpful?

Thanks for your feedback!

Can we improve
 this doc? 
Send us feedback!

Product

For Developers

Company

Support

© 2025 Segment.io, Inc.

=== Content from https://segment.com/docs/engage/trait-activation/ ===

Trait Activation Overview
        

Engage Foundations requires a Business tier account and includes Unify.


See the 
available plans
, or 
contact Support
.

On this page

Use Trait Activation to configure sync payloads that you send from Engage Audiences and Journeys to a Destination or Destination Function. Map traits from user profiles to Destinations, configure identifiers to sync, and choose a sync strategy that fits your use cases.

Trait Activation includes both 
Trait Enrichment
 and 
ID Sync
. With Trait Enrichment, use custom, SQL, computed, and predictive traits to enrich the data you map to your destinations or destination functions. Use ID Sync to select identifiers and a sync strategy for each identifier when syncing Engage Audiences to Destinations.

Trait Activation setup

To get started with Trait Activation, you’ll need to set up the destination that you’ll use with 
Trait Enrichment
 and 
ID Sync
.

Set up your destination

Select your destination, view its Segment documentation, then follow the corresponding required setup steps.

Resyncs

Segment recommends creating a new audience for Trait Enrichment and ID Sync. For existing audience destinations, both Trait Enrichment and ID Sync won’t resync the entire audience. Only new data flowing into Segment will adhere to new trait settings.

Contact Segment support
 if you’d like your Audience resynced with Trait Enrichment and ID Sync.

For Audiences larger than 50 million users, it may take several hours, or even days, to sync. Only one resync is allowed at a time for each workspace.

Use cases

Trait Enrichment and ID Sync can help you:

Increase advertising match rates
: Expand the pool of users you advertise to and increase match rates by using traits and identifiers to find the right customers.

Include more personalized message content
: Include traits in your payload for more up-to-date, accurate data.

Configure how you send identifiers to Destinations
: Send the right identifiers to your destinations. For profiles with multiple identifiers, choose a strategy to select identifiers and send them downstream.

Next steps

To learn more about Trait Activation, visit the following docs:

This page was last modified: 10 Sep 2024

Need support?

Questions? Problems? Need more info? Contact Segment Support for assistance!

Help improve these docs!

Was this page helpful?

Thanks for your feedback!

Can we improve this doc? 
Send us feedback!

Get started with Segment

On this page

Was this page helpful?

Thanks for your feedback!

Can we improve
 this doc? 
Send us feedback!

Product

For Developers

Company

Support

© 2025 Segment.io, Inc.

=== Content from https://segment.com/docs/unify/traits/custom-traits/ ===

Custom Traits
        

On this page

Custom traits are user or account traits collected from the Identify calls you send to Segment. For example, these could be demographics like 
age
 or 
gender
, account-specific like 
plan
, or even things like whether a user has seen a particular A/B test variation. From your sources, send custom traits as pieces of information that you know about a user in an Identify call.

As opposed to 
computed traits
 which are computed from your source data, or 
SQL Traits
 which are computed from warehouse data, custom traits are created from source events you pass into Segment and have no trait limits.

Comparing trait types

View the table below to better understand how Segment collects custom, computed, and SQL traits.

You can use the Profile explorer (
Unify > Profile explorer
) to view traits attached to a profile.

Using custom traits

Here’s the payload of a typical Identify call with custom traits (with most 
common fields
 removed):

{

  
"type"
:
 
"identify"
,

  
"traits"
:
 
{

    
"name"
:
 
"John Smith"
,

    
"email"
:
 
"john@example.com"
,

    
"plan"
:
 
"premium"
,

    
"logins"
:
 
5

  
},

  
"userId"
:
 
"97980cfea0067"


}



And here’s the corresponding JavaScript event that would generate the above payload:

analytics
.
identify
(
"
97980cfea0067
"
,
 
{

  
name
:
 
"
John Smith
"
,

  
email
:
 
"
john@example.com
"
,

  
plan
:
 
"
premium
"
,

  
logins
:
 
5


});



Any source event where there’s a 
traits
 object and key value pairs generates custom traits.

Custom traits are mutable and update to the latest value seen by the user’s Identify events.

When an audience that previously generated Identify events is deleted, the data for the audience key is still attached to profiles that entered the audience and becomes visible in Segment as a custom trait.

Reserved custom traits

Segment has reserved some custom traits that have semantic meanings for users, and will handle them in special ways. For example, Segment always expects 
email
 to be a string of the user’s email address. Segment sends this on to destinations like 
Mailchimp
 that require an email address for their tracking.

Only use reserved custom traits for their intended meaning.

Reserved custom traits Segment has standardized:

To learn more about using an Identify call to tie custom traits to profiles, 
visit Segment’s Identify documentation
.

This page was last modified: 12 Apr 2024

Need support?

Questions? Problems? Need more info? Contact Segment Support for assistance!

Help improve these docs!

Was this page helpful?

Thanks for your feedback!

Can we improve this doc? 
Send us feedback!

Get started with Segment

On this page

Was this page helpful?

Thanks for your feedback!

Can we improve
 this doc? 
Send us feedback!

Product

For Developers

Company

Support

© 2025 Segment.io, Inc.

=== Content from https://segment.com/docs/engage/trait-activation/id-sync/ ===

ID Sync
        

Engage Foundations requires a Business tier account and includes Unify.


See the 
available plans
, or 
contact Support
.

On this page

Use ID Sync to select identifiers and a sync strategy when you send Audience or Journeys data to your destinations or destination functions. Configure how you send identifiers, which provides more control over the data you send downstream.

On this page, you’ll learn how to configure and begin using ID Sync.

Set up ID Sync

Use the following steps to set up ID Sync with Audiences or Journeys.

Set up ID Sync with Audiences

To set up ID Sync with 
Audiences
:

Set up ID Sync with Journeys

You can configure ID Sync with Journeys as you’re creating or editing your journey in the 
builder
.

Default setup

Default setup uses default Segment Destination behavior. To use the default settings, click 
Save
 and resume building your audience or journey.

You can customize additional event settings at any time.

Customized setup

With Customized setup, you can choose which identifiers you want to map downstream to your destination.

Review your settings before configuring an ID strategy

If you want to send 
ios.idfa
 as a part of your ID strategy, confirm that you’ve enabled the Send Mobile IDs setting when connecting your destination to an audience or journey.

Limits and best practices

FAQs

Trait Enrichment
 lets you map the traits data you’ve collected with Engage to use when syncing audiences and Journeys to destinations and destination functions.

ID Sync
 lets you map the identities data gathered for a profile for use when syncing audiences and Journeys to destinations and destination functions.

Audiences without ID Sync aren’t allowed to select any strategy, and by default will send all values of an identifier to the destination. Also, audiences without ID Sync don’t send any custom identifiers that are present in your space.

Yes, you can edit configuration in the Destination 
Settings
 tab at any time. However, changes will only take place in subsequent audience syncs, or in new audiences connected to the destination.

This page was last modified: 26 Feb 2025

Need support?

Questions? Problems? Need more info? Contact Segment Support for assistance!

Help improve these docs!

Was this page helpful?

Thanks for your feedback!

Can we improve this doc? 
Send us feedback!

Get started with Segment

On this page

Was this page helpful?

Thanks for your feedback!

Can we improve
 this doc? 
Send us feedback!

Product

For Developers

Company

Support

© 2025 Segment.io, Inc.

=== Content from https://segment.com/docs/privacy/faq/ ===

Privacy Frequently Asked Questions
        

On this page

Privacy Portal questions

Why aren’t fields from my Cloud Object Sources (like Salesforce and Zendesk) showing up in the Privacy Portal Inbox and Inventory?

The Privacy Portal doesn’t doesn’t support fields from Cloud Object Sources like Salesforce or Zendesk.

Why does Segment suggest classifying my fields as Yellow or Red?

Segment provides suggested classifications based on 
default PII matchers
. These suggestions include exact and fuzzy matches for potential PII. You can update these classifications by following the instructions to 
change a recommended classification
.

Who can access the Privacy Portal?

Only Workspace Owners can access the portal.

Which Segment plan types include access to the Privacy Portal?

All Segment plans include access to the Privacy Portal. Data privacy is a fundamental Segment feature, not an add-on.

If I block data at the source level, can I reverse it or recover the data using Segment’s Data Replay feature?

When you block data at the source level using Privacy Controls, the data never enters Segment. As a result, Segment can’t replay the data. Segment recommends exercising caution when blocking data at the source level.

The Privacy Portal classified my property as Yellow, but my destinations require it to function. What should I do?

Segment classifications are recommendations. If a destination requires a field classified as Yellow, you can override the recommended classification to ensure the field gets sent downstream.

User deletion and suppression questions

How can I find a specific 
userId
?

To locate a specific 
userId
, query your Segment 
data warehouse
 for the 
users
 table. Use other known details about the user, like their email address, to identify the correct row and retrieve the 
userId
.

How many deletion requests can I send?

You can send batches of up to 5,000 
userIds
, or 4 MB, per payload. Segment processes these batches asynchronously. 
Contact Segment
 if you need to process more than 110,000 users within a 30-day period.

Which destinations can I send deletion requests to?

In addition to your Raw Data destinations (Amazon S3 and data warehouses), Segment can forward requests to the following streaming destinations:

Segment forwards deletion requests but cannot guarantee that data is deleted from downstream destinations. You must contact these destinations to confirm that they executed the request.

Which destinations require additional configuration to process deletion requests?

To process deletion requests in Amplitude, add your Amplitude secret key to the destination settings under “Secret Key.” You can find this key in your Amplitude project’s 
General Settings
.

To send deletion requests to Google Analytics, authenticate your account with Segment using OAuth. Go to the 
User Deletion
 settings in your Segment Google Analytics destination and use your email and password to complete authentication.

What regulation types does Segment support?

Segment supports the following regulation types:

Using 
SUPPRESS_WITH_DELETE
 or 
DELETE_ONLY
 regulation types might lead to additional charges levied by your destination providers.

This page was last modified: 14 Feb 2025

Need support?

Questions? Problems? Need more info? Contact Segment Support for assistance!

Help improve these docs!

Was this page helpful?

Thanks for your feedback!

Can we improve this doc? 
Send us feedback!

Get started with Segment

On this page

Was this page helpful?

Thanks for your feedback!

Can we improve
 this doc? 
Send us feedback!

Product

For Developers

Company

Support

© 2025 Segment.io, Inc.

=== Content from https://segment.com/docs/unify/unify-gdpr/ ===

Unify and GDPR
        

Unify requires a Business tier account and is included with Engage.


See the 
available plans
, or 
contact Support
.

On this page

All 
Segment GDPR features
 apply to Unify.

Segment never shares or sells user data. Unify inherits Segment’s holistic approach to security and privacy, using 256-bit AES standard encryption to safeguard data stores both at rest and in transit.

User Rights

End-user privacy and the GDPR principles informed the design of Unify, a product powered by first-party data. Unify integrates Segment’s existing end-user privacy features with several user rights:

Below, learn how each of these rights protects the integrity of users and their data.

Right to Erasure

Using Segment’s platform, you can 
manage user deletion
 across all Segment products and supported Destinations. User deletion requests remove user data from all internal Segment archives and environments, including Engage audiences, within 30 days.

Right to Object

With 
one-click suppression
, you can block data collection for specific users. Segment discontinues profile building around suppressed users and prevents them from joining future audiences.

Right to Rectification

When Segment receives new information, the platform updates user profiles and traits in both Segment and its downstream tools. Use the 
Profile API
 to confirm that an update has been processed.

Rights to Access and Portability

Identity Resolution
 connects information you’ve gathered about a customer into a single profile. Using the Profile API, you can provide end users with this data. You can also enable raw data integrations and warehouses to share a user’s data in a structured format.

Next Steps

Visit the Segment site to learn 
how Segment products simplify GDPR compliance
, and reference Segment’s 
complying with the GDPR
 documentation to incorporate 
GDPR best practices
 into your workflow.

This page was last modified: 28 Mar 2023

Need support?

Questions? Problems? Need more info? Contact Segment Support for assistance!

Help improve these docs!

Was this page helpful?

Thanks for your feedback!

Can we improve this doc? 
Send us feedback!

Get started with Segment

On this page

Was this page helpful?

Thanks for your feedback!

Can we improve
 this doc? 
Send us feedback!

Product

For Developers

Company

Support

© 2025 Segment.io, Inc.

=== Content from https://segment.com/docs/segment-app/iam/concepts/ ===

Access Management Concepts
        

Advanced Access Management is available for all Business plans.


See the 
available plans
, or 
contact Support
.

On this page

Team Members

A Segment 
Team Member
 is an individual with access to a workspace. A Segment user can be associated with one or more workspaces, either as an 
owner
 or 
member
 of each.
Check out the 
roles documentation
 for a complete list of roles.

The user session for a Segment Team Member is 7 days. Team Members in a 
HIPAA eligible workspace
 have a 15 minute user session across all workspaces.

If you are a Team Member in a HIPAA eligible workspace and want to access a non-HIPAA eligible workspace with a 7 day user session, you can create an alias (for example 
name+workspace@gmail.com
).

User Groups

A 
User Group
 is a set of Team Members with a set of shared policies. A Segment Team Member can be a member of one or many Groups. All roles in the Segment App are additive, which means that group membership can be assigned in addition to individual roles for a single team member. For example, a single user could inherit roles from a Group definition AND have access to additional resources through individually assigned roles.

Tokens

You can generate tokens to programmatically access Segment resources using the 
Segment Public API
.



Resources

Resources are the building blocks of Segment, and represent the different parts of your Segment deployment to which you can grant access. These include:

Labels

Workspace owners can use Labels to grant users access to groups of resources. When you add a Label to a Source or Spaces, any users who are granted access to that Label gain access to those resources.

To create or configure labels, go to the 
Labels
 tab in your workspace settings. Only workspace Owners can manage labels for the entire workspace.

Quick Links:

This page was last modified: 23 Oct 2023

Need support?

Questions? Problems? Need more info? Contact Segment Support for assistance!

Help improve these docs!

Was this page helpful?

Thanks for your feedback!

Can we improve this doc? 
Send us feedback!

Get started with Segment

On this page

Was this page helpful?

Thanks for your feedback!

Can we improve
 this doc? 
Send us feedback!

Product

For Developers

Company

Support

© 2025 Segment.io, Inc.

=== Content from https://segment.com/docs/guides/usage-and-billing/account-management/ ===

Account Management
        

On this page

What is the difference between an account and a workspace?

An account is associated to a single user and is attached to the email address you sign up with. A workspace houses all of your sources, and can have one or several user accounts as owners and/or read-only members.

What if I change my domain name?

You don’t need to do anything if you change your domain name. If the new domain name will serve as the same Segment source, make sure you use the same Segment write key that you used with the old domain.

You may claim ownership of your domain for the purposes of single sign-on login association, but it currently has no bearing on data collection.

I’m on a legacy API plan. Why can’t I add the integration I want?

Some of Segment’s previous plans, including the legacy API plan, limited integration usage. If you want to add an integration that’s not available on your current plan, move to a new Team plan. Team plans include all integrations, along with other additional features.

Will deleting my account cancel my subscription?

No. Deleting your account only stops you from accessing workspaces through your login. The workspace is where the subscription is managed, and it will not be deleted. Data will still flow into Segment and your Destinations, and you will still be charged if you delete your account but don’t delete your workspace.

How do I delete my account?

To delete your account, go to the 
User Settings
 menu, and click 
Delete Account
 at the bottom of the page.

Once the account is deleted you will not have access to workspaces associated with your account that are attached to the email address you signed up with.

How do I delete my workspace entirely?

To delete your workspace, go to your 
Workspace Settings
, click the 
General Settings
 tab, then click 
Delete Workspace
.  Segment will irrevocably delete your workspace 5 days after you initiate your deletion request.

If you want to revoke the workspace deletion request during the 5 days after you initiated your request, open the 
Workspace Settings
 page, select the 
General Settings
 tab and click 
Revoke Workspace Deletion
.

You should also change your write keys for each source and remove all Segment snippets from your codebase.

How do I change my account email address?

To update the email address associated with your Segment account:

You’ll need to authenticate and verify your new email address for the change to take effect. Note that 
workspace owners
 can’t make this change for other users.

What happens if I change my workspace name or slug?

Changing your workspace name or slug won’t impact any sources or destinations you’ve already configured. If you’re using 
Segment’s Public API
, you’ll need to change the slug in your request URLs.

We were unable to save your changes, please try again

If you see this error message when trying to change a workspace slug, it often means the slug is already taken.

Can I recover a source or workspace after I delete it?

No. Deleted sources and workspaces cannot be recovered.

Can I move a source from one workspace to another?

Though workspaces can’t be merged, you can move an existing source to a single workspace to the same effect. For example, you might move existing sources to one workspace so that you can unify all of your data across teams and gain a broader view of your customer data tracking.

To move a source between workspaces, navigate to the source’s 
Settings
 tab, then click 
Transfer to Workspace
. Choose the workspace you’re moving the source to, then click 
Transfer Source
.

When you transfer a source from one workspace to another, Segment migrates all connected non-storage destinations.

The person who transfers the source must be a 
workspace owner
 for both the origin and recipient workspaces, otherwise the recipient workspace won’t appear in the dropdown list.



Tracking Plans do not transfer

Segment recommends that you disconnect Tracking Plans from Sources before you initiate a workspace transfer. Once the transfer is complete, add and reconnect your Tracking Plans in the new workspace.

Sources can't be transferred to EU workspaces

Though transferring sources to the EU workspace is not blocked in the UI, the transfer will not work as expected. This feature is not supported for cross region migration. Segment recommends that you re-create the source in the new workspace.

This page was last modified: 09 Jan 2025

Need support?

Questions? Problems? Need more info? Contact Segment Support for assistance!

Help improve these docs!

Was this page helpful?

Thanks for your feedback!

Can we improve this doc? 
Send us feedback!

Get started with Segment

On this page

Was this page helpful?

Thanks for your feedback!

Can we improve
 this doc? 
Send us feedback!

Product

For Developers

Company

Support

© 2025 Segment.io, Inc.

=== Content from https://segment.com/docs/connections/data-export-options/ ===

Data Export Options
        

There are a few ways to export your Segment data. Segment 
Business customers
 have the most data flexibility, but our self-service customers also have options.

Business plan customers

Customers on our 
business plan
 can take advantage of Replay when they change vendors or add a vendor to their marketing and analytics stack.

When you want to trial or start using a new vendor, Segment can 
replay
 your timestamped, historical data so it’s like you’ve been using that app all along.

Replay works for all server-side destinations that have or accept timestamps, including our Amazon S3 destination, so you can get all your data history since the first event you sent to Segment.

Free, Team, and Business plan customers

If you are on any of our plans, there are multiple options available to you to gain access to your raw data.

All customers can connect a 
data warehouse
 to Segment – Free and Team customers can connect one, while Business customers can connect as many as they need. We translate and load your raw data logs into your warehouse for more powerful analysis in SQL.

We store all your API calls as line-separated JSON objects in Amazon S3. If you 
enable Amazon S3
 in your destinations catalog, we will copy the same data to your own S3 bucket. The data copied will only include data sent to Segment after you turn on the destination. Read our 
Amazon S3 docs
 to learn more about how we structure that data.

You can use our 
webhooks destination
 to fire off requests in realtime to an endpoint that you would need to spin up and manage on your side. This is basically re-creating how our business system works but takes a bit of work on your side. If your event volume is high it can be difficult to keep a server up to receive those messages in realtime.

Another one of our destinations is 
Iron.io
. They function similar to webhooks, but they will manage the message queue and allow you to run scripts on your data before routing it to another end point. Again this is similar to what Segment does for our business customers, but will require a decent amount of work from your team, however it will be much more reliable if your event volume gets high.

This option is the most restrictive but might be the easiest if you need only basic data to be exported. A few examples would be to use the reporting APIs 
Clicky
 or 
Google Analytics
 provide (after turning those tools on in Segment and sending them data). Those APIs aren’t super flexible and you won’t see all the data from Segment, but for basic metrics they should work. One tool that’s a bit more flexible when it comes to a reporting API is 
Keen.io
, which is also available on the Segment platform.

This page was last modified: 01 Dec 2022

Need support?

Questions? Problems? Need more info? Contact Segment Support for assistance!

Help improve these docs!

Was this page helpful?

Thanks for your feedback!

Can we improve this doc? 
Send us feedback!

Get started with Segment

Was this page helpful?

Thanks for your feedback!

Can we improve
 this doc? 
Send us feedback!

Product

For Developers

Company

Support

© 2025 Segment.io, Inc.

=== Content from https://segment.com/docs/unify/data-graph/setup-guides/bigquery-setup// ===

BigQuery Data Graph Setup
        

Unify requires a Business tier account and is included with Engage.


See the 
available plans
, or 
contact Support
.

On this page

BigQuery for Data Graph is in beta and Segment is actively working on this feature. Some functionality may change before it becomes generally available. This feature is governed by Segment’s 
First Access and Beta Preview Terms
.

Set up your BigQuery data warehouse to Segment for the 
Data Graph
.

Step 1: Roles and permissions

You need to be an account admin to set up the Segment BigQuery connector as well as write permissions for the 
__segment_reverse_etl
 dataset.

To set the roles and permissions:

Step 2: Create a dataset for Segment to store checkpoint tables

Create a new dataset as Segment requires write access to the dataset for internal bookkeeping and to store checkpoint tables for the queries that are executed.

Segment recommends you to create a new dataset for the Data Graph. If you choose to use an existing dataset that has also been used for 
Segment Reverse ETL
, you must follow the 
additional instructions
 to update user access for the Segment Reverse ETL catalog.

To create your dataset, navigate to the BigQuery SQL editor and create a dataset that will be used by Segment.

CREATE SCHEMA IF NOT EXISTS `__segment_reverse_etl`;
GRANT `roles/bigquery.dataEditor` ON SCHEMA `__segment_reverse_etl` TO "serviceAccount:<YOUR SERVICE ACCOUNT EMAIL>";


Step 3: Grant read-only access for the Data Graph

Grant the 
BigQuery Data Viewer
 role to the service account at the project level. Make sure to grant read-only access to the Profiles Sync project in case you have a separate project.

To grant read-only access for the Data Graph:

(Optional)
 Step 4: Restrict read-only access

If you want to restrict access to specific datasets, grant the BigQuery Data Viewer role on datasets to the service account. Make sure to grant read-only access to the Profiles Sync dataset.

To restrict read-only access:

You can also run the following command:

GRANT `roles/bigquery.dataViewer` ON SCHEMA `YOUR_DATASET_NAME` TO "serviceAccount:<YOUR SERVICE ACCOUNT EMAIL>";


Step 5: Validate permissions

Step 6: Connect your warehouse to Segment

Update user access for Segment Reverse ETL dataset

If you ran Segment Reverse ETL in the project you are configuring as the Segment connection project, a Segment-managed dataset is already created and you need to provide the new Segment user access to the existing dataset.

If you run into an error on the Segment app indicating that the user doesn’t have sufficient privileges on an existing 
__segment_reverse_etl
 dataset, grant the 
BigQuery Data Editor
 role on the 
__segment_reverse_etl
 dataset to the service account . Note that the 
__segment_reverse_etl
 dataset is hidden in the console. Run the following SQL command:

GRANT `roles/bigquery.dataEditor` ON SCHEMA `__segment_reverse_etl` TO "serviceAccount:<YOUR SERVICE ACCOUNT EMAIL>";


This page was last modified: 05 Dec 2024

Need support?

Questions? Problems? Need more info? Contact Segment Support for assistance!

Help improve these docs!

Was this page helpful?

Thanks for your feedback!

Can we improve this doc? 
Send us feedback!

Get started with Segment

On this page

Was this page helpful?

Thanks for your feedback!

Can we improve
 this doc? 
Send us feedback!

Product

For Developers

Company

Support

© 2025 Segment.io, Inc.

=== Content from https://segment.com/docs/engage/content/whatsapp/ ===

WhatsApp Template
        

Engage Premier requires a Business tier account and includes Engage Foundations and Unify.


See the 
available plans
, or 
contact Support
.

On this page

Engage Premier entered an End of Sale (EOS) period effective  June 10, 2024. Existing Segment customers will continue to have access and support to Engage Premier until an end-of-life (EOL) date is announced. We recommend exploring the following pages in preparation of a migration or future MCM needs:

Twilio Marketing Campaigns

Preferred ISV Partners:

Airship Blog
 


Bloomreach Blog
 


Braze Blog
 


Insider Blog
 


Klaviyo Blog
 


Twilio Engage Foundations Documentation
 

With Twilio Engage, you can build personalized WhatsApp templates to store and use throughout marketing campaigns.

This page explains how to create, build, and submit WhatsApp templates for approval.

WhatsApp Template Approval

WhatsApp templates must be approved by Meta before you can use them in campaigns.

WhatsApp template types

You can choose between three WhatsApp template types:

Build a WhatsApp message template

Before you begin

If you’re new to Engage Premier, you’ll need to sign up for the 
Twilio Content Editor beta
 before you can use WhatsApp templates.

Follow these steps to build a WhatsApp template:

Submit a saved template for approval

If you saved your template without submitting it for approval, it won’t be available for use in campaigns until you submit it for approval.

Follow these steps to submit saved templates for approval:

Personalize with merge tags

You can personalize your WhatsApp templates with merge tags based on profile traits.

To include merge tags in your template, click 
+ Add merge tag
 in the template builder and select the profile trait(s) you want to include in your message.

Segment displays the merge tag in the body as a numerical value surrounded by curly braces, like 
{{1}}
. When a susbcriber triggers your WhatsApp campaign, Segment will replace the merge tag with the specific value associated with that subscriber’s profile.

If a merge tag doesn’t apply to a subscriber, Engage will use the content you enter into the 
Default content
 field.



To learn more about profile traits, visit Segment’s 
Computed Traits
 and 
SQL Traits
 documentation.

Template approvals

Meta must first review and approve your WhatsApp template before you can use it in a campaign. Meta approves most templates in under an hour, but some approvals can take up to 48 hours. Keep this time frame in mind if you plan to send time-sensitive campaigns.

For more on the template approval process, view 
recommendations and best practices for creating WhatsApp Message Templates
.

Next steps

Once your template has been approved, you can 
create a Journey to send a WhatsApp campaign
.

This page was last modified: 15 Jul 2024

Need support?

Questions? Problems? Need more info? Contact Segment Support for assistance!

Help improve these docs!

Was this page helpful?

Thanks for your feedback!

Can we improve this doc? 
Send us feedback!

Get started with Segment

On this page

Was this page helpful?

Thanks for your feedback!

Can we improve
 this doc? 
Send us feedback!

Product

For Developers

Company

Support

© 2025 Segment.io, Inc.

=== Content from https://segment.com/docs/connections/storage/data-lakes/sync-reports/ ===

Data Lakes Sync Reports and Errors
        

Data Lakes is available for the listed account plans only.


See the 
available plans
, or 
contact Support
.

On this page

Segment Data Lakes generates reports with operational metrics about each sync to your data lake so you can monitor sync performance. These sync reports are stored in your S3 bucket and Glue Data Catalog. This means you have access to the raw data, so you can query it to answer questions and set up alerting and monitoring tools.

Sync Report schema

Your sync_report table stores all of your sync data. You can query it to answer common questions about data synced to your data lake.
The table has the following columns in its schema:

The Glue Database named 
__segment_datalake
 stores the schema of the 
sync_reports
 table. The 
__segment_datalake
 database has the following format:

The 
sync_reports
 table is available in S3 and Glue only once a sync completes. Sync reports are not available for syncs in progress.

Data location

Data Lakes sync reports are stored in Glue and in S3.

Segment automatically creates a Glue Database and table when you set up Data Lakes to store all sync report tables. The Glue Database is named 
__segment_datalake
, and the table is named 
sync_reports
.

The S3 structure is:

s3://my-bucket/segment-data/reports/day=YYYY-MM-DD/source=$SOURCE_ID/run_id=$RUN_ID/report.json

Data format

The data in the sync reports is stored in JSON format to ensure that it is human-readable and can be processed by other systems.

Each table involved in the sync is a separate JSON object that contains the sync metrics for the data loaded to that table.

The example below shows the raw JSON object for a 
successful
 sync report.

  
{

    
"type"
:
 
"source"
,

    
"workspace_id"
:
 
"P3IMS7SBDH"
,

    
"source_id"
:
 
"9IP56Shn6"
,

    
"run_id"
:
 
"1597581273464733073"
,

    
"start_time"
:
 
"2020-08-19 22:15:59.044084423"
,

    
"finish_time"
:
 
"2020-08-19 22:18:12.891"
,

    
"duration_mins"
:
 
2
,

    
"status"
:
 
"finished"
,

    
"table_name"
:
 
""
,

    
"database"
:
 
"ios_prod"
,

    
"row_count"
:
 
81020
,

    
"emr_cluster_id"
:
 
"j-3SXSUSDNPIS"
,

    
"s3_bucket"
:
 
"my-segment-datalakes-bucket"

  
}

  
{

    
"type"
:
 
"event"
,

    
"workspace_id"
:
 
"P3IMS7SBDH"
,

    
"source_id"
:
 
"9IP56Shn6"
,

    
"run_id"
:
 
"1597581273464733073"
,

    
"start_time"
:
 
"2020-08-19 22:15:59.044084423"
,

    
"finish_time"
:
 
"2020-08-19 22:18:12.891"
,

    
"duration_mins"
:
 
2
,

    
"status"
:
 
"finished"
,

    
"table_name"
:
 
"track_order_completed"
,

    
"database"
:
 
"ios_prod"
,

    
"partitions"
:
 
[

      
{

        
"day"
:
 
"2020-08-16"
,

        
"hr"
:
 
"10"

      
},

      
{

        
"day"
:
 
"2020-08-16"
,

        
"hr"
:
 
"11"

      
}

    
],

   
"new_columns"
:
 
[

      
{

        
"name"
:
 
"properties_billing_address"
,

        
"type"
:
 
"string"

      
}

    
],

    
"row_count"
:
 
20020
,

    
"emr_cluster_id"
:
 
"j-3SXSUSDNPIS"
,

    
"s3_bucket"
:
 
"my-segment-datalakes-bucket"

  
}

  
{

    
"type"
:
 
"event"
,

    
"workspace_id"
:
 
"P3IMS7SBDH"
,

    
"source_id"
:
 
"9IP56Shn6"
,

    
"run_id"
:
 
"1597581273464733073"
,

    
"start_time"
:
 
"2020-08-19 22:15:59.044084423"
,

    
"finish_time"
:
 
"2020-08-19 22:18:12.891"
,

    
"duration_mins"
:
 
2
,

    
"status"
:
 
"finished"
,

    
"table_name"
:
 
"track_product_added"
,

    
"database"
:
 
"ios_prod"
,

    
"partitions"
:
 
[

      
{

        
"day"
:
 
"2020-08-16"
,

        
"hr"
:
 
"10"

      
}

    
],

    
"row_count"
:
 
20260
,

    
"emr_cluster_id"
:
 
"j-3SXSUSDNPIS"
,

    
"s3_bucket"
:
 
"my-segment-datalakes-bucket"


}



The example below shows the raw JSON object for a 
failed
 sync report.

{

    
"type"
:
 
"source"
,

    
"workspace_id"
:
 
"P3IMS7SBDH"
,

    
"source_id"
:
 
"9IP56Shn6"
,

    
"run_id"
:
 
"1597867438900010296"
,

    
"start_time"
:
 
"2020-08-19 20:04:58.368616813"
,

    
"finish_time"
:
 
"2020-08-19 20:49:48.308318686"
,

    
"duration_mins"
:
 
44
,

    
"status"
:
 
"failed"
,

    
"error"
:
 
"Data Lakes Destination has invalid configuration for 
\"
AWS Role ARN
\"
: field is required."
,

    
"error_code"
:
 
"Segment.Internal"
,

    
"table_name"
:
 
""
,

    
"database"
:
 
"ios_prod"
,

    
"emr_cluster_id"
:
 
"j-3SXSUSDNPIS"
,

    
"s3_bucket"
:
 
"segment-datalakes-demo-stage"


}



Querying the Sync Reports table

You can use SQL to query your Sync Reports table to explore and analyze operational sync metrics.
A few helpful and commonly used queries are included below.

SELECT
 
day
,
sum
(
row_count
)


FROM
 
"__segment_datalake"
.
"sync_reports"


WHERE
 
source_id
=
'9IP56Shn6'
 
and
 
table_name
=
'checkout_started'


GROUP
 
BY
 
day


ORDER
 
BY
 
day



SELECT
 
day
,
 
table_name
,
sum
(
row_count
)


FROM
 
"__segment_datalake"
.
"sync_reports"


WHERE
 
source_id
=
'9IP56Shn6'
 
AND
 
type
=
'event'


GROUP
 
BY
 
day
,
 
table_name


ORDER
 
BY
 
day



SELECT
 
max
(
finish_time
)


FROM
 
"__segment_datalake"
.
"sync_reports"


WHERE
 
source_id
=
'9IP56Shn6'
 
AND
 
status
=
'finished'
 
AND
 
date
(
day
)
 
=
 
CURRENT_DATE


LIMIT
 
1



SELECT
 
run_id
,
 
status
,
 
error
,
 
error_code


FROM
 
"__segment_datalake"
.
"sync_reports"


WHERE
 
source_id
=
'9IP56Shn6'
 
AND
 
status
=
'failed'
 
AND
 
date
(
day
)
 
>=
 
(
CURRENT_DATE
 
-
 
interval
 
'2'
 
day
)



Sync errors

The following error types can cause your data lake syncs to fail:

Insufficient permissions

If Data Lakes does not have the correct access permissions for S3, Glue, and EMR, your syncs will fail.

If permissions are the problem, you might see one of the following permissions-related error messages:

Check the set up guide
 to ensure that you set up the required permission configuration for S3, Glue and EMR.

Invalid settings

One or more settings might be incorrectly configured in the Segment app, preventing your Data Lakes syncs from succeeding.

If you have invalid settings, you might see one of the error messages below:

The most common error occurs when you do not list all Source IDs in the External ID section of the IAM role. You can find your Source IDs in the Segment workspace, and you must add each one to the list of 
External IDs
 in the IAM policy. You can either update the IAM policy from the AWS Console, or re-run the 
Data Lakes set up Terraform job
.

Internal error

Internal errors occur in Segment’s internal systems, and should resolve on their own. If sync failures persist, 
contact the Segment Support team
.

FAQ

How are Data Lakes sync reports different from the sync data for Segment Warehouses?

Both Warehouses and Data Lakes provide similar information about syncs, including the start and finish time, rows synced, and errors.

However, Warehouse sync information is only available in the Segment app: on the Sync History page and Warehouse Health pages. With Data Lakes sync reports, the raw sync information is sent directly to your data lake. This means you can query the raw data and answer your own questions about syncs, and use the data to power alerting and monitoring tools.

What happens if a sync is partly successful?

Sync reports are currently generated only when a sync completes, or when it fails. Partial failure reporting is not currently supported.

This page was last modified: 03 Aug 2023

Need support?

Questions? Problems? Need more info? Contact Segment Support for assistance!

Help improve these docs!

Was this page helpful?

Thanks for your feedback!

Can we improve this doc? 
Send us feedback!

Get started with Segment

On this page

Was this page helpful?

Thanks for your feedback!

Can we improve
 this doc? 
Send us feedback!

Product

For Developers

Company

Support

© 2025 Segment.io, Inc.

=== Content from https://segment.com/docs/connections/storage/warehouses/choose-warehouse/ ===

Choosing a Warehouse
        

On this page

Comparing Redshift and Postgres

In most cases, you will get a much better price-to-performance ratio with Redshift for typical analyses.

Redshift lacks some 
features
, 
datatypes
, and 
functions
 supported by Postgres and also implements 
some features
 differently. If you need any of the features or functions missing in Redshift and BigQuery, choose Postgres. If not (or you’re not sure), Segment recommends choosing Redshift.

If you’d like more information, Amazon wrote 
about this in their documentation
.

Comparing Redshift and BigQuery

Both Redshift and BigQuery are attractive cloud-hosted, affordable, and performant analytical databases. The differences between the two are around their architecture and pricing.

Architecture

When you provision a Redshift cluster, you’re renting a server from Amazon Web Services. Your cluster consists of 
nodes
, each with dedicated memory, CPU, and disk storage. These nodes handle data storage, query execution, and - if your cluster contains multiple nodes - a leader node will handle coordination across the cluster.

Redshift performance and storage capacity is a function of cluster size and cluster type. As your storage or performance requirements change, you can scale up or down your cluster as needed.

With BigQuery, you’re not constrained by the storage capacity or compute resources of a given cluster. Instead, you can load large amounts of data into BigQuery without running out of memory, and execute complex queries without maxing out CPU.

This is possible because BigQuery takes advantage of distributed storage and networking to separate data storage from compute power. Google’s
Colossus distributed file system
 distributes data across many servers in the Google cloud. When you execute a query, the 
Dremel query engine
 splits the query into smaller sub-tasks, distributes the sub-tasks to computers across Google data centers, and then re-assembles them into your results.

Pricing

The difference in architecture translates into differences in pricing.

Redshift prices
 are based on an hourly rate determined by the number and types of nodes in your cluster. They offer dense storage - optimized for storage - and dense compute nodes - optimized for query performance.

BigQuery has two 
pricing options
: variable and fixed pricing. With the variable, pay-as-you-go plan, you pay for the data you load into BigQuery, and then pay for the amount of data you query. BigQuery allows you to set up 
Cost Controls and Alerts
 to help control and monitor costs.

Fixed-price plans are more for high-volume customers and allow you to rent a fixed amount of compute power.

Resource Management

Redshift does require you to create a cluster, choose sort and distribution keys, and resize your cluster as storage and performance needs change over time.

BigQuery is “fully-managed”, which means that you’ll never have to resize or adjust distribution or sort keys. BigQuery handles all of that.

This page was last modified: 20 Mar 2024

Need support?

Questions? Problems? Need more info? Contact Segment Support for assistance!

Help improve these docs!

Was this page helpful?

Thanks for your feedback!

Can we improve this doc? 
Send us feedback!

Get started with Segment

On this page

Was this page helpful?

Thanks for your feedback!

Can we improve
 this doc? 
Send us feedback!

Product

For Developers

Company

Support

© 2025 Segment.io, Inc.

=== Content from https://segment.com/docs/connections/storage/ ===

Data Storage overview
        

Off-the-shelf analytics tools (like Google Analytics and Mixpanel) offer quick and easy insights about common business questions, and often meet the needs of marketing teams and product managers. However, data analysts and data scientists need access to an organization’s raw data to derive deeper and more customized insights to support their organization.

Only users with Business or Team plans can add Warehouse destinations.

Segment offers several 
Data Storage Destinations
 to help you store your raw Segment data, including:

Although the sharing economy is eroding the idea of “ownership,” when it comes to analytics data, we strongly believe that you should own it.

This page was last modified: 30 Mar 2023

Need support?

Questions? Problems? Need more info? Contact Segment Support for assistance!

Help improve these docs!

Was this page helpful?

Thanks for your feedback!

Can we improve this doc? 
Send us feedback!

Get started with Segment

Was this page helpful?

Thanks for your feedback!

Can we improve
 this doc? 
Send us feedback!

Product

For Developers

Company

Support

© 2025 Segment.io, Inc.

=== Content from https://segment.com/docs/privacy/consent-management/consent-in-unify/ ===

Consent in Unify
        

Consent Management is available to customers on Business tier plans.


See the 
available plans
, or 
contact Support
.

On this page

Consent in Unify and Twilio Engage is currently unavailable.

However, Segment’s OneTrust consent wrappers automatically generate the Segment Consent Preference Updated Track event, which will be required for future integrations with Unify and Twilio Engage.

Segment uses Profiles in 
Unify
 as the source of truth of an end user’s consent preference when enforcing consent in Twilio Engage. To get consent preference on the Profile, Segment requires the use of the 
Segment Consent Preference Updated event
 and 
Identify
 events to route events to Unify. The Segment Consent Preference Updated and Identify events should include the 
consent object
.

Segment Consent Preference Updated event

Every time an end user provides or updates their consent preferences, Segment requires you to generate a 
Segment Consent Preference Updated
 event. If you are using 
Segment’s OneTrust consent wrappers
, Segment automatically generates a Segment Consent Preference Updated event. This event is required to add the end user’s consent preference on their Profile in Unify.

For example, if an end user agreed to share their information for functional and advertising purposes but not for analytics or data sharing, the Segment Consent Preference Updated 
Track call
 demonstrating their new consent preferences would have the following format:

{

  
"anonymousId"
:
 
"23adfd82-aa0f-45a7-a756-24f2a7a4c895"
,

  
"type"
:
 
"track"
,

  
"event"
:
 
"Segment Consent Preference Updated"
,

  
"userId"
:
 
"u123"
,

  
"traits"
:
 
{

     
"email"
:
 
"peter@example.com"
,

     
"phone"
:
 
"555-555-5555"
,

  
}

  
"timestamp"
:
 
"2023-01-01T00:00:00.000Z"
,

  
"context"
:
 
{

    
"consent"
:
 
{

      
"categoryPreferences"
 
:
 
{

        
"Advertising"
:
 
true
,

        
"Analytics"
:
 
false
,

        
"Functional"
:
 
true
,

        
"DataSharing"
:
 
false

      
}

    
}

  
}


}



If you use Protocols, the Segment app automatically adds the Segment Consent Preference Updated event to all your existing Tracking Plans and for every new Tracking Plan. Segment recommends you don’t edit or delete the default fields in the Segment Consent Preference Updated events, but you can add new fields as needed.

Segment Consent Preference Updated is a reserved event name

Segment has standardized a series of reserved event names that have special semantic meaning and maps these events to tools that support them.

See the 
Semantic Events
 docs for more details.

Sharing consent with Actions destinations

In addition to enforcing consent in Connections, you may want these preferences to flow to each destination so your destinations can be aware when an end-user revokes their consent. You can use the 
Destination Actions framework
 to edit the destination’s mapping and copy the consent preferences from the Segment Consent Preference Updated event to a destination-specified consent field.

If you use Destination Actions to send consent information to your destinations, the Segment Consent Preference Updated event should 
only
 include information about a user’s consent preferences because this event is sent regardless of an end-user’s consent preferences.

Sharing consent with Classic Destinations is not available

Segment only supports sharing consent with Actions Destinations.

This page was last modified: 25 Feb 2025

Need support?

Questions? Problems? Need more info? Contact Segment Support for assistance!

Help improve these docs!

Was this page helpful?

Thanks for your feedback!

Can we improve this doc? 
Send us feedback!

Get started with Segment

On this page

Was this page helpful?

Thanks for your feedback!

Can we improve
 this doc? 
Send us feedback!

Product

For Developers

Company

Support

© 2025 Segment.io, Inc.

=== Content from https://segment.com/docs/connections/storage/data-lakes/comparison/ ===

Comparing Data Lakes and Warehouses
        

Data Lakes is available for the listed account plans only.


See the 
available plans
, or 
contact Support
.

On this page

As Segment builds new data storage products, each product evolves from prior products to best support the needs of customers. Segment Data Lakes is an evolution of the Warehouses product that meets the changing needs of customers.

Data Lakes and Warehouses are not identical, but are compatible with a configurable mapping. This mapping helps you to identify and manage the differences between the two storage solutions, so you can easily understand how the data in each is related.

Data freshness

Data Lakes and Warehouses offer different sync frequencies:

Duplicates

Segment’s 
99% guarantee of no duplicates
 for data within a 24 hour look-back window applies to data in Segment Data Lakes and Warehouses.

Warehouses
 and 
Data Lakes
 also have a secondary deduplication system to further reduce the volume of duplicates to ensure clean data in your Warehouses and Data Lakes.

Object vs event data

Warehouses support both event and object data, while Data Lakes supports only event data.

See the table below for information about the 
source
 types supported by Warehouses and Data Lakes.

Schema

Data types

Warehouses and Data Lakes both infer data types for the events each receives. Since events are received by Warehouses one by one, Warehouses look at the first event received every hour to infer the data type for subsequent events. Data Lakes uses a similar approach, however because it receives data every hour, Data Lakes is able to look at a group of events to infer the data type.

This approach leads to a few scenarios where the data type for an event may be different between Warehouses and Data Lakes. Those scenarios are:

Variance in data types between Warehouses and Data Lakes don’t happen often for booleans, strings, and timestamps, however it can occur for decimals and integers.

If a bad data type is seen, such as text in place of a number or an incorrectly formatted date, Warehouses and Data Lakes attempt a best effort conversion to cast the fields to the target data type. Fields that cannot be casted may be dropped. 
Contact Segment Support
 if you want to correct data types in the schema and perform a 
replay
 to ensure no data is lost.

Tables

Tables between Warehouses and Data Lakes will be the same, except for in these two cases:

Columns

Similar to tables, columns between Warehouses and Data Lakes will be the same, except for in a few specific scenarios:

This page was last modified: 03 Aug 2023

Need support?

Questions? Problems? Need more info? Contact Segment Support for assistance!

Help improve these docs!

Was this page helpful?

Thanks for your feedback!

Can we improve this doc? 
Send us feedback!

Get started with Segment

On this page

Was this page helpful?

Thanks for your feedback!

Can we improve
 this doc? 
Send us feedback!

Product

For Developers

Company

Support

© 2025 Segment.io, Inc.

=== Content from https://segment.com/docs/getting-started/implementation-guide/ ===

Getting Started Guide
        

On this page

Welcome to Segment! This doc mirrors Segment’s in-product guide, and walks you through each of the tasks to level up your workspace strength and become familiar with Segment.

The guide is broken into three categories of tasks:

Basics

The tasks included in Basics help you send and debug your very first data from a 
Source
 (a library that sends data to Segment), and into a 
Destination
 (tools you use to analyze or act on your data).

The Basic tasks include:

Invite Teammates

Segment allows you to invite team members to your workspace. To decide who on your team should be added to Segment, think about who might be responsible for implementing, owning, or using your data in downstream tools.

For example, as a developer, you might invite:

To invite team members to your workspace:

Add a Source

A Source is a website, server library, mobile SDK, or cloud application which can send data into Segment. It’s where your data originates. Add a Source to collect data to understand who your customers are and how they’re using your product. Create a source for each website or app you want to track.

To add a Source:

Learn More

Add page or screen tracking

Once you’ve added your Segment Source, you’re ready to send data into Segment. The simplest data to send into Segment is a Page call (for website Sources) or Screen call (for mobile Sources). Page and screen calls send automatically once you install the Segment snippet or SDK on your website or mobile app. Page and screen calls allow you to record whenever a user sees a page of your website or screen of your app, along with any optional properties about the page or screen.

Learn how to 
install the Segment snippet or SDK on your website or mobile app
 to start sending data.

Learn More

Add a Destination

Destinations are the business tools or apps that Segment forwards your data to. Adding Destinations allow you to act on your data and learn more about your customers in real time.

To add a Destination:

Learn More

Testing and Debugging

The Source Debugger is a real-time tool that helps you validate that API calls made from your website, mobile app, or servers arrive at your source. You can use the Source Debugger to make sure that your source functions properly and your events actively send.

The Debugger shows a live stream of events that flow through your Segment Source, so that you can check that your events send in the correct format. When you click on a specific event, you’ll be able to see these two views of an event:

To access your Source Debugger:

Learn More

Instrumentation

The tasks in this phase help you create a data strategy and send additional types of data (identify and track calls) to get a clearer picture of who your users are and what actions they’re taking.

The Instrumentation tasks include:

Send an Identify call

The Identify call allows you to tie a user to their actions and record traits about them. It includes a unique User ID and any optional traits you know about the user, like their email, name, and address. Sending an Identify call is your first step towards understanding who your users are.

An example of the types of details you might want to learn and track about your users in an Identify call are:

Learn More

Send a Track call

The Segment Track call allows you to record any actions your users perform, along with any properties that describe the action. Sending a track call is your first step towards understanding what your users are doing.

Each action that a user takes is known as an event. Each event has a name and properties. For example, the User Registered event might have properties like 
plan
 or 
accountType
.

To save time on instrumentation, be sure to check if 
one of Segment’s Business Specs
 meets your needs.

Learn More

Choose what to track

Segment recommends you to create and maintain a Tracking Plan to have data clarity and team alignment about what customer data you need to collect and why. It’s best to think about the measurable business outcomes you’re trying to track or improve, and then drill down to track the events needed for each business outcome.

For example, if you’re looking to reduce cart abandonment, you may want to engage cart abandoners by sending emails and in-app messaging to them using Customer.io and Intercom. You also might want to track events like Product Added or Cart Viewed along this customer journey.

Segment maintains a number of industry or product-specific specs to help you get started:

Learn More

Event anatomy and naming standards

When it comes to data collection, the best way to set your company up for success is to establish consistent naming conventions. This makes your code easier to read, and it helps everyone at your company understand what your events mean.

Segment recommends the best practice of using an “Object Action” (Noun Verb) naming convention for all Track events (for example, Menu Clicked) and using noun_noun snake case for property names (for example, property_name). You can view all the event names you’re currently tracking in the Schema view to ensure you’re using consistent conventions and casing.

To view your event names in the Source Schema:

Learn More

Add a data warehouse

A data warehouse is a central location where you can store your raw customer data from multiple sources. A data warehouse gives you flexibility to query your data, which allows you to answer analytical questions that may not be possible with a standard analytics tool.

A data warehouse also allows you to collect and compile data from third party tools as 
Cloud Sources
 in Segment, to help you gain a 360 view of your customer touchpoints.

Learn More

Add more destinations

Adding more destinations allows you to connect all your business tools to run through Segment. This gives you the confidence that they are all acting on the same data. Most users connect a variety of marketing, advertising, product, and analytics tools.

With all your tools acting on the same set of customer data, you can personalize your customer engagement and deliver a consistent message across multiple channels

To add more destinations:

Learn More

Optimization

The tasks in this phase help you to optimize your Segment implementation and take it to the next level.

The optimization tasks include:

Add more sources

Adding any additional data sources that you might have, like a mobile app, marketing website, server, or cloud tool will give you a more complete view of your customer.
Each touchpoint you have with your customers is a potential area to gain a better understanding of them.

To add more sources:

Learn More

Add a cloud source

Cloud sources allow you to pull in customer data from third-party tools (like Twilio or Stripe) into a data warehouse for complex querying. Consolidating your customer data enables you to eliminate data silos to get a single view of your customer.

Before adding a cloud source, you need to make sure you:

Once you have the necessary credentials, to add a cloud source:

Learn More

Explore Protocols

Protocols automate and scale the 
data quality best practices
 developed over years of helping users implement Segment. Investing in data quality improves trust in your data, reduces time spent by your engineering and business teams navigating and validating data, and allows your business to grow faster.

There are steps to take when you use Protocols:

Learn More

Explore Engage

Engage is a powerful personalization platform that enables you to create unified customer profiles in Segment, to build and enrich audiences, and to activate audiences across marketing tools.

Engage allows you to enrich user profiles with custom traits, allowing you to create granular audiences for campaigns, advertising, and analysis.

This page was last modified: 24 Jan 2023

Need support?

Questions? Problems? Need more info? Contact Segment Support for assistance!

Help improve these docs!

Was this page helpful?

Thanks for your feedback!

Can we improve this doc? 
Send us feedback!

Get started with Segment

On this page

Was this page helpful?

Thanks for your feedback!

Can we improve
 this doc? 
Send us feedback!

Product

For Developers

Company

Support

© 2025 Segment.io, Inc.

=== Content from https://segment.com/docs/protocols/ ===

Protocols Overview
        

Protocols is available as an add-on for Business plans only.


See the 
available plans
, or 
contact Support
.

On this page

Protocols is 
only
 available for 
event stream
 (website, mobile, and server sources) and 
Engage
 sources.

Segment helps customers collect and integrate customer data across a wide range of tools and Destinations. To do so reliably, the data Segment receives must be clean, consistent and adhere to a well thought out tracking plan.

Protocols was built to automate and scale the 
data quality best practices
 developed over years of helping customers implement Segment. Investing in data quality will improve trust in your data, reduce time spent by your engineering and business teams navigating and validating data, and ultimately allow your business to grow faster.

Protocols is a premium add-on feature available to Business Tier customers. If your plan includes Protocols, you can access it 
from the 
protocols
 path in your workspace
. If your plan doesn’t include Protocols, contact your Segment account executive.

There are four steps to using Protocols

Good data quality starts with a well thought out Tracking Plan. With Protocols, you can define your events and corresponding properties in a Tracking Plan. This tracking plan becomes a central source of truth for product, engineering, analytics, and business teams.

With your tracking plan living in Segment, you can apply it to 1 or more data sources. Any event or property that does not match the tracking plan will generate a violation. Violations are displayed in aggregated form to spot trends, and detailed form to help you quickly find and resolve discrepancies.

To maintain a high degree of quality over time, we offer strict controls to block non-conforming events. Blocked events can be forwarded to a separate quarantined Segment source for analysis and review.

Even the most exacting data collection processes are subject to human error and organizational complexity. Transformations can be applied from within Protocols to change event and property names without touching code.

Learn more

Learn more about tracking plans and why you need them.

Create a Tracking Plan to standardize your collected data.

Get answers to Protocols questions that come up the most.


  Get Started: Learn more about Tracking Plans ->


This page was last modified: 13 Jul 2023

Need support?

Questions? Problems? Need more info? Contact Segment Support for assistance!

Help improve these docs!

Was this page helpful?

Thanks for your feedback!

Can we improve this doc? 
Send us feedback!

Get started with Segment

On this page

Was this page helpful?

Thanks for your feedback!

Can we improve
 this doc? 
Send us feedback!

Product

For Developers

Company

Support

© 2025 Segment.io, Inc.

=== Content from https://segment.com/docs/engage/campaigns/broadcasts/ ===

Broadcasts
        

Engage Premier requires a Business tier account and includes Engage Foundations and Unify.


See the 
available plans
, or 
contact Support
.

On this page

Engage Premier entered an End of Sale (EOS) period effective June 10, 2024. Existing Segment customers will continue to have access and support to Engage Premier until an end-of-life (EOL) date is announced. Segment recommends exploring the following pages in preparation of a migration or future MCM needs:

Twilio Marketing Campaigns

Preferred ISV Partners:

Airship Blog
 


Bloomreach Blog
 


Braze Blog
 


Insider Blog
 


Klaviyo Blog
 


Twilio Engage Foundations Documentation
 

Broadcasts are one-time email or SMS campaigns that you can send with Twilio Engage. Use broadcasts for single, one-off occasions like the following:

For more on the different types of Engage campaigns, read 
Audiences, Journeys, and Broadcasts
.

On this page, you’ll find step by step instructions for how to create a broadcast, as well as information on broadcast best practices and analytics.

Create and send an email broadcast

Follow these steps to create an email broadcast:

Segment recommends sending email broadcasts to users with a 
subscribed
 status. However, if you need to send an email broadcast to someone who hasn’t subscribed, you can configure an email to 
send to all users
.

Blind carbon copy

Broadcasts doesn’t support BCC (blind carbon copy). If your use case requires BCC, 
contact Segment
.

Create and send an SMS broadcast

Follow these steps to create an email broadcast:

Cancel a scheduled broadcast

Follow these steps to cancel a scheduled broadcast:

Unscheduled broadcasts revert to draft status and can be found under the Drafts tab of the Broadcasts page.

Working with broadcasts

Keep the following information in mind as you work with broadcasts.

SMS segments

SMS broadcasts longer than 160 characters are split into segments and then joined together by the recipient’s device. As a result, you can send SMS broadcasts longer than 160 characters, but each 160-character segment is billed individually.

For more on message segments, view 
SMS character limits
.

Email template limits

The total size of your email must be less than 30MB.

Attachments are not supported in email templates, but you can upload files to an external storage service and include a link within the email using a button or image.

To learn more, view SendGrid’s 
email limits
.

Scale and throughput

The following table lists geographic availability, scale, and speed details for email and SMS broadcasts:

Long-code message throughput depends on a number of factors, including your 
10DLC trust score
.

Segment recommends that you use short code phone numbers for SMS broadcasts sent to more than 5000 recipients.

Broadcast analytics

Segment provides analytics for each broadcast. By selecting a sent broadcast from the broadcasts list, you can view both high-level performance metrics and granular insights on what actions individual recipients have taken on the Broadcast campaign.

Engage powers analytics for both email and SMS broadcasts. For more information on Engage analytics, view 
Analytics Overview
.

Review sent broadcasts

To view information for a sent broadcast, navigate to 
Engage > Broadcasts
, and select a broadcast from the broadcasts list.

Content tab

The content tab shows the message content and email or SMS settings that you configured for the broadcast.

Recipients tab

Segment maintains a recipients list for broadcasts. The recipients list lets you filter through several analytics statuses. Selecting an individual profile from the recipients list opens a preview pane with that profile’s details.

Settings tab

The settings tab shows your broadcast’s setup info, the recipient audience and its subscription status, as well as the broadcast’s scheduled time.

On the settings tab, you can also find the broadcast’s campaign key, which you can use to reference the broadcast. For example, you can use the campaign key to create an audience for future targeting or to create a suppression list of recipients you don’t want to receive future broadcasts.

This page was last modified: 13 Feb 2025

Need support?

Questions? Problems? Need more info? Contact Segment Support for assistance!

Help improve these docs!

Was this page helpful?

Thanks for your feedback!

Can we improve this doc? 
Send us feedback!

Get started with Segment

On this page

Was this page helpful?

Thanks for your feedback!

Can we improve
 this doc? 
Send us feedback!

Product

For Developers

Company

Support

© 2025 Segment.io, Inc.

=== Content from https://segment.com/docs/protocols/apis-and-extensions/anomaly_detection/ ===

Anomaly Detection
        

Protocols is available as an add-on for Business plans only.


See the 
available plans
, or 
contact Support
.

On this page

If you’re using Protocols, you might want to get notifications when an anomaly in event volumes or 
Protocols violation
 counts occurs. This document clarifies what Segment means by anomaly detection, gives examples of anomalies that might be relevant to your business, and provides some example solutions of how to monitor and alert on anomalies using some standard tools available today.

What is anomaly detection?

Anomaly detection means finding out when your data collection is broken, missing, or incorrect.

When you first start using Protocols, you might focus on fixing data quality issues for a limited set of business critical events. After those first issues are resolved, you might get notifications if new issues occur or if old issues reoccur, so you can avoid manually monitoring data quality. New issues often occur when a new app version is released, and for many companies, that’s weekly.

The issues you care about for anomaly detection are different for each business. An anomaly for one company could be completely normal for another company. For example, an B2B company would expect a steep drop-off of traffic and event volume on weekends, while a media or entertainment company would expect to see a rise in activity in the evenings and on weekends for their different locales.

Other types of issues you can monitor for include:

Anomaly Detection solutions

There are several easy ways to set up anomaly detection using the destination partner tools you probably already use. Many of these solutions come from Segment customers using Protocols. They use these solutions to help manage data quality and get notified when relevant anomalies are detected.

You can send anomalous events directly from your source to a Slack channel using the Slack (Actions) destination. To get started:

If you’re not using the Slack (Actions) destination to forward violations, Segment recommends that you create a new Segment source to collect all violations and Segment workspace activity. To do this, create a new HTTP source in your workspace, and assign a name that you can easily understand (for example, 
Protocols Audit Source
).

Next, set up 
Violation forwarding
 for each Tracking Plan connected to the Source. Once connected, your sources will look like the following diagram:



Note: When you enable violation forwarding, it counts as 1 MTU toward your monthly MTU limit. If you are on an API plan, all forwarded violations count against your API limit. Violations might also generate costs in downstream destinations and data warehouses.

Once violation forwarding is enabled, you can build a custom anomaly detection solution that works for your business. The examples Segment covers here include:

Forward violations to a Slack Channel

After you’ve enabled 
Violation Forwarding
, 
enable the Slack destination
 for your Protocols Audit Source. In the destination’s settings, add an Incoming Webhook URL for the Slack channel you want to push notifications to. Next, add the 
Violation Generated
 event to the 
Event Templates settings
.

You can copy and paste the example snippet below into the Event Template field to format the Slack message with the event name, violation description and source name. You can customize this message however you want, including adding @ mentions, and any of the 
properties included in the Violation Generated event
.

Source: `{{properties.sourceName}}` \nEvent: `{{properties.eventName}}` \nViolation: `{{properties.violationDescription}}`


When you’re done, it’ll look like the screenshot below.



Create customized Anomaly Detection dashboards in a BI tool

Custom dashboards are a great way to focus your teams around the metrics and events that matter most to your business. With a few simple queries you can build a dashboard to share with teams, so everyone can understand how well they’re doing against your data quality objectives. Here’s an example dashboard that combines 
forwarded Violations
 with production event data to track data quality. See below for detailed SQL queries.



Note: For all queries below, replace 
protocols_audit_source
 with whatever schema name you set for your forwarded violations source.

Source-level event to violation count comparison:

This query produces a table showing the total event and violation counts, along with a ratio of the two, broken out by day. A bar chart from this data can show when violations increase or decrease disproportionately to event volume in a source. This is the first place you would want to check to see if anomalies are occurring.

with


total_track_event_volume
 
as
 
(

      
select
  
sent_at
::
date
 
as
 
date
,

              
count
(
*
)
 
as
 
total_event_count

        
from
  
<<
YOUR_PROD_SOURCE_SCHEMA_NAME
>>
.
tracks

    
group
 
by
  
sent_at
::
date


),



total_violations
 
as
 
(

      
select
  
event_sent_at
::
date
 
as
 
date
,

              
count
(
id
)
 
as
 
violation_count

        
from
  
protocols_audit_source
.
violation_generated

       
where
  
source_slug
 
=
 
'<<YOUR SOURCE SLUG>>'

         
and
  
event_type
 
=
 
'track'

    
group
 
by
  
event_sent_at
::
date


)


      
select
  
v
.
date
,

              
t
.
total_event_count
 
"Total Violation Count"
,

              
nvl
(
v
.
violation_count
,
 
0
)
 
"Total Event Count"
,

              
nvl
(
v
.
violation_count
,
 
0
)::
float
/
t
.
total_event_count
::
float
 
as
 
"Violations Per Event"

        
from
  
total_track_event_volume
 
t

   
left
 
join
  
total_violations
 
v

          
on
  
t
.
date
 
=
 
v
.
date

    
order
 
by
  
v
.
date
 
desc



Ratio of High priority events to violation counts:

This query produces a table showing all violations and event counts by day for a single event sent to Segment. A bar chart from this data can show when violations increase or decrease disproportionately to event volume for the single event. Segment recommends selecting a few events that are important for your business (for example, 
Order Completed
, 
Video Viewed
, 
User Signed Up
) and creating a separate query and chart for each event.

with


distinct_track_event_volume
 
as
 
(

      
select
  
sent_at
::
date
 
as
 
date
,

              
count
(
*
)
 
as
 
event_count

        
from
  
<<
YOUR_PROD_SOURCE_SCHEMA_NAME
>>
.
<<
IMPORTANT_EVENT_TABLE_NAME
>>

    
group
 
by
  
sent_at
::
date


),



distinct_track_event_violations
 
as
 
(

      
select
  
event_sent_at
::
date
 
as
 
date
,

              
count
(
id
)
 
as
 
violation_count

        
from
  
protocols_audit_source
.
violation_generated

       
where
  
source_slug
 
=
 
'<<YOUR SOURCE SLUG>>'

         
and
  
event_name
 
=
 
'<<IMPORTANT EVENT NAME>>'

    
group
 
by
  
event_sent_at
::
date


)


      
select
  
v
.
date
,

              
t
.
event_count
 
as
 
"Distinct Event Count"
,

              
nvl
(
v
.
violation_count
,
 
0
)
 
as
 
"Violation Count"
,

              
nvl
(
v
.
violation_count
,
 
0
)::
float
/
t
.
event_count
::
float
 
as
 
"Violations Per Distinct Event"

        
from
  
distinct_track_event_volume
 
t

   
left
 
join
  
distinct_track_event_violations
 
v

          
on
  
t
.
date
 
=
 
v
.
date

    
order
 
by
  
v
.
date
 
desc



Source-level distinct and total violation count (Last 7 days):

This query produces a table that lists all sources connected to a Tracking Plan. For each source, the table shows distinct violations and total violations seen in the source. This table is similar to the 
violations summary
 view in the Segment app.

      
select
  
source_name
,

              
count
(
distinct
(
violation_description
))
 
as
 
distinct_violations
,

              
count
(
*
)
 
as
 
total_violations

        
from
  
protocols_audit_source
.
violation_generated

       
where
  
event_sent_at
 
>=
 
current_date
 
-
 
7

    
group
 
by
  
source_name



Event violation count (Top 10):

This query produces a table listing the top 10 events with the most violations. A bar chart showing the worst offending events is a great way to focus your efforts on fixing the least reliable events.

      
select
  
event_name
,

              
count
(
*
)
 
as
 
total_violations

        
from
  
protocols_audit_source
.
violation_generated

    
group
 
by
  
event_name

    
order
 
by
  
total_violations
 
desc

       
limit
  
10



This page was last modified: 02 Nov 2023

Need support?

Questions? Problems? Need more info? Contact Segment Support for assistance!

Help improve these docs!

Was this page helpful?

Thanks for your feedback!

Can we improve this doc? 
Send us feedback!

Get started with Segment

On this page

Was this page helpful?

Thanks for your feedback!

Can we improve
 this doc? 
Send us feedback!

Product

For Developers

Company

Support

© 2025 Segment.io, Inc.

=== Content from https://segment.com/docs/partners/ ===

Developer Center Overview
        

On this page

Welcome! Here are the steps you’ll follow to build an integration on Dev Center 2.0, launch your destination to Private Beta so customers can test it, and then launch it as Public in the Segment catalog.

Build on Segment

Over 19,000 companies use Segment as their central hub for collecting and synthesizing first-party customer data. Customers use Segment 
sources
 to collect data across all their properties (for example, web, mobile, CRMs, or email) and send this data into 
destinations
 (SaaS tools, internal databases or queues, or a data warehouse) to perform analytics, run marketing campaigns and much more.

Integration types

Segment provides two different integration types to support bringing your data into Segment, and sending your data downstream to other third-party tools.

Sources
 bring users’ first-party data into Segment. While there are several 
types
 of sources (for example, web or server libraries, mobile integrations, and Cloud), the Developer Center enables you to build your own 
Cloud Event
 sources. These sources enable users to import data directly from your application into Segment.

Destinations
 send data to other tools for processing or analysis. For example, a Segment user may want to send their data to your advertising platform or analytics tool. To accomplish this, they’ll connect your Segment destination to their workspace.

All new Segment Destinations are built on the 
Actions framework
, which enables a simplified build experience for you and a more straightforward configuration experience for your users.

Development process

To develop your integration in the Developer Center, complete the following steps:

Become a Segment Partner

Sign up for the 
Segment Select Partner Program
. During the sign-up process, you’ll agree to the 
Segment Partner Program Agreement
 and 
Privacy Policy
.

Understand Segment’s conceptual model and Spec

Segment’s 
Conceptual Model
 is a high-level overview of how Segment works and explains how your integration fits into the Segment catalog.

The 
Segment Spec
 provides best practices for the specific data you should capture and the best way to format that data based on your use case. The Spec outlines the semantic definition of the customer data that Segment captures across all its libraries and APIs, and will be a main building block for your integration.

Follow Segment’s security guidance

Security for both customers and partners is a priority at Segment. Before you start building on the Developer Center, review the 
Acceptable Use Policy
 and ensure you’re following these guidelines:

Request access to the Segment Developer Center

Segment provides access to the Developer Portal on request. Open the Developer Portal page and click 
Sign up
 to request access. A Segment account is required for this step.

Segment receives a large volume of requests so please include a valid company website and email address, answer all questions with details about integration’s use case as well as highlighting specific customer requests to expedite the approval process.

Create your integration

Follow the steps to build your 
source
 or 
destination
.

Write your integration’s documentation

Documentation is integral to enabling Segment’s users to self-serve and onboard with your integration. Segment’s documentation team will work with you during this part of the process to ensure your documentation matches the Segment style and is as instructive as possible.

Submit your integration for review

Before users can go hands on with your integration, a review by Segment engineers is required to ensure the integration meets security and usability standards.

To submit your destination for review, follow the destination-specific instructions in the 
Submit a pull request
 docs.

To submit your source for review, complete the steps described in the Developer Portal and click 
Submit for review
.

This page was last modified: 12 Aug 2024

Need support?

Questions? Problems? Need more info? Contact Segment Support for assistance!

Help improve these docs!

Was this page helpful?

Thanks for your feedback!

Can we improve this doc? 
Send us feedback!

Get started with Segment

On this page

Was this page helpful?

Thanks for your feedback!

Can we improve
 this doc? 
Send us feedback!

Product

For Developers

Company

Support

© 2025 Segment.io, Inc.

=== Content from https://segment.com/docs/connections/sources/schema/schema-unique-limits/ ===

Segment Schema Limits
        

While you can technically track unlimited events with Segment, only the first 4,000 events will be visible on the Schema page for a given Source. After you hit the 4,000 event limit, all future events will still be tracked and sent to your Destinations. They will not, however, be logged in the Segment Schema table.

While you can track unlimited event properties and traits with Segment, the Schema page has the following default limits:

After you hit the limit for both properties or traits, future properties and traits are still tracked and sent to your Destinations, but they won’t appear on the event details page. This limit includes nested properties in an event’s properties object.

These limits can also affect the traits and properties that you can see in the Computed Trait and Audience builder tools in Engage. If expected traits or properties do not appear in these tools, contact the 
Segment Support team
.

If you hit any of the limits or would like to clear out old events or properties, you can clear the Schema data from your Source Settings. In your Source, navigate to Settings, then Schema Configuration. Scroll down to the 
Clear Schema History
 setting.

You can’t clear Identify/Group traits if your Source is connected to a Tracking Plan.



Clearing events from the Source Schema only clears them from the Segment interface. It does not impact the data sent to your destinations or warehouses. Once you clear the events, the Schema page starts to repopulate new events.

You can archive events in order to declutter the Source Schema. If your Source Schema is connected to a Tracking Plan, events need to be blocked or unplanned for you to archive them. If your Source Schema not connected to a Tracking Plan, you must disable the event to see the archive button.

Archiving an event triggers an “Schema Event Archived” activity to the Audit Trail.

To view archived events, you can filter your view by “Archived”.

While this is particularly useful for Protocols customers that want to keep events “Unplanned yet acknowledged” and build a process to monitor for unplanned events, Protocols is not required to use this feature.

At this time, you cannot clear or archive old event properties individually. An alternative to this is to archive the event itself and then clear the archive. After you clear the archive, the event will re-populate in the schema with only the current properties.

This page was last modified: 28 Jan 2025

Need support?

Questions? Problems? Need more info? Contact Segment Support for assistance!

Help improve these docs!

Was this page helpful?

Thanks for your feedback!

Can we improve this doc? 
Send us feedback!

Get started with Segment

Was this page helpful?

Thanks for your feedback!

Can we improve
 this doc? 
Send us feedback!

Product

For Developers

Company

Support

© 2025 Segment.io, Inc.

=== Content from https://segment.com/docs/protocols/faq/ ===

Protocols Frequently Asked Questions
        

Protocols is available as an add-on for Business plans only.


See the 
available plans
, or 
contact Support
.

On this page

Protocols Notifications

How can I subscribe to Protocols notifications?

You can subscribe to a variety of Protocols specific alerts through the workspace Activity Feed settings. To subscribe, visit your workspace 
Settings
 > 
User Preferences
 > 
Activity Notifications
 > 
Protocols
.

How can I get notified when someone makes a change to my tracking plan?

You can forward notifications from Protocols to a new Segment source, which can then send them to notification tools such as Slack webhook.

You can also forward these Protocols alerts to any (cloud-mode) Segment destination that accepts Track calls, including data warehouses. Most customers record these activity feed events to a data warehouse for analysis.

How do I get notified when new violations are generated? Can I create custom violation notifications?

You can enable 
violation event forwarding
 to start delivering violations as Track calls to a Segment source. From there, you can forward the events to any Segment destination that accepts Track calls.

You can also use the Slack Actions destination to set event triggers for context fields, meaning events with violations are sent as Track calls directly from the source.

Protocols Tracking Plan

What is the Segment Consent Preference Updated event, and who added it to my Tracking Plans?

Consent Management
 users see the 
Segment Consent Preference Updated
 event automatically added to all existing Tracking Plans after they create their first consent category, or when they create a new Tracking Plan after configuring Consent Management. Segment recommends that you do not remove this event.

How do I add Page and Screen events to my Tracking Plan?

To consolidate the views in the Schema tab, Segment automatically converts 
page
 and 
screen
 calls into 
Page Viewed
 and 
Screen Viewed
 events that appear in the Schema Events view. Segment recommends adding a 
Page Viewed
 or 
Screen Viewed
 event to your Tracking Plan with any properties you want to validate against. At this time, to validate that a specific named page/screen (
analytics.page('Homepage') | analytics.screen('Home')
) has a specific set of required properties, you will need to use the 
JSON Schema
.

How can I see who made changes to my Tracking Plan?

Each Tracking Plan includes a Changelog, which shows which changes were made by which users. To view it, open a Tracking Plan, click the 
…
 button (also known as the dot-dot-dot, or ellipses menu) next to the Edit Tracking Plan button, and click 
View Changelog
.

How many Sources can I connect to a Tracking Plan?

The Tracking Plan to Source relationship is a one-to-many relationship. This means you can connect as many Sources to a Tracking Plan as you need. However Segment recommends connecting 1-3 Sources per Tracking Plan, because it’s rare to have more than three Sources that share an identical set of events, especially when tracking events across platforms. For example, many Segment mobile SDKs (iOS and Android) automatically collect events that would not make sense to collect in a web app. Segment doesn’t recommend including events in a Tracking Plan that would never be tracked in a Source.

Can I duplicate a Tracking Plan in the Segment UI?

You can duplicate Tracking Plans in the Segment web app by following the 
instructions to copy a tracking plan
. You can also use the 
Public API
 to copy the underlying JSON schema from one Tracking Plan to another.

How do I handle versioning with mobile apps?

Segment currently supports the ability to 
create multiple versions of an event
 in a Tracking Plan. This is ideal for mobile apps, where a breaking change like adding a new required property to an event could cause all previous app versions on user devices to generate violations. You must manually add a 
context.protocols.event_version
 property to the specific track call so that Segment can correctly validate the event against the defined version. Learn more in the 
Tracking Plan event versioning documentation
.

How do I handle null property values?

In the Tracking Plan editor, click on the data type dropdown for a given property and toggle “Allow Null Values”. Enabling null values means only 
null
 values will be accepted for that property.

Can I group specific events in a Tracking Plan?

Yes. 
Tracking Plan Labels
 are an excellent way to organize events in a Tracking Plan by priority, platform, product, or similar metadata for each event.

How do I send someone a specific event or group of events to implement?

You can search in a Tracking Plan to find a specific event, and then copy the URL for the search results page and share it. You can also filter by label to share a group of events. The person you send the URL to must have access to the Workspace and tracking plan to see the results page. (See 
the Access Management documentation
 for more details.)

Can I create a master Tracking Plan that supersedes all other Tracking Plans?

Yes. 
Tracking Plan Libraries
 makes it easy to create groups of events or properties that can be easily imported into multiple Tracking plans.

Can I copy a Tracking Plan into a library?

No. Unfortunately it’s not yet possible to automatically transfer events from a Tracking Plan to Libraries. To import events into a new event library, import them directly from a source.

Can I transfer a Tracking Plan between production and staging environments?

Yes. Using the 
Public API
, you can copy the underlying JSON schema from a Tracking Plan in one Workspace to a Tracking Plan in another Workspace.

If you 
discarded events
 as a part of your original Tracking Plan, you must connect to the same Source and configure identical Schema Controls in your other Workspace so that blocked events behave as expected.

Can I connect a Source to more than one Tracking Plan?

Unfortunately, Sources cannot be connected to more than one Tracking Plan. If you were able to connect more than one Tracking Plan to a Source, it could create conflict if events overlapped.

How do Tracking Plans work?

Segment’s code uses built-in logic to verify if an event exists in the Tracking Plan. If an event does not exist, it will follow the configuration the 
Schema Configuration settings
 for each source connected to the Tracking Plan.

Why are my unplanned properties still getting sent to my destinations even though I’ve set the dropdown to “Omit Properties”?

Unplanned property omission is only supported for cloud-mode destinations. Unplanned properties will not be omitted when they’re sent to device-mode destinations.

Why do I have two different Tracking Plan IDs?

When you access a Tracking Plan, you’ll come across two IDs: 
tp_
 and 
rs_
. Segment uses the two IDs to identify your Tracking Plan in the two APIs you can use to manage your workspace: the 
Public API
 and the 
Config API
.

To view the two IDs for your Tracking Plan, navigate to the Tracking Plan you’d like to view the ID for and select the dropdown next to 
Tracking Plan ID
.

If you’re using the Public API, you’ll need the ID that starts with 
tp_
.

If you’re using the Config API, you’ll need the ID that starts with 
rs
.

How do I import events from a Source Schema into a Tracking Plan?

When you first create your Tracking Plan, you can add events from your Source Schema by selecting the 
Import events from Source
 button on the Tracking Plan editor page. You can manually add these events after you’ve connected your Source Schema to your Tracking Plan by clicking the (+) next to the event on your Source Schema page.

Can I import events from my Source Schema into a Tracking Plan?

When you initially create your Tracking Plan, you can import events into it from a Source Schema. Manually add these events by clicking the the (+) next to the event in your Source Schema page after connecting your Tracking Plan.

Can I recover a Tracking Plan that was deleted?

You cannot recover a deleted Tracking Plan and Segment cannot recover it on your behalf. Please delete Tracking Plans with caution.

Protocols Validation

What is the difference between Violations Emails and the Violations page in the Segment UI?

Violations Daily Digest

The Violations Daily Digest is a great way to keep informed of new violations that might be easy to overlook on the Protocols Violations page. The digest sends one email digest per source, every day at approximately 12AM EST. You cannot currently opt in or out of specific sources.

The digest contains all violations for that source that are 
unique
 in the previous 48 hours. For example, if an event 
testEvent
 had violations on the first day of the month, then those violations won’t appear in the digest until the third of the month.

The email includes information about the violation to help you track down its source and correct it. It includes the event name and property name fields, the violation type, the number of times that specific type of violation was seen, and the last time it was seen.

Protocols Violations Page

The Protocols Violations page shows a live count for violations. You can adjust the timeframe to show violations in the last hour, the last 24 hours, or the last seven days.

You might see a difference between the count on the Violations page and the count in the Violations email digests. This can happen due to differences between the time periods available (24 hours in in the live page, 48 hours in the daily digest email), and the fact that the digest only shows 
unique
 violations. The fields displayed on the Violations page are more detailed than those included in the email digest.

Why do I see root listed on my Violations page?

You may see violations related to (root). For example:

(
root
)


Must
 
validate
 
all
 
the
 
schemas


// Or


(
root
)


Must
 
validate
 
"
then
"
 
as
 
"
if
"
 
was
 
valid



These violations are related to your common JSON Schema if you’ve applied custom rules. In this instance (root), refers to the top level of the JSON object (Segment event).

Protocols Enforcement

Why can’t I use the Schema to filter my events?

The schema functionality is a 
reactive
 way to clean up your data, where the Tracking Plan functionality is a 
proactive
, intentional way to clean and unify all future data. Segment has found that the best data driven companies invest the time required to build strong processes and controls around their data. The investment pays off exponentially.

That being said, there are plenty of scenarios where the reactive Schema functionality solves immediate needs for customers. Often times, customers will use both Schema Controls and Tracking Plan functionality across their Segment Sources. For smaller volume Sources with less important data, the Schema functionality often works perfectly.

If I enable blocking are events just blocked from specific Destinations or the entire Segment pipeline?

Segment can block events from all Segment Destinations except for mobile device mode destinations.

Events that are delivered from a mobile source in device mode bypass the point in the Segment pipeline where Segment blocks events, so mobile events sent using device mode are not blocked and are delivered to your Destinations. If you are a Business Tier customer using Segment’s 
Swift
 or 
Kotlin
 SDKs, you can use 
destination filters
 to block events.

When an event is blocked using a Tracking Plan, it does not count towards your MTU limit. If you use 
blocked event forwarding
, blocked events forwarded to a new source will count toward your MTU limit.

If I omit unplanned properties or properties that generate JSON schema violations, what happens to them?

Segment doesn’t store unplanned properties and properties omitted due to JSON Schema Violations in Segment logs. Segment drops omitted properties from the events. You can find the omitted properties in the 
context.violations
 object of an event payload. If you forward Violations to a new source, then you can also see the omitted properties in the Violation Generated event under 
violationField
 in the 
properties
 object.

Segment only stores fully blocked events for 30 days.

Why am I seeing unplanned properties/traits in the payload when violations are triggered, despite using schema controls to omit them?

If you’re seeing unplanned properties/traits in your payload despite using Schema Controls, you might want to select a new degree of blocking controls.

Segment’s 
Schema Controls
 provide three options to omit properties/traits. Select the one that aligns with your requirements:

Why am I still seeing unplanned properties in my Source Schema when I’ve added the properties to a new version of my Tracking Plan?

The source schema only validates events against the oldest event version in a Tracking Plan. If, for example, you have a version 1 and version 2 of your Tracking Plan, the schema only checks against version 1 of your Tracking Plan.

Do blocked and discarded events count towards my MTU counts?

Blocking events within a 
Source Schema
 or 
Tracking Plan
 excludes them from API call and MTU calculations, as the events are discarded before they reach the pipeline that Segment uses for calculations.

Do warehouse connectors use the data type definitions when creating a warehouse schema?

Warehouse connectors don’t use data type definitions for schema creation. The 
data types
 for columns are inferred from the first event that comes in from the source.

Why are unplanned properties not showing up as blocked in my Source Schema, even though I’ve set the Schema Configuration to omit them?

Next to the Event Name column in your 
Source Schema
 are two columns:  Allowed and Blocked. If you configure your 
Schema Configuration
 to Block Unplanned Events and Omit Properties, the Source Schema only shows a property or trait as blocked when the 
entire event is blocked
 because it’s unplanned and not part of the Tracking Plan. The Block Unplanned Events and Omit Properties settings are only be enforced if the property is an unplanned name, not an unplanned value.

To show a blocked value for a property/trait in your Source Schema, you’ll need to trigger a violation, which can only be done using the JSON Schema. Once you configure your Schema Configuration to Omit Properties, the property or trait is shown as blocked.

See an example payload below:

"protocols"
:
 
{

      
"omitted"
:
 
[

        
"newProperty"

      
],

      
"omitted_on_violation"
:
 
[

        
"integer"
,

        
"string"

      
],

      
"sourceId"
:
 
"1234"
,

      
"violations"
:
 
[

        
{

          
"type"
:
 
"Invalid Type"
,

          
"field"
:
 
"properties.integer"
,

          
"description"
:
 
"Invalid type. Expected: integer, given: number"

        
},

        
{

          
"type"
:
 
"Invalid Type"
,

          
"field"
:
 
"properties.string"
,

          
"description"
:
 
"Invalid type. Expected: string, given: integer"

        
}

      
]





Can I use schema controls to block events forwarded to my source from another source?

You can only use schema controls to block events at the point that they are ingested into Segment. When you forward an event that Segment has previously ingested from another source, that event bypasses the pipeline that Segment uses to block events and cannot be blocked a second time.

Protocols Transformations

Do transformations work with Segment replays?

If you create a destination scoped transformation and request a replay for that destination, the transformation will transform events into the destination. Segment doesn’t recommended requesting a replay to resend events to a destination as that will likely result in duplicate events in the destination.

Why can’t I create multiple transformations of the same type for the same event?

To reduce the risk of creating circular and conflicting transformations, Segment only allows a single transformation to be created for each distinct source, event, destination and type pairing. That means you cannot create two 
Rename track event
 transformations for a 
order_completed
 event. This eliminates the possibility of different stakeholders creating conflicting transformations to satisfy their own needs. It also simplifies the Transformations list view, making it much easier to sort and filter by source, event, destination, etc.

Why can’t I select multiple events or destinations in a single transformation?

In early transformations prototypes, Segment allowed users to select multiple events and destinations for a single transformation rule. Segment realized, however, that this created a structure that was impossible to scale, and likely to generate unintended consequences. For example, if Segment allows multiple track events to be selected for a property name change, it’d be possible to create conflicting changes. Instead, by enforcing a single event, Segment can check to see if a transformation rule exists and smartly link you to that rule using a warning.

What permissions are required to create and edit transformations?

Only workspace admins are allowed to create transformations.

What permissions are required to view transformations?

All users with Protocols admin or read-only permissions can view transformations.

Why can’t Segment support transformations for device-mode destinations?

Transformations introduce advanced logic that at scale may impact performance of client-side libraries. If you are interested in testing new functionality which supports device-mode destination transformations in analytics.js, contact your account rep.

Are Destination Filters applied before or after my Protocols Transformations?

That depends. If you are working with source-level Transformations, the Protocols conversion will come first. If you are dealing with a destination scoped transformation (which is set to only impact data going to a specific destination), Destination Filters will be applied prior to Protocols Transformations.

Why do I need Protocols to use transformations?

Transformations are but one tool among many to help you improve data quality. Segment highly recommends that all customers interested in improving data quality start with a well defined Tracking Plan. The Tracking Plan serves as a roadmap for how you want to collect data. Without a clear roadmap, it’s nearly impossible to build alignment around how transformations should be used to improve data quality, leading to more data quality issues than it solves.

Are transformations applied when using the Event Tester?

Transformations are not applied to events sent through the 
Event Tester
. The Event Tester operates independently from the Segment pipeline, focusing solely on testing specific connections to a destination. For a transformation to take effect, the event must be processed through the Segment pipeline.

Why am I getting the error “rules must contain less than or equal to 200 items” when using the Public API? Can I increase this limit?

This error occurs because there is a limit of 200 rules per API update. This restriction is by design to ensure stable API performance. Segment is not able to increase this limit on your behalf. To work around this, split your update into smaller batches, each with 200 or fewer rules.

This page was last modified: 27 Jan 2025

Need support?

Questions? Problems? Need more info? Contact Segment Support for assistance!

Help improve these docs!

Was this page helpful?

Thanks for your feedback!

Can we improve this doc? 
Send us feedback!

Get started with Segment

On this page

Was this page helpful?

Thanks for your feedback!

Can we improve
 this doc? 
Send us feedback!

Product

For Developers

Company

Support

© 2025 Segment.io, Inc.

=== Content from https://segment.com/docs/unify/identity-resolution/space-setup/ ===

Space Setup
        

Unify requires a Business tier account and is included with Engage.


See the 
available plans
, or 
contact Support
.

On this page

Step one: Create a new Dev space

When starting with Unify, begin by creating a 
Dev
 space. This will be your sandbox instance of Unify to test new Identity settings, audiences, and traits before applying the same changes to a 
Prod
 space that would immediately affect production data flowing to downstream destinations.

Step two: Configure Identity settings

Before you connect any source to the Dev space, Segment recommends that you first start by reviewing and configuring your Identity settings, as changes to the Identity rules will only be applied to new events received following any updates. Read more on those settings in the 
Identity Resolution Settings
 docs.

Step three: Set up a connection policy

If you haven’t already, Segment highly recommends labeling all your sources with 
Dev
 or 
Prod
 
environments
. Once your sources have been labeled, visit the 
Connection Policy
 page by navigating to 
Unify > Unify settings > Space management
. Here, you can enforce that only sources labeled 
Dev
 can be connected to your 
Dev
 Unify instance.



The Identity Resolution table can only be edited by Workspace Owners and users with the Identity Admin role.

Step four: Connect sources and create test audiences

Once your connection policy is in place, select the 
Profile sources
 tab in 
Unify settings
. Now you can connect a few sources that will automatically begin to replay.

Once the sources have finished replaying, check user profiles to ensure that profiles are merging as expected. This would also be an ideal time to create test audiences and confirm that these populate the expected number of users.

Step five: Connect audiences to a Dev instance of a downstream destination

Connect test audiences or traits to a dev instance of your downstream destination. Confirm that users are appearing as expected.

Step six: Apply changes to Prod sources

Once everything looks good to go, create a new 
Prod
 space, following all the same steps above, and connect a live instance of your downstream destination to your 
Prod
 space.

This page was last modified: 07 Feb 2025

Need support?

Questions? Problems? Need more info? Contact Segment Support for assistance!

Help improve these docs!

Was this page helpful?

Thanks for your feedback!

Can we improve this doc? 
Send us feedback!

Get started with Segment

On this page

Was this page helpful?

Thanks for your feedback!

Can we improve
 this doc? 
Send us feedback!

Product

For Developers

Company

Support

© 2025 Segment.io, Inc.

=== Content from https://segment.com/docs/guides/filtering-data/ ===

Filtering your Segment Data
        

On this page

There are many ways you can use Segment to filter event and object based data to control which destinations it reaches. This document lists the most commonly used ways you can filter data in Segment, and explains when you’d use each.



Filtering with the Integrations Object

The Integrations object is the only filtering method that cannot be edited using the Segment web app. As such, it is both the most reliable, and the most complicated filtering option to change. The integrations object is available to all customers regardless of Segment plan.

Use this option when you absolutely, for sure, 100% know that you 
always
, or 
never
 want this data in a specific destination or set of destinations. You can also build logic in your app or site to conditionally enable or disable destinations by rewriting this object, however this is not recommended as it is time consuming to change, especially for mobile apps.

The Integrations object filters 
track
, 
page
, 
group
, 
identify
, and 
screen
 events from both client and cloud based sources, and routes or prevents them from getting to the listed destinations.

You can use the 
integrations
 JSON object as part of your Segment payloads to control how Segment routes your data to specific destinations. An example payload is below:

{

  
"anonymousId"
:
 
"507f191e810c19729de860ea"
,

  
"context"
:
 
{

    
"locale"
:
 
"en-US"
,

    
"page"
:
 
{

      
"title"
:
 
"Analytics Academy"
,

      
"url"
:
 
"https://segment.com/academy/"

    
}

  
},

  
"integrations"
:
 
{

    
"All"
:
 
true
,

    
"Mixpanel"
:
 
false
,

    
"Salesforce"
:
 
false
,

    
"My Destination Function (My Workspace)"
:
 
true

  
}


}



By 
default
, the 
integrations
 object is set to 
'All': true
. You do not need to include this flag in the object to use this behavior, but if you’ll be using the integrations object frequently to control destination filtering, you might want to do this to make it explicit for later readers. Change this to 
'All': false
 to prevent any downstream destinations from receiving data, not including data warehouses. If you set 
'Segment.io': false
 in the integrations object, Analytics.js 2.0 drops the event before it reaches your Source Debugger. You can also add destinations to the object by key, and provide a 
true
 or 
false
 value to allow or disallow data to flow to them on an individual basis. The Destination Info box at the top of each destination page lets you know how to refer to each destination in the Integrations object.

If you are using 
multiple instances of a destination
, any settings you set in the integrations object are applied to all instances of the destination. You cannot specify an instance of a destination to apply Integrations object settings to. 

Note that destination flags are 
case sensitive
 and match the destination’s name in the docs (for example, “AdLearn Open Platform”, “awe.sm”, or “MailChimp”).

The syntax to filter data to a data warehouse is different. Refer to the 
Warehouse FAQs
 for more details.

Destination filters

Destination filters
 allow you to control the data flowing into each specific destination, by examining event payloads, and conditionally preventing data from being sent to destinations. You can filter out entire events, or just specific fields in the properties, in the traits, or in the context of your events. Destination filters support cloud-based (server-side), actions-based, and mobile and web device-mode destinations.  Destination filters aren’t available for, and don’t prevent data from reaching your warehouse(s) or S3 destinations.

Destination filters are only available in workspaces that are on a Business Tier plan.

Keep 
these limitations
 in mind when using destination filters.



To set up destination filters from the Segment web app for the destination from which you want to exclude data:

You can set up destination filters using the options presented in the Segment web app, or using Segment’s Filter Query Logic (FQL). If you use FQL, your query syntax is limited to 5KB per query.

Per-Source schema integrations filters

Integration filters allow you to quickly change which destinations receive specific Track, Identify, or Group events. Access this tool in any Source that is receiving data by navigating to the Schema tab. Schema integration filters are available to workspaces that are on a Business Tier plan only.

You can apply Integrations filters to specific events regardless of whether the source is connected to a Tracking Plan. To update which destination an event can be sent to, click the 
Integrations
 dropdown menu to see a list of the destinations each call is sent to. You can turn those destinations on or off from within the dropdown menu.



The events filtered out of individual destinations using this method still arrive in your data warehouse(s). Warehouses do not appear in the integration filters dropdown, and you cannot prevent data from flowing to Warehouses using this feature - to do that use 
Warehouse Selective Sync
.

Integration filters are all-or-nothing for each event.
 If you require more detailed control over which events are sent to specific destinations, you can use Destination Filters to inspect the event payload, and conditionally drop the data or forward it to the destination.

Integration filters won’t override an existing value in the integrations object.
 If the integration object already has a value for the integration, the per source schema integration filters will not override this. For example, if you’re sending events to Appsflyer with the 
appsflyerId
 passed into the integration object:

integrations
:
 
{

  
Appsflyer
:
 
{

    
appsflyerId
:
 
'
xxxxxx
'

  
}


}



For the same event you have Appsflyer turned off using the per source schema integrations filter, this filter won’t override the above object with a false value, and events still send downstream. In this scenario, you can use 
destination filters
 to drop the event before it sends downstream.

Schema event filters

You can use Schema Event Filters to discard and permanently remove Page, Screen and Track events from event-based sources, preventing them from reaching any destinations or warehouses, as well as omit identify traits and group properties. Use this if you know that you’ll never want to access this data again. This functionality is similar to filtering with the Integrations object, however it can be changed from within the Segment app without touching any code.

When you enable these filters, Segment stops forwarding the data to all of your Cloud- and device-mode destinations, including warehouses, and your data is no longer stored in Segment’s warehouses for later replay.

Use this when you need to disable an event immediately, but may need more time to remove it from your code, or when you want to temporarily disable an event for testing. In addition to blocking track calls, you can block all page and screen calls, as well as omit identify traits and group properties.

If the Source is not connected to a tracking plan, you’ll find event filter toggles next to the Integration filters in the source’s schema tab. When an event is set to block, the entire event is blocked. This means no destinations receive it, including data warehouses.

When you block an event using Schema filters, it won’t be considered in the MTU count unless blocked event forwarding is enabled.



When an event is blocked, the name of the event or property appears on your Schema page with a counter which shows how many times it has been blocked. By default, data from blocked events and properties is not recoverable. You can always re-enable the event to continue sending it to downstream destinations.

In most cases, blocking an event immediately stops that event from sending to destinations. In rare cases, it can take 
up to 6 hours
 for an event to completely stop arriving in all Destinations.

This feature is only available if the Source is not connected to a Tracking Plan, and is only available in workspaces that are on a Business Tier plan.

Protocols Tracking Plan blocking and property omission

If you’re using Protocols, and you’re confident that your tracking plan includes exactly the events and properties you want to record, you can tell Segment to 
block unplanned events or malformed JSON
. When you do this, Segment discards any data coming from the Source that doesn’t conform to the tracking plan.

By default, the blocked events are permanently discarded: they do not flow to Destinations, and cannot be Replayed (similar to Schema Controls). However, you can opt to send data in violation of the tracking plan to a new Segment Source so you can monitor it. (This source can affect your MTU count.)

If you have Protocols in your workspace, 
and
 have a tracking plan associated with the Source, you’ll see additional options in the Schema Configuration section of the Source’s Settings page. From this page you can choose how to handle data violations across different types of calls and properties, whether that be blocking events entirely or omitting violating properties.



Destination Insert Function

A customizable way to filter or alter data going from a source to a cloud-mode destination is to use 
Insert Functions
). This feature gives you the ability to receive data from your Segment source, write custom code to alter or block it, and then pass that altered payload to a downstream cloud-mode destination.

Warehouse Selective Sync

Warehouse Selective Sync allows you to stop sending specific data to specific warehouses. You can use this to stop syncing specific events or properties that aren’t relevant, and could be slowing down your warehouse syncs. See the 
Warehouse Selective Sync documentation
 to learn more.

This feature is only available to Business Tier customers, and you must be a Workspace Owner to change Selective Sync settings.

Privacy Portal filtering

The 
Privacy Portal
 is available to all Segment customers, because Segment believes that data privacy is a right, and that anyone collecting data should have tools to help ensure their users’ privacy. More enhancements are available to BT customers who may need tools for managing complex implementations.

The Privacy Portal tools allow you to inspect your incoming calls and their payloads, detect potential Personally Identifiable Information (PII) in properties using matchers, classify the information by different categories of risk, and use those categories to determine which Destinations may or may not receive the data. Learn more about these features in the 
Privacy Portal documentation
.



This page was last modified: 02 Feb 2024

Need support?

Questions? Problems? Need more info? Contact Segment Support for assistance!

Help improve these docs!

Was this page helpful?

Thanks for your feedback!

Can we improve this doc? 
Send us feedback!

Get started with Segment

On this page

Was this page helpful?

Thanks for your feedback!

Can we improve
 this doc? 
Send us feedback!

Product

For Developers

Company

Support

© 2025 Segment.io, Inc.

=== Content from https://segment.com/docs/connections/storage/catalog/ ===

Data Storage catalog
        

Beta

Was this page helpful?

Thanks for your feedback!

Can we improve
 this doc? 
Send us feedback!

Product

For Developers

Company

Support

© 2025 Segment.io, Inc.

=== Content from https://segment.com/docs/protocols/apis-and-extensions/ ===

Protocols: APIs and Extensions
        

Protocols is available as an add-on for Business plans only.


See the 
available plans
, or 
contact Support
.

On this page

Built from the ground up, Protocols addresses a wide range of customer needs.

With Protocols, you can help engineers reduce tracking errors, create issue notifications, and get the most out of your Tracking Plan. Below, learn about several Protocols resources that can help you address these and other common use cases.

Anomaly detection

If you’re using Protocols, you might want to get notifications when event volume anomalies or Protocols violation counts occur. Read Segment’s 
anomaly detection documentation
 to learn about common anomalies, as well as monitoring and alerting solutions you can implement using standard tools.

Public API

Protocols customers can access 
Segment’s Public API
, which enables programmatic creation, configuration, and fetching of core Segment platform resources like Sources, Destinations, and Tracking Plans.

The Public API represents Segment’s commitment to developers, helping you extend your workflow around customer data collection and activation.

Supported Operations

Typewriter

Typewriter is a tool for generating strongly-typed Segment analytics libraries based on your pre-defined Tracking Plan spec. View Segment’s 
Typewriter documentation
 to get started.

This page was last modified: 03 Aug 2023

Need support?

Questions? Problems? Need more info? Contact Segment Support for assistance!

Help improve these docs!

Was this page helpful?

Thanks for your feedback!

Can we improve this doc? 
Send us feedback!

Get started with Segment

On this page

Was this page helpful?

Thanks for your feedback!

Can we improve
 this doc? 
Send us feedback!

Product

For Developers

Company

Support

© 2025 Segment.io, Inc.

=== Content from https://segment.com/docs/connections/sources/schema/destination-data-control/ ===

Using Schema Controls
        

On this page

Once you have enabled destinations for a given source, all of the 
data
 you track will be routed to your connected tools and warehouses. If you no longer wish to send all data to a particular destination, you can disable the destination from the Source overview page. 

Segment gives you the power to control exactly what data is allowed into your destinations, so you can protect the integrity of your data, and the decisions you make with it. You can send all of your data to a warehouse and only two specific events to an analytics tool. You can also block rogue events from all of your warehouses and end tools.

Filter specific events from being sent to specific destinations

An 
integrations object
 may be passed in the 
options
 of  
group
, 
identify
, 
page
 and 
track
 methods, allowing selective destination filtering. By default all destinations are enabled.

All customers can filter specific events from being sent to specific destinations (except for warehouses) by updating their tracking code. Here is an example showing how to send a single message only to Intercom and Google Analytics:

analytics
.
identify
(
'
user_123
'
,
 
{

  
email
:
 
'
jane.kim@example.com
'
,

  
name
:
 
'
Jane Kim
'


},
 
{

  
integrations
:
 
{

    
'
All
'
:
 
true
,

    
'
Intercom
'
:
 
true
,

    
'
Google Analytics
'
:
 
true
,

    
'
Mixpanel
'
:
 
false

  
}


});



Destination flags are 
case sensitive
 and match 
the Destination’s name in the docs
 (for example, “AdLearn Open Platform”, “awe.sm”, “MailChimp”, etc.).

If you’re on Segment’s Business plan, you can filter track calls right from the Segment UI on your Source Schema page by clicking on the field in the 
Integrations
 column and then adjusting the toggle for each tool. Segment recommends using the UI if possible since it’s a much simpler way of managing your filters and can be updated with no code changes on your side.



Block or disable specific events and properties from being sent to all destinations

If you no longer want to track an event, you can either remove it from your code or, if you’re on the Business plan, you can block track calls right from the Segment UI on your Source Schema page by adjusting the toggle for each event.



Once you block an event in Segment, Segment stops forwarding it to all of your destinations, including your warehouses. You can remove it from your code at your leisure. In addition to blocking track calls, Business plan customers can block all Page and Screen calls, as well as Identify traits and Group properties. 

Add a new event using the 
New Event
 button

The 
New Event
 button in your source schema adds the event to the source schema only. It does not add any events to your tracking code. If you want to track an event, you still need to manually add it to your source code.

A use case for this feature might be to enable 
schema filtering
 for a new event before it arrives in the source to prevent it from reaching specific downstream destinations.

Export your Source Schema

Segment allows users with Source Read-only permissions to download Source Schemas as a CSV file, maximizing portability and access to event data. You can download a copy of your schema by visiting the Source Schema page.

You can export Track, Identify, and Group Source Schemas.

Download a CSV

You can only download one Source Schema CSV schema type (Track, Identify, or Group) per source at the same time.

To download a Source Schema CSV file:

All events and properties are now included in the CSV file

When you export a Source Schema, all events and properties are included in the CSV file regardless of the filters or search parameters currently applied to the Source Schema view.

Difference between Schema UI and CSV Export

When exporting a CSV from the Schema UI, there are differences in how event data is structured:

This allows you to see how Segment tracks different properties for the same event.

View download history

You can view the Source Schema exports from the last 14 days on the Download History page.

To access the Download History page:

Track event CSV format

The Track event CSV file contains the following columns:

Labels in your exported CSV

If you use 
labels
, they appear as columns in your CSV. The column headers are keys, and the column data contains values.

Identity and Group event CSV format

The Identify and Group CSV files contain the following columns:

The exported schema doesn’t include actual values (for example, personal data) for the events, properties, and traits you are tracking for a specific source.

See the 
Segment Schema Limits
 for more information on how to manage the Source Schema.

This page was last modified: 31 Jan 2025

Need support?

Questions? Problems? Need more info? Contact Segment Support for assistance!

Help improve these docs!

Was this page helpful?

Thanks for your feedback!

Can we improve this doc? 
Send us feedback!

Get started with Segment

On this page

Was this page helpful?

Thanks for your feedback!

Can we improve
 this doc? 
Send us feedback!

Product

For Developers

Company

Support

© 2025 Segment.io, Inc.

=== Content from https://segment.com/docs/unify/quickstart/ ===

Unify Onboarding Guide
        

Unify requires a Business tier account and is included with Engage.


See the 
available plans
, or 
contact Support
.

On this page

This guide walks you through the set up process for a simple Unify space, which you can use if your Segment implementation is simple. If your implementation is complex, you can use this to demonstrate and test Unify before working on a more complex configuration.

If you’re using Engage, visit the 
Engage Foundations Onboarding Guide
 for additional steps to create audiences, connect to destinations, and more.

Unify configuration requirements

To configure and use Unify, you need the following:

Step 1: Create a new Developer space

When you first start working with Unify, you should start by creating a “Developer” space. This is your experimental and test environment while you learn more about how Unify works. You can validate that identity resolution is working correctly in the Developer space, and then apply those changes to your 
Production
 space once you’re sure everything is working as expected.

This two-space method prevents you from making untested configuration changes that immediately affect production data.

Step 2: Invite teammates to your Segment space

You probably have teammates who help set up your Segment Workspace with the data you need. Invite them to your Unify dev space and grant them access to the space. Navigate to 
Access Management
 in your workspace settings to add them.

Step 3: Connect production sources

If the source you want to add doesn’t appear on the list, then check if the source is enabled. If the source is enabled, verify that you have set up a connection policy which enforces that you can only add sources with specific labels to this space. Read more about Segment’s connection policy in the 
Space Setup
 docs.

Tip:
 It sounds a little counter- intuitive to connect a production source to a developer space, but your production sources have rich user data in them, which is what you need to build and validate user profiles.

Once you select sources, Segment starts a replay of one month of historical data from these sources into your Unify space. Segment does this step first so you have some user data to build your first profiles.

The replay usually takes several hours, but the duration will vary depending on how much data you have sent through these sources in the past one month. When the replay finishes, you are notified in the Sources tab under Settings, shown below.

Note
: Data replays start with the earliest (oldest) chronological events in the one month window, and finish with the most recent. Don’t continue to the next step until all replays are marked complete. If you do, the data in your Unify data will be stale.

Once the Source(s) finish replaying, data from your connected Sources flows into Unify in near real time, just like it does for sources in your Segment workspace.

Step 4: Check your profile data

Once the replay finishes, you can see the data replayed into Unify using the Profile explorer. You should have a lot! The data should include information from multiple sources and multiple sessions, all resolved into a single profile per user.

Before you continue, check a few user profiles to make sure they show an accurate and recent snapshot of your users.

A good test is to look at 
your own
 user profile, and maybe some colleagues’ profiles. Look in the Profile explorer for your Profile, and look at your event history, custom traits and identifiers. If these identifiers look correct across a few different profiles (and you can verify that they are all correct), then you’re ready to create an audience.

If your user profiles look wrong, or you aren’t confident users are being accurately defined and merged, stop here and troubleshoot. It’s important to have accurate identity resolution before you continue. See the 
detailed Identity Resolution documentation
 to better understand how it works, and why you may be running into problems. (Still need help? 
Contact Segment
 for assistance.)

Identify events triggered by a user don’t appear in the Events tab of their profile. However, the traits from these events are still assigned to the profile. You can view them under the Traits tab.

Step 5: Create your production space

Once you validate that your data is flowing through Unify, you’re ready to create a Production space. Segment recommends that you repeat the same steps outlined above, focusing on your production use cases and data sources.

If you’re using Engage, view additional steps to complete your space set up in the 
Engage Foundations Onboarding Guide
.

You can rename the Segment space UI name, but can’t modify the space slug. As a result, you can’t change the URL of a space.

This page was last modified: 27 Feb 2025

Need support?

Questions? Problems? Need more info? Contact Segment Support for assistance!

Help improve these docs!

Was this page helpful?

Thanks for your feedback!

Can we improve this doc? 
Send us feedback!

Get started with Segment

On this page

Was this page helpful?

Thanks for your feedback!

Can we improve
 this doc? 
Send us feedback!

Product

For Developers

Company

Support

© 2025 Segment.io, Inc.

=== Content from https://segment.com/docs/protocols/validate/forward-violations/ ===

Forward Violations
        

Protocols is available as an add-on for Business plans only.


See the 
available plans
, or 
contact Support
.

You can forward Violations (data that does not conform to your Protocols tracking plan) to a Segment Source to enable custom notifications, dashboards and further analysis in any Segment destination that accepts cloud-mode data.

To set up forwarding, navigate to the settings tab of the Source, then Schema Configuration. Select the source you’ll forward events to from the Forwarding Settings Violations dropdown. Similar to 
Blocked Event forwarding
, Segment recommends that you create a new Source for violations.



Violations are sent to the selected Source as 
analytics.track()
 calls. The call payload includes the following properties, along with the 
context.app
 and 
context.library
 objects to aid in filtering violations.

    
{

      
"context"
:
 
{

        
"app"
:
 
null
,

        
"library"
:
 
{

          
"name"
:
 
"analytics-node"
,

          
"version"
:
 
"2.1.0"

        
}

      
},

      
"event"
:
 
"Violation Generated"
,

      
"integrations"
:
 
{},

      
"messageId"
:
 
"sch-Vir1JNrorwlBmCHrHWuPJekMx5c59T7c"
,

      
"properties"
:
 
{

        
"appVersion"
:
 
""
,

        
"eventMessageID"
:
 
"node-YhqA1DK9mgV55INDzOSd542fCFbIkc1o"
,

        
"eventName"
:
 
"Order Completed"
,

        
"eventSentAt"
:
 
"2018-08-23T21:35:02.85860964Z"
,

        
"eventTimestamp"
:
 
"0001-01-01T00:00:00Z"
,

        
"eventType"
:
 
"track"
,

        
"sourceID"
:
 
"dInN1HJ4bi"
,

        
"sourceName"
:
 
"Acme Store"
,

        
"sourceSlug"
:
 
"acme_store_test"
,

        
"trackingPlanID"
:
 
"rs_16KrwVbouFLrYbDkGh4LNslkrNp"
,

        
"trackingPlanName"
:
 
"Acme Ecommerce TP"
,

        
"violationDescription"
:
 
"properties.products is required"
,

        
"violationField"
:
 
"properties.products"
,

        
"violationType"
:
 
"Required"

      
},

      
"receivedAt"
:
 
"2018-09-05T23:05:25.862826965Z"
,

      
"timestamp"
:
 
"2018-09-05T23:05:25.862826855Z"
,

      
"type"
:
 
"track"
,

      
"userId"
:
 
"schema-violations"
,

      
"forwardedFromProject"
:
 
"dInN1HJ4bi"

    
}



Enabling Violation forwarding generates 1 additional MTU in your workspace. If you are on an API billing plan, you are charged for the increased API volume generated by the forwarded violations.

`Violation Generated` events

Violation Generated
 events do not appear in the source’s Schema tab, but they do appear as Violation Generated events in the 
debugger
.

This page was last modified: 07 Feb 2025

Need support?

Questions? Problems? Need more info? Contact Segment Support for assistance!

Help improve these docs!

Was this page helpful?

Thanks for your feedback!

Can we improve this doc? 
Send us feedback!

Get started with Segment

Was this page helpful?

Thanks for your feedback!

Can we improve
 this doc? 
Send us feedback!

Product

For Developers

Company

Support

© 2025 Segment.io, Inc.

=== Content from https://segment.com/docs/config-api/fql/ ===

Redirecting…

=== Content from https://segment.com/docs/engage/quickstart/ ===

Twilio Engage Foundations Onboarding Guide
        

Engage Foundations requires a Business tier account and includes Unify.


See the 
available plans
, or 
contact Support
.

On this page

This guide walks you through the set up process for a simplified Engage space. If your implementation is complex, use this to demonstrate and test Engage before working on a more complex configuration.

The first four steps in this guide also apply to Unify set up. To learn more, visit the 
Unify Onboarding Guide
.

Regional Segment

Engage Foundations is available on Segment’s regional infrastructure. For more information, see the article 
Regional Segment
.

Engage configuration requirements

Engage requires both 
Connections
 and 
Unify
.

The following are prerequisites to configuring and using Engage:

Step 1: Create a new Developer space

When you first start working with Engage, start by creating a “Developer” Engage space. This is your experimental and test environment while you learn more about how Engage works.

Step 2: Invite teammates to your Engage space

Invite teammates to your Engage dev space and grant them access to the space. Navigate to 
Access Management
 in your workspace settings to add them.

Step 3: Connect production sources

```Segment Source Details:

Details for replay:

All the events or only a subset of event names? Provide event names and/or method calls (page/identify/track/group) if only a subset of events is needed.
```

To learn more, visit 
Connect production sources
.

Step 4: Check your profile data

After the replay finishes, you can see the data replayed into Engage using the Profile Explorer. Visit the 
Profiles Onboarding Guide
 for more info.

Step 5: Create an Audience

You can build an audience using any source data that flows into your Engage space.

In this step, use the Audience Builder UI to create an Audience using properties you’re familiar with. For example, you might know the number of new website user signups in the last seven days, if you’ve connected your production website source to Engage.

To build your own audience:

After you build your audience, click 
Preview Results
 to see the total number of users who meet the audience criteria, for example all users who signed up within the last seven days.

Step 6:  Connect the Audience to a Destination

After you create your test audience, click 
Select Destinations
. Engage guides you through configuration steps to set up a destination for your audience. If you don’t already have destinations configured for the Engage space, you are prompted to select one or more. Finally, enter a name for the audience.

The larger the audience you’re creating, the longer it takes Engage to successfully compute the Audience. The Audience page shows a status that indicates if the audience is still being calculated. When the total number of users appears in the Audience overview, as in the example screenshot below, the audience has successfully finished computing, and Engage then sends the audience to the destination you selected.



Step 7: Validate that your audience is appearing in your destination

Audiences are either sent to destinations as a boolean user-property (for example 
New_Users_7days=true
 or a user-list, depending on what the destination supports. Read more about 
which destinations support which types of data
.

The UIs for the destination tools you send the audience data to are different, so the process of validating the audience varies per tool. However, the guiding principle is the same. You should be able to identify the full group of users who are members of your audience in your destination.

Step 8: Create your production space

After you validate that your full audience is arriving in your destination, you’re ready to create a Production space. Segment recommends that you repeat the same steps outlined above, focusing on your production use cases and data sources.

This page was last modified: 21 Jun 2024

Need support?

Questions? Problems? Need more info? Contact Segment Support for assistance!

Help improve these docs!

Was this page helpful?

Thanks for your feedback!

Can we improve this doc? 
Send us feedback!

Get started with Segment

On this page

Was this page helpful?

Thanks for your feedback!

Can we improve
 this doc? 
Send us feedback!

Product

For Developers

Company

Support

© 2025 Segment.io, Inc.

=== Content from https://segment.com/docs/connections/destinations/destination-filters/ ===

Destination Filters
        

On this page

Destination filters are only available to Business Tier customers.

Use destination filters to prevent certain data from flowing into a destination. You can conditionally filter out event properties, traits, and fields, or even filter out the event itself.

You can configure destination filters on cloud-mode, mobile cloud-mode destinations, and web device-mode and actions-based destinations.  With device-mode destinations, you can use the same user interface or API mechanism that you use for your cloud-mode destinations, and have those filters acted upon for device-mode destinations on web.

Common use cases for destination filters include:

Limitations

Keep the following limitations in mind when you use destination filters:

Contact Segment
 if these limitations impact your use case.

Create a destination filter

To create a destination filter:

Enable destination filters for Analytics.js sources

If you are currently using Analytics.js as your source and want to apply filters to device-mode destinations, you need to enable device mode destination filters for your Analytics.js source. To do this, go to your Javascript source, navigate to Settings > Analytics.js, and turn on the toggle for 
Destination Filters
. This will ensure the filters are effectively applied to device-mode destinations.

Destination filters API

The destination filters API provides more power than Segment’s dashboard destination filters settings. With the API, you can create complex filters that are conditionally applied using Segment’s 
Filter Query Language (FQL)
.

The destination filters API offers four different filter types:

To learn more, read Segment’s 
Destination Filters API docs
.

Examples

The following examples illustrate common destinations filters use cases:

PII management

Example: Remove email addresses from 
context
 and 
properties
:

Property-level allowlisting is available with Segment’s API. Using destination filters, you can configure a rule that removes email addresses from 
context
 and 
properties
. As a result, Segment only sends traits without PII to the destination.



Healthcare and Life Sciences (HLS) customers can encrypt data flowing into their destinations

HLS customers with a HIPAA eligible workspace can encrypt data in fields marked as Yellow in the Privacy Portal before they flow into an event stream, cloud-mode destination.

To learn more about data encryption, see the 
HIPAA Eligible Segment documentation
.

Control event volume

This example shows a filter that controls event volume by only sending 
User Signed Up
 and 
Demo Requested
 events.



Cleaner data

This example shows a rule that only sends track calls to Google Analytics.



Remove internal and test events from production tools

In the example below, the rule targets email addresses with internal domains to stop test events from reaching Destinations.



In the example below, the rule prevents an event from sending if 
Order Completed
 and 
properties.email
 contain an internal 
@segment.com
 email address.



Sample a percentage of events

Using the 
destination filters API
, you can create a rule to randomly sample video heartbeat events.

Drop events

Watch this destination filters walkthrough
 to learn how to use event names to filter events sent to destinations.

Only send events with userId

Use the 
Public API
 to only send events to your destination if they contain a 
userId
. Here’s an example of how you might format this request:

{

    
"sourceId"
:
 
"<SOURCE_ID>"
,

    
"destinationId"
:
 
"<DESTIANTION_ID>"
,

    
"title"
:
 
"Don't send event if userId is null"
,

    
"description"
:
 
"Drop event if there is no userId on the request"
,

    
"if"
:
 
"length( userId ) < 1"
,

    
"actions"
:
 
[

      
{

        
"type"
:
 
"DROP"

      
}

    
],

    
"enabled"
:
 
true

  
}



Remove userId from payload

There are certain destinations to which you may not want to send the 
userId
. To accomplish this, you can use the 
Public API
 to create a Filter that will target and remove the 
userId
 (or any other top-level field) like this:

{

    
"sourceId"
:
 
"<sourceId>"
,

    
"destinationId"
:
 
"<destinationId>"
,

    
"title"
:
 
"Don't send userId at all"
,

    
"description"
:
 
"Drop userId on all requests"
,

    
"if"
:
 
"all"
,

    
"actions"
:
 
[

       
{

        
"type"
:
 
"DROP_PROPERTIES"
,

          
"fields"
:
 
{

            
""
:[
"userId"
]

            
}

       
}

      
],

      
"enabled"
:
 
true


}



Filter conditional operators

Important notes

Some destinations offer settings that also allow you to filter data. For example, the Facebook App Events destination allows you to map 
Screen
 events to 
Track
 events. Because destination filters are evaluated and applied 
before
 the destination settings are applied, they can conflict with your settings.

For example, if you have a destination filter that filters Track events 
and
 you have the 
Use Screen Events as Track Events
 setting enabled, 
Track
 events drop, but 
Screen
 events still process. The destination settings transform it into a 
Track
 event - 
after
 the filters.

Segment makes effort to ensure that destination filters can handle unexpected situations. For example, if you use the 
contains()
 FQL function on the 
null
 field, Segment returns 
false
 instead of returning an error. If Segment can’t infer your intent, Segment logs an internal error and drops the event. Segment defaults to this behavior to prevent sensitive information, like a PII filter, from getting through.

Errors aren’t exposed in your Destination’s Event Deliverability tab. For help diagnosing missing destination filter events, 
contact Segment
.

FAQs

Destination filters can filter properties out of objects nested in an array. For example, you can filter out the 
price
 property of every object in an array at 
properties.products
. You can also filter out an entire array from the payload. However, you can’t drop nested objects in an array or filter properties out of a single object in an array.

To block a specific property from all of the objects within a properties array, set the filter using the following the format: 
<propertyType>.<arrayName>.<arrayElementLabel>​
.

For example, the 
properties.products.newElement
 filter blocks all 
newElement
 property fields from each 
products
 object of an array within the 
properties
 object of a Track event.



To block the Identify event trait 
products.newElement
, select the option under the 
User Traits
 list instead. To block the context object field 
products.newElement
, select it from the 
Context Fields
 list.

Segment supports 10 filters per destination. If you need help consolidating filters or would like to discuss your use case, 
contact Segment
.

Segment evaluates multiple 
Only Send
 filters against each other and resolves destination filters in order. If multiple 
Only Send
 filters conflict with each other, Segment won’t send information downstream.

Segment displays the most recent 15,000 properties. To find a property not in the filter dropdown, enter the property manually.

To filter out events from warehouses, use Selective Sync.

Generally, only Track calls have 
name
 properties, which correspond to the 
event
 field in an event.

The Activity Feed shows the action, date, and user who performed the action when a destination filter is created, modified, enabled, disabled, or deleted. You can also subscribe to notifications for any of these changes in the 
Activity Feed
 settings page.

You must have write access to save and edit filters. Read permission access only allows viewing and testing access.

Use the destination filter tester during setup to verify that you’re filtering out the right events. Filtered events show up on the schema page but aren’t counted in event deliverability graphs.

Destination Filters can’t target properties or traits with spaces in the field name. As an alternative, use 
Insert Functions
, which let you write code to take care of such filtering.

The check for unsupported events types happens before any destination filter checks. As a result, Destination Filters can’t prevent unsupported event type errors. To filter these events, use the 
Integrations Object
.

Destination filters only filter events sent after filter setup. If you just added a destination filter but still see some events going through, you’re likely seeing retries from failed events that occurred before you set up the filter.

When Segment sends an event to a destination but encounters a timeout error, it attempts to send the event again. As a result, if you add a destination filter while Segment is trying to send a failed event, these retries could filter through, since they reflect events that occurred before filter setup.

Destination filters are case-sensitive. Make sure to test your filter conditions with a test event before saving and enabling the filter.

This page was last modified: 23 Dec 2024

Need support?

Questions? Problems? Need more info? Contact Segment Support for assistance!

Help improve these docs!

Was this page helpful?

Thanks for your feedback!

Can we improve this doc? 
Send us feedback!

Get started with Segment

On this page

Was this page helpful?

Thanks for your feedback!

Can we improve
 this doc? 
Send us feedback!

Product

For Developers

Company

Support

© 2025 Segment.io, Inc.

=== Content from https://segment.com/docs/protocols/tracking-plan/libraries/ ===

Tracking Plan Libraries
        

Protocols is available as an add-on for Business plans only.


See the 
available plans
, or 
contact Support
.

On this page

Tracking Plan Libraries make it easy to scale Tracking Plan creation within your workspace. You can create libraries for track events or track event properties. Editing Tracking Plan Libraries is identical to 
editing Tracking Plans
.

Segment does support advanced JSON schema implementations and Identify/Group trait libraries.

Once created, you can import event or property Libraries into a Tracking Plan using a simple wizard flow.

Create a Tracking Plan Library

To create a new Library:



Tracking Plan Event Libraries

Tracking Plan Event Libraries support Track events and associated properties. Event Libraries are helpful when you want to track a group of events consistently across tracking plans. For example, if you are an eCommerce company with more than one application, you may need to track 
eCommerce Spec
 events consistently across those sources. Instead of needing to re-create the eCommerce spec across all tracking plans, create a library and import the events to each Tracking Plan.

Tracking Plan Property Libraries

Tracking Plan property Libraries support Track event property groups. Property Libraries are helpful when you have more than one event in a Tracking Plan that share a common set of properties. For example, if you want to consistently include 
order_id
, 
currency
, 
cart_id
 and a 
products
 array of objects in your checkout flow events, you can create a Library with these properties including descriptions, data types and conditional filters.

Import Libraries into a Tracking Plan

You can import event and property libraries into a Tracking Plan. Enter the Draft mode for a Tracking Plan and click the 
Import from Event or Property Library
 dropdown. A wizard will appear allowing you to either add the events to a tracking plan, or add properties to selected events already in the tracking plan. After adding your events or properties, remember to merge your changes!



Library syncing

When you import events or properties from a Library, you can enable syncing to ensure that changes made to the Library pass down to all synced Tracking plans. Syncing is important when you want to make sure all Tracking Plans define events and properties consistently. For example, it’s best practice to create separate tracking plans for mobile and web sources as these two sources share some but not all events or properties. Library syncing is the best way to ensure that the shared events are consistently tracked across Tracking Plans, even as you make changes to the Library.

To enable syncing, select the desired Library from the Tracking Plan import flow, and toggle the syncing option. This selects all events or properties in the Library for import. Partial syncs are not supported.

Syncing a Library makes events and properties un-editable, and bypasses the Tracking Plan merge step. You can add properties to synced events, but cannot remove those synced events unless you also remove the Library sync. To unsync a library, click 
View Synced Libraries
 from the Tracking Plan and click the overflow menu to unsync the Library.

All changes made to a synced library pass through to the Tracking Plans and may impact data deliverability



This page was last modified: 07 Feb 2025

Need support?

Questions? Problems? Need more info? Contact Segment Support for assistance!

Help improve these docs!

Was this page helpful?

Thanks for your feedback!

Can we improve this doc? 
Send us feedback!

Get started with Segment

On this page

Was this page helpful?

Thanks for your feedback!

Can we improve
 this doc? 
Send us feedback!

Product

For Developers

Company

Support

© 2025 Segment.io, Inc.

=== Content from https://segment.com/docs/unify/profiles-sync/overview/ ===

Profiles Sync Overview
        

Unify requires a Business tier account and is included with Engage.


See the 
available plans
, or 
contact Support
.

On this page

Profiles Sync connects identity-resolved customer profiles to a data warehouse of your choice.

With a continual flow of synced profiles, teams can enrich and use these data sets as the basis for new audiences and models. Profiles Sync addresses a number of use cases, with applications for identity graph monitoring, attribution analysis, machine learning, and more. View 
Profiles Sync Sample Queries
 for an in-depth guide to Profiles Sync applications.

Profiles Sync use cases

To help you get started, here are a few example use cases:

Understand how Segment creates Profiles

Use Profiles Sync for more insight into profiles generated by Segment’s 
Identity Resolution
. Query the Profile Sync data set to answer questions such as:

Understanding how Segment creates profiles helps you detect potential instrumentation errors.

Create golden profiles

Join Segment’s profile data with existing object data from your warehouse to create a single view of the customer. You can then use this data set to create personalized  experiences on any channel. For example, B2B companies can build a report that maps sales executives (object data synced with a source like Salesforce) with customers who are most likely to buy a certain product (Profile Traits data synced with Profiles Sync).

Understand a customer’s journey

With Profiles Sync, your data teams can better understand profile merge events. Connect anonymous IDs, User IDs, and emails to understand your customer’s journey with details such as:

Build attribution models

Use Profiles Sync to build data models that marketing partners can trust. Trace prospective customer journeys before buying products, and build models that help you understand which channels provide the most value.

Use machine learning

Access profile traits and see how they change over time. This will help you to better understand how your customer’s behavior evolves, and build models that predict LTV, churn, and propensity scores.

Next steps

To learn more about Profiles Sync, visit the following docs:

For more on Profiles Sync logic, table mappings, and data types, download this 
Profiles Sync ERD
.

This page was last modified: 31 Oct 2023

Need support?

Questions? Problems? Need more info? Contact Segment Support for assistance!

Help improve these docs!

Was this page helpful?

Thanks for your feedback!

Can we improve this doc? 
Send us feedback!

Get started with Segment

On this page

Was this page helpful?

Thanks for your feedback!

Can we improve
 this doc? 
Send us feedback!

Product

For Developers

Company

Support

© 2025 Segment.io, Inc.

=== Content from https://segment.com/docs/?ref=nav#segment-documentation ===

Segment Documentation
          

Learn how to use Segment to collect, responsibly manage, and integrate your customer data with hundreds of tools.

Learn about Segment, plan and work through a basic implementation, and explore features and extensions.

How can Segment help you?

Integrate the tools you need for analytics, growth, marketing, and more.

Prevent data quality issues with a tracking schema and enforcement with Protocols.

Build audiences and journeys from real-time customer data to personalize experiences on every channel.

Keep customer data private with Segment's data discovery and policy enforcement tools.

Get Data into Segment

The Segment Spec helps you identify, capture, and format meaningful data for use with Segment libraries and APIs as well as downstream tools.

Use Track, Page, Identify, and other Segment tracking calls.

Save time by letting Segment calls collect information for you.

Use our business-case specs to ensure that your tools get the most from your data.

Learning about Segment

The basics of your Segment implementation.

Over a dozen how-to guides that help you accomplish common tasks.

Connect your app to Segment

Additional Resources

Segment's Analytics Academy walks you through the wide world of analytics, including best practices, an overview of the most popular tools, and case studies of how other developers have achieved success.

For a more hands-on tutorial of Segment, check out Segment University. It offers step-by-step instructions, starting with first steps and going through some of our more advanced features.

Need support?

Questions? Problems? Need more info? Contact Segment Support for assistance!

Help improve these docs!

Was this page helpful?

Thanks for your feedback!

Can we improve this doc? 
Send us feedback!

Get started with Segment

Product

For Developers

Company

Support

© 2025 Segment.io, Inc.

=== Content from https://segment.com/docs/getting-started/use-cases/reference// ===

Use Cases Reference
        

On this page

This reference guide provides detailed information on the suggested events, sources, and destinations for each Segment use case. Use this guide to ensure you’re tracking the right events and connecting the best sources and destinations for your specific needs.

Use Cases by business goal

The business goal you select during onboarding determines the use cases that Segment shows you.

This table lists each business goal and each of its corresponding use cases:

Suggested events, sources, and destinations

This section contains tables for the different events, sources, and destinations that Segment recommends for each use case.

Optimize advertising

Click on each use case in this section to view Segment’s recommendations for the Optimize advertising business goal, which helps you improve return on ad spend.

This table shows the event and properties Segment recommends you track for the Build high value lookalikes use case, which helps you build from high-value purchasers through specific channels:





And this table shows the source and destination types that Segment recommends you set up for the Build high-value lookalikes use case:




This table shows the events and properties Segment recommends you track for the Build lookalikes for app install use case, which helps you build lookalikes from app installers in specific channels:





And this table shows the source and destination types that Segment recommends you set up for the Build lookalikes for app install use case:




This table shows the events and properties Segment recommends you track for the Increase signups with lookalikes use case, which helps you build lookalikes from signups through specific channels.:





And this table shows the source and destination types that Segment recommends you set up for the Increase signups with lookalikes use case:




This table shows the events and properties Segment recommends you track for the Mitigate cart abandonment use case, which helps you win back users to drive purchases and understand funnel:





And this table shows the source and destination types that Segment recommends you set up for the Mitigate cart abandonment use case, which helps you win back users to drive purchases and understand funnel:




This table shows the event and properties Segment recommends you track for the Mitigate high value churn use case, which helps you anticipate churn for your highest value users and prevent them from churning:





And this table shows the source and destination types that Segment recommends you set up for the Mitigate high value churn use case:




This table shows the event and properties Segment recommends you track for the Suppress based on time use case, which helps you suppress users after a few days to keep campaigns fresh:





And this table shows the source and destination types that Segment recommends you set up for the Suppress based on time use case:




This table shows the events and properties Segment recommends you track for the Suppress with purchase use case, which helps you suppress converted users immediately after a conversion:





And this table shows the source and destination types that Segment recommends you set up for the Suppress with purchase use case:




Personalize first conversion

Click on each use case in this section to view Segment’s recommendations for the Personalize first conversion business goal, which helps you convert prospective or free customers.

This table shows the events and properties Segment recommends you track for the Accelerate app install use case, which helps you encourage app install with personalized messaging:





And this table shows the source and destination types that Segment recommends you set up for the Accelerate app install use case:




This table shows the events and properties Segment recommends you track for the Accelerate onboarding use case, which helps you optimize new user activation based on real-time behavior:





And this table shows the source and destination types that Segment recommends you set up for the Accelerate onboarding use case:




This table shows the events and properties Segment recommends you track for the Accelerate signup use case, which helps you encourage anonymous users to sign up with messaging:





And this table shows the source and destination types that Segment recommends you set up for the Accelerate signup use case:




This table shows the events and properties Segment recommends you track for the Acquire paid subscriptions use case, which helps you engage customers at the right time to drive conversions:





And this table shows the source and destination types that Segment recommends you set up for the Acquire paid subscriptions use case:




This table shows the events and properties Segment recommends you track for the Convert trials to paid subscriptions use case, which helps you get customers to upgrade through personalized messaging:





And this table shows the source and destination types that Segment recommends you set up for the Convert trials to paid subscriptions use case:




This table shows the events and properties Segment recommends you track for the Mitigate cart abandonment use case, which helps you win back users to drive purchases and understand funnel:





And this table shows the source and destination types that Segment recommends you set up for the Mitigate cart abandonment use case:




Boost retention, upsell, and cross-sell

Click on each use case in this section to view Segment’s recommendations for the Boost retention, upsell, and cross-sell business goal, which helps you increase repeat visits or purchases.

This table shows the event and properties Segment recommends you track for the Build high value lookalikes use case, which helps you build from high-value purchasers through specific channels:





And this table shows the source and destination types that Segment recommends you set up for the Build high value lookalikes use case:




This table shows the events and properties Segment recommends you track for the Increase repeat purchases use case, which helps you convert single-purchase buyers with personalized communications:





And this table shows the source and destination types that Segment recommends you set up for the Increase repeat purchases use case:




This table shows the event and properties Segment recommends you track for the Mitigate high value churn use case, which helps you anticipate churn for your highest-value users and prevent them from churning:





And this table shows the source and destination types that Segment recommends you set up for the Mitigate high value churn use case:




This table shows the events and properties Segment recommends you track for the Nurture with content use case, which helps you use content personalized by interest to nurture leads or customers:





And this table shows the source and destination types that Segment recommends you set up for the Nurture with content use case:




This table shows the events and properties Segment recommends you track for the Personalize upsell content use case, which helps you personalize upsell and cross-sell messaging while understanding behavior:





And this table shows the source and destination types that Segment recommends you set up for the Personalize upsell content use case:




This table shows the events and properties Segment recommends you track for the Personalize winback use case, which helps you design personalized messaging based on user behavior:





And this table shows the source and destination types that Segment recommends you set up for the Personalize winback use case:




Personalize communications and product experiences

Click on each use case in this section to view Segment’s recommendations for the Personalize communications and product experiences business goal, which helps you engage your customers with relevant content.

This table shows the events and properties Segment recommends you track for the Accelerate onboarding use case, which helps you optimize new user activation based on real-time behavior:





And this table shows the source and destination types that Segment recommends you set up for the Accelerate onboarding use case:




This table shows the events and properties Segment recommends you track for the Increase repeat purchases use case, which helps you convert single-purchase buyers with personalized communications:





And this table shows the source and destination types that Segment recommends you set up for the Increase repeat purchases use case:




This table shows the event and properties Segment recommends you track for the Mitigate high value churn use case, which helps you anticipate churn for your highest-value users and prevent them from churning:





And this table shows the source and destination types that Segment recommends you set up for the Mitigate high value churn use case:




This table shows the events and properties Segment recommends you track for the Nurture with content use case, which helps you use content personalized by interest to nurture leads or customers:





And this table shows the source and destination types that Segment recommends you set up for the Nurture with content use case:




This table shows the events and properties Segment recommends you track for the Personalize upsell content use case, which helps you personalize upsell and cross-sell messaging while understanding behavior:





And this table shows the source and destination types that Segment recommends you set up for the Personalize upsell content use case:




This table shows the events and properties Segment recommends you track for the Personalize winback use case, which helps you design personalized messaging based on user behavior:





And this table shows the source and destination types that Segment recommends you set up for the Personalize winback use case:




This page was last modified: 08 Oct 2024

Need support?

Questions? Problems? Need more info? Contact Segment Support for assistance!

Help improve these docs!

Was this page helpful?

Thanks for your feedback!

Can we improve this doc? 
Send us feedback!

Get started with Segment

On this page

Was this page helpful?

Thanks for your feedback!

Can we improve
 this doc? 
Send us feedback!

Product

For Developers

Company

Support

© 2025 Segment.io, Inc.

=== Content from https://segment.com/docs/unify/identity-resolution/ ===

Identity Resolution Overview
        

Unify requires a Business tier account and is included with Engage.


See the 
available plans
, or 
contact Support
.

On this page

Identity Graph

Identity Resolution sits at the core of Segment. The Identity Graph merges the complete history of each customer into a single profile, no matter where they interact with your business. Identity Resolution allows you to understand a user’s interaction across web, mobile, server, and third-party partner touch-points in real time, using an online and offline ID graph with support for cookie IDs, device IDs, emails, and custom external IDs. If you are sending the 
Group call
, you can also understand user behavior at the account-level.



Highlights

Technical highlights

FAQs

For security reasons, Segment requires that the 
Profile API
 only be used server-side. The Profile API allows you to look up data about any user given an identifier (for example, email, 
anonymousId
, or 
userId
) and an authorized access secret. While this enables powerful personalization workflows, it could also let your customers’ data fall into the wrong hands if the access secret were exposed on the client.

Instead, by creating an authenticated personalization endpoint server-side backed by the Profile API, you can serve up personalized data to your users without the risk of their information falling into the wrong hands.

This page was last modified: 22 Nov 2023

Need support?

Questions? Problems? Need more info? Contact Segment Support for assistance!

Help improve these docs!

Was this page helpful?

Thanks for your feedback!

Can we improve this doc? 
Send us feedback!

Get started with Segment

On this page

Was this page helpful?

Thanks for your feedback!

Can we improve
 this doc? 
Send us feedback!

Product

For Developers

Company

Support

© 2025 Segment.io, Inc.

=== Content from https://segment.com/docs/connections/spec/ecommerce-tracking-plan/ ===

Ecommerce Tracking Plans
        

On this page

When tracking your data, it’s important to set yourself up for success. E-commerce and retail companies want to use their data to understand why some customers fall out of their funnels or why customers become repeat buyers. They want to understand the important lifecycle events that lead up to the sale of a physical item, so they can, for example, test whether personalized shopping experiences yield higher conversions, or build a multi-channel cart abandonment campaign. But first, they need to make sure those lifecycle events are being captured in their datasets.

That’s where a tracking plan comes in. A tracking plan is a living document that can be used across your organization to record what events and properties to track, where you’ll be tracking them in your code base, and why you’re tracking them.

Learn more about the value and function of a tracking plan.

In this guide, you’ll learn the core events most relevant to e-commerce companies that can get you started immediately in understanding your customers and driving sales.

Talk to a product specialist today
 about building a clean, high-quality data spec so you can focus on brand engagement and sales growth.

Identifying your customers

Before diving into specific event tracking, you’ll want to make sure you track who your users are with the the 
.identify()
 call. You should call 
.identify()
 whenever a visitor provides you with a piece of information, at which point they become “known users.” The 
.identify()
 call creates or updates a record of your customer with a set of traits in your tools and warehouse. But how do you choose which traits about your user to include?

Traits are pieces of data that you track about a specific user. Read the guide about selecting traits to learn more.

Here are the most common user traits e-commerce companies include in their tracking:

Here are a few examples of other helpful user traits:

Here is a sample 
.identify()
 call for Segment:

In analytics.js:

 
 
 
 
analytics
.
identify
({


 
 
 
 
 
 
first_name
:
 
'
Andy
'
,


 
 
 
 
 
 
last_name
:
 
'
Jiang
'
,


 
 
 
 
 
 
email
:
 
'
andy@segment.com
'


 
 
 
 
});



In analytics-ios:

 
 
 
 
[[
SEGAnalytics
 
sharedAnalytics
]
 
identify
:
nil


 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
traits
:@{
 
@"email"
:
 
@"andy@segment.com"
,


 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
@"first_name"
:
 
@"Andy"
,


 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
@"last_name"
:
 
@"Jiang"
,


 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
@"experiment_viewed"
:
 
@"Coupon"
 
}];



In analytics-android:

 
 
 
 
Analytics
.
with
(
context
).
identify
(
new
 
Traits
().
putValue
(
"first_name"
,
 
"andy"
).
putValue
(
"last_name"
,
 
"jiang"
).
putValue
(
"experiment_viewed"
,
 
"Coupon"
).
putValue
(
"email"
,
 
"andy@segment.com"
),
 
null
);



The main benefits of using the traits listed above are:

You can use the traits to personalize content in your email or push notification tools by inserting their information. For example:

You can create cohorts based on the traits in any of your tools. For example, you could use 
order_count
 to determine how many of your users are first or repeat, or 
experiments
 to determine how many have seen this particular experiment variant

Given the power you have in your downstream tools to create cohorts based on these dimensions, you may be tempted to throw more contextual data into the 
traits
 , such as UTM params, IP addresses, and userAgents. But if you’re using Segment’s client-side analytics.js library, then all of these contextual pieces of data are automatically collected.

Selecting key e-commerce and marketing events

Now that you are tracking who your users are, you can work on what they’re doing on your website or in your app with a 
.track()
 call. Tracking customer events lets you learn about your customers, measure the impact of your marketing efforts and product decisions, and proactively engage your customers in a meaningful way that drives sales.

Read the event tracking guide to learn more about tracking the right events.

E-commerce businesses, unlike SaaS or consumer apps that optimize for product engagement, focus on directing users down their funnels to a conversion goal, like purchasing a product. To best understand why customers convert, it’s important to track and measure all key funnel events.

There are also auxiliary actions to track to measure your customer’s engagement with your site. These actions are good to track so you can better understand their intent on your website.

Lastly, Segment has a set of semantic campaign events that are automatically collected so you can understand the conversions in these specific channels:

Check out the full list of e-commerce events you should track.

You may notice a pattern in the event names. Segment selected the “Object Action” naming convention to ensure that all event data is clean and easily analyzable, while choosing “snake_case” for the traits and properties. It doesn’t matter what you choose, so long as it’s consistent. Without a uniform and enforced naming framework to guide developers that add tracking code later, your data could get marred with conflicting naming structures. Learn more about the importance of naming conventions.

Segment recommends tracking core checkout activity on the server-side. Learn more about
 
tracking on the client vs. server
.

Selecting your properties

Properties are similar to traits, but they’re associated with specific actions, rather than with an individual user. Each 
.track()
 call can accept an optional dictionary of 
properties
, which can contain any key-value pair you want. These 
properties
 act as dimensions that allow your end tool to group, filter, and analyze the events. They give you additional detail on broader events.

Learn more about properties and what they mean for your downstream analysis.

For e-commerce, since most events are customers choosing, browsing, and checking out products, all of the traits must contain key information about the products themselves:

These traits must be included because many tools rely on them for analysis. If there was one of them missing, the call would be ignored.

Using a specific tool and want to see how Segment handles sending calls to it?
 
Check out the documentation.

Here is an example 
.track()
 call:

In analytics-node:

 
 
 
 
analytics
.
track
({


 
 
 
 
 
 
userId
:
 
'
019mr8mf4r
'
,


 
 
 
 
 
 
event
:
 
'
Order Completed
'
,


 
 
 
 
 
 
properties
:
 
{


 
 
 
 
 
 
 
 
order_id
:
 
'
50314b8e9bcf000000000000
'
,


 
 
 
 
 
 
 
 
total
:
 
20
,


 
 
 
 
 
 
 
 
currency
:
 
'
USD
'
,


 
 
 
 
 
 
 
 
products
:
 
[


 
 
 
 
 
 
 
 
 
 
{


 
 
 
 
 
 
 
 
 
 
 
 
product_id
:
 
'
507f1f77bcf86cd799439011
'
,


 
 
 
 
 
 
 
 
 
 
 
 
sku
:
 
'
201
'
,


 
 
 
 
 
 
 
 
 
 
 
 
name
:
 
'
Folsom
'
,


 
 
 
 
 
 
 
 
 
 
 
 
price
:
 
10
,


 
 
 
 
 
 
 
 
 
 
 
 
quantity
:
 
1


 
 
 
 
 
 
 
 
 
 
},


 
 
 
 
 
 
 
 
 
 
{


 
 
 
 
 
 
 
 
 
 
 
 
product_id
:
 
'
505bd76785ebb509fc183733
'
,


 
 
 
 
 
 
 
 
 
 
 
 
sku
:
 
'
204
'
,


 
 
 
 
 
 
 
 
 
 
 
 
name
:
 
'
Brennan
'
,


 
 
 
 
 
 
 
 
 
 
 
 
price
:
 
10
,


 
 
 
 
 
 
 
 
 
 
 
 
quantity
:
 
1


 
 
 
 
 
 
 
 
 
 
}


 
 
 
 
 
 
 
 
]


 
 
 
 
 
 
}


 
 
 
 
});



In analytics-ios

 
 
 
 
[[
SEGAnalytics
 
sharedAnalytics
]
 
track
:
@"Order Completed"


 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
properties
:@{
 
@"order_id"
:
 
@"50314b8e9bcf000000000000"
,


 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
@"total"
:
 
@"20"
,


 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
@"currency"
:
 
@"USD"
,


 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
@"products"
:
 
@"Coupon"
 
}];



In analytics-android:

 
 
 
 
Analytics
.
with
(
context
).
track
(
"Order Completed"
,
 
new
 
Properties
().
putValue
(
"order_id"
,
 
"50314b8e9bcf000000000000"
).
putValue
(
"total"
,
 
20
).
putValue
(
"currency"
,
 
"USD"
).
putValue
(
"products"
,
 
"Coupon"
));



It’s important that these events contain particular properties, such as 
sku
 , otherwise the downstream tools won’t be able to create out-of-the-box revenue and sales reports. Learn more about semantic properties in the e-commerce spec.

Using data to understand why your customers don’t convert

The most successful e-commerce businesses not only efficiently move their customers through the funnel towards conversion, but also have the infrastructure to collect and use customer data. Having a tracking plan focused on key funnel events can help e-commerce businesses get a sense of the health of their funnel.

Aside from funnel health, having these key pieces of customer data gives companies the ability to tailor and personalize each interaction, as well as build marketing campaigns around actions taken or omitted.

Without taking this critical step of mapping out key customer events, businesses often spend too much time revisiting their data model or analyzing impartial data sets. Instead they could spend that time understanding and addressing customers’ needs.

Talk to a product specialist today
 
about building a clean, high-quality data spec so you can focus on brand engagement and sales growth.

This page was last modified: 01 Jul 2022

Need support?

Questions? Problems? Need more info? Contact Segment Support for assistance!

Help improve these docs!

Was this page helpful?

Thanks for your feedback!

Can we improve this doc? 
Send us feedback!

Get started with Segment

On this page

Was this page helpful?

Thanks for your feedback!

Can we improve
 this doc? 
Send us feedback!

Product

For Developers

Company

Support

© 2025 Segment.io, Inc.

=== Content from https://segment.com/docs/guides/how-to-guides/cross-channel-tracking/ ===

Tracking Customers Across Channels and Devices
        

On this page

The paths consumers take to your app or website are more complex than ever, often involving a variety of online communities and multiple devices. Your next repeat customer might stumble across your display ad on a newsletter you’ve never heard about, or receive a recommendation from a co-worker in a Slack channel.

But these off-domain and cross-device brand interactions are equally, if not more, important to track and understand. With this data, you can identify more sources of qualified traffic and determine the best shopping experiences for conversion.

In this guide, you’ll learn where and how to track these critical events so that you can understand your customer’s journey before they even get to your storefront, as well as their preferred shopping experiences.

If you’re interested in learning about what to track, 
check out Segment’s guide on creating an e-commerce tracking plan
.

Talk to a product specialist today
 about building a clean, high-quality data spec so you can focus on brand engagement and sales growth.

Where are they coming from? Off-domain tracking

Digital marketing consists of owned marketing, earned marketing, and paid marketing.

Owned
 marketing encompasses all activities you have full control over. It can be further split into first- and second-party data. First-party data is customer data generated on your site or in your app. Second-party data is customer data generated when your customers interact with your email or push notifications (for example, “Email Opened” or “Push Notification Received”).

Earned
 marketing is when publications, newsletters, or blogs organically create some content that refers to, or promotes you.

Paid acquisition
, like display ads or embedded advertorials, don’t exist on your domain. To track the inbound traffic from both “earned” and paid acquisition sources, Segment uses UTM parameters (and deep links if you’re directing a customer to a specific screen in your mobile app that has the product to purchase).

Track engagement on your email channels

While these are still under “owned” marketing, they happen off your domain. An example is sending an engagement email to your customer base with a call-to-action to visit your store. If you’re using Segment and an email or push notification tool on Segment’s platform, you can easily collect second-party data such as “Email Sent” and “Push Notification Opened”.

Learn more about which 
email
 and 
push notification tools
 Segment supports.

Here are some of the most commonly used and popular events tracked through email and push notifications on Segment:

Email Delivered

Email Opened

Push Notification Received

Push Notification Opened

Deep Link Clicked

If your email tool is not supported on Segment, you can still track email opens with Segment’s tracking pixel. This pixel functions like an advertising pixel in that it embeds an image onto pages where JavaScript and POST requests are disabled.

View a list of tools Segment supports
.

In your email template HTML, include an image tag where the 
src
 is a URL that is carefully constructed to hit Segment’s appropriate endpoint with a JSON payload that is base64 encoded.

An example of the payload that will be sent to Segment upon an email open is:

{

  
"
writeKey
"
:
 
"
YOUR_WRITE_KEY
"
,

  
"
userId
"
:
 
"
025waflo3d65
"
,

  
"
event
"
:
 
"
Email Opened
"
,

  
"
properties
"
:
 
{

    
"
subject
"
:
 
"
Try Our New $10 Toast
"
,

    
"
email
"
:
 
"
andy@segment.com
"

  
}


}



Then, you would base64 encode that and append it to the Segment endpoint:

https://api.segment.io/v1/pixel/track?data=<base64-ENCODED-JSON>


Add the complete URL as the 
src
 in the image tag.

<img src="https://api.segment.io/v1/pixel/track?data=eyJ3cml0ZUtleSI6ICJZT1VSX1dSSVRFX0tFWSIsICJ1c2VySWQiOiAiMDI1cGlrYWNodTAyNSIsICJldmVudCI6ICJFbWFpbCBPcGVuZWQiLCAicHJvcGVydGllcyI6IHsgICAic3ViamVjdCI6ICJUaGUgRWxlY3RyaWMgRGFpbHkiLCAgICJlbWFpbCI6ICJwZWVrQXRNZUBlbWFpbC5wb2tlIiB9fQ">


Learn more about Segment’s Pixel API.

Track earned traffic with UTM Parameters

UTM parameters are types of query strings added to the end of a URL. When clicked, they let the domain owners track where incoming traffic is coming from and understand what aspects of their marketing campaigns are driving traffic.



UTM parameters are only used when linking to your site from outside of your domain. When a visitor arrives to your site using a link containing UTM parameters, Segment’s client-side analytics.js library will automatically parse the URL’s query strings, and store them within the 
context
 object as outlined in the 
Spec: Common
 docs. These parameters do not persist to subsequent calls unless you pass them explicitly.

UTM parameters contain three essential components:

utm_campaign
: This is the name of your campaign. All marketing activities that support this campaign, needs to have the same utm_campaign so that downstream analysis to measure performance for this specific campaign can be done off this primary key. (Example: “national-toastday”)

utm_medium
: How the traffic is coming to your site. Is it through email, a display ad, or an online forum? This ensures Segment’s downstream analysis can easily see which channel performs the best. (Examples: “email”, “paid-display”, “paid-social”, “organic-social”)

utm_source
: Where the traffic is specifically coming from. You can be specific here. This ensures Segment’s downstream analysis can measure which specific source brings the most conversions. (Examples: “twitter”, “customer.io” (email tool), “facebook”, “adroll”)

With these being optional:

utm_content
: For multiple calls to action on a single page, utm_content indicates which one. For example, on a website, there may be three different display ads. While the link on each display ad will have the same utm_campaign, utm_medium, and utm_source, the utm_content will be different. (Examples: “banner”, “left-side”, “bottom-side”)

utm_term
: This is the parameter suggested for paid search to identify keywords for your ad. If you’re using Google Adwords and have enabled “autotagging”, then you don’t need to worry about this. Otherwise, you can manually pass the keywords from your search terms through this parameter so that you can see which keywords convert the most. Note that this parameter is reserved explicitly for search. (Examples: “toast”, “butter”, “jam”)

If you’d like UTM parameters to persist in subsequent calls, you’ll need to manually add those fields in the 
context.campaign
 object of your event call. For example:

analytics
.
page
(
"
97980cfea0067
"
,
 
{},
 
{
 
 
campaign
:
 
{

   
name
:
 
"
TPS Innovation Newsletter
"
,

   
source
:
 
"
Newsletter
"
,

   
medium
:
 
"
email
"
,

   
term
:
 
"
tps reports
"
,

   
content
:
 
"
image link
"

  
},


});



You can also store the values in cookies and/or localStorage and use 
Analytics.js Middleware
 to enrich the payload for subsequent calls.

Learn more about the semantics with each UTM parameter.
 
The key isn’t to stick with the definitions that closely, but to be consistent within your own analytics system.

Proper UTMs use

A marketing campaign is a single marketing message across several platforms, media, and channels, with a consistent and clear call-to-action.

Since the marketing campaign is from off-domain to your storefront (on your property or domain), then it’s critical to use the proper and consistent UTM params across all of your channels:

Emails

Paid acquisition

Guest blog post in partner’s newsletter

Article in the news

Offline events / in real life / meat space

Your UTM parameters would match a pattern such as:

Having the same utm_campaign across all channels

Different utm_source and utm_medium depending on the channel

If you were on paid acquisition, the placement of the display ad would determine what goes in utm_content

If you were using paid search, the term would be utm_term

An example would be a National Toast Day campaign. This campaign would include emails, paid acquisition (with AdRoll and Facebook Ads), organic social (Twitter), and promotional content on partners’ blogs.

Having the consistent UTM parameters naming convention simplifies the downstream analysis and the ease of querying across dimensions, such as within the campaign, which medium or source was the best. Or which placement of the display ad led to the most conversions.

Learn more about measuring ROI of marketing campaigns with SQL and UTM parameters.

What device are they using? Cross-device tracking

It’s common for customers to discover you on their desktop before making the purchase much later on their phone. How do you tie all of these events back to the same customer so you can understand which marketing activities on what screens are responsible for conversions?

Track server-side when possible

Tracking with JavaScript in the browser has its benefits, such as using browser technologies to automatically track things like UTM parameters, referring domain, IP address, and user agent. But here are a few reasons why it might make sense for your store to track on the server side.

Are your customers technically savvy and use ad blockers? Ad blockers restrict requests from a list of blocklisted domains to your browser, which means that none of your event tracking will work properly. If you sell to a technical audience, it is possible that you may be underreporting your analytics by a material amount.

Do you have multiple devices? If you have multiple devices with the same customer check out flow, moving those events to the server-side will reduce your surface area of your code base. This means less maintenance and faster changes.

Learn more about client vs server tracking
.

If you do move key checkout events to the server side, you will have to manually send the data automatically collected by Segment’s client-side JavaScript library to your server. These pieces of tracking data are still important for the following reasons:

UTM parameters
: Collecting the UTM params will allow you to tie conversion events to your marketing campaign or activities. This is valuable in that you can immediately measure performance and calculate ROI on your campaigns.

IP address
: The IP address can provide location intelligence for your customers. This means you can personalize your shopping experience or engagement emails with inventory that might be more relevant depending on your customers’ locations.

User Agent
: The User Agent will inform you of your customers’ preferred device and shopping experience. Are they converting on a mobile web browser? Native app? Or on their laptop?

Learn how to use
context
 to manually send this information on the server side.

Track the same user across devices

If your store allows user registration and users are logged in when they shop on your site or app, then you can track them across devices.

This works by using a 
userId
 instead of an 
anonymousId
 to track key events and where they occur. This 
userId
 serves as the primary key in your downstream tools and data warehouse, allowing you to join all of a profile’s anonymous activities with logged in activities. You also can get a complete picture of a profiles location, and what device they are on while using your app or website.

Learn more about pulling the entire user journey for a single user given a userId.

Unfortunately, tracking the same user across devices only works if they log in to each device. Anonymous browsing in each distinct “experience” (for example, mobile safari, native iPhone, browser on laptop) generates its own unique 
anonymousId
 . Each 
anonymousId
 is limited to the scope of that browser or app, only measuring activities in those sessions. It’s not until the user logs in when the 
userId
 is generated (if registering for a new account) or the 
userId
 is retrieved from your database, and then mapped to the 
anonymousId
 of that session. Segment keeps a table of 
anonymousId
s mapped to a single 
userId
 so you can analyze a user’s activity across multiple devices.

If a user logs in on multiple devices, then you would be able to analyze even the anonymous activity across those devices. Consequently, it’s important to encourage your users to log in so that you have this capability.

Attribute offline conversions to online impressions

One of the biggest challenges for brick-and-mortar stores is to measure the impact of their online advertising campaigns on their in-store purchases. Attributing offline conversions has traditionally been difficult to achieve, due to the lack of offline data and robust infrastructure to route that data.

For Facebook advertisers, 
Facebook Offline Conversions
 allow you to tie offline conversions to your campaigns. It’s important to note that the offline data is labeled to an event set that has been assigned to a Facebook campaign. Here are the two ways to attribute offline conversions to Facebook advertisements:

Uploading offline event data about actions that aren’t captured with Facebook Pixel or App Events to Facebook for them to match actions to your Facebook ads

Enable and configure 
Segment’s Facebook Offline Conversions destination
, which automates attributing offline events to your Facebook ads in real-time

Learn more about the benefits of Segment’s Facebook Offline Conversions destination
.

Most other advertising networks provide some functionality of manually uploading offline data to match with their online advertising data. Here is a short list of other services:

Attributing in-store purchases to an impression from a display ad online is critical to help marketers and advertisers understand which campaigns or creatives are driving sales. The more real-time the data and insights, the more nimble your business can be in altering course so that additional resources can be put towards the right marketing actions.

Learn about the funnel before your website or app

The internet has made it easy for customers to come from nearly anywhere to your digital storefront. But there are ways to track and collect data to better understand these complicated paths so you can be intentional with your marketing efforts to tap into these communities.

By tracking in these locations with the above mentioned techniques, your downstream analysis will also be simpler. With UTM params, you’ll be able to quickly measure the performance of a campaign or a particular channel. By properly tracking on multiple devices, you can understand which shopping experiences are most preferred. These tracking techniques are invaluable to understanding the source of your highest quality customers.

Talk to a product specialist today
 about building a clean, high-quality data spec so you can focus on brand engagement and sales growth.

This page was last modified: 12 Aug 2024

Need support?

Questions? Problems? Need more info? Contact Segment Support for assistance!

Help improve these docs!

Was this page helpful?

Thanks for your feedback!

Can we improve this doc? 
Send us feedback!

Get started with Segment

On this page

Was this page helpful?

Thanks for your feedback!

Can we improve
 this doc? 
Send us feedback!

Product

For Developers

Company

Support

© 2025 Segment.io, Inc.

=== Content from https://segment.com/docs/engage/user-subscriptions/csv-upload/ ===

Update Subscriptions with a CSV
        

Engage Premier requires a Business tier account and includes Engage Foundations and Unify.


See the 
available plans
, or 
contact Support
.

On this page

Engage Premier entered an End of Sale (EOS) period effective  June 10, 2024. Existing Segment customers will continue to have access and support to Engage Premier until an end-of-life (EOL) date is announced. We recommend exploring the following pages in preparation of a migration or future MCM needs:

Twilio Marketing Campaigns

Preferred ISV Partners:

Airship Blog
 


Bloomreach Blog
 


Braze Blog
 


Insider Blog
 


Klaviyo Blog
 


Twilio Engage Foundations Documentation
 

Use the CSV Uploader to add or update user subscription states.

When you upload a CSV file, Engage adds new profiles and updates existing user profiles. Each CSV row corresponds to a user profile and columns to an identifier in your 
identity resolution configuration
.

You can also 
set subscription states
 for each email and phone number that you upload in the CSV. Subscription states help you track which email addresses and numbers you have permission to market to.

Uploading a CSV creates new profiles and updates existing profiles. These profile updates may lead to users entering existing audiences or message campaigns.

Using the CSV Uploader to upload user profiles to Engage will 
not
 increase your MTUs count. 
Learn more
 about MTUs and Engage.

Upload a CSV file

Follow these steps to add subscribers with a CSV file upload

1. Download your CSV template

Click 
Download Template
 to download a CSV template with identifier columns from your identity resolution configuration. Engage adds subscription columns next to email and SMS identifiers, where you can update subscription states for email addresses and phone numbers.

CSV files can only have a single 
email
 and 
phone
 identifier column. Include any additional email addresses or phone numbers for a user profile as a separate row.

Navigate to 
Unify > Unify settings
 and select the 
Identity resolution
 tab to view or add identifiers in your Segment workspace.

2. Fill out your CSV file

Enter values for the identifiers in your CSV file. You can also 
set email, phone, and WhatsApp subscriptions
 using the 
email_subscription_status
, 
sms_subscription_status
, and 
whatsapp_subscription_status
 columns.

A few best practices to keep in mind as you fill out your CSV:

3. Upload your CSV file

Upload a CSV file to Twilio Engage in two ways:

Engage processes CSV rows sequentially. Column values, except for a blank subscription status, override previous values for a user profile.

A blank subscription status in the CSV doesn’t overwrite current 
email
 or 
phone
 
subscription states
 in your Segment space.

4. Name your custom trait

Every time you upload a file, you have the option to add a custom trait to user profiles in the CSV. Use custom traits to help you 
create audiences
 or send messages to a specific group of users. You can also add an existing custom trait name from your Segment workspace to the list of users in the CSV file.

Custom traits
 display in the Custom Traits tab of a user profile in the Profile explorer.

View Update History

Use the Update History page to view CSV file uploads in your workspace over the last 30 days.

To view the Update History page:

View the status of the file upload and the custom trait name added to user profiles in the CSV upload. The error report only shows rows that Segment couldn’t successfully process.

Error reports

Use error reports to fix invalid rows and quickly re-upload data.

From the Update History page:

Engage uses the following error codes on the report:

Validation errors

The following table lists validation errors you may run into with your CSV upload:

Set user subscriptions

Use the CSV Uploader to set subscription states for user email addresses and phone numbers.

Each user profile in a Segment workspace can have multiple email addresses and phone numbers, all with different subscription states.

For each CSV file, Engage adds:

In the 
email_subscription_status
, 
sms_subscription_status
, and 
whatsapp_subscription_status
 columns, set subscription states for email and phone numbers with the following values:

Engage accepts both uppercase and lowercase subscription status values.

Only contact users that subscribe to your communications. View 
User Subscription States
 to learn more.

Message consent

Segment recommends sending to subscribed users. If a recipient deletes or flags an unwanted message as spam, inbox providers might start to filter your messages straight to spam folders. View more SendGrid delivery 
Best Practices
 to prevent email from going to spam.

This page was last modified: 15 Jul 2024

Need support?

Questions? Problems? Need more info? Contact Segment Support for assistance!

Help improve these docs!

Was this page helpful?

Thanks for your feedback!

Can we improve this doc? 
Send us feedback!

Get started with Segment

On this page

Was this page helpful?

Thanks for your feedback!

Can we improve
 this doc? 
Send us feedback!

Product

For Developers

Company

Support

© 2025 Segment.io, Inc.

=== Content from https://segment.com/docs/engage/audiences/account-audiences/ ===

Account-level Audiences
        

Engage Foundations requires a Business tier account and includes Unify.


See the 
available plans
, or 
contact Support
.

On this page

Account-level audiences are audiences for businesses that sell to other businesses. They return the set of accounts which match a combination of account-level traits, user-level traits, and user events. You can sync these accounts and associated users with downstream destinations.

You can use account-level audiences to accomplish the following use cases:

Enable account-level audiences

Account-level audience conditions

A single account-level audience can incorporate any combination of the following condition types:

To access account-level audience conditions:



The three types of user-level conditions are:

You can create conditions which operate on the set of events collectively triggered by all users associated with an account with 
account-level computed and SQL traits
.

Account-level computed and SQL traits

Workspaces with access to account-level audiences can create account-level 
computed
 and 
SQL
 traits. All user-level computed trait types are supported (see the 
Types of computed traits
 docs for a full list). Account-level computed traits operate on the set of events triggered by all users associated with a given account.

Use-cases for account-level computed traits include:

Use SQL traits for complex calculations not supported by computed traits. For example, you would use SQL traits to calculate the number of unique users associated with an account who have logged in during the past month.

Use account-level SQL traits to associate users to an account

To associate users to an account with SQL traits, you must return both the 
group_id
 and 
user_id
 in the account level SQL trait. This fires a Group call which Segment uses to add users to groups in destinations.

When a group (account) contains more than one user, the query returns duplicate 
group_id
s (mapped to unique 
user_id
s). However, Segment doesn’t return duplicate 
group_id
s in the account-level SQL trait. As a result, you can’t map users to accounts in a many-to-many situation.

Use account-level computed and SQL traits as account-level audience conditions

Once created, you can connect account-level computed and SQL traits to downstream destinations. You can also use them as conditions in account-level audiences, enabling you to build audiences based on the set of events triggered by all users associated with a given account.

For example, you can create an audience which selects all accounts where users associated with the account have collectively logged in fewer than 10 times in the past 30 days. To accomplish this, you would:

Create an account-level event counter computed trait, selecting the “Logged In” event and specifying a time window of the past 30 days.

This computed trait will count all “Logged In” events collectively triggered by users associated with each account over the past 30 days, and append the resulting counts to each Account profile.

Create an account-level audience, containing a single condition using the new computed trait (for example, 
logins_past_30_days<10
).

Connect account-level audiences, computed traits, and SQL traits to destinations

When you connect an account-level audience or trait to a destination, you select which events to send to the destination when the audience or trait is calculated. Each destination supports a subset of the following events:

Enable group calls when you want to update a destination’s account records based on audience membership. Enable identify calls when you want to update a destination’s user records based on audience membership.

Use account-level traits in user-level audiences

Account-level audiences let you target all users associated with accounts that match your audience criteria. However, you may need to target a subset of users based on the traits of their associated accounts.

Account-level traits are not available in the user-level audience builder. However, account-level audience membership is available in user-level audiences through the “Part of an audience” condition. This enables you to use account-level audiences as a “passthrough” for account-level traits.

For example, you may wish to create an audience which selects all admin-level users associated with accounts in the software industry (for example, 
user.role=Admin AND account.industry=Software
). To accomplish this, you would:

Known limitations of account-level audiences

If you find that these limitations impede your ability to use account-level audiences, contact 
friends@segment.com
 with details about your use case.

This page was last modified: 12 Aug 2024

Need support?

Questions? Problems? Need more info? Contact Segment Support for assistance!

Help improve these docs!

Was this page helpful?

Thanks for your feedback!

Can we improve this doc? 
Send us feedback!

Get started with Segment

On this page

Was this page helpful?

Thanks for your feedback!

Can we improve
 this doc? 
Send us feedback!

Product

For Developers

Company

Support

© 2025 Segment.io, Inc.

=== Content from https://segment.com/docs/engage/journeys/step-types/ ===

Journeys Step Types
        

Engage Foundations requires a Business tier account and includes Unify.


See the 
available plans
, or 
contact Support
.

On this page

On this page, you’ll find information about the steps you can add to a Journey.

Conditions and delays

Journeys has two steps that you can use to determine how and when users move to the following step.

The 
Add a condition
 step defines the conditions that a user must satisfy to move from one step to the next. You can define new conditions or import conditions from an existing audience.

The 
Add a delay
 step defines the length of time in minutes, hours, days, or weeks that a user must wait before moving to the next step.

Flow control steps

Journeys offers four steps that help you control how users flow through your Journey.

True/false splits

A true/false split divides the previous step’s user group into two branches, based on Boolean logic against a defined condition. Users who satisfy the condition(s) move to the 
True
 branch. Otherwise, they move to the 
False
 branch. To enforce mutual exclusivity, Journeys evaluates true/false conditions when a user reaches the relevant step.

You can add Step Names to describe the users who end up in both the True and False branches.

Multi-branch splits

Multi-branch split
 divides the group of users from a previous step into two or more branches based on each branch’s defined conditions.

Define the number of branches you want to create, then add an 
Add a condition
 step to define each branch’s condition.

Journeys doesn’t enforce mutual exclusivity in branch conditions. For more information about ensuring branch exclusivity, see 
Best Practices
.

Randomized splits

A randomized split lets you experiment with and test the performance of a Journey’s branches. When you create a randomized split, you add up to five Journey branches, each with a different step. Journeys then sends eligible users down one of the branches at random. Each branch receives a portion of the eligible users based on percentages that you assign to the branches.

If the Journey has a re-entry condition, users will join the same split branches upon re-entry.

To test your messaging channels, for example, you might create a randomized split with three different branches, assigning 40% of users to an email campaign, 40% to an SMS campaign, and 20% to a control group. Once users flow through the split, you can determine the success of the email and SMS campaigns compared to each other and the control group.

Follow these steps to add a randomized split to a Journey:

Users who meet the Journey’s entry condition will then enter the Journey and flow through the randomized split.

Once users complete your Journey’s randomized split step, you’ll have insight into how each split performed. You can take action on the results by cloning the Journey and sending a new set of users through the highest performing branch.

Connecting to existing steps

You can merge split Journey branches by using the 
Connect to existing steps
 option. Connecting to existing steps lets you apply a single step to more than one group. For example, you may want to target some Journey group members with email campaigns while targeting others with ad campaigns. Instead of duplicating steps, you can connect these steps to steps that already exist.

Keep the following in mind when connecting to existing steps:

Follow the instructions below to connect branches to an existing step:

Actions steps

With Journey actions steps, you can send marketing campaigns to groups of users and deliver Journey information to downstream tools.

Show an ad

The 
Show an ad
 step lets you send users to an advertising destination. You can also configure exit settings that remove users from the ad step after specific periods of time.

For example, you may want to show an ad for only one week to users who abandoned a cart during a purchase. With the Show an ad step, you can remove users from the ad destination seven days after they enter it.

Ad-based exit settings

Ad step exit settings don’t impact other Journey steps. A user can exit an ad step but remain in the overall Journey. For more on Journeys exit settings, view 
Journey exit and re-entry times
.

Follow these steps to add a Show an ad step to a Journey:

Channels steps

The 
Send an email
, 
Send an SMS
, and 
Send a WhatsApp
 steps are only available on 
Engage Premier
.

Use Twilio Engage to send email as a step in a Journey.

To send email in Engage, you must connect a 
SendGrid subuser account
 to your Segment space. Visit the 
onboarding steps
 for more information.

Subscribed
 users will receive an email upon entering the step. To send an email to users regardless of their subscription state, you can use Engage to 
send a message to all users
. Visit 
Email Campaigns
 for more information.

Use Twilio Engage to send an SMS message as a step in a Journey.

To send SMS in Engage, you must connect a Twilio messaging service to your Segment workspace. Visit the 
onboarding steps
 for more information.

As soon as a 
subscribed
 user enters the Send SMS step, they’ll receive the text. Visit 
SMS Campaigns
 for more information.

Use Twilio Engage to send a WhatsApp message as a step in a Journey.

WhatsApp Public Beta

WhatsApp as an Engage channel is in public beta.

Send to Destinations

The 
Send to Destinations
 step delivers information about the Journey to the selected Destination. For more information, see 
Send data to Destinations
.

This page was last modified: 07 Feb 2025

Need support?

Questions? Problems? Need more info? Contact Segment Support for assistance!

Help improve these docs!

Was this page helpful?

Thanks for your feedback!

Can we improve this doc? 
Send us feedback!

Get started with Segment

On this page

Was this page helpful?

Thanks for your feedback!

Can we improve
 this doc? 
Send us feedback!

Product

For Developers

Company

Support

© 2025 Segment.io, Inc.

=== Content from https://segment.com/docs/engage/content/email/html-editor/ ===

HTML Editor
        

On this page

Engage Premier entered an End of Sale (EOS) period effective  June 10, 2024. Existing Segment customers will continue to have access and support to Engage Premier until an end-of-life (EOL) date is announced. We recommend exploring the following pages in preparation of a migration or future MCM needs:

Twilio Marketing Campaigns

Preferred ISV Partners:

Airship Blog
 


Bloomreach Blog
 


Braze Blog
 


Insider Blog
 


Klaviyo Blog
 


Twilio Engage Foundations Documentation
 

Use the HTML Editor to design your email template with both code and visual editing capabilities. Build your email template with code, copy and paste existing code, or use the Visual Editor for a code free design experience.

On this page, you’ll learn how to use the HTML Editor to build personalized email templates for your Engage campaigns.

Getting started

You can navigate to the HTML Editor in two ways:

From the 
Select Editor
 screen, select 
HTML Editor
 and click 
Build Email
.

Visual Editor

Use the Visual Editor for a no-code option to design your email. With the Visual Editor, you can:

Engage updates any changes you make in the Visual Editor to the HTML Editor in real-time.

Insert an image

To insert an image from the Visual Editor:

Preview for desktop or mobile display

To preview your email template, click the preview icon in the Visual Editor toolbar.

From the preview screen, you can toggle between desktop or mobile to view the email in both displays.

HTML Editor

Use the HTML Editor to maintain your email template with code. Copy and paste existing code or build a new template in the editor.

Engage displays any changes you make in a preview screen to the right of your code. You can preview your email in both desktop and mobile display.

Click 
Format
 at any time to properly indent and format your code in the HTML Editor.

When you toggle from the HTML Editor to the Visual Editor, Engage may make minor changes to your code formatting. If Engage re-formats your code, it will not affect the email layout.

Error flagging and content validation

Engage displays in-line error flags in the code editor to help you debug your code. If there are errors, you might not see content as expected in the preview screen until you’ve debugged your code.

For all content editors in Engage, you’ll see alerts for any issues in your template, such as invalid profile traits or incorrect 
liquid syntax
. Engage both flags template issue(s), and displays recommended next steps. While you can save these templates, you must fix any issues before using them in Engage campaigns.

Personalize with merge tags

Add merge tags to personalize your message with user profile traits.

You can also add merge tags to your email right from the code editor.

Liquid templating

Engage supports liquid templating to create dynamic content in the HTML Editor.

For example, use  
{% if %}
, 
{% elseif %}
, and 
{% else %}
 tags to call a product by name if known, or use a default message:

{% if profile.traits.product_title == "Sneakers" %}
  Hey, view our latest sneakers!
{% elsif profile.traits.product_title == "Sandals" %}
  Hey, check out these sandals!
{% else %}
  Hey, check out our latest footwear.
{% endif %}


If you use liquid templating, be sure to 
test your email
 to make sure that everything renders properly.

While both the HTML and Visual Editor support liquid templating, Segment recommends using the HTML Editor to write liquid templating.

Engage doesn’t support liquid template syntax that produces partial blocks of HTML.

To view more examples related to your use case, visit the 
LiquidJS docs
.

Add unsubscribe links

It’s always best practice to include an unsubscribe link in the emails you build. Engage adds an unsubscribe link to email templates, which you can edit at any time.

You can add unsubscribe links from the visual or HTML Editor.

From the Visual Editor:

To add a link from the code editor, use 
<a href = "[unsubscribe]"> </a>
 in your HTML.

For more on email unsubscribe links, view SendGrid’s 
best practices
.

Toggle between editors

From the editor screen, you can click 
Use HTML Editor
 or 
Use Visual Editor
 to toggle between the two editors.

The Visual Editor renders your HTML in an editable preview (similar to an email client), so you might need to accept formatting changes to your HTML to use the Visual Editor. In this case, Segment displays a confirmation modal with HTML differences.

Potential HTML changes include formatting, removing attributes with potentially unsuported scripts in your HTML (for example, 
onclick
 or 
onblur
), attribute reordering, and adding missing tags.

If you don’t want to accept the changes required to use the Visual Editor, you can continue editing in the HTML Editor.

Formatting your HTML

In the HTML Editor, you can use the 
Format
 button to properly indent and format your code. Note that the Format button may not implement all changes necessary to use the Visual Editor.

Save the template

After you design the email, click 
Create Email Template
. You can navigate to 
Engage > Content > Templates
 to view and maintain your email template.

Next steps

Learn more about 
building email templates
 to include in your Engage campaigns.

You can also learn about the 
Drag and Drop Editor
 in Engage to build Email templates with drag and drop functionality.

Once you create an email with the HTML Editor, you can’t modify it with the Drag and Drop Editor, and vice versa.

This page was last modified: 15 Jul 2024

Need support?

Questions? Problems? Need more info? Contact Segment Support for assistance!

Help improve these docs!

Was this page helpful?

Thanks for your feedback!

Can we improve this doc? 
Send us feedback!

Get started with Segment

On this page

Was this page helpful?

Thanks for your feedback!

Can we improve
 this doc? 
Send us feedback!

Product

For Developers

Company

Support

© 2025 Segment.io, Inc.

=== Content from https://segment.com/docs/getting-started/whats-next/ ===

What's Next
        

On this page

You’re just getting started with Segment, but there’s so much more to explore!

Privacy tools and filtering

Segment includes a free suite of Privacy tools to help your organization comply with regulations like the GDPR and the CCPA.

The 
Privacy Portal
 allows you to easily audit, monitor, and enforce privacy rules against your Segment data, to proactively protect your customers.

Improve data quality with Protocols

You had a taste of the planning needed to set up clear, consistent, reliable and extensible data schemas in 
Planning a Full Install
.

Business tier customers can use Segment’s 
Protocols
 package to help with this process, to keep track of what data is being collected where, and to normalize their data as it flows through Segment. Clean, consistent data helps you move faster to build marketing campaigns and act on analytics insights.

With Protocols, you can use 
Tracking Plans
 to build consensus in your organization about which events and property you intend to collect across your web, mobile or server-side data sources. Once defined, you can connect the Tracking Plan to your Sources to automatically 
validate
 the data is flowing correctly. You can also turn on 
enforcement
 to block bad data, and even fix incorrect data with 
Transformations
.

Single view of the customer with Engage

Engage
 is a powerful personalization platform that enables you to create unified customer profiles in Segment, to build and enrich audiences, and to activate audiences across marketing tools.

With Engage, you can create unified customer profiles, enrich those profiles with new traits, build Audiences using those profiles, and sync audiences to marketing tools to power personalized experiences, and better understand and market to your customers.

More learning resources

Segment University
 is Segment’s free, online classroom for learning the basics of Segment.

Analytics Academy
 is a series of lessons designed to help you understand the value of analytics as a discipline, and to help you think through your analytics needs, and get started creating robust and flexible analytics systems to help you grow.

Need ideas or prior art? 
Segment Recipes
 are some cool things you can do by hooking your Segment workspace up to different Destination tools. Everything from sending tailored onboarding emails, to joining and cleaning your data with third party tools

Other Resources

Still hungry for more? Check out our list of 
other Segment Resources
!

Technical Support

If you’re experiencing problems, have questions about implementing Segment, or want to report a bug, you can fill out our 
support contact form here
 and our Product Support Engineers will get back to you.

You need a Segment account in order to file a support request. If you don’t already have a Segment account, you can sign up for a free workspace.

Back to the Getting Started index

This page was last modified: 07 Feb 2025

Need support?

Questions? Problems? Need more info? Contact Segment Support for assistance!

Help improve these docs!

Was this page helpful?

Thanks for your feedback!

Can we improve this doc? 
Send us feedback!

Get started with Segment

On this page

Was this page helpful?

Thanks for your feedback!

Can we improve
 this doc? 
Send us feedback!

Product

For Developers

Company

Support

© 2025 Segment.io, Inc.

=== Content from https://segment.com/docs/guides/how-to-guides/segment-and-attribution/ ===

Segment's Role in Attribution
        

On this page

At a higher level, attribution tools allow you to connect a specific campaign to user acquisition, giving you more visibility into campaign performance.  See 
the destination catalog
 for a list of attribution tools that Segment supports. 

There are three stages of mobile attribution as it relates to Segment. 

Customer installs your app

The install is attributed by an attribution provider (Adjust, AppsFlyer, etc)

Attribution information is sent back to Segment

Here is a bit more information on what is happening at each of those stages. 

Customer installs your app

When 
lifecycle events
 are enabled, the 
Application Installed
 and 
Application Opened
 events are triggered on the first app open after the app is installed.  Note, if the app is deleted and then later reinstalled on the device, these events will be triggered again on first app open. 

Situations where install counts look lower in Segment than in other tools. 

Some tools, like iTunes or Google Play, count install on download rather than on app open like Segment.  iTunes and Google Play is able to easily collect data on download but not as easily able to collect first-party data on app open. Whereas other tools, such as Segment, need their SDK to be loaded in app and initialized on app open before they are able to collect the install information. For example, if a user downloads your app but does not open it, the install will be counted in iTunes/Google Play but not counted in Segment or other tools.

Situations where install counts look higher in Segment than in other tools

Many tools deduplicate install data. Some tools only allow one install event per lifetime of deviceId. Others deduplicate by deviceId accepting only one install per UTC day.  Each and every tool is different.  

Segment, on the other hand, does not deduplicate.  We don’t believe our role in your data pipeline should be deduping particular events.  In fact, there may be situations where you may want to account for multiple 
Application Installed
 events such as: user sells their phone, user uninstalls and later decides to reinstall, etc. It is better to think about the 
Application Installed
 data in your Segment warehouse as the raw source of data, giving you flexibility to query 

For more information on how installs are counted in different tools, here are a few resources from our partners: 

Adjust - Discrepancies and Why Data Does not Always Match Up

The install is attributed by an attribution provider

Device-Mode Connection

When you enable an attribution destination in device-mode, our integration code will also load that tool’s SDK. Upon app launch, the destination’s SDK will send install information which is then use to attribute that install to a campaign on their backend.  Segment loads the destination’s SDK, but attribution happens outside of Segment. 

Cloud-Mode Connection

Destination receives the 
Application Installed
 event and attributes the installation on their backend. 

Attribution information is sent back to Segment

Device-Mode Connection

For tools that support this, if you have enabled “Track Attribution Data” in your Segment dashboard, our integration listens to the attribution tool’s SDK for a change in attribution state. 
Note
: Not all device-mode attribution tools offer “Track Attribution Data” functionality.  See the settings section for a particular tool in your Segment dashboard for confirmation. 

When there is a change in attribution state, the integration code triggers an 
Install Attributed
 call to be sent back to your Segment source (and on to all other enabled destinations - in device and cloud-mode).  

Here is an example of how that call is triggered in the 
AppsFlyer integration code
. This is the similar for other attribution providers such as 
Adjust
. 

Cloud-Mode Connection

For tools that support server-side postback, after install is attributed, an 
Install Attributed
 event is triggered and sent server-side to your Segment source and forwarded on to all enabled cloud-mode destinations. 

Example 
Install Attributed
 event: 

analytics
.
track
(
'
Install Attributed
'
,
 
{


 
 
provider
:
 
'
Tune/Adjust/AppsFlyer
'
,


 
 
campaign
:
 
{


 
 
 
 
source
:
 
'
Network/FB/AdWords/MoPub/Source
'
,


 
 
 
 
name
:
 
'
Campaign Name
'
,


 
 
 
 
content
:
 
'
Organic Content Title
'
,


 
 
 
 
ad_creative
:
 
'
Red Hello World Ad
'
,


 
 
 
 
ad_group
:
 
'
Red Ones
'


 
 
}


});



For more detailed information on a particular attribution destination and functionality, see our 
Destinations docs
.

This page was last modified: 07 Feb 2023

Need support?

Questions? Problems? Need more info? Contact Segment Support for assistance!

Help improve these docs!

Was this page helpful?

Thanks for your feedback!

Can we improve this doc? 
Send us feedback!

Get started with Segment

On this page

Was this page helpful?

Thanks for your feedback!

Can we improve
 this doc? 
Send us feedback!

Product

For Developers

Company

Support

© 2025 Segment.io, Inc.

=== Content from https://segment.com/docs/guides/audiences-and-journeys/ ===

Audiences, Journeys, and Broadcasts
        

Engage Foundations requires a Business tier account and includes Unify.


See the 
available plans
, or 
contact Support
.

On this page

Audiences, Journeys, and Broadcasts are fundamental to 
Twilio Engage
 and let you segment your users, send them personalized content, and show them ads from platforms like Facebook or Google.

In this guide, you’ll learn how to choose between an Audience, a Journey, and a Broadcast for a number of marketing use cases across the customer lifecycle.

Back to basics

First, consider the following definitions for an Audience, Journey, and Broadcast.

Audience

In Engage, an 
Audience
 is a group of users that share certain characteristics. When you create an Audience, you group users who meet certain conditions, like having performed an event or having a 
Computed Trait
.

Once you’ve created an Audience, you can sync it to 
marketing automation tools
, 
ads platforms
, 
analytics tools
, or 
data warehouses
. Depending on the Audience’s conditions and 
connected Destination(s)
, Segment syncs the Audience’s users in batches or in real time, as they meet the Audience’s conditions.

Journey

A 
Journey
 is a logic-driven workflow that progresses users through steps based on conditions and time delays. You add users to a Journey with an entry condition, then users progress through 
the Journey’s steps
 based on conditions you define during Journey setup.

As with Audiences, Segment can sync users to Destinations at designated points in the Journey. Unlike an Audience, a Journey can send users to Twilio Engage’s 
native email and SMS channels
.

Broadcast

A 
Broadcast
 is a one-time SMS or email campaign sent to a group of users. Whereas Segment continously updates Audience membership, Segment only calculates the users who will receive your Broadcast once. Marketers commonly use Broadcasts for newsletters, promotional campaign, and events.

Engage and the customer lifecycle

The customer lifecycle provides a helpful framework for thinking about Audiences, Journeys, and Broadcasts.



Audiences and Broadcasts tend to be most effective at the top of the customer lifecycle funnel, where brand awareness and discovery occurs.

A Journey becomes a better option as customers progress down the funnel, where a more complex strategy involving messaging, social ads, and newsletters helps move customers closer to conversion.

Choosing between Audiences, Journeys, and Broadcasts

With the customer lifecycle in mind, use the following table as a starting point for selecting an Audience or Journey for common marketing use cases:

While these suggestions will work for most use cases, you may need to consider other factors before you implement your own campaign. Asking the following questions will help you identify the right approach.

Over the course of a campaign, how many touchpoints do I want to create?

Audiences and Broadcasts work best for single, one-off messages or touchpoints. If you need a campaign with time delays and branching logic, opt for a Journey.

For example, an Audience works well if you want to show a single ad when a user abandons a cart. If, however, you want to show an ad, wait several days, then send the user an email if they’ve not completed their purchase, go with a Journey.

Do I want to use Engage Premier Channels like SMS and email?

You can message users with 
Engage Premier Channels
. If you’d like to send an SMS or email campaign to a customer, use a Journey.

Do I need branching logic?

Create a Journey if you want to incorporate branching logic into your campaign.

Do I want to conduct an A/B test or create a holdout group?

A number of Journeys step types, like 
randomized splits
, let you run experiments and test your campaigns. If you want to experiment with different groups, use a Journey.

Do I want my customers to receive the same campaign more than once?

With Journeys, you can allow customers to 
re-enter a Journey they’ve exited
 or restrict them to a one-time Journey.

Audiences, on the other hand, admit users whenever they meet the Audience’s criteria. For example, you may want to retarget a user with an ad whenever they view a page on your website. In this case, an Audience works well since the user can re-enter the Audience regardless of how many times they’ve already done so.

Putting it together

With this guidance in mind, take your next steps with Engage by learning 
how to build a Journey
, 
work with Engage Audiences
, and 
send a Broadcast
.

This page was last modified: 12 Jun 2023

Need support?

Questions? Problems? Need more info? Contact Segment Support for assistance!

Help improve these docs!

Was this page helpful?

Thanks for your feedback!

Can we improve this doc? 
Send us feedback!

Get started with Segment

On this page

Was this page helpful?

Thanks for your feedback!

Can we improve
 this doc? 
Send us feedback!

Product

For Developers

Company

Support

© 2025 Segment.io, Inc.

=== Content from https://segment.com/docs/connections/spec/track/ ===

Spec: Track
        

On this page

The Track API call is how you record any actions your users perform, along with any properties that describe the action.

Check out our high-level overview of the Track method in Segment University. (Must be logged in to access.)

Each action is known as an event. Each event has a name, like 
User Registered
, and properties. For example, a 
User Registered
 event might have properties like 
plan
 or 
accountType
. Calling Track in one of our 
sources
 is one of the first steps to getting started with Segment.

Here’s the payload of a typical Track call with most 
common fields
 removed:

{

  
"type"
:
 
"track"
,

  
"event"
:
 
"User Registered"
,

  
"properties"
:
 
{

    
"plan"
:
 
"Pro Annual"
,

    
"accountType"
 
:
 
"Facebook"

  
}


}



And here’s the corresponding JavaScript event that would generate the above payload:

analytics
.
track
(
"
User Registered
"
,
 
{

  
plan
:
 
"
Pro Annual
"
,

  
accountType
:
 
"
Facebook
"


});



Based on the library you use, the syntax in the examples might be different. You can find library-specific documentation on the 
Sources Overview
 page.

Beyond the common fields, the Track call has the following fields:

Example

Here’s a complete example of a Track call:

{

  
"anonymousId"
:
 
"23adfd82-aa0f-45a7-a756-24f2a7a4c895"
,

  
"context"
:
 
{

    
"library"
:
 
{

      
"name"
:
 
"analytics.js"
,

      
"version"
:
 
"2.11.1"

    
},

    
"page"
:
 
{

      
"path"
:
 
"/academy/"
,

      
"referrer"
:
 
""
,

      
"search"
:
 
""
,

      
"title"
:
 
"Analytics Academy"
,

      
"url"
:
 
"https://segment.com/academy/"

    
},

    
"userAgent"
:
 
"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_0) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/46.0.2490.86 Safari/537.36"
,

    
"ip"
:
 
"108.0.78.21"

  
},

  
"event"
:
 
"Course Clicked"
,

  
"integrations"
:
 
{},

  
"messageId"
:
 
"ajs-f8ca1e4de5024d9430b3928bd8ac6b96"
,

  
"properties"
:
 
{

    
"title"
:
 
"Intro to Analytics"

  
},

  
"receivedAt"
:
 
"2015-12-12T19:11:01.266Z"
,

  
"sentAt"
:
 
"2015-12-12T19:11:01.169Z"
,

  
"timestamp"
:
 
"2015-12-12T19:11:01.249Z"
,

  
"type"
:
 
"track"
,

  
"userId"
:
 
"AiUGstSDIg"
,

  
"originalTimestamp"
:
 
"2015-12-12T19:11:01.152Z"


}



Create your own Track call

Use the following interactive code pen to see what your Track calls look like with user-provided information:

 Sample Track call


Sample output goes here!


Identities

The User ID is a unique identifier for the user performing the actions. Check out the 
User ID docs
 for more detail.

The Anonymous ID can be any pseudo-unique identifier, for cases where you don’t know who the user is, but you still want to tie them to an event. Check out the 
Anonymous ID docs
 for more detail.

Note: In our browser and mobile libraries a User ID is automatically added
 from the state stored by a previous 
identify
 call, so you do not need to add it yourself. They will also automatically handle Anonymous IDs under the covers.

Event

Every Track call records a single user action. Segment calls these “events”, and recommend that you make your event names human-readable, so that everyone on your team (even you, after all that caffeine) can know what they mean instantly.

Don’t use nondescript names like 
Event 12
 or 
TMDropd
. Instead, use unique but recognizable names like 
Video Recorded
 and 
Order Completed
.

Segment recommends event names built from a noun and past-tense verb.

For more information about best practices in event naming, check out Segment’s 
Analytics Academy lesson on best practices for naming conventions for clean data
.

Segment has standardized a series of reserved event names that have special semantic meaning. We map these events to tools that support them whenever possible. See the 
Semantic Events docs
 for more detail.

Properties

Properties are extra pieces of information you can tie to events you track. They can be anything that will be useful while analyzing the events later. Segment recommends sending properties whenever possible because they give you a more complete picture of what your users are doing.

Segment has reserved some properties that have semantic meanings, and handle them in special ways. For example, we always expect 
revenue
 to be a dollar amount that we send to tools that handle revenue tracking.

You should 
only use reserved properties for their intended meaning
.

The following are all of the reserved properties Segment has standardized that apply to all events. Check out the 
Semantic Events docs
 for properties specific to individual reserved events.

Note:
 You might be used to some destinations recognizing special properties differently. For example, Mixpanel has a special 
track_charges
 method for accepting revenue. Luckily, you don’t have to worry about those inconsistencies. Just pass along 
revenue
.  
Segment will handle all of the destination-specific conversions for you automatically.
 Same goes for the rest of the reserved properties.

Sending Traits in a Track Call - Destination Actions

All events have the ability to include additional event data in the 
context
 object. There may be instances when your team may want to include user traits or group traits in a Track event, such as having a single event trigger multiple events in an Actions destination. Since user Traits are not standard fields for a Track event, in order to do this, you’ll need to explicitely pass the user’s traits into the event payload’s 
context.traits
 object.

For instructions on how to pass fields to the context object for a specific library, please see the related library’s Source documentation.

Segment’s Actions destinations allows your team to build individual actions that are triggered based on a set of configured conditions. By adding the user’s latest traits to the Track event’s 
context.traits
 object, its possible to build two separate Actions to be triggered by this single event. For example, if your team would like to send an Identify event anytime the specific Track event “Button Clicked” is triggered, simply add the available traits into the Track event’s payload, then build a destination Actions for the Track event : 
Event Name is Button Clicked
, and a destination Action for the Identify event : 
All of the following conditions are true: Event Name is Button Clicked, Event Context traits exists
, and then both Actions will have access to reference the 
context.traits
 fields within its mappings.

For more information on the context object, please see the 
Spec: Common Fields
 documentation.

If the 
Example Payload
 shared above is modified as the event 
Button Clicked
 with 
"username": "testing-123"
 in the 
context.traits
 object, then the event’s payload would be :

{
  "anonymousId": "23adfd82-aa0f-45a7-a756-24f2a7a4c895",
  "context": {
    "library": {
      "name": "analytics.js",
      "version": "2.11.1"
    },
    "page": {
      "path": "/academy/",
      "referrer": "",
      "search": "",
      "title": "Analytics Academy",
      "url": "https://segment.com/academy/"
    },
    "userAgent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_0) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/46.0.2490.86 Safari/537.36",
    "ip": "108.0.78.21",
    "traits": {
      "username": "testing-123"
    }
  },
  "event": "Button Clicked",
  "integrations": {},
  "messageId": "ajs-f8ca1e4de5024d9430b3928bd8ac6b96",
  "properties": {
    "title": "Intro to Analytics"
  },
  "receivedAt": "2015-12-12T19:11:01.266Z",
  "sentAt": "2015-12-12T19:11:01.169Z",
  "timestamp": "2015-12-12T19:11:01.249Z",
  "type": "track",
  "userId": "AiUGstSDIg",
  "originalTimestamp": "2015-12-12T19:11:01.152Z"
}


Here’s what that Identify Action would look like : 


Context

Context is a dictionary of extra information that provides useful context about a datapoint, for example the user’s 
ip
 address or 
locale
. You should 
only use
 Context fields for their intended meaning.

This page was last modified: 12 Apr 2024

Need support?

Questions? Problems? Need more info? Contact Segment Support for assistance!

Help improve these docs!

Was this page helpful?

Thanks for your feedback!

Can we improve this doc? 
Send us feedback!

Get started with Segment

On this page

Was this page helpful?

Thanks for your feedback!

Can we improve
 this doc? 
Send us feedback!

Product

For Developers

Company

Support

© 2025 Segment.io, Inc.

=== Content from https://segment.com/docs/engage/warehouses/ ===

Engage and Warehouses
        

Engage Foundations requires a Business tier account and includes Unify.


See the 
available plans
, or 
contact Support
.

On this page

Engage provides a complete, up-to-date view of your users customer journey as it unfolds, and one of the best ways to understand the data produced by this journey is by analyzing the data in your data warehouse using SQL.

With Engage, you can send Computed Traits and Audiences to a data warehouse like Redshift, BigQuery, or Snowflake. This allows you to perform analysis and reporting around key customer audiences and campaigns, as well set up your user data as input into predictive models.

Segment makes it easy to load your customer profile data into a clean schema, so your analysts can help answer some of your toughest business questions.

Set up

When you build an audience or computed trait, you can configure it to send an identify call or a track call to your data warehouse, and additionally include mobile ids.



Identify calls for audiences

If you chose to send your Engage data as an identify call, Engage usually sends one call per user.

When you send 
audiences
 as an identify call, Engage includes a boolean trait that matches the audience name. When a user enters an audience the boolean is set to 
true
, and when they exit, the boolean is set to 
false
.

In the example below, you can see that the 
identify
 payload includes a trait of the audience 
first_time_shopper
 with the value of 
true.

{

  
"
type
"
:
 
"
identify
"
,

  
"
userId
"
:
 
u123
,

  
"
traits
"
:
 
{

     
"
first_time_shopper
"
:
 
true
 
// false when a user exits the audience

  
}


}



Identify calls for computed traits

When you send 
computed traits
 as an identify call, Engage sends a similar call with the computed value for that trait. In the example below, the trait 
total_revenue_180_days
 includes the calculated value of 
450.00
.

{

  
"
type
"
:
 
"
identify
"
,

  
"
userId
"
:
 
u123
,

  
"
traits
"
:
 
{

     
"
total_revenue_180_days
"
:
 
450.00

  
}


}



Warehouse schema for Engage identify calls

Engage identify calls appear in your warehouse using a similar format as normal Connections identify calls. Identify calls appear in two tables per Engage space. These tables are named with a prefix of 
engage_
, then the Engage space name, followed by 
identifies
 or 
users
. The 
identifies
 table contains a record of every identify call, and the 
users
 table contains one record per 
user_id
 with the most recent value.

The 
engage_
 schema name is specific to the Engage space and cannot be modified. Additional audiences and computed traits appear as additional columns in these tables.

engage_default.identifies

engage_default.users

Track calls for audiences

When you send 
audiences
 using track calls, Engage sends an 
Audience Entered
 event when a user enters, and an 
Audience Exited
 event when the user exits, by default. These event names are configurable.

Engage also sends two event properties about the audience: the 
audience_key
, which records the name of the audience that the event modifies, and the audience name and its value, as a separate key and value pair. The value of the audience key is populated with a boolean value.

In the example below, you can see that the 
audience_key
 is set to record a modification to the  
first_time_shopper
 audience, and the 
first_time_shopper
 value is set to 
true
.

{

  
"
type
"
:
 
"
track
"
,

  
"
userId
"
:
 
u123
,

  
"
event
"
:
 
"
Audience Entered
"
,

  
"
traits
"
:
 
{

     
"
audience_key
"
:
 
"
first_time_shopper
"
,

     
"
first_time_shopper
"
:
 
true

  
}


}



Track calls for computed traits

When you send 
computed traits
, Engage sends a 
Trait Computed
 event that records which computed trait it updates, then records the updated key and value. You can also customize this event name.



In the example below, the Trait Computed event contains the 
trait_key
 which records which computed trait is being modified, and then includes the key 
total_revenue_180_days
 with the updated value of 
450.00
.

{

  
"
type
"
:
 
"
track
"
,

  
"
userId
"
:
 
u123
,

  
"
event
"
:
 
"
Trait Computed
"
,

  
"
traits
"
:
 
{

     
"
trait_key
"
:
 
"
total_revenue_180_days
"
,

     
"
total_revenue_180_days
"
:
 
450.00

  
}


}



Warehouse schema for Engage track calls

Similar to track calls in Connections, Engage track calls appear in your warehouse as one table per event name. For example, if you configure your events called 
Audience Entered
, 
Audience Exited
, and 
Trait Computed
, Engage would create tables like the following examples in your warehouse:

engage_default.audience_entered

engage_default.audience_exited

engage_default.trait_computed

Users table

The users table is an aggregate view based on the 
user_id
 field. This means that anonymous profiles with just an 
anonymous_id
 identifier aren’t included in this view. You can still view identify calls for anonymous audiences and computed traits in the 
identifies
 table.

The users table is synced as soon as the warehouse is connected as a destination in Engage, if you’ve previously created Engage computations. As a result, the table might contain data from computations not directly connected to the warehouse.

Sync frequency

Although Engage can compute audiences and traits in real-time, these calculations are subject to the sync schedule allowed by your warehouses plan, which is usually hourly. You can check the warehouse sync history to see details about past and upcoming syncs. When you look at the sync schedule, sources with the 
engage_
 prefix sync data from Engage.

Common questions

Can I prevent a table, a computed trait, or audience from syncing to my warehouse?

Yes. You can use 
Warehouses Selective Sync
 to manage which traits, audiences, and tables get synced from Engage.

Why are there multiple schemas prefixed with 
engage_
 in my warehouse when I only have one space?

Segment can only connect a source to one instance of each destination. For example, one source cannot send to two different Amplitude instances. As a workaround, Engage creates multiple sources to send events to the destinations connected to your space.

For example, if you have three webhook destinations in your space, Engage creates three different sources to send events to them. This creates three different warehouse schemas, and is usually the reason you have more schemas than spaces.

This approach doesn’t apply to messaging destinations, however. Messaging destinations connected from journeys and broadcasts don’t generate multiple background sources.

This page was last modified: 30 Jan 2024

Need support?

Questions? Problems? Need more info? Contact Segment Support for assistance!

Help improve these docs!

Was this page helpful?

Thanks for your feedback!

Can we improve this doc? 
Send us feedback!

Get started with Segment

On this page

Was this page helpful?

Thanks for your feedback!

Can we improve
 this doc? 
Send us feedback!

Product

For Developers

Company

Support

© 2025 Segment.io, Inc.

=== Content from https://segment.com/docs/connections/reverse-etl/reverse-etl-source-setup-guides/snowflake-setup/ ===

Snowflake Reverse ETL Setup
        

On this page

Set up Snowflake as your Reverse ETL source.

At a high level, when you set up Snowflake for Reverse ETL, the configured user/role needs read permissions for any resources (databases, schemas, tables) the query needs to access. Segment keeps track of changes to your query results with a managed schema 
(
__SEGMENT_REVERSE_ETL
), which requires the configured user to allow write permissions for that schema.

Segment now supports key-pair authentication for Snowflake Reverse ETL sources. Key-pair authentication is available for Business Tier users only.

Snowflake Reverse ETL sources support Segment's dbt extension

If you have an existing dbt account with a Git repository, you can use 
Segment’s dbt extension
 to centralize model management and versioning, reduce redundancies, and run CI checks to prevent breaking changes.

Set up guide

Follow the instructions below to set up the Segment Snowflake connector. Segment recommends you use the 
ACCOUNTADMIN
 role to execute all the commands below, and that you create a user that authenticates with an encrypted key pair.

Segment has a Terraform provider, powered by the Public API, that you can use to create a Snowflake Reverse ETL source. See the 
segment_source (Resource)
 documentation for more information.

Enter and run the code below to create a database.
Segment uses the database specified in your connection settings to create a schema called 
__segment_reverse_etl
 to avoid collision with your data. The schema is used for tracking changes to your model query results between syncs.
An existing database can be reused, if desired. Segment recommends you to use the same database across all your models attached to this source to keep all the state tracking tables in 1 place.

-- not required if another database is being reused


CREATE
 
DATABASE
 
segment_reverse_etl
;



Enter and run the code below to create a virtual warehouse.
Segment Reverse ETL needs to execute queries on your Snowflake account, which requires a Virtual Warehouse to handle the compute. You can also reuse an existing warehouse.

-- not required if reusing another warehouse


CREATE
 
WAREHOUSE
 
segment_reverse_etl

 
WITH
 
WAREHOUSE_SIZE
 
=
 
'XSMALL'

   
WAREHOUSE_TYPE
 
=
 
'STANDARD'

   
AUTO_SUSPEND
 
=
 
600
 
-- 5 minutes

   
AUTO_RESUME
 
=
 
TRUE
;



Enter and run the code below to create specific roles for Reverse ETL.
All Snowflake access is specified through roles, which are then assigned to the user you’ll create later.

-- create role


CREATE
 
ROLE
 
segment_reverse_etl
;



-- warehouse access


GRANT
 
USAGE
 
ON
 
WAREHOUSE
 
segment_reverse_etl
 
TO
 
ROLE
 
segment_reverse_etl
;



-- database access


GRANT
 
USAGE
 
ON
 
DATABASE
 
segment_reverse_etl
 
TO
 
ROLE
 
segment_reverse_etl
;


GRANT
 
CREATE
 
SCHEMA
 
ON
 
DATABASE
 
segment_reverse_etl
 
TO
 
ROLE
 
segment_reverse_etl
;



Enter and run one of the following code snippets below to create the user Segment uses to run queries. For added security, Segment recommends creating a user that authenticates using a key pair.

To create a user that authenticates with a key pair, 
create a key pair
 and then execute the following SQL commands:

-- create user (key-pair authentication)


CREATE
 
USER
 
segment_reverse_etl_user


DEFAULT_ROLE
 
=
 
segment_reverse_etl


RSA_PUBLIC_KEY
 
=
 
'enter your public key'
;



-- role access


GRANT
 
ROLE
 
segment_reverse_etl
 
TO
 
USER
 
segment_reverse_etl_user
;



To create a user that authenticates with a password, execute the following SQL commands:

-- create user (password authentication)


CREATE
 
USER
 
segment_reverse_etl_user

 
MUST_CHANGE_PASSWORD
 
=
 
FALSE

 
DEFAULT_ROLE
 
=
 
segment_reverse_etl

 
PASSWORD
 
=
 
'my_strong_password'
;
 
-- Do not use this password



-- role access


GRANT
 
ROLE
 
segment_reverse_etl
 
TO
 
USER
 
segment_reverse_etl_user
;



Learn more about the Snowflake Account ID in Snowflake’s 
Account identifiers
 documentation.

After you’ve successfully added your Snowflake source, 
add a model
 and follow the rest of the steps in the Reverse ETL setup guide.

Security

Allowlisting IPs

If you create a network policy with Snowflake and are located in the US, add  
52.25.130.38/32
 and 
34.223.203.0/28
 to the “Allowed IP Addresses” list.

If you create a network policy with Snowflake and are located in the EU, add 
3.251.148.96/29
 to your “Allowed IP Addresses” list.

This page was last modified: 04 Nov 2024

Need support?

Questions? Problems? Need more info? Contact Segment Support for assistance!

Help improve these docs!

Was this page helpful?

Thanks for your feedback!

Can we improve this doc? 
Send us feedback!

Get started with Segment

On this page

Was this page helpful?

Thanks for your feedback!

Can we improve
 this doc? 
Send us feedback!

Product

For Developers

Company

Support

© 2025 Segment.io, Inc.

=== Content from https://segment.com/docs/segment-app/iam/sso/ ===

Single Sign On team management
        

Single Sign-on is only available in Business plan workspaces.


See the 
available plans
, or 
contact Support
.

On this page

Segment supports Single Sign On for Business Tier accounts. You can use any SAML-based Identity Provider (IdP), for example Okta, Bitium, OneLogin, or Centrify, or use GSuite to serve as your identity provider, delegating access to the application based on rules you create in your central identity management solution.

With SSO, you have centralized control over your users’ ability to authenticate or not in your IdP. You can also enforce rules like two-factor authentication or password rotation at the IdP level.

You can configure as many IdP connections to your workspace as needed to support IdP-initiated authentication. This allows seamless migration from one system to a new one, if, for example, your organization switches IdP vendors or switches from GSuite to a dedicated SAML IdP like Okta or OneLogin.

To enable SSO-based login from the Segment login page (app.segment.com/login), you must first verify that you own the domain, and connect it to your organization’s Segment account. After you have done that, SSO users from your domain can use the Segment login page to access your default Segment workspace.

The Segment login page can only be connected to one workspace. To use your IdP with multiple workspaces, you will have to initiate login to the other workspaces from the IdP instead of through the login portal.

Set up — SAML

Segment’s SSO configuration is entirely self-service. Additionally, Segment has prebuilt connections with 
Okta

, 
OneLogin
, and 
Microsoft Entra ID

 which can help you get set up faster. 
Reach out to support
 if you run into any questions or issues.

To get started, go to your workspace settings and navigate to 
Authentication > Connections > Add new Connection
. Follow the steps to create a SAML connection.





Prepare your IdP for the connection

Segment officially supports apps for Okta, Microsoft Entra ID, and OneLogin. Next, find Segment in your IdP’s app catalog, and follow the set up instructions they provide.

If you’re using a different IdP, you must create a custom SAML-based application.

Your provider will ask you for a few things from Segment, which Segment provides in the setup flow:



A few gotchas to look out for:

For GSuite configurations, make sure the 
Start URL
 field in Service Provider Details is left blank.

Different IdPs have different names for the Audience URL. Some call it “Audience URI”, some call it “Entity ID”, some call it “Service Provider Entity ID.” It’s likely there are only two required fields without correct defaults, and they correspond to the 
SSO URL
 and 
Audience URL
 values above.

In all IdPs Segment works with, the default 
NameID
 option is the correct one. Make sure it’s using the 
emailAddress
 schema.

In all IdPs Segment works with, the default connection encryption options are the correct ones. (Signed Response & Assertion Signature with SHA256, Unencrypted Assertions).

Different IdPs store records of your employees differently. The only attribute mapping Segment requires is to make sure you’re sending 
email
 . In Okta this is at 
user.email
. In Duo this is 
mail
.

Make sure you’ve enabled “send all attributes” (not just NameID) if applicable for your IdP.

No 
RelayState
 is required. This is also sometimes called 
Target
.

After you create the application in your IdP, you can come back to Segment and click “Next”.

Configure Segment to Talk to Your IdP

Your IdP provides a URL and x.509 certificate. Copy them into their respective fields in Segment.



Then, click “Configure Connection.”

You’re all set.

Test your connection with IdP-initiated SSO

Back at the connections page, make sure your connection is enabled with the switch on the right.



You can now test using IdP-initiated SSO (by clicking login to Segment from within your IdP) is working correctly. If not, double check the IdP configuration gotchas section above.

Require SSO

For most customers, Segment recommends requiring SSO for all users. If you do not require SSO, users can still log in with a username and password. If some members cannot log in using SSO, Segment also supports SSO exceptions.

These options are off by default, but you can configure them on the 
Advanced Settings
 page. Log in using SSO to toggle the 
Require SSO
 setting.



Setup — GSuite

To configure GSuite for use with Segment, go to your workspace settings and choose the “Connections” tab under “Authentication” and click “Add New Connection.” Follow the steps to create a “Google Apps For Work” connection.

Enter your domain (or, if you’ve verified it already, choose it from the dropdown) and then click the resulting link to authorize the connection.

Enabling Segment-initiated login

Segment supports SSO on the login page for emails that match your workspace’s domain.

In order to enable this, you’ll need to verify your domain with Segment. To do that, go to the “Domains” tab under “Authentication” in the workspace settings page.



Enter your domain and click “Add Domain.” When you click verify, you’re given two options to verify your domain, either using a meta tag to add to your 
/index.html
 at the root, or a DNS text record that you can add through your DNS provider. After you do so and click verify, you can move to the next step.

Domain tokens expire 14 days after they are verified.

Configuring SSO to access multiple workspaces

To configure SSO for multiple workspaces, your admin must configure access to each workspace as a separate app in your identity provider. You are unable to use verified domain(s) across multiple workspaces and will encounter the following error if you add a domain that is already verified in another workspace:

Warning
: This domain has already been claimed.

After your administrator configures separate apps for each workspace in your IdP, the end-users can log in to the IdP and click on the relevant app for the workspace you are trying to access. This is also referred to as IdP-initiated SSO.

Okta setup

The Okta/Segment SAML integration supports the following features:

For more information on these features, visit the 
Okta Glossary
.

Configuration steps

To set up the Okta/Segment SAML integration, you’ll first carry out several steps in Segment, then finish in Okta.

Follow these steps in Segment to set up the Okta/Segment SAML integration:



Finish setting up the Okta/Segment SAML integration by carrying out these steps in Okta:




You’ve now completed setup. For SP-initiated SSO, follow these steps:

SSO Frequently Asked Questions

Segment supports “just in time” user permissioning; new users who authenticate using your IdP are automatically created in Segment as minimal-access (read-only) members. If the user already exists in Segment then Segment associates the IdP-identity with the existing Segment user account. Segment also supports user provisioning and role mapping 
using SCIM
.

Yes, users can be automatically de-provisioned 
if you use SCIM
.

Segment allows users to own their own workspaces. While your IdP authentication will ensure that any non-owners must have logged in with SSO to access 
your workspace
, they can still log into Segment with a username and password to access their own workspaces.

Workspace owners can invite additional owners with any domain using the traditional invite mechanism. If the workspace is configured to require SSO, and the user is not on your IdP, you can add an Exemption under 
Workspace Settings > Authentication > Advanced Settings
.

After SSO is configued to access multiple workspaces, you will have slightly different signin experience in the below scenarios

This page was last modified: 07 Feb 2025

Need support?

Questions? Problems? Need more info? Contact Segment Support for assistance!

Help improve these docs!

Was this page helpful?

Thanks for your feedback!

Can we improve this doc? 
Send us feedback!

Get started with Segment

On this page

Was this page helpful?

Thanks for your feedback!

Can we improve
 this doc? 
Send us feedback!

Product

For Developers

Company

Support

© 2025 Segment.io, Inc.

=== Content from https://segment.com/docs/connections/spec/copilot/ ===

Spec: AI Copilot
        

On this page

This page is a guide for developers who want to track interactions with AI copilots using Segment. It explains what data to send to Segment, letting you understand customer interactions with AI copilots.

Overview

AI copilots are like virtual assistants that help customers in conversations.

Each conversation starts when a customer sends their first message or question. Throughout the conversation, Segment can track various events that capture key moments, like messages sent and received, tools invoked, and media generated.

While some copilot conversations have clear ending points, which occur when the customer explicitly indicates that the conversation is over, the tracked events provide valuable insights into the entire conversation flow.

Tracked events

In this section, you’ll find the tracked semantic events that serve as a starting point for AI copilot events. You can extend them based on your own requirements.

This table lists the events that you can track from any conversation:

Live chat events

Segment can also track the following live chat events:

Event details

This section contains the structure and properties of each AI copilot tracked event.

Conversation Started

The Conversation Started event should be sent when a customer sends their first message.

This event supports the following semantic properties:

Here’s an example of a Conversation Started call:

{

  
"userId"
:
 
"123"
,

  
"action"
:
 
"track"
,

  
"event"
:
 
"Conversation Started"
,

  
"properties"
:
 
{

    
"conversationId"
:
 
"1238041hdou"

  
}


}



Message Sent

The Message Sent event should be sent when a user adds a new message to a thread. The default for 
role
 is 
"customer"
.

This event supports the following semantic properties:

Here’s an example of a Message Sent call:

{

  
"userId"
:
 
"123"
,

  
"action"
:
 
"track"
,

  
"event"
:
 
"Message Sent"
,

  
"properties"
:
 
{

    
"conversationId"
:
 
"1238041hdou"
,

    
"messageId"
:
 
"msg123"
,

    
"message_body"
:
 
"What's the best stock in the Nasdaq right now?"
,

  
"role"
:
 
"customer"

  
}


}



Message Received

The Message Received event should be sent when the copilot gives a non-custom response (either text or voice) to something the user asked.

The default for 
role
 is 
"agent"
. You can extend 
role
 to different agent type, like 
ai_agent
, 
human_agent
, 
task_automation_agent
, and so on.

This event supports the following semantic properties:

{

  
"userId"
:
 
"123"
,

  
"action"
:
 
"track"
,

  
"event"
:
 
"Message Received"
,

  
"properties"
:
 
{

    
"conversationId"
:
 
"1238041hdou"
,

    
"messageId"
:
 
"msg124"
,

    
"message_body"
:
 
"Thank you for reaching out. How can I assist you today?"

  
},

  
"role"
:
 
"agent"


}



Conversation Ended

The Conversation Ended event should be sent when a customer or agent explicitly indicates that the conversation has ended or deletes the chat.

This event supports the following semantic property:

Here’s an example of a Conversation Ended call:

{

  
"userId"
:
 
"123"
,

  
"action"
:
 
"track"
,

  
"event"
:
 
"Conversation Ended"
,

  
"properties"
:
 
{

    
"conversationId"
:
 
"1238041hdou"

  
}


}



Action Invoked

The Action nvoked event should be sent when the copilot or user uses a custom capability or tool, like making a call to an external API.

This event supports the following semantic properties:

Here’s an example of an Action Invoked call:

{

  
"userId"
:
 
"123"
,

  
"action"
:
 
"track"
,

  
"event"
:
 
"Action Invoked"
,

  
"properties"
:
 
{

    
"conversationId"
:
 
"1238041hdou"
,

    
"messageId"
:
 
"msg125"
,

    
"type"
:
 
"Inventory Request"
,

    
"action"
:
 
"check stock level"
,

  
"role"
:
 
"customer"

  
}


}



Media Generated

This event should be sent when an image, video, or custom audio is generated by the model.

This event supports the following semantic properties:

Here’s an example of a Media Generated call:

{

  
"userId"
:
 
"123"
,

  
"action"
:
 
"track"
,

  
"event"
:
 
"Media Generated"
,

  
"properties"
:
 
{

    
"conversationId"
:
 
"1238041hdou"
,

    
"messageId"
:
 
"msg126"
,

    
"role"
:
 
"agent"
,

    
"type"
:
 
"image"
,

    
"sub_type"
:
 
"gif"

  
}


}



Component Loaded

This event should be sent when a new, custom component is shown to the user that isn’t text or voice.

This event supports the following semantic properties:

Here’s an example of a Component Loaded call:

{

  
"userId"
:
 
"123"
,

  
"action"
:
 
"track"
,

  
"event"
:
 
"Component Loaded"
,

  
"properties"
:
 
{

    
"conversationId"
:
 
"1238041hdou"
,

    
"messageId"
:
 
"msg127"
,

    
"type"
:
 
"Stock Price Chart"

  
}


}



Feedback Submitted

This event should be sent when a user rates a conversation or message.

This event supports the following semantic properties:

Here’s an example of a Feedback Submitted call:

{

  
"userId"
:
 
"123"
,

  
"action"
:
 
"track"
,

  
"event"
:
 
"Feedback Submitted"
,

  
"properties"
:
 
{

    
"conversationId"
:
 
"1238041hdou"
,

    
"messageId"
:
 
"msg128"
,

    
"rating"
:
 
5

  
}


}



Identify

This event should be sent when a new user is identified, either anonymously or as a known user.

This event supports the following semantic properties:

Here’s an example of an Identify call:

{

  
"
userId
"
:
 
"
123
"
 
||
 
"
anonymousId
"
 
:
 
"
768923ihuy32
"
,

  
"
action
"
:
 
"
identify
"
,

  
"
properties
"
:


}



Standard Track calls

When sending events to Segment based on user actions, like items purchased or support requested, make sure to include relevant identifiers for accurate tracking and analysis.

These identifiers include 
conversationId
 and 
messageId
, among others, depending on the specific tracked action:

For example, to track an event where a user makes a purchase, the standard Track call could look like this:

{

  
"userId"
:
 
"user123"
,

  
"action"
:
 
"track"
,

  
"event"
:
 
"Item Purchased"
,

  
"properties"
:
 
{

    
"conversationId"
:
 
"conv456"
,

    
"messageId"
:
 
"msg789"
,

    
"itemId"
:
 
"item101112"
,

    
"itemName"
:
 
"Super Widget"
,

    
"itemPrice"
:
 
19.99
,

    
"currency"
:
 
"USD"

  
}


}



This page was last modified: 04 Apr 2024

Need support?

Questions? Problems? Need more info? Contact Segment Support for assistance!

Help improve these docs!

Was this page helpful?

Thanks for your feedback!

Can we improve this doc? 
Send us feedback!

Get started with Segment

On this page

Was this page helpful?

Thanks for your feedback!

Can we improve
 this doc? 
Send us feedback!

Product

For Developers

Company

Support

© 2025 Segment.io, Inc.

=== Content from https://segment.com/docs/engage/audiences/product-based-audiences/ ===

Product Based Recommendation Audiences
        

Engage Foundations requires a Business tier account and includes Unify.


See the 
available plans
, or 
contact Support
.

On this page

Product Based Recommendation Audiences lets you select a product, article, song, or other piece of content from your catalog, and then build an audience of the people that are most likely to engage with it. Segment optimized the personalized recommendations built by Product Based Recommendation Audiences for user-based commerce, media, and content affinity use cases.

You can use Product Based Recommendation Audiences to power the following common marketing campaigns:

Create a Product Based Audience

Set up your Recommendation Catalog

Segment uses your interaction events (
order_completed
, 
product_added
, 
product_searched
, 
song_played
, 
article_saved
) and the event metadata of those interaction events to power the Recommendations workflow.

To create your Recommendation Catalog:

Segment can take several hours to create your Recommendation Catalog.

Create your Product Based Audience

Once you’ve created your Recommendation Catalog, you can build a Recommendation Audience. A Recommendation Audience lets you select a parameter and then build an audience of the people that are most likely to engage with that parameter.

To create a Product Based Audience:

Segment can take up to a day to calculate your Product Based Audience.

Best practices

This page was last modified: 15 Jan 2025

Need support?

Questions? Problems? Need more info? Contact Segment Support for assistance!

Help improve these docs!

Was this page helpful?

Thanks for your feedback!

Can we improve this doc? 
Send us feedback!

Get started with Segment

On this page

Was this page helpful?

Thanks for your feedback!

Can we improve
 this doc? 
Send us feedback!

Product

For Developers

Company

Support

© 2025 Segment.io, Inc.

=== Content from https://segment.com/docs/connections/spec/common/ ===

Spec: Common Fields
        

On this page

In the Segment 
Spec
 all the 
API calls
 have a common structure, and a few common fields.

However, not all destinations accept all fields included in the Spec. Not sure which fields a destination accepts? Refer to the destination’s documentation page, or check out the 
open-source destination code on GitHub
.

Check out our high-level overview of these APIs in Segment University. (Must be logged in to access.)

Structure

Every API call has the same core structure and fields. These fields describe user identity, timestamping, and mechanical aides like API version.

Here’s an example of these common fields in raw JSON:

{

  
"anonymousId"
:
 
"507f191e810c19729de860ea"
,

  
"context"
:
 
{

    
"active"
:
 
true
,

    
"app"
:
 
{

      
"name"
:
 
"InitechGlobal"
,

      
"version"
:
 
"545"
,

      
"build"
:
 
"3.0.1.545"
,

      
"namespace"
:
 
"com.production.segment"

    
},

    
"campaign"
:
 
{

      
"name"
:
 
"TPS Innovation Newsletter"
,

      
"source"
:
 
"Newsletter"
,

      
"medium"
:
 
"email"
,

      
"term"
:
 
"tps reports"
,

      
"content"
:
 
"image link"

    
},

    
"device"
:
 
{

      
"id"
:
 
"B5372DB0-C21E-11E4-8DFC-AA07A5B093DB"
,

      
"advertisingId"
:
 
"7A3CBEA0-BDF5-11E4-8DFC-AA07A5B093DB"
,

      
"adTrackingEnabled"
:
 
true
,

      
"manufacturer"
:
 
"Apple"
,

      
"model"
:
 
"iPhone7,2"
,

      
"name"
:
 
"maguro"
,

      
"type"
:
 
"ios"
,

      
"token"
:
 
"ff15bc0c20c4aa6cd50854ff165fd265c838e5405bfeb9571066395b8c9da449"

    
},

    
"ip"
:
 
"8.8.8.8"
,

    
"library"
:
 
{

      
"name"
:
 
"analytics.js"
,

      
"version"
:
 
"2.11.1"

    
},

    
"locale"
:
 
"en-US"
,

    
"network"
:
 
{

      
"bluetooth"
:
 
false
,

      
"carrier"
:
 
"T-Mobile US"
,

      
"cellular"
:
 
true
,

      
"wifi"
:
 
false

    
},

    
"os"
:
 
{

      
"name"
:
 
"iPhone OS"
,

      
"version"
:
 
"8.1.3"

    
},

    
"page"
:
 
{

      
"path"
:
 
"/academy/"
,

      
"referrer"
:
 
""
,

      
"search"
:
 
""
,

      
"title"
:
 
"Analytics Academy"
,

      
"url"
:
 
"https://segment.com/academy/"

    
},

    
"referrer"
:
 
{

      
"id"
:
 
"ABCD582CDEFFFF01919"
,

      
"type"
:
 
"dataxu"

    
},

    
"screen"
:
 
{

      
"width"
:
 
320
,

      
"height"
:
 
568
,

      
"density"
:
 
2

    
},

    
"groupId"
:
 
"12345"
,

    
"timezone"
:
 
"Europe/Amsterdam"
,

    
"userAgent"
:
 
"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/113.0.0.0 Safari/537.36"
,

    
"userAgentData"
:
 
{

      
"brands"
:
 
[

        
{

          
"brand"
:
 
"Google Chrome"
,

          
"version"
:
 
"113"

        
},

        
{

          
"brand"
:
 
"Chromium"
,

          
"version"
:
 
"113"

        
},

        
{

          
"brand"
:
 
"Not-A.Brand"
,

          
"version"
:
 
"24"

        
}

      
],

      
"mobile"
:
 
false
,

      
"platform"
:
 
"macOS"

    
}

  
},

  
"integrations"
:
 
{

    
"All"
:
 
true
,

    
"Mixpanel"
:
 
false
,

    
"Salesforce"
:
 
false

  
},

  
"event"
:
 
"Report Submitted"
,

  
"messageId"
:
 
"022bb90c-bbac-11e4-8dfc-aa07a5b093db"
,

  
"receivedAt"
:
 
"2015-12-10T04:08:31.909Z"
,

  
"sentAt"
:
 
"2015-12-10T04:08:31.581Z"
,

  
"timestamp"
:
 
"2015-12-10T04:08:31.905Z"
,

  
"type"
:
 
"track"
,

  
"userId"
:
 
"97980cfea0067"
,

  
"version"
:
 
2


}




In more detail these common fields for every API call are:

Beyond this common structure, each API call adds a few specialized top-level fields.

Context

Context is a dictionary of extra information that provides useful context about a datapoint, for example the user’s 
ip
 address or 
locale
. You should 
only use
 Context fields for their intended meaning.

Context fields automatically collected

Below is a chart that shows you which context variables are populated automatically by the iOS, Android, and analytics.js libraries.

Other libraries only collect 
context.library
, any other context variables must be sent manually.

IPv6

Segment doesn’t support automatically collecting IPv6 addresses.

The Android library collects 
screen.density
 with 
this method
.

userAgentData is only collected if the 
Client Hints API
 is available on the browser.

Segment doesn’t collect or append to the context of subsequent calls in the new mobile libraries (Swift, Kotlin, and React Native).

To pass the context variables which are not automatically collected by Segment’s libraries, you must manually include them in the event payload. The following code shows how to pass 
groupId
 as the context field of Analytics.js’s 
.track()
 event:

analytics
.
track
(
"
Report Submitted
"
,
 
{},
 
{

  
context
:
 
{

    
groupId
:
 
"
1234
"

  
}


});



To add fields to the context object in the new mobile libraries, you must utilize a custom plugin. Documentation for creating plugins for each library can be found here:

Integrations

A dictionary of destination names that the message should be sent to. 
'All'
 is a special key that applies when no key for a specific destination is found.

Integrations defaults to the following:

{

  
All
:
 
true
,

  
Salesforce
:
 
false
,


}



This is because 
Salesforce
 has strict limits on API calls.

Sending data to the rest of Segment’s destinations is opt-out so if you don’t specify the destination as false in this object, it will be sent to rest of the destinations that can accept it.

Timestamps

Every API call has four timestamps, 
originalTimestamp
, 
timestamp
, 
sentAt
, and 
receivedAt.
  They’re used for very different purposes.

All timestamps are 
ISO-8601
 date strings, and are in the UTC timezone.
 To see the user’s timezone information, check the 
timezone
 field that’s automatically collected by 
client-side libraries
.

You must use ISO-8601 date strings that include timezones when you use timestamps with 
Engage
. If you send custom traits without a timezone, Segment doesn’t save the timestamp value.

Timestamp overview

originalTimestamp

The 
originalTimestamp
 tells you when call was invoked on the client device or the value of 
timestamp
 that you manually passed in.

Note:
 The 
originalTimestamp
 timestamp is not useful for any analysis since it’s not always trustworthy as it can be easily adjusted and affected by clock skew.

sentAt

The 
sentAt
 timestamp specifies the clock time for the client’s device when the network request was made to the Segment API. For libraries and systems that send batched requests, there can be a long gap between a datapoint’s 
timestamp
 and 
sentAt
. Combined with 
receivedAt
, Segment uses 
sentAt
 to correct the original 
timestamp
 in situations where a user’s device clock cannot be trusted (mobile phones and browsers). The 
sentAt
 and 
receivedAt
 timestamps are assumed to occur at the same time (maximum a few hundred milliseconds), and therefore the difference is the user’s device clock skew, which can be applied back to correct the 
timestamp
.

Note:
 The 
sentAt
 timestamp is not useful for any analysis since it’s tainted by user’s clock skew.

Segment now adds `sentAt` to a payload when the batch is complete and initially tried to the Segment API for the Swift, Kotlin, and C# mobile libraries

This update changes the value of the Segment-calculated 
timestamp
 to align closer with the 
receivedAt
 value rather than the 
originalTimestamp
 value. For most users who are online when events are sent, this does not significantly impact their data. However, if your application utilizes an offline mode where events are queued up for any period of time, the 
timestamp
 value for those users now more closely reflects when Segment received the events rather than the time they occurred on the users’ devices.

receivedAt

The 
receivedAt
 timestamp is added to incoming messages as soon as they hit the API. It’s used in combination with 
sentAt
 to correct clock skew, and also to aid with debugging libraries and systems that deliver events in batches.

The 
receivedAt
 timestamp is most important as the sort key in Segment’s Warehouses product. Use this for max query speed when retrieving data from your Warehouse.

Note:
 Chronological order of events is not ensured with 
receivedAt
.

timestamp

The 
timestamp
 timestamp specifies when the data point occurred, corrected for client-device clock skew. This is the timestamp that is passed to downstream destinations and used for historical replays. It is important to use this timestamp for importing historical data to the API.

If you are using the Segment server Source libraries, or passing calls directly to the HTTP API endpoint, you can manually set the 
timestamp
 field. This change updates the 
originalTimestamp
 field of the Segment event. If you use a Segment Source in device mode, the library generates 
timestamp
 and you cannot manually set one directly in the call payload.

Segment calculates 
timestamp
 as 
timestamp = receivedAt - (sentAt - originalTimeStamp)
.

For client-side tracking it’s possible for the client to spoof the 
originalTimeStamp
, which may result in a calculated 
timestamp
 value set in the future.

FAQ

Why Are Events Received with Timestamps Set in the Past or Future?

If you’re using one of Segment’s client-side libraries, please note that several factors can cause timestamp discrepancies in your event data.

This page was last modified: 06 Feb 2025

Need support?

Questions? Problems? Need more info? Contact Segment Support for assistance!

Help improve these docs!

Was this page helpful?

Thanks for your feedback!

Can we improve this doc? 
Send us feedback!

Get started with Segment

On this page

Was this page helpful?

Thanks for your feedback!

Can we improve
 this doc? 
Send us feedback!

Product

For Developers

Company

Support

© 2025 Segment.io, Inc.

=== Content from https://segment.com/docs/guides/how-to-guides/migrate-from-other-tools/ ===

Migrating Code From Other Analytics Tools
        

On this page

Switching from your current client-side JavaScript event tracking to Segment is easy. Below you can find migration guides for the following tools:

If you’d like us to add more tools or mobile/server-side examples to this guide 
let us know
!

Google Analytics

Custom Events

Google Analytics Custom Events are simple to record in Segment. You’ll record them with our track method and use the same properties you would when sending to Google Analytics directly.

The only mapping exception is the Event Action. That will automatically be populated by the Event Name you include in the track call.

Here’s an example:

ga
(
'
send
'
,
 
{

  
'
hitType
'
:
 
'
event
'
,

  
'
eventCategory
'
:
 
'
Account
'
,

  
'
eventAction
'
:
 
'
Signed Up
'
,

  
'
eventLabel
'
:
 
'
Premium
'
,

  
'
eventValue
'
:
 
4


});



Becomes:

analytics
.
track
(
'
Signed Up
'
,
 
{

  
category
:
 
'
Account
'
,

  
label
:
 
'
Premium
'
,

  
value
:
 
4


});



Since Event Category is required we’ll populate it with 
All
 if you don’t specify one. You can read more about this in 
our Google Analytics docs
.

Ecommerce

Segment has full support for the Google Analytics E-Commere API and the 
Enhanced E-Commerce API
 as well. Make sure you follow 
our e-commerce tracking plan
 to make sure you’ll be able to use all e-commerce features in the tools we support.

For an e-commerce transaction to appear in Google Analytics you’ll need to enable e-commerce for your Google Analytics view and send an Order Completed event to Segment. This simplifies things a lot compared to the direct Google Analytics code.

Here’s an example:

ga
(
'
require
'
,
 
'
ecommerce
'
);



ga
(
'
ecommerce:addTransaction
'
,
 
{


'
id
'
:
 
'
93745
'
,


'
revenue
'
:
 
'
30
'
,


'
shipping
'
:
 
'
3
'
,


'
tax
'
:
 
'
2
'
,


'
currency
'
:
 
USD


});



ga
(
'
ecommerce:addItem
'
,
 
{


'
id
'
:
 
'
23423
'
,


'
name
'
:
 
'
Monopoly: 3rd Edition
'
,


'
sku
'
:
 
'
J90-32
'
,


'
category
'
:
 
'
Games
'
,


'
price
'
:
 
'
19.00
'
,


'
quantity
'
:
 
'
1
'


});



ga
(
'
ecommerce:addItem
'
,
 
{


'
id
'
:
 
'
22744
'
,


'
name
'
:
 
'
Uno Card Game
'
,


'
sku
'
:
 
'
Q93-32
'
,


'
category
'
:
 
'
Cards
'
,


'
price
'
:
 
'
3.00
'
,


'
quantity
'
:
 
'
2
'


});



ga
(
'
ecommerce:send
'
);



Becomes:

analytics
.
track
(
'
Order Completed
'
,
 
{

  
order_id
:
 
'
93745
'
,

  
total
:
 
46
,

  
shipping
:
 
3
,

  
tax
:
 
2
,

  
currency
:
 
USD
,

  
products
:
 
[{

    
id
:
 
'
23423
'
,

    
name
:
 
'
Monopoly: 3rd Edition
'
,

    
sku
:
 
'
J90-32
'
,

    
category
:
 
'
Games
'
,

    
price
:
 
19
,

    
quantity
:
 
1

  
},
 
{

    
id
:
 
'
22744
'
,

    
name
:
 
'
Uno Card Game
'
,

    
sku
:
 
'
Q93-32
'
,

    
category
:
 
'
Cards
'
,

    
price
:
 
3
,

    
quantity
:
 
2

  
}]


})



At the very minimum you must include an orderId for each Order and for each product inside that order you must include an id and name. All other properties are optional.

Custom Dimensions

Through Segment you can record user-scope custom dimensions using our identify, page, or track methods.

A full explanation can be found in 
our Google Analytics docs
 page, but here’s a quick example:

ga
(
'
set
'
,
 
'
dimension5
'
,
 
'
Male
'
);


ga
(
'
send
'
,
 
'
pageview
'
);



Becomes:

analytics
.
identify
({

  
gender
:
 
'
Male
'


});


analytics
.
page
();



(This example assumes you have already mapped Gender to the correct dimension in your Segment source settings for Google Analytics.)

Everything Else

To see a full list of Google Analytics features and how they work through Segment read 
our Google Analytics docs page
.

Mixpanel

Event Tracking

Event tracking is Mixpanel’s bread and butter. Below are all the relevant Mixpanel functions and how you can map them to Segment functions.

Switching your event tracking from Mixpanel to Segment couldn’t be easier. Our trackmethod maps directly to Mixpanel’s. The event name is the first argument and the event properties are the second argument.

mixpanel
.
track
(
'
Registered
'
,{

  
type
:
 
'
Referral
'


});



Becomes:

analytics
.
track
(
'
Registered
'
,
 
{

  
type
:
 
'
Referral
'


});



The identify method in Mixpanel is used to merge together events from multiple environments so your unique events number is accurate and your funnels don’t break.

Since 
mixpanel.identify
 only takes a single argument (a userID) it maps directly to our identify method:

mixpanel
.
identify
(
'
123
'
);



Becomes:

analytics
.
identify
(
'
123
'
);



Mixpanel has the idea of Super Properties, which are user traits that get attached to every event that the user does. In Segment you can set Mixpanel Super Properties using our identify method. Super properties are only supported in client-side libraries 
Analytics.js
, 
iOS
, 
Android
.

Here’s an example:

mixpanel
.
register
({

  
"
gender
"
:
 
"
male
"
,

  
"
hairColor
"
:
 
"
brown
"


});



Becomes:

analytics
.
identify
({

  
gender
:
 
'
male
'
,

  
hairColor
:
 
'
brown
'


});



This also works when you include a userId argument in your identify call.

Alias

Alias is necessary in Mixpanel to tie together an anonymous visitor with an identified one. The Mixpanel and Segment alias methods both work the same.

In client-side javascript passing a single argument will alias the current anonymous or identified visitor distinct_id to the userId you pass into it:

mixpanel
.
alias
(
'
1234
'
);



Becomes:

analytics
.
alias
(
'
1234
'
);



Track Links

If you are tracking links with Mixpanel’s 
track_links
 helper you can switch that code to the Segment 
trackLink helper function
 in 
Analytics.js
.

And here’s an example:

// track click for link id #nav


mixpanel
.
track_links
(
"
#free-trial-link
"
,
 
"
Clicked Free-Trial Link
"
,
 
{


 
 
plan
:
 
'
Enterprise
'


})



Becomes:

var
 
link
 
=
 
document
.
getElementById
(
'
free-trial-link
'
);


analytics
.
trackLink
(
link
,
 
'
Clicked Free-Trial Link
'
,
 
{


 
 
plan
:
 
'
Enterprise
'


});



Track Forms

If you are tracking forms with 
Mixpanel’s track_forms helper
 you can switch that code to
the Segment trackForm helper function
 in Analytics.js.

And here’s an example:

// track submission for form id "register"


mixpanel
.
track_forms
(
"
#register
"
,
 
"
Created Account
"
,


 
 
plan
:
 
'
Premium
'


});



Becomes:

var
 
form
 
=
 
document
.
getElementById
(
'
register
'
);


analytics
.
trackForm
(
form
,
 
'
Created Account
'
,


 
 
plan
:
 
'
Premium
'


});



People Tracking

Mixpanel people tracking is a separate database from the event tracking outlined above. For that reason there are separate API methods to record data to Mixpanel People.

This method sets people properties in Mixpanel People. In Segment you will use ouridentify method to accomplish this.

Here’s an example:

mixpanel
.
people
.
set
({


 
 
"
$email
"
:
 
"
jake.peterson@example.com
"
,


 
 
"
$name
"
:
 
"
Jake Peterson
"


});



Becomes:

analytics
.
identify
({


 
 
email
:
 
'
jake.peterson@example.com
'
,


 
 
name
:
 
'
Jake Peterson
'


});



This also works when you include a userId argument in your identify call.

As you can see Segment also recognizes special traits like email and name and translates them to the keys that Mixpanel expects (we automatically add the dollar sign).

For more information check out 
our Mixpanel docs
.

Increment

To use Mixpanel increment through Segment you won’t event need anything in your code! All you have to do is list the events you’d like to increment automatically in your Mixpanel destination settings.

Read more in 
our Mixpanel Increment Docs
.

Revenue

Mixpanel’s Revenue report requires the use of a special function called 
track_charge
. In Segment that special function becomes a simple track call. By using the event name 
Order Completed
 we’ll also use that event for any tools you use that recognize our 
ecommerce spec
.

mixpanel
.
people
.
track_charge
(
30.50
,


 
 
'
orderId
'
:
 
'
F9274
'


});



Becomes:

analytics
.
track
(
'
Order Completed
'
,


 
 
revenue
:
 
30.50
,


 
 
orderId
:
 
'
F9274
'


});



This page was last modified: 07 Nov 2023

Need support?

Questions? Problems? Need more info? Contact Segment Support for assistance!

Help improve these docs!

Was this page helpful?

Thanks for your feedback!

Can we improve this doc? 
Send us feedback!

Get started with Segment

On this page

Was this page helpful?

Thanks for your feedback!

Can we improve
 this doc? 
Send us feedback!

Product

For Developers

Company

Support

© 2025 Segment.io, Inc.

=== Content from https://segment.com/docs/unify/profiles-sync/tables/ ===

Profiles Sync Tables and Materialized Views
        

Unify requires a Business tier account and is included with Engage.


See the 
available plans
, or 
contact Support
.

On this page

Through Profiles Sync, Segment provides data sets and models to help you enrich customer profiles using your warehouse data.

This page compares raw tables and materialized views, explaining their roles and use cases. It also outlines the tables Segment lands and the tables you can materialize as part of Profiles Sync.

Understanding raw tables and materialized views

Profiles Sync creates two types of tables in your data warehouse: raw tables and materialized views. These tables help you work with profile and event data at different levels of detail.

The following table shows how raw tables map to their corresponding materialized views:

Raw tables are best for detailed, event-level analysis or debugging specific updates in the Identity Graph. They show every single change and event in your Profiles Sync pipeline.

Materialized views are better for reporting, analytics, and when you need an up-to-date view of profile traits or identifiers. Materialized views reduce complexity by summarizing data from the raw tables.

For example, if you want to debug why a specific profile trait was updated, you’d look at the 
profile_traits_updates
 raw table. But if you want to see the current profile data for a marketing campaign, you’d probably opt for the 
user_traits
 materialized view.

Case study: anonymous site visits lead to profile merge

This section uses a practical example of how Segment connects and merges anonymous profiles to illustrate how Profiles Sync populates and updates its tables.

Explore the following event tabs to learn how these examples result in profile creation and merging.

Suppose these four events lead to the creation of two separate profiles:

// An anonymous visit to twilio.com triggers a Page call:

anonymous_id: 5285bc35-05ef-4d21
context.url: twilio.com
timestamp: May 2, 14:01:00

// Segment generates Profile 1, with a single known ID: 5285bc35-05ef-4d21



// Moments later, the same user signs up to Twilio with their email address.
// This triggers an Identify call:

anonymous_id: 5285bc35-05ef-4d21
context.url: twilio.com/try-twilio
timestamp: May 2, 14:01:47
email: jane.kim@segment.com

// Segment modifies Profile 1, adding an email address to the anonymous ID generated in Event 1.


// Weeks later, an anonymous visit to twilio.com triggers a Page call:

anonymous_id: b50e18a5-1b8d-451c
context.url: twilio.com/education
timestamp: June 22, 10:47:15

// Segment generates Profile 2, with a single known ID: b50e18a5-1b8d-451c.


// Moments later, the same user signs up for a Twilio webinar.
// This triggers an Identify call:

anonymous_id: b50e18a5-1b8d-451c
context.url: twilio.com/events/webinars
timestamp: June 22, 10:48:00
email: jane.kim@segment.com

// Segment understands that Profile 2 and Profile 1 are the same user.
// Segment merges Profile 2 into Profile 1.
// Profile 1 now has two values for anonymous_id: 5285bc35-05ef-4d21 and b50e18a5-1b8d-451c.


Initially, Segment generates two profiles for the first three calls. In the final event, though, Segment understands that Profile 2 should be merged into Profile 1.  Segment then merges Profile 2 into Profile 1, merging away Profile 2 in the process.

Profiles Sync tracks and provides information about these events through a set of tables, which you’ll learn about in the next section.

Profile raw tables

Profile raw tables contain records of changes to your Segment profiles and Identity Graph over time.

With raw tables, you have full control over the materialization of Profiles in your warehouse, as well as increased observibility.

Raw tables contain complete historical data when using historical backfill.

The id_graph_updates table

The 
id_graph_updates
 table maps between the following:

As a result, this table contains information about the creation and merging of profiles, as well as the specific events that triggered those changes.

Using the events from the profile merge case study, Segment would generate three new entries to this table:

In this example, the table shows 
profile_2
 mapping to two places: first to itself, then, later, to 
profile_1
 after the merge occurs.

Segment shows the complete history of every profile. If, later, 
profile_1
 merges into a different 
profile_0
, Segment adds recursive entries to show that 
profile_1
 and 
profile_2
 both map to 
profile_0
.  These entries give you a comprehensive history of all profiles that ever existed.

If you’ll use Profiles Sync to build models, refer to the 
id_graph
 model, which can help you put together a complete view of a customer.

The external_id_mapping_updates table

This table maps Segment-generated identifiers, like 
segment_id
, to external identifiers that your users provide. It has the following columns:

The anonymous site visits sample used earlier would generate the following events:

In this table, Segment shows three observed identifiers. For each of the three identifiers, Segment outputs the Segment ID initially associated with the identifier.

The profile_traits_updates table

The 
profile_traits_updates
 table maps each 
segment_id
 with all associated profile traits.

Segment updates this table:

In the event that two profiles merge, Segment only updates the 
profile_traits_updates
 table for the 
canonical_segment_id
, or the fully merged id.

From the 
profile_traits_updates
 table, use Segment’s 
open-source dbt models
, or your own tools to materialize the 
profile_traits
 table with all profiles and associated profile traits in your data warehouse.

Event type tables

Event type tables provide a complete history for each type of event. Segment syncs events based on the event sources you’ve connected to Unify.

Identity Resolution processes these events, and includes a 
segment_id
, enabling the data to be joined into a single Profile record.

Event type tables will have 2 months of historical data on backfill.

Event type tables includes the following tables:

These event tables are similar to the tables landed by Segment warehouse integrations, with the following exceptions:

The previous result would generate two entries in the 
pages
 table:

And two entries in the 
identifies
 table:

All these events were performed by the same person. If you use these tables to assemble your data models, though, always join them against 
id_graph
 to resolve each event’s 
canonical_segment_id
.

You might see columns appended with 
hidden_entry
 or 
hidden_entry_joined_at
 in profile data of users in Journeys. Segment uses these for internal purposes, and they do not require any attention or action.

Profiles Sync schema

Profiles Sync uses the following schema: 
<profiles_space_name>.<tableName>
.

Note that the Profiles Sync schema is different from the Connections Warehouse schema: 
<source_name>.<tableName>
.

If your space has the same name as a source connected to your Segment Warehouse destination, Segment overwrites data to the Event tables.

For more on Profiles Sync logic, table mappings, and data types, download this 
Profiles Sync ERD
 or visit 
schema evolution and compatibility
.

Track event tables

Track event tables provide a complete event history, with one table for each unique named Track event. Segment syncs events based on the event sources you’ve connected to Unify.

These tables include a full set of Track event properties, with one column for each property.

Segment’s Identity Resolution has processed these events, which contain a 
segment_id
, enabling the data to be joined into a single profile record.

These tables will have two months of historical data on backfill.

To view and select individual track tables, edit your sync settings after you enable Profiles Sync, and wait for the initial sync to complete.

Tables Segment materializes

With Profiles Sync, you can access the following three tables that Segment materializes for a more complete view of your profile:

These materialized tables provide a snapshot of your Segment profiles, batch updated according to your sync schedule.

Switching to materialized Profile Sync

If you’re not using materialized views for Profile Sync and would like to switch, follow these steps:

For materialized view tables, you must have delete permissions for your data warehouse.

Why materialized views?

Materialized views offer several advantages:

The user_traits table

With the 
user_traits
 table, you’ll see all traits that belong to a profile, represented by the 
canonical_segment_id
. Use this table for a complete picture of your Profiles Sync data with external data sources such as customer purchase history, product usage, and more.

When a merge occurs, two things happen:

This table has the following columns:

The user_identifiers table

The 
user_identifiers
 table contains all external ID values that map to a profile, which is represented by the 
canonical_segment_id
.

With the 
user_identifiers
 table:

When a merge occurs:

This table has the following columns:

The profile_merges table

The 
profile_merges
 table contains all mappings from a 
segment_id
 to a profile, represented by the 
canonical_segment_id
. This mapping indicates that a profile has been created within Segment.

With the 
profile_merges
 table:

When a merge occurs:

This table has the following columns:

Tables you materialize

You can materialize the following tables with your own tools, or using Segment’s 
open-source dbt models
:

You might want to materialize your own tables if, for example, you want to transform additional data or join Segment profile data with external data before materialization.

You can alternatively use tables that Segment materializes and syncs to your data warehouse. 
Learn more
 about the tables Segment materializes.

Please note that dbt models are in beta and need modifications to run efficiently on BigQuery, Synapse, and Postgres warehouses. Segment is actively working on this feature.

Every customer profile (or 
canonical_segment_id
) will be represented in each of the following tables.

The id_graph table

This table represents the current state of your identity graph, showing only where a 
segment_id
 is now understood to point.

The most recent entry for each 
segment_id
 from 
id_graph_updates
 reflects this. After the four example events, 
id_graph
 would show the following:

Segment drops most diagnostic information from this table, since it’s designed for reference use. In this case, you’d learn that any data references to 
profile_2
 or 
profile_1
 now map to the same customer, 
profile_1
.

The external_id_mapping table

Use this table to view the full, current-state mapping between each external identifier you’ve observed and its corresponding, fully-merged 
canonical_segment_id
.

In the case study example, you’d see the following:

The profile_traits table

Use the 
profile_traits
 table for a singular view of your customer. With this table, you can view all custom traits, computed traits, SQL traits, audiences, and journeys associated with a profile in a single row.

The 
profile_traits
 table contains the last seen value for any of your customer profile traits that Segment processes as an Identify call.

If Segment later merges away a profile, it populates the 
segment_id
 it merged in the 
merged_to
 column.

In the case study example, Segment only collected email.  As a result, Segment would generate the following 
profile_traits
 table:

Merged profiles

Profiles that Segment merges away are no longer canonical.

This page was last modified: 29 Jan 2025

Need support?

Questions? Problems? Need more info? Contact Segment Support for assistance!

Help improve these docs!

Was this page helpful?

Thanks for your feedback!

Can we improve this doc? 
Send us feedback!

Get started with Segment

On this page

Was this page helpful?

Thanks for your feedback!

Can we improve
 this doc? 
Send us feedback!

Product

For Developers

Company

Support

© 2025 Segment.io, Inc.

=== Content from https://segment.com/docs/engage/audiences/generative-audiences-nutrition-facts/ ===

Generative Audiences Nutrition Facts Label
        

Twilio’s 
AI Nutrition Facts
 provide an overview of the AI feature you’re using, so you can better understand how the AI is working with your data. Twilio outlines AI qualities in Generative Audiences in the Nutrition Facts label below. For more information, including the AI Nutrition Facts label glossary, refer to the 
AI Nutrition Facts
 page.



AI Nutrition Facts






Generative Audiences

Description






Generate user audiences from text instructions 

Privacy Ladder Level 


1



Feature is Optional


Yes

Model Type 


Generative 

Base Model 


OpenAI - GPT-4 

Trust Ingredients

Base Model Trained with Customer Data


No

Customer Data is Shared with Model Vendor


No
 

Training Data Anonymized   


 N/A

Data Deletion


Yes

Human in the Loop 


Yes

Data Retention  


30 days

Input/Output Consistency


No

Other Resources

This page was last modified: 04 Mar 2024

Need support?

Questions? Problems? Need more info? Contact Segment Support for assistance!

Help improve these docs!

Was this page helpful?

Thanks for your feedback!

Can we improve this doc? 
Send us feedback!

Get started with Segment

Was this page helpful?

Thanks for your feedback!

Can we improve
 this doc? 
Send us feedback!

Product

For Developers

Company

Support

© 2025 Segment.io, Inc.

=== Content from https://segment.com/docs/unify/identity-resolution/identity-resolution-onboarding/ ===

Identity Resolution Onboarding
        

Unify requires a Business tier account and is included with Engage.


See the 
available plans
, or 
contact Support
.

On this page

The steps in this guide pertain to spaces created after 
October 5th, 2020
. For spaces created before 
October 5th, 2020
, please refer to 
Identity Resolution Settings
.

Workspace owners, administrators, and users with the Identity Admin role can edit Identity Resolution Settings.

Segment creates and merges user profiles based on a space’s Identity Resolution configuration. Segment searches for identifiers such as 
userId
, 
anonymousId
, and 
email
 on incoming events and matches them to existing profiles or creates new profiles. These identifiers display in the Identities tab of a User Profile in the Profile explorer.

Navigate to 
Unify > Profile explorer
 to view identities attached to a profile, along with custom traits, event history, and more.



Flat matching logic

After receiving a new event, Segment looks for profiles that match any of the identifiers on the event.

Based on the existence of a match, one of three actions can occur:

1: Create a new profile

When there are no pre-existing profiles that have matching identifiers to the event, Segment creates a new user profile.

2: Add to existing profile

When there is one profile that matches all identifiers in an event, Segment attempts to map the traits, identifiers, and events on the call to that existing profile. If there is an excess of any identifier on the final profile, Segment defers to the Identity Resolution rules outlined below.

3: Merge existing profiles

When there are multiple profiles that match the identifiers in an event, Segment checks the Identity Resolution rules outlined below, and attempts to merge profiles.

Identity Resolution settings

Identity Admins should first configure Identity Resolution Settings to protect the identity graph from inaccurate merges and user profiles.

During the space creation process, the first step is to choose an Identity Resolution configuration. If this is your first space, you have the option to choose a Segment-suggested Out-of-the-Box configuration or a custom Identity Resolution setup. All other spaces have a third option of importing settings from a different space.



Out-of-the-box

For most first-time users, Segment recommends that you use the out-of-the-box configuration and answer a short series of questions for a best-fit setup for your use-case.

If you have custom unique identifiers or don’t have a canonical 
user_id
, you’re automatically redirected to the Identity Resolution Settings page to complete your setup.

Custom rules

If you’re familiar with identity or have custom identifiers, Segment recommends that you select Custom Rules.

Segment redirects you to the Identity Resolution Settings page where you can add Default Identifiers or Custom Identifiers.

Segment’s 11 default are:

You can also provide a trait or property key to match on to add custom identifiers. You can preview the locations where Segment looks for the identifier. Segment accepts both camelCase and snake_case for context.traits, traits, and properties, but accepts lowercase types for identifiers only in the context.externalIds object.



Segment recommends that you proactively prevent using certain values as identifiers. While these values remain in the payload on the event itself, it is not promoted to an identifier Segment uses to determine user profiles.

This is important when developers have a hard-coded value for fields like 
user_id
 during QA or development that then erroneously make it to production. This may cause hundreds of profiles to merge incorrectly and can have costly consequences if these spaces already feed data into a production email marketing tool or push notification tool downstream.

In the past, Segment has seen certain default values that cause large amounts of profiles to merge incorrectly. Segment suggests that for every identifier, customers opt into automatically blocking the following suggested values:



Before sending data through, Segment also recommends that you add any default hard-coded values that your team uses during the development process, such as 
void
 or 
abc123
.

Identity Admins can specify the total number of values allowed per identifier type on a profile during a certain period. For example, in the image below, the 
anonymous_id
 field has a limit of 
5 Weekly
.


This will vary depending on how companies define a user today. In most cases, companies rely on 
user_id
 to distinguish user profiles and Segment defaults to the following configurations:

Specific cases may deviate from this default. For example, a case where a user can have more than one 
user_id
 but one email, like when 
shopify_id
 and an internal UUID define a user. In this case, an example configuration may be:

When you choose the limit on an identifier, ask the following questions about each of the identifiers you send to Segment:

Segment considers the priority of an identifier once that identifier exceeds the limit on the final profile.

For example, consider a Segment space with the following Identity Resolution configurations:

A profile already exists with 
user_id
 
abc123
 and 
email
 
jane@example1.com
. A new event comes in with new 
user_id
 
abc456
 but the same 
email
 
jane@example1.com
.

If this event maps to this profile, the resulting profile would then contain two 
user_id
 values and one 
email
. Given that 
user_id
 has a limit of 1, this exceeds the limit of that identifier. As a result, Segment checks the priority of the 
user_id
 identifier. Because 
email
 and 
user_id
 are the two identifiers on the event and 
email
 ranks lower than 
user_id
, Segment demotes 
email
 as an identifier on the incoming event and tries again.

At this point, the event searches for any profiles that match just the identifier user_id 
abc456
. Now there are no existing profiles with this identifier, so Segment creates a new profile with user_id 
abc456
.

By default, Segment explicitly orders user_id and email as rank 
1
 and 
2
, respectively. All other identifiers are in alphabetical order beginning from rank 
3
. This means that if the identifiers sent with events flowing into Segment are user_id, email, anonymous_id, and ga_client_id, the rank would be as follows:

If a new android.id identifier appeared without first giving it explicit order, the order would automatically reshuffle to:

If you require an explicit order for all identifiers, configure this in the Identity Resolution Settings page before sending in events.



When choosing the priority of your identifier, ask the following questions about each of the identifiers you send to Segment:

Importing from an existing space

This option is available to new spaces after you create an initial Dev space. Segment recommends this option when identity settings are validated as correct in the initial Dev space and should be copied into the Production space.

You can review the identifiers, priorities, limits, and blocked values before you complete the import.



Connect a source

After you configure Identity Resolution settings, the next step is to connect a 
source
 to the Segment space.

Create an audience

After you connect a source, Segment creates user profiles based off of replayed and newly incoming data.



The next step, which is important in the Dev space, is to create an audience to ensure that user profiles have populated correctly and that the Identity Resolution settings follow expected business logic.

For example, if there should be 100,000 distinct users who have a 
user_id
, this would be a great way to validate that the Identity Resolution settings have calculated profiles correctly.

For more information about how to create audiences and traits, see Segment’s 
Audiences docs
.

This page was last modified: 15 Oct 2024

Need support?

Questions? Problems? Need more info? Contact Segment Support for assistance!

Help improve these docs!

Was this page helpful?

Thanks for your feedback!

Can we improve this doc? 
Send us feedback!

Get started with Segment

On this page

Was this page helpful?

Thanks for your feedback!

Can we improve
 this doc? 
Send us feedback!

Product

For Developers

Company

Support

© 2025 Segment.io, Inc.

=== Content from https://segment.com/docs/guides/usage-and-billing/mtus-and-throughput/ ===

MTUs, Throughput and Billing
        

On this page



The graphic illustrates an example billing model with data flowing through Segment within a monthly period. Each event on the different touch points (for example, Sign-ups or Product Added) is calculated as one API call.

Segment detects that the user across two of the touch points is the same user based on their userID (userID 123) and deduplicates them, counting for one 
MTU
.

With Engage, this user falls into one audience, has one computed trait, and falls into one Journeys step, accounting for three compute credits total. Compute credits are not tied to an individual user, so multiple people could fall into these buckets, still accounting for a single compute credit for each trait/audience/journey step.

Finally, the example sends some user events to a destination function, which is charged according to function execution time.

What is an MTU?

MTU stands for “monthly tracked user”. (Keep reading to learn 
how Segment counts MTUs
.)

What is an API call?

When you use Segment to collect your data, you use the Segment tracking methods (Track, Page, Screen, Identify, Group, and Alias) which collect the data 
in a structured way
, and then send it to 
api.segment.io
. If you are using the Segment HTTP API, or sending batched data using a cloud-source, that data also goes through this Segment API endpoint.

Each data blob (with its properties or traits) goes through this endpoint, and is considered one “API call”.

What is throughput?

Depending on your Segment account type, your plan might include a throughput limit. The throughput limit tells you how many API calls and objects Segment allows you per MTU.

For example, if your workspace’s throughput limit is set to 250, this means that you can send a combined total of 250 API calls and objects to Segment each month per MTU you’ve paid for in your plan. If you have a 10,000 MTU plan, this means you can send up to a total of 2.5 million API calls and objects each month.

These objects and API calls are not tied to a specific user, but are an aggregate number applied to your workspace. Most customers never hit this limit, and Business tier plans often have custom limits.

You can sometimes “batch” API calls to reduce send times, however batching doesn’t reduce your throughput usage. Batched calls are unpacked as they are received, and the objects and calls the batch contains are counted individually. While batching does not reduce your throughput, it does reduce the possibility of rate limit errors.

How does Segment calculate MTUs?

Segment counts the number of 
unique
 
userId
s, and then adds the number of 
unique
 
anonymousId
s that were not associated with a 
userId
 during the billing period. Segment counts these IDs over all calls made from all sources in your workspace, over a billing month. Segment only counts each user once per month, even if they perform more than one action or are active across more than one source.

Imagine that you have both a website and a mobile app. Both the website and mobile app have pages that you can use without being logged in, and both send Identify calls when a user 
does
 log in.

As a simple example, imagine that a user is already logged in on both the website and the mobile app. When the user’s activity generates events on the website, these events are sent using Analytics.js, and include the user’s 
userId
. When they do things on the mobile app, these events are sent from a mobile source, and also include the 
userId
. When Segment counts the MTUs, all the events from the same 
userId
 only generate one MTU, regardless of the source it came from.

Now imagine a new user, who has never logged in. At first, they have two 
anonymousId
s, one for the mobile app and one for the website. However, if they log in during the course of the month, you now know who they are, and can attach their 
anonymousId
 to a 
userId
.

If the user logs in on 
just
 the app, you would still see two MTUs: one 
anonymousId
 for the website source, and one 
anonymousId
 with an attached 
userId
 from the mobile app source.
If the user logs in on 
both
 the app and website, they would count as one MTU: two different 
anonymousId
s attached to one 
userId
.

How do I see my usage data?

If you have questions about your data usage or how it relates to your bill, log into your Segment workspace, click 
Settings > Usage and Billing > Usage
.

The Usage page shows what plan the workspace is on, what data volume that plan includes, and how much data you have already used in the current billing period. If you have used more data volume than your plan includes, the page shows information about how much data is in overage.

Click the billing period dropdown at the top of the page to see a cumulative daily report of data volumes (by source) for the current billing period. The last five billing periods are also available, along with an overview of the last twelve months of data volumes.

What is the difference between an event and an object?

Understanding the difference between events and objects helps you understand how MTUs are calculated.

An event is a data collection triggered in response to a user action: a 
Track call
 (or a 
Page
/
Screen
 call if the action was to navigate to a new page). Events take place in a single moment in time, and include a name, timestamp, and 
properties
. When an event happens more than once, it creates a new Event record (with a new timestamp) rather than updating an existing one. For example, a user browsing a product catalog might generate several “Product Viewed” events, which might include the product name, price, and category.

This is in contrast to “Objects” which represent a single thing that persists over time and can be updated. Objects have “traits” (instead of properties) which record information about that object, and which can change over time. For example a “user” object could have a trait of “email” which doesn’t change often, but could also have a 
computed trait
 like 
logged_in_last_7_days
 that changes between 
true
 and 
false
 based on how much they use your site.

How is object throughput calculated?

Object Cloud Sources
 retrieve records from integration partners on a scheduled basis. Segment processes these records before writing them out to connected 
Storage Destinations
. Segment counts one object for each record retrieved from a Cloud Source. The number of objects ingested during a billing period has a direct impact on 
throughput
, which is calculated as 
(objects ingested + API calls received) / MTU allowance
.

Depending on the capabilities of the partner’s API, Segment may need to retrieve all available records and then deduplicate them prior to writing them out to a connected storage destination. In such cases, all retrieved records are still counted as ingested objects, even if the same records are retrieved multiple times in a given billing period. If you experience overages due to high object throughput, contact 
friends@segment.com
 to request a less frequent sync cadence.

MTUs, object throughput, and Cloud sources

If you use 
Cloud sources
 to pull in data from your third party services (in addition to tracking your users with Segment library sources), the data from these cloud apps 
can
 increase your MTU counts and object counts.

There are two types of cloud sources: 
object sources
, and 
event sources
. Object sources bring in information about entities, such as a person or company, which can change and have their properties updated over time. Events happen once in time, so while their properties don’t change, they can also happen more than once over time. (See 
above
 for more on objects vs events.)

Object sources 
do not
 increase your MTU count
 because the data included doesn’t usually contain an ID. Object sources can only send to Warehouses, and 
do
 affect the total object count which is used to calculate your 
throughput
. Some examples of object-sources are 
Salesforce
, 
Zendesk
, and 
Stripe
.

Event sources 
can
 create new MTUs
 because each event coming from this source includes either a 
userId
 or an 
anonymousId
 associated with the event. Some examples of event sources are 
Vero
, 
Drip
, and 
Youbora
.

Tip!
 You can check the 
Collections
 section of a cloud-source’s Segment documentation to see what type of data it sends. The Collections table lists each data type sent from the cloud source, and tells you if that data is an Object or an Event.

MTUs and Protocols

Protocols is a Business Tier feature. If you are on a Free or Team plan, this section does not apply to you.

Segment’s Protocols product allows you to selectively 
filter and block your incoming data
 to prevent malformed data from reaching destinations including your data warehouses and other storage solutions.

Tracking plan blocking: Blocked events are blocked from sending to all Segment Destinations, including warehouses and streaming Destinations. They’re blocked from reaching the entire Segment data pipeline. When you block an Event using a Tracking Plan, it does not count towards your MTU limit.

Blocked events (sometimes called “violations”) only count toward your MTU limit if you enable 
blocked event forwarding
 in your Source settings. You might do this to monitor issues with your incoming data as you continue to develop your tracking.

If you enable violation forwarding, it generates one (1) additional MTU in your workspace, total. If you are on an API billing plan, you are charged for the increased API volume generated by the forwarded violations. Forwarded violations might also generate costs in downstream destinations and data warehouses connected to the violations source.

MTUs and Engage

Engage
 is a Business Tier addon feature. If you are on a Free or Team plan this section does not apply to you (because you do not have this feature).

All Engage data are omitted from billing MTU and API throughput calculations, including 
computed traits
, 
SQL traits
, and 
audiences
.

MTUs and Replays

Replay
 is a Business Tier feature. If you are on a Free or Team plan, this section does not apply to you.

Replays only affect your MTU count if you are using a 
Repeater
 destination, which might send data that hasn’t yet been seen this month back through a source.

How Reverse ETL affects MTUs

Extracting data with Reverse ETL does 
not
 count toward your MTU usage. However, if you send that data through the 
Segment Connections destination
, it 
will
 affect your MTUs.

The Segment Connections destination is built for Reverse ETL and treats events as if they’re coming from a standard source, meaning they contribute to your MTU count.

For more information, see 
Reverse ETL usage limits
.

Why is my MTU count different from what I see in my destinations and other tools?

Different tools count users under different conditions, so comparing numbers between any two tools, or between Segment and a tool, rarely produces the same number. Each tool accepts slightly different incoming data, and they often reject or process the incoming data differently. Included are some example explanations of why you might see differing numbers below.

Contact 
Segment Product Support
 if for more information about a specific tool, or if you’re concerned that differing numbers might be due an implementation error.

Google Analytics requires that you include a 
url
 in any Page calls from a Segment server library. If you don’t include a 
url
, Google Analytics silently rejects the call, which can reduce the number of users you see in GA.

Segment does not pass data from 
Identify calls
 to Google because it is against Google’s terms of service to pass Personally Identifiable Information (PII) to the Google Analytics reporting interface. If you need to pass data from an Identify call, you can set up a 
Custom Dimension mapping
 to override this.

To pass the 
userId
 from your 
Identify calls
 to Google Analytics, go to the Google Analytics destination settings in the Segment web app, locate the 
Advanced Google Analytics settings
, and enable 
Send User-ID to GA
.

By default, Segment doesn’t send standard 
Page calls
 or 
Screen calls
 to Amplitude, which might reduce the number of unique users Amplitude sees.

To send Page and Screen calls to Amplitude, go to the Amplitude destination settings in the Segment web app, and locate the 
Advanced Options
 tab.

Amplitude can only automatically link an anonymous user to their logged-in 
userId
 if the events or traits come from a device-mode source (such as Analytics.js or a mobile library). If you use a server library or the Segment HTTP API, Amplitude can’t 
automatically
 connect the anonymous user to their logged-in identity. To work around this so Amplitude can connect the anonymous and identified user, make your Identify call when the user logs in, and include both the 
anonymousId
 from before the user logged in 
and
 the 
userId
 the user provided at log-in.

For Amplitude to associate both client-side and server-side activity with the same user, you must pass the same 
deviceId
 to Amplitude. Otherwise, Amplitude creates two users - one associated with the user’s 
deviceId
 and another user associated with the user’s Segment 
anonymousId
.

What might cause a spike in my MTU count?

There are several reasons why you might see a sudden increase in MTUs. Most of them are due to traffic fluctuations, however, some changes you make in code might also increase your MTU count, usually because you are (unexpectedly) generating a new 
anonymousId
 or 
userId
 for a single user.

If you think an implementation problem is causing an increase in your MTU count, 
contact Segment Product Support
 as soon as possible for help troubleshooting and resolving the issue.

MTU counts usually increase when the number users or visitors to parts of your site or application that use Segment tracking increase. Sometimes you’ll see a spike when you post a big press release, or marketing campaign that leads to an influx of visitors. Another potential cause of big increases is adding tracking to parts of your site or app that didn’t have tracking before.

Another possibility is an increase in the number of interactions with your users outside your app (emails, help desk, push notifications, etc) that you are importing using cloud sources. Tracking users you weren’t tracking before increases your MTU count unless you are able to pass a 
userId
 so they can be resolved with existing users. If you’re already tracking those users elsewhere with Segment, they are not counted a second time.

Users who are very privacy-conscious might cause your tracking to generate more MTUs; however in most cases these users are a fraction of a percentage of total traffic.

Check to see if you changed how you call 
analytics.reset()
. This utility method clears the old user identity information, and generates a new 
anonymousId
 each time you call it. This creates a user that Segment cannot resolve with an existing user until they are further identified.

Segment’s analytics libraries include methods that allow you to overwrite both the 
userId
 (using 
identify(xxx)
) and 
anonymousId
 (using 
analytics.user().anonymousId(xxx)
). Using these methods on a user whose tracking information already includes an ID can cause the user to be counted more than once.

If you find you need to use one of these overwrite methods, you should check to make sure that the field you are changing is 
null
 first. If the field is 
not
 null, you probably don’t want to overwrite it and lose the user’s original tracked identity.

If the pages you track are on more than one domain (for example, 
mydomain.com
 and 
mydomain.net
), the user generates a new 
anonymousId
 for each domain. However, if the domain is a subdomain (for example 
mydomain.com
 and 
app.mydomain.com
), they can share a user cookie and so share identity data and count as only one MTU.

If the user goes from one page to another and the second page loads in an iFrame, the page in the iFrame generates its own 
anonymousId
.

Where can I find information about Twilio Engage Channels billing?

Segment does not bill for SMS and Email sends from Engage Channels. For actual billed usage, refer to the Twilio and SendGrid accounts that you’ve linked to Engage.

This page was last modified: 06 Feb 2025

Need support?

Questions? Problems? Need more info? Contact Segment Support for assistance!

Help improve these docs!

Was this page helpful?

Thanks for your feedback!

Can we improve this doc? 
Send us feedback!

Get started with Segment

On this page

Was this page helpful?

Thanks for your feedback!

Can we improve
 this doc? 
Send us feedback!

Product

For Developers

Company

Support

© 2025 Segment.io, Inc.

=== Content from https://segment.com/docs/unify/debugger/ ===

Using the Profile Source Debugger
        

Unify requires a Business tier account and is included with Engage.


See the 
available plans
, or 
contact Support
.

On this page

The Profile Source Debugger enables you to inspect and monitor events that Segment sends downstream.

Because Segment generates a unique source for every destination connected to a Space, the Debugger gives you insight into how Segment sends events before they reach their destination. Even when a destination is removed, you can’t delete and shouldn’t disable this source for Segment to function as designed. The source will be reused by Segment as needed.

The Debugger provides you with the payload information you need to troubleshoot potential formatting issues and ensure Segment sends events as your destinations expect.

Modifying or disabling these system-generated sources could result in unforeseen issues and data loss.

Turning off these sources might impede profile updates, while removing them could compromise the stability of the Engage space, potentially making it irrecoverable. To ensure the system’s integrity, Segment highly recommends leaving these sources enabled.

Working with the Debugger

Navigate to the Debugger tab on the Unify settings page of the space you want to debug. Select the source you want to inspect in the Debugger.

The Debugger presents a stream of incoming events. The event inspector displays three tabs for each event:

Similar to the Connections Debugger, you can search through events using information contained within the event’s payload.

This page was last modified: 22 Nov 2024

Need support?

Questions? Problems? Need more info? Contact Segment Support for assistance!

Help improve these docs!

Was this page helpful?

Thanks for your feedback!

Can we improve this doc? 
Send us feedback!

Get started with Segment

On this page

Was this page helpful?

Thanks for your feedback!

Can we improve
 this doc? 
Send us feedback!

Product

For Developers

Company

Support

© 2025 Segment.io, Inc.

=== Content from https://segment.com/docs/getting-started/use-cases// ===

Use Cases Overview
        

On this page

Use Cases are pre-built Segment setup guides tailored to common business goals.

Use Cases eliminate guesswork with a structured approach to onboarding, helping you configure Segment correctly and align its features to your business objectives.

You can onboard to Segment with a Use Case if you’re a new Business Tier customer or haven’t yet connected a source and destination.

Onboard to Segment with Use Cases

Not sure where to start? Read through Segment's Choosing a Use Case guide, which breaks down the available business goals and their associated use cases.

Follow the steps in the Use Cases Setup guide to get up and running with Segment.

Looking for something more technical? View the Use Cases Reference, which lists the tracking events, connections, and destinations Segment recommends for each use case.

Take the next step

Explore the following core Segment features, all of which power Use Cases.

Collect event data from your mobile apps, websites, and servers.

Forward your data to the business tools and apps your business uses.

Track user interactions, resolve their identities, and explore Profiles.

Build, enrich, and activate audiences with Segment's personalization platform.

This page was last modified: 08 Oct 2024

Need support?

Questions? Problems? Need more info? Contact Segment Support for assistance!

Help improve these docs!

Was this page helpful?

Thanks for your feedback!

Can we improve this doc? 
Send us feedback!

Get started with Segment

On this page

Was this page helpful?

Thanks for your feedback!

Can we improve
 this doc? 
Send us feedback!

Product

For Developers

Company

Support

© 2025 Segment.io, Inc.

=== Content from https://segment.com/docs/unify/profiles-sync/sample-queries/ ===

Profiles Sync Sample Queries
        

Unify requires a Business tier account and is included with Engage.


See the 
available plans
, or 
contact Support
.

On this page

On this page, you’ll find queries that you can run with Profiles Sync to address common use cases.

The examples in this guide are based on a Snowflake installation. If you’re using another warehouse, you may need to adjust the syntax.

About example schemas

The queries on this page use two example schemas:

These schema names may not match your own.

Monitor and diagnose identity graphs

These queries let you view and manage identity graphs, which give you insight into unified customer profiles generated by 
identity resolution
.

Show how many profiles Segment creates and merges per hour

This example queries the 
id_graph_udpates
 table to measure the rate at which Segment creates and merges profiles, as well as the type of event that triggered the profile change:

SELECT

    
DATE_TRUNC
(
'hour'
,
timestamp
)
 
as
 
hr
,

    
CASE

      
WHEN
 
canonical_segment_id
=
segment_id

      
THEN
 
'profile creation'
 
ELSE
 
'profile merge'

    
END
 
as
 
profile_event
,

    
triggering_event_type
,

    
COUNT
(
DISTINCT
 
triggering_event_id
)
 
as
 
event_count


FROM
 
ps_segment
.
id_graph_updates


GROUP
 
BY
 
1
,
2
,
3



Isolate profiles that have reached an identifier’s maximum configured value

Segment’s 
configurable identifier limits
 let you set maximum values for identifiers like email. These maximum configured values help prevent two separate users from being merged into a single Profile.

The following query lets you view Profiles that have reached a configured limit for the email identifier:

WITH
 
agg
 
AS
 
(

    
SELECT

        
canonical_segment_id
,

        
COUNT
(
LOWER
(
TRIM
(
external_id_value
)))
 
as
 
value_count
,

        
LISTAGG
(
external_id_value
,
', '
)
 
as
 
external_id_values

    
FROM
 
ps_materialize
.
external_id_mapping

    
WHERE
 
external_id_type
=
'email'

    
GROUP
 
BY
 
1


)


SELECT

    
canonical_segment_id
,

    
external_id_values
,

    
value_count


FROM
 
agg


WHERE
 
value_count
 
>
 
5
 
-- set to your configured limit



Reconstruct a profile’s traits

Identify the source that generated the value for a particular trait for a canonical profile as well as its child profiles

When a merge occurs, Segment selects and associates a single trait value with a profile. This logic depends on how you materialize the 
profile_traits
 table.

You can break out a profile, though, to see the trait versions that existed before the merge. As a result, you can identify a particular trait’s origin.

The following example inspects a particular profile, 
use_XX
, and trait, 
trait_1
. The query reports the profile’s last observed trait, its source ID, and any profiles Segment has since merged into the profile:

SELECT
 
*
 
FROM
 
(

  
SELECT

    
ids
.
canonical_segment_id
,

    
ident
.
segment_id
,

    
ident
.
event_source_id
,

    
ident
.
trait_1
,

    
row_number
()
 
OVER
(
PARTITION
 
BY
 
ident
.
segment_id
 
ORDER
 
BY
 
ident
.
timestamp
 
DESC
)
 
as
 
rn

  
FROM
 
ps_segment
.
identifies
 
as
 
ident

  
INNER
 
JOIN
 
ps_materialize
.
id_graph
 
as
 
ids


ON
 
ids
.
segment_id
 
=
 
ident
.
segment_id


AND
 
ids
.
canonical_segment_id
 
=
 
'use_XXX'


AND
 
ident
.
trait_1
 
IS
 
NOT
 
NULL


)
 
WHERE
 
rn
=
1



Measure and model your customer base

Pull a complete list of your customers, along with their merges, external identifiers, or traits

The following three snippets will provide a full list of your customers, along with:

SELECT

  
canonical_segment_id
,

  
LISTAGG
(
segment_id
,
 
', '
)
 
as
 
associated_segment_ids


FROM
 
ps_materialize
.
id_graph


GROUP
 
BY
 
1



SELECT

  
canonical_segment_id
,

  
LISTAGG
(
external_id_value
 
||
 
'('
 
||
 
external_id_type
 
||
 
')'
,
 
', '
)
 
as
 
associated_segment_ids


FROM
 
ps_materialize
.
external_id_mapping


GROUP
 
BY
 
1



SELECT
 
*
 
FROM
 
ps_materialize
.
profile_traits
 
WHERE
 
merged_to
 
IS
 
NULL



Pull the latest subscription status set for every profile identifier in the space

Provides the latest subscription status set for all identifiers in the space. This query will not include identifiers that have no subscription status ever set.

SELECT
 
evt1
.
user_id
,
  
evt1
.
channel
,
 
evt1
.
_id
 
id
,
 
evt1
.
status
,
 
evt1
.
received_at


FROM
 
ps_segment
.
CHANNEL_SUBSCRIPTION_UPDATED
 
evt1


JOIN
 
(

  
SELECT
 
_id
,
 
MAX
(
received_at
)
 
AS
 
max_received_at

  
FROM
 
ps_segment
.
CHANNEL_SUBSCRIPTION_UPDATED

  
GROUP
 
BY
 
_id


)
 
evt2


ON
 
evt1
.
_id
 
=
 
evt2
.
_id
 
AND
 
evt1
.
received_at
 
=
 
evt2
.
max_received_at


ORDER
 
BY
 
1



Show all pages visited by a user

To get complete user histories, join event tables to the identity graph and aggregate or filter with 
id_graph.canonical_segment_id
:

SELECT

    
id_graph
.
canonical_segment_id
,

    
pages
.
*


FROM
 
ps_segment
.
pages


LEFT
 
JOIN
 
ps_materialize
.
id_graph

    
ON
 
id_graph
.
segment_id
 
=
 
pages
.
segment_id


WHERE
 
canonical_segment_id
 
=
 
‘
use_XX
..
’



Show the complete history of a trait or audience membership associated with a customer

Suppose you want to track a user’s entrances and exits of the audience 
aud_1
. Running the following query would return all qualifying entrance and exits:

SELECT

    
id_graph
.
canonical_segment_id
,

    
identifies
.
aud_1
,

    
identifies
.
timestamp


FROM
 
ps_segment
.
identifies


INNER
 
JOIN
 
ps_materialize
.
id_graph

    
ON
 
id_graph
.
segment_id
 
=
 
identifies
.
segment_id

    
AND
 
identifies
.
aud_1
 
IS
 
NOT
 
NULL



This query works with any Trait or Audience membership, whether computed in Engage or instrumented upstream.

FAQs

Yes. Engage sends updates to Audience membership (as a boolean) and computed trait value updates as traits on an Identify call that Segment forwards to your data warehouse.

The column name corresponds to the Audience or Trait key shown on the settings page:

Surface these values the same way as any other trait value:

Identity merges change Segment’s understanding of who performed historical events.

For example, if 
profile_b
 completed a “Product Purchased” event but Segment understands that 
profile_b
 should be merged into 
profile_a
, Segment deduces that 
profile_a
 performed that initial “Product Purchased” event.

With that in mind, here’s how to differentiate between 
segment_id
 and 
canonical_segment_id
:

The mapping between these two identifiers materializes in your 
id_graph
 table. If a profile has not been merged away, then 
segment_id
 is equivalent to 
canonical_segment_id
. If a profile has been merged away, 
id_graph
 reflects that state.

As a result, you can retrieve a customer’s complete event history by joining an event table, like 
product_purchased
 to 
id_graph
.

For more information, view the 
Profiles Sync tables guide
.

Profiles Sync mimics the materialization performed by 
Segment Unify
. A user’s merges, external IDs, and traits should be expected whether they’re queried in the warehouse, Profile API, or viewed in the UI.

The following edge cases might drive slight (<0.01%) variation:

By contrast, Segment Unify and incrementally-built Profiles Sync materializations won’t combine already-computed traits across two merged profiles at the moment of merge. Instead, one profile’s traits will be chosen across the board.

The 
external_id_hash
 is a hash of the 
external_id_type
 and 
external_id_value
 using SHA-1. This field corresponds to the 
primary_key
 for the table: 
hash (external_id_type and external_id_value)
.
For example, in BigQuery the logic is: 
TO_HEX(SHA1(concat(external_id_type, external_id_value))) as seg_hash
.

This page was last modified: 04 Jul 2024

Need support?

Questions? Problems? Need more info? Contact Segment Support for assistance!

Help improve these docs!

Was this page helpful?

Thanks for your feedback!

Can we improve this doc? 
Send us feedback!

Get started with Segment

On this page

Was this page helpful?

Thanks for your feedback!

Can we improve
 this doc? 
Send us feedback!

Product

For Developers

Company

Support

© 2025 Segment.io, Inc.

=== Content from https://segment.com/docs/connections/reverse-etl/reverse-etl-source-setup-guides/redshift-setup/ ===

Redshift Reverse ETL Setup
        

On this page

Set up Redshift as your Reverse ETL source.

Redshift Reverse ETL sources support Segment's dbt extension

If you have an existing dbt account with a Git repository, you can use 
Segment’s dbt extension
 to centralize model management and versioning, reduce redundancies, and run CI checks to prevent breaking changes.

To set up Redshift with Reverse ETL:

Run the SQL commands below to create a user named 
segment
.

 
--
 
create
 
a
 
user
 
named
 
"
segment
"
 
that
 
Segment
 
will
 
use
 
when
 
connecting
 
to
 
your
 
Redshift
 
cluster
.

 
CREATE
 
USER
 
segment
 
PASSWORD
 
'
<enter password here>
'
;


 
--
 
allows
 
the
 
"
segment
"
 
user
 
to
 
create
 
new
 
schemas
 
on
 
the
 
specified
 
database
.
 
(
this
 
is
 
the
 
name
 
you
 
chose
 
when
 
provisioning
 
your
 
cluster
)

 
GRANT
 
CREATE
 
ON
 
DATABASE
 
"
<enter database name here>
"
 
TO
 
"
segment
"
;



Extra Permissions

Give the 
segment
 user read permissions for any resources (databases, schemas, tables) the query needs to access.

Give the 
segment
 user write permissions for the Segment managed schema (
__segment_reverse_etl
), which keeps track of changes to the query results.

Troubleshooting

Extraction failures: relation does not exist

If you are able to run the query in the Query Builder, but the sync fails with the 
relation does not exist
 error, please make sure the schema name is included before the database table name, and check that the schema name is correct:

SELECT
 
id
 
FROM
 
<
schema_name
>
.
<
table_name
>



After you’ve successfully added your Redshift source, 
add a model
 and follow the rest of the steps in the Reverse ETL setup guide.

This page was last modified: 10 Jun 2024

Need support?

Questions? Problems? Need more info? Contact Segment Support for assistance!

Help improve these docs!

Was this page helpful?

Thanks for your feedback!

Can we improve this doc? 
Send us feedback!

Get started with Segment

On this page

Was this page helpful?

Thanks for your feedback!

Can we improve
 this doc? 
Send us feedback!

Product

For Developers

Company

Support

© 2025 Segment.io, Inc.

=== Content from https://segment.com/docs/connections/storage/warehouses/redshift-tuning/ ===

Speeding Up Redshift Queries
        

On this page

Waiting minutes and minutes, maybe even an hour, for your queries to compute is an unfortunate reality for growing companies. Whether your data has grown faster than your cluster, or you’re running too many jobs in parallel, there are lots of reasons your queries might be slowing down.

To help you improve your query performance, this guide takes you through common issues and how to mitigate them.

Common Causes for Slow Queries

1. Not enough space

As your data volume grows and your team writes more queries, you might be running out of space in your cluster.

To check if you’re getting close to your max, run this query. It will tell you the percentage of storage used in your cluster. Segment recommends that you don’t exceed 75-80% of your storage capacity. If you approach that limit, consider adding more nodes to your cluster.

SELECT
 
sum(pct_used);


FROM
 
svv_table_info;



Learn how to resize your cluster.

2. Inefficient queries

Another thing you’ll want to check is if your queries are efficient. For example, if you’re scanning an entire dataset with a query, you’re probably not making the best use of your compute resources.

Some tips for writing performant queries:

Consider using 
INNER joins
 as they are more efficient than 
LEFT joins
.

Stay away from 
UNION
 whenever possible.

Specify multiple levels of conditionals when you can.

Use 
EXPLAIN
 to show the query execution plan and cost.

To learn more about writing beautiful SQL, check out these resources:

Periscope on Query Performance

Mode on Performance Tuning SQL Queries

Chartio on Improving Query Performance

3. Running multiple ETL processes and queries

Some databases like Redshift have limited computing resources. Running multiple queries or ETL processes that insert data into your warehouse at the same time will compete for compute power.

If you have multiple ETL processes loading into your warehouse at the same time, especially when analysts are also trying to run queries, everything will slow down. Try to schedule them at different times and when your cluster is least active.

If you’re a Segment Business Tier customer, you can schedule your sync times under Warehouses Settings.



You also might want to take advantage of Redshift’s 
Workload Management
 that helps ensure fast-running queries won’t get stuck behind long ones.

4. Default WLM Queue Configuration

As mentioned before, Redshift schedules and prioritizes queries using 
Workload Management
. Each queue is configured to distribute resources in ways that can optimize for your use-case.

The default configuration is a single queue with only 5 queries running concurrently, but Segment discovered that the default only works well for low-volume warehouses. More often than not, adjusting this configuration can improve your sync times.

Before Segment’s SQL statements, Segment uses 
set query_group to "segment";
 to group all the queries together. This allows you to create a queue that isolates Segment’s queries from your own. The maximum concurrency that Redshift supports is 50 across 
all
 query groups, and resources like memory distribute evenly across all those queries.

Segment’s initial recommendation is for 2 WLM queues:

a queue for the 
segment
 query group with a concurrency of 
10

leave the default queue with a concurrency of 
5



Generally, Segment is responsible for most writes in the databases Segment connects to, so having a higher concurrency allows Segment to write as fast as possible. If you’re also using the same database for your own ETL process, you may want to use the same concurrency for both groups. You may even require additional queues if you have other applications writing to the database.

Each cluster may have different needs, so feel free to stray from this recommendation if another configuration works better for your use-case. AWS provides some 
guidelines
, and you can always 
contact us
 as Segment is more than happy to share the learnings while working with Redshift.

Pro-tips for Segment Warehouses

In addition to following performance best practices, here are a some more optimizations to consider if you’re using Segment Warehouses.

Factors that affect load times

When Segment is actively loading data into your data warehouse, Segment is competing for cluster space and storage with any other jobs you might be running. Here are the parameters that influence your load time for Segment Warehouses.

Performance optimizations

To make sure you have enough headroom for quick queries while using Segment Warehouses, here are some tips!

Hopefully these steps will help to speed up your workflow! If you need any other help, feel free to 
contact us
.

This page was last modified: 21 Apr 2023

Need support?

Questions? Problems? Need more info? Contact Segment Support for assistance!

Help improve these docs!

Was this page helpful?

Thanks for your feedback!

Can we improve this doc? 
Send us feedback!

Get started with Segment

On this page

Was this page helpful?

Thanks for your feedback!

Can we improve
 this doc? 
Send us feedback!

Product

For Developers

Company

Support

© 2025 Segment.io, Inc.

=== Content from https://segment.com/docs/protocols/transform/ ===

Use Transformations to fix bad data
        

Protocols is available as an add-on for Business plans only.


See the 
available plans
, or 
contact Support
.

On this page

What is a Transformation?

With Transformations, you can change data as it flows through Segment to either correct bad data or customize data for a specific destination. Change event and property names to align events with your Tracking Plan, or to conform to a destination-specific requirement. For example, you can create a Transformation to change an event name from 
completed_order
 to 
Order Completed
 to conform to 
Segment’s ecommerce spec
.

You can also use 
Segment’s Public API
 to transform events, properties, and property values for many 
use cases
.

Transformations are very powerful and should be applied with care.

Transformations 
irrevocably
 change the event payloads flowing through Segment and immediately affect either all destinations, or a single downstream destination, depending on your settings.

As soon as you apply a Transformation, the original tracking payloads are not easily recoverable.

Segment’s goal is to make Transformations a powerful tool that complements a well structured Tracking Plan. Together, these features help your organization scale and achieve high data quality. For that reason Segment recommends that you start your data quality strategy with a clearly defined Tracking Plan. Without this critical component, the risk of creating conflicting or detrimental transformations increases.

Other important notes

View all Transformations

All Protocols Transformations are listed in the Transformations tab in the Protocols section. The list view supports filtering and sorting to organize transformations by transformation type, source, and destination.



Transformations can be enabled and disabled directly from the list view using the toggle.

Transformations can be deleted and edited by clicking on the overflow menu. When editing a Transformation, only the resulting event or property names, and Transformation name can be edited. If you want to select a different event or source, create a separate Transformation rule.

Transformations created using the Public API

On the Transformations page in the Segment app, you can view and rename transformations that you created with the Public API. In some cases, you can edit these transformations in the UI.

Create a Transformation

To create a Transformation, navigate to the Transformations tab in Protocols and click 
New Transformation
 in the top right. A three-step wizard guides you through creating a transformation.

Workspace Owner or Source Admin permissions are required to create and edit transformations.
Source Read-only permissions are required to view transformations.



Step 1: Select the transformation type

To create a Transformation, you first need to select which type of transformation you want to create. For each transformation type, Segment displays a description, use cases, and example payload. Current transformation types available in your Segment workspace include:

Rename track event:
 Rename track event name at the source or per destination. The events listed in the event names dropdown menu correspond to the events listed on the 
source schema view
.


Edit track event properties:
 Rename multiple properties and/or change property data structure at the source or per destination


Edit identify or group event traits:
 Rename multiple traits and/or change trait data structure at the source or per destination


View more 
use cases
 of Transformations available in both your workspace and 
Segment’s Public API
.

Step 2: Set up the transformation

Depending on the transformation type you selected, relevant drop-down selectors and fields are presented to define how you want to transform the data.

Multiple transformations cannot be created for the same source + type + event + destination combo. This restriction blocks circular transformations (for example, 
order_updated
 to 
orderUpdated
 to 
order_updated
), minimizes unexpected transformations, and enables easy filtering across each dimension.

Regardless of the type of transformation selected, first select a source. Each Transformation can only apply to a single source. While this makes it more difficult to apply transformations broadly, it ensures you are only transforming data relevant to the selected source.

After selecting the source, you will need to select a scope. Scope determines where Segment applies the transformation.

Source-scoped Transformations only apply to cloud-mode, S3, and data warehouse destinations.



Source scope:

Events are transformed in all 
active Segment cloud-mode destinations, warehouses, and S3 destinations.
 This scope is best when you want to fix malformed events before sending them to all destinations. These transformations should be treated as a temporary solution to hold you over while your engineering team fixes the root event.

Destination scope:

Events are transformed in ONLY the selected cloud-mode destination. 
Device-mode destinations, S3, and data warehouses are not currently supported.
 Use the Destination scope when you want to customize an event to the unique requirements of a destination. These transformations can exist permanently.

Depending on the type of transformation you selected, you will need to enter the relevant event, property, or trait mappings to create the transformation.

Select an Event:

After you select the scope, use the search box to choose the event to transform. You can 
only
 select a single track event, identify or group call. If you are renaming the event, simply enter the new name in the provided text field.

Rename properties or traits:

To rename properties or traits within a selected event, click 
+ Add Property
. The dropdown that appears contains the properties or traits sent with the selected event. Segment supports JSON Path notation to select nested objects up to four levels deep. For example, 
order.id
 selects the 
id
 property in the 
order
 object. Segment does not support 
.$.
 notation to select a property from an array of objects. For example, the following event, which generates 
products.$.product_id
, is unsupported.

analytics
.
track
(
'
Example
'
,
 
{

  
products
:
 
[{

      
product_id
:
 
"
123
"

  
}],


})



In this scenario, we do not support the transformation of product_id.

After selecting a property/trait, select JSON Path or Simple String to change the property/trait. Simple string will change the name in-line, while JSON path allows you to move the property/trait in or out of an object.

When you see properties that have the escape character 
\
 in them - this escape character 
\
 is added to differentiate between a property name that has a . in it, and a nested field, like so:

ingredients.salad → "ingredients": { "salad": "yum" }
ingredients\.salad → "ingredients.salad": "yum"


Step 3: Name the transformation and enable it

Enter a descriptive name to act as a label for the transformation. This label helps you organize your Transformations, and Segment recommends that you make this descriptive and focused on the problem you’re solving. For example 
Fix misnamed order_completed event for ecommerce spec
 is much better than 
Map order_completed
.

In this step, you can also choose to keep the Transformation disabled, so you can come back and edit it later. To update, enable, or disable a Transformation, click on the overflow menu and select 
Edit Transformation
.

Use Cases

Here’s a list of Segment Transformations with some use case examples.

Rename an event:
 Change an event name from 
viewed_product
 to 
Product Viewed
.

Rename a property or trait:
 Change the property name 
revenue
 to 
total
 for a specific destination.

Update a property value:
 Use 
Segment’s Public API
 to transform the property 
currency
 to have the value 
USD
.

Property Transformations

fqlDefinedProperties": [{"fql": "uppercase("United States)", "propertyName": "properties.propertyValue1"}]


or dynamic casing functions:

fqlDefinedProperties": [{"fql": "lowercase(properties.propertyValue1)", "propertyName": "properties.propertyValue1"}]


fqlDefinedProperties": [{"fql": "lowercase(properties.prop1)", "propertyName": "properties.prop2"}]


Segment displays an error if the following property conflicts occur:

This page was last modified: 07 Feb 2025

Need support?

Questions? Problems? Need more info? Contact Segment Support for assistance!

Help improve these docs!

Was this page helpful?

Thanks for your feedback!

Can we improve this doc? 
Send us feedback!

Get started with Segment

On this page

Was this page helpful?

Thanks for your feedback!

Can we improve
 this doc? 
Send us feedback!

Product

For Developers

Company

Support

© 2025 Segment.io, Inc.

=== Content from https://segment.com/docs/connections/sources/ ===

Sources Overview
        

On this page

Related content

What is a source?

A source is a website, server library, mobile SDK, or cloud application which can send data into Segment. It’s where your data originates. Add a source to collect data to understand who your customers are and how they’re using your product. Create a source for each website or app you want to track. While it’s not required that you have a single source for each server, site, or app, you should create a source for each unique source of data.

Each source you create has a write key, which is used to send data to that source. For example, to load 
analytics.js
, the Segment JavaScript library
 on your page, the snippet on the 
Quickstart Guide
 includes:

analytics
.
identify
(
'
user_123
'
,
 
{

  
email
:
 
'
jane.kim@example.com
'
,

  
name
:
 
'
Jane Kim
'

  
});



If you don't see the source you're looking for in our catalog

If a tool is not listed as a supported source in Segment’s 
catalog
, then it is not possible to incorporate the integration out-of-the-box within a Segment workspace. However, as an alternative, you can use the 
HTTP API
 source to collect data from the tool’s API. You can also use 
Functions
 to send or receive data from other tools.

Types of sources

Segment has three types of sources:

Event streams sources

Event streams sources collect data from your website or app to monitor user actions. These sources include 
website libraries
, 
mobile
, and 
server sources
.

Source Overview

The Source Overview page for an event stream source shows you a 
pipeline view
 of all events Segment receives from your source, events that failed on ingest, events that are filtered at the source level, and “eligible events”, which are the events that will flow into your destinations. If you select one of the steps in the pipeline view, you can see a line chart that reflects the fluctuations in volume alongside a 
breakdown table
 that has more details about the events impacted by the selected step.

The pipeline view shows each of the four steps Segment encounters when processing data from your source:

You can use the time picker located on the Source Overview page to specify a time period (last 10 minutes, 1 hour, 24 hours, 7 days, 2 weeks, or a custom date range over the last two weeks) for which you’d like to see data. Segment sets the time picker to show data for the last 24 hours by default.



The breakdown table displays three tabs, 
Event type
, 
Event name
, and 
App version
.

Each of these tabs displays an event count, which is the total number of events that Segment received in a particular step.

The Unnamed or batched events under the 
Event Name
 tab is a collection of all identify and page/screen calls in the source.

Website libraries

Analytics.js
, the JavaScript library, is the most powerful way to track customer data from your website. If you’re just starting out, Segment recommends it over server-side libraries as the simplest installation for any website.

Analytics and data collection is a very broad topic and it can be quite overwhelming. How do you get started?

Mobile

Segment’s Mobile SDKs are the best way to simplify your iOS, Android, and Xamarin app tracking. Try them over server-side sources as the default installation for any mobile app.

Analytics-Flutter library

The Analytics-Flutter library is currently only available in pilot phase and is governed by Segment’s 
First Access and Beta Preview Terms
. If you’d like to try out this library, access the 
Analytics-Flutter GitHub repository
.

Server

Segment’s server-side sources let you send analytics data directly from your servers. Segment recommends tracking from your servers when device-mode tracking (tracking on the client) doesn’t work. Check out the 
guide on server-side tracking
 if you’re not sure whether it makes sense for your use case.

Cloud-mode tracking

Server-side data management is when tag sends data to the Segment servers, which then passes that data to the destination system.

Cloud app sources

Cloud app sources empower you to pull together data from all of your different third-party tools into a Segment warehouse or to your other enabled integrated tools. They send data about your users from your connected web apps. There are two types of Cloud Apps: 
Object cloud sources
 and 
Event cloud sources
.

Wondering which cloud-apps send which types of data? Check out the Cloud Sources comparison.

Object Cloud Sources

These Cloud App Sources can export data from its third party tool and import it directly into your Segment warehouse. Make sure you have a Segment warehouse enabled before you enable any of the following sources:

Facebook Ads

Google Ads

HubSpot

Intercom

Mailchimp

Mandrill

Marketo

Salesforce

Salesforce Marketing Cloud

SendGrid

Stripe

Twilio

Zendesk

Event Cloud Sources

These Cloud App Sources can not only export data into your Segment warehouse, but they can 
also
 federate the exported data into your other enabled Segment integrations:

ActiveCampaign

Aircall

Airship

Alloy Flow

Amazon S3

Amplitude Cohorts

Antavo

Authvia

AutopilotHQ

Beamer

Blip

Bluedot

Blueshift

Braze

Candu

Chatlio

CleverTap

CommandBar

ConfigCat

Customer.io

Delighted

Drip

Dub

Elastic Path

Elastic Path CX Studio

Facebook Lead Ads

Factual Engine

Foursquare Movement

Freshchat

Friendbuy

Gladly

GWEN Webhooks

Herow

IBM Watson Assistant

Inflection

Insider

Iterable

Jebbit

Klaviyo

Klenty

LaunchDarkly

Leanplum

Listrak

LiveLike (Source)

Looker

Mailjet

Mailmodo

Mixpanel Cohorts

MoEngage (Source)

Moesif API Analytics

Navattic

Nudgespot

One Creation

OneSignal

OneTrust

Paytronix

Pendo

ProveSource

Pushwoosh Source

Qualtrics

Quin AI

Radar

Refiner

Selligent Marketing Cloud

SendGrid Marketing Campaigns

Shopify (by Littledata)

Shopify - Powered by Fueled

Statsig

Synap

Upollo

UserGuiding

Vero

Voucherify

White Label Loyalty

WorkRamp

Yotpo

HTTP

If Segment doesn’t have a library for your environment, you can send your data directly to the 
HTTP Tracking API
. All of Segment’s other sources and platforms use the HTTP API to work their magic behind the scenes.

Pixel

Segment’s 
Pixel Tracking API
 lets you track events from environments where you can’t execute code, like tracking email opens.

Reverse ETL sources

Reverse ETL sources are data warehouses that enable you to use 
Reverse ETL
 to send data from your warehouse source to your destinations.

Reverse ETL supports these sources:

Segment is actively working on adding more sources. If you’d like to request Segment to add a particular source, please note it on the 
feedback form
.

Create a source

To create a source:

Once you’ve created a source, the source is automatically enabled and can immediately receive events. You can review your new events in that source’s 
Debugger
 tab.

Sources not connected to an enabled destination are disabled after 14 days

If your source is not connected to any destinations or is only connected to disabled destinations, Segment automatically disables this source after 14 days, even if the source is receiving events. Disabled sources will no longer receive data. 
You can view when Segment disables your destination in your workspace’s 
Audit Trail
 as 
Event : Source Disabled
 with 
Actor : Segment
.
Workspace members receive an email notification before Segment disables your source so that your team has time to take action.
If you would like to prevent this behavior in your workspace, fill out 
this Airtable form
.

One source or multiple sources?

Segment suggests that you create one source for each type of data you want to collect. For example, you might have one source for all of your website tracking and a different source for any mobile tracking. Creating one source per data type provides the following benefits:

Library tiers

Segment has defined three tiers for libraries: Flagship, Maintenance, and Community. These tiers indicate the level of support, enhancements, and maintenance each library receives from Segment.

The criteria for assigning a library to a tier include its overall usage by customers and the availability of newer versions. Here’s how Segment defines each tier:

If a library falls into one of these tiers, you’ll see the tier label at the beginning of the library’s page.

This page was last modified: 18 Nov 2024

Further reading

A list of the available sources on the Segment platform.

Need support?

Questions? Problems? Need more info? Contact Segment Support for assistance!

Help improve these docs!

Was this page helpful?

Thanks for your feedback!

Can we improve this doc? 
Send us feedback!

Get started with Segment

On this page

Related content

Was this page helpful?

Thanks for your feedback!

Can we improve
 this doc? 
Send us feedback!

Product

For Developers

Company

Support

© 2025 Segment.io, Inc.

=== Content from https://segment.com/docs/engage/campaigns/mobile-push/ ===

Mobile Push Onboarding
        

Engage Premier requires a Business tier account and includes Engage Foundations and Unify.


See the 
available plans
, or 
contact Support
.

On this page

Engage Premier entered an End of Sale (EOS) period effective  June 10, 2024. Existing Segment customers will continue to have access and support to Engage Premier until an end-of-life (EOL) date is announced. We recommend exploring the following pages in preparation of a migration or future MCM needs:

Twilio Marketing Campaigns

Preferred ISV Partners:

Airship Blog
 


Bloomreach Blog
 


Braze Blog
 


Insider Blog
 


Klaviyo Blog
 


Twilio Engage Foundations Documentation
 

This page walks you through the process of setting up mobile push notifications using Segment, Twilio, and Firebase/Apple Developer.

Prerequisites

Please reach out to your CSM or AE prior to trying out this feature.
This guide assumes familiarity with Swift and Kotlin and is intended for a developer audience.

Overview

You’ll set up mobile push in four stages:

1. Set up analytics for mobile push

Before you can send mobile pushes, you’ll need to set up analytics. In this step, you’ll integrate Segment’s mobile SDK into your app.

Add the Segment base SDK

This section outlines the process for adding Segment’s base SDK to your app, including the Analytics Kotlin, Analytics-Swift, and React Native libraries.

You must initialize your Analytics instance in the Application class, otherwise you may experience issues with customization and delivery confirmation.

Follow these steps to integrate Analytics Kotlin:

 
<
uses
-
permission
 
android:
name
=
"android.permission.INTERNET"
/>

 
<
uses
-
permission
 
android:
name
=
"android.permission.ACCESS_NETWORK_STATE"
/>




For detailed instructions on integrating Analytics Kotlin, follow the steps in the 
Analytics Kotlin getting started section
.

Follow these steps to integrate Analytics-Swift for iOS & Apple:

For detailed instructions on integrating Analytics-Swift, follow the steps in the 
Analytics-Swift getting started section
.

Follow these steps to integrate the React Native library:

For detailed instructions on integrating Analytics for React Native, follow the steps in the 
Analytics for React Native getting started section
.

2. Add the Engage SDK plugin

Next, you’ll add the Engage SDK plugins for both iOS and Android to your application.

Instructions for iOS

Now that you’ve integrated Analytics-Swift, follow the steps in this section to add the Engage Plugin for iOS.

You can add the Engage SDK plugin using either Xcode or 
Package.swift
.

Instructions for adding the plugin with Xcode

In the Swift packages search dialog, enter the following URL:

 https://github.com/segment-integrations/analytics-swift-engage


Instructions for adding the plugin with 
Package.swift

.package(
            name: "Segment",
            url: "https://github.com/segment-integrations/analytics-swift-engage.git",
            from: "1.1.2"
        ),


Import the plugin in the file where you configure your Analytics instance:

 import Segment
 import TwilioEngage // <-- Add this line.


After your Analytics-Swift library setup, call 
analytics.add(plugin: ...)
 to add an instance of the plugin to the Analytics timeline:

 let analytics = Analytics(configuration: Configuration(writeKey: "<YOUR WRITE KEY>")
                     .flushAt(3)
                     .trackApplicationLifecycleEvents(true))

 let engage = TwilioEngage { previous, current in
     print("Push Status Changed /(current)")
 }

 analytics.add(plugin: engage)


To start receiving and handling mobile push notifications, add or modify the following methods in your 
AppDelegate
:

   
func
 
application
(
_
 
application
:
 
UIApplication
,
 
didFinishLaunchingWithOptions
 
launchOptions
:
 
[
UIApplication
.
LaunchOptionsKey
:
 
Any
]?)
 
->
 
Bool
 
{


    
//Add the following:


        
let
 
center
  
=
 
UNUserNotificationCenter
.
current
()

        
center
.
delegate
 
=
 
self

        
center
.
requestAuthorization
(
options
:
 
[
.
sound
,
 
.
alert
,
 
.
badge
])
 
{
 
(
granted
,
 
error
)
 
in

            
guard
 
granted
 
else
 
{

                
Analytics
.
main
.
declinedRemoteNotifications
()

                
Tab1ViewController
.
addPush
(
s
:
 
"User Declined Notifications"
)

                
return

            
}

            
DispatchQueue
.
main
.
async
 
{

                
UIApplication
.
shared
.
registerForRemoteNotifications
()

            
}

        
}

        
        
// The following conditional statement is necessary to handle remote notifications in older versions of iOS.

        
if
 
let
 
notification
 
=
 
launchOptions
?[
UIApplication
.
LaunchOptionsKey
.
remoteNotification
]
 
as?
 
[
String
:
 
Codable
]
 
{

            
Tab1ViewController
.
addPush
(
s
:
 
"App Launched via Notification 
\(
notification
)
"
)

            
Analytics
.
main
.
receivedRemoteNotification
(
userInfo
:
 
notification
)

        
}


        
...


        
return
 
true


}



func
 
application
(
_
 
application
:
 
UIApplication
,
 
didRegisterForRemoteNotificationsWithDeviceToken
 
deviceToken
:
 
Data
)
 
{

    
// Segment event to register for remote notifications

    
Analytics
.
main
.
registeredForRemoteNotifications
(
deviceToken
:
 
deviceToken
)


}



func
 
application
(
_
 
application
:
 
UIApplication
,
 
didFailToRegisterForRemoteNotificationsWithError
 
error
:
 
Error
)
 
{

    
// Segment event for failure to register for remote notifications

    
Analytics
.
main
.
failedToRegisterForRemoteNotification
(
error
:
 
error
)


}



func
 
application
(
_
 
application
:
 
UIApplication
,
 
didReceiveRemoteNotification
 
userInfo
:
 
[
AnyHashable
 
:
 
Any
])
 
async
 
->
 
UIBackgroundFetchResult
 
{

    
// Segment event for receiving a remote notification

    
Analytics
.
main
.
receivedRemoteNotification
(
userInfo
:
 
userInfo
)


    
// TODO: Customize notification handling based on the received userInfo.

    
// Implement actions or UI updates based on the notification content.


    
return
 
.
noData


}



func
 
userNotificationCenter
(
_
 
center
:
 
UNUserNotificationCenter
,
 
didReceive
 
response
:
 
UNNotificationResponse
)
 
async
 
{

    
let
 
userInfo
 
=
 
response
.
notification
.
request
.
content
.
userInfo

    
//Segment event for receiving a remote notification

    
Analytics
.
main
.
receivedRemoteNotification
(
userInfo
:
 
userInfo
)


    
// TODO: Customize notification response handling based on the received userInfo.

    
// Implement actions based on the user's response to the notification.

    
// Example: Navigate to a specific screen or perform an action based on the notification.



}



The previous steps are required. For configuration options, including subscription statuses and media handling, visit the 
getting started section
 of Segment’s Twilio Engage Plugin documentation on GitHub.

Instructions for Android

Now that you’ve integrated Analytics-Kotlin, follow these steps to add the Engage Plugin for Android:

Add the following to your Gradle dependencies:

     
implementation
 
'com.segment.analytics.kotlin.destinations:engage:<LATEST_VERSION>'



Add the following service to the 
application
 tag of your 
AndroidManifest.xml
 file:

     
<service

         
android:name=
"com.segment.analytics.kotlin.destinations.engage.EngageFirebaseMessagingService"

         
android:exported=
"true"
>

         
<intent-filter>

         
<action
 
android:name=
"com.google.firebase.INSTANCE_ID_EVENT"
/>

         
<action
 
android:name=
"com.google.firebase.MESSAGING_EVENT"
 
/>

         
</intent-filter>

     
</service>



Add this plugin to your Analytics instance:

     
analytics
.
add
(
TwilioEngage
(
applicationContext
))



The previous steps are required. For configuration options, including subscription statuses and customized actions, visit the 
getting started section
 of Segment’s Twilio Engage Destination documentation on GitHub.

Next, you’ll configure your iOS and Android push credentials for use with Twilio Notify and Twilio Notifications.

3. Configure iOS push notifications

3a. Set up an App ID

Before you begin, log into your 
Apple development account
 and click on 
Identifiers
 under the 
Certificates, Identifiers & Profiles
 section. This will show a list of identifiers, including App IDs.

3b. Create a certificate

Next, you’ll create a push notification certificate, which lets your app receive notifications. You can either make a development certificate or a production certificate. This guide explains how to make a development certificate. Segment recommends that you use Xcode managed certificates.

Segment recommends that you use Xcode managed certificates for your application. If you prefer to create your certificate manually, follow these steps:

You just created an Apple Development iOS Push Services certificate, which you can now download and double-click to add to your Mac’s keychain.

3c. Create a credential for Twilio

You’ll extract your certificate key and private key from this file — you need these two keys to create a Twilio credential. First, run this command in Terminal:

openssl pkcs12 
-in
 cred.p12 
-nokeys
 
-out
 cert.pem 
-nodes



cert.pem
 is your certificate key file. Next, run the following command in the terminal:

openssl pkcs12 
-in
 cred.p12 
-nocerts
 
-out
 key.pem 
-nodes



key.pem
 is your private key file. Next, run this command to process this key:

openssl rsa 
-in
 key.pem 
-out
 key.pem


You can now paste your credentials into the modal found in the Twilio Console. Make sure that you strip anything 
outside
 of the 
-----BEGIN CERTIFICATE-----
 and 
-----END CERTIFICATE-----
 boundaries and outside of the 
-----BEGIN RSA PRIVATE KEY-----
 and 
-----END RSA PRIVATE KEY-----
 boundaries before pasting your credentials. Check the 
Sandbox
 button if you made a development certificate. Sandbox is synonymous with development mode.

Once you save a credential, the 
CERTIFICATE
 and 
PRIVATE KEY
 fields are hidden for security reasons.

After you’ve pasted your credentials, click 
Save
. You should see an SID appear on the new page; copy it to your clipboard, as you’ll need it in the next step.

3d. Configure your Twilio Service to use your APNS credentials

Twilio lets you build multiple applications within a single account. To separate those applications, you need to create Service instances that hold all the data and configuration for a given application.

To do so, you’ll need to configure your Service instance to use the Credential that contains your APNS certificate and private key. You can do that using the Services page in the Console. You’ll need to update your Service with the Twilio Push Credential SID.

If you’re just getting started, set up the APN credential first, then create your Service by clicking the blue plus button on the 
Services Console
 page.

4. Configure Android push notifications

Follow the steps in Twilio’s 
Configuring Android Push Notifications
.

During Step 5, 
Upload your API Key to Twilio
, follow these steps:

With your server key copied, finish steps 5 and 6 in the Twilio documentation.

5. Configure mobile push in Engage

Follow these steps to set up mobile push in Twilio Engage.

5a. Set up Twilio credentials

Follow the steps in 5a only if you’re new to Twilio Engage Premier. If you’ve already 
configured messaging services
 as part of Twilio Engage Premier onboarding, you can skip to 5b.

Removing messaging services

To remove a messaging service, navigate to Engage > Engage settings > Channels and click the pencil icon under 
Twilio messaging service
. Enter the account credentials by either using the API key secret or creating a new API key. Once you’ve selected the desired services, they will override the existing ones, effectively removing the ones you no longer need.

5b. Create a new push service

Complete mobile push onboarding by creating a new push service:

Build a mobile push template

Now that you’ve completed mobile push setup, you’re ready to 
build a mobile push template
.

This page was last modified: 13 Aug 2024

Need support?

Questions? Problems? Need more info? Contact Segment Support for assistance!

Help improve these docs!

Was this page helpful?

Thanks for your feedback!

Can we improve this doc? 
Send us feedback!

Get started with Segment

On this page

Was this page helpful?

Thanks for your feedback!

Can we improve
 this doc? 
Send us feedback!

Product

For Developers

Company

Support

© 2025 Segment.io, Inc.

=== Content from https://segment.com/docs/ ===

Segment Documentation
          

Learn how to use Segment to collect, responsibly manage, and integrate your customer data with hundreds of tools.

Learn about Segment, plan and work through a basic implementation, and explore features and extensions.

How can Segment help you?

Integrate the tools you need for analytics, growth, marketing, and more.

Prevent data quality issues with a tracking schema and enforcement with Protocols.

Build audiences and journeys from real-time customer data to personalize experiences on every channel.

Keep customer data private with Segment's data discovery and policy enforcement tools.

Get Data into Segment

The Segment Spec helps you identify, capture, and format meaningful data for use with Segment libraries and APIs as well as downstream tools.

Use Track, Page, Identify, and other Segment tracking calls.

Save time by letting Segment calls collect information for you.

Use our business-case specs to ensure that your tools get the most from your data.

Learning about Segment

The basics of your Segment implementation.

Over a dozen how-to guides that help you accomplish common tasks.

Connect your app to Segment

Additional Resources

Segment's Analytics Academy walks you through the wide world of analytics, including best practices, an overview of the most popular tools, and case studies of how other developers have achieved success.

For a more hands-on tutorial of Segment, check out Segment University. It offers step-by-step instructions, starting with first steps and going through some of our more advanced features.

Need support?

Questions? Problems? Need more info? Contact Segment Support for assistance!

Help improve these docs!

Was this page helpful?

Thanks for your feedback!

Can we improve this doc? 
Send us feedback!

Get started with Segment

Product

For Developers

Company

Support

© 2025 Segment.io, Inc.

=== Content from https://segment.com/docs/guides/intro-user/ ===

Segment for Data Users
        

On this page

If you aren’t involved in setting up your Segment implementation, or are just starting to set up Destinations for your organization’s workspace, this guide is for you.

What is Segment?

If you read the 
detailed explanation of Segment
 on the previous page, you can skip ahead!

Segment is a system for sending messages from your websites, mobile apps, and servers. These messages contain data about events on, or users of those systems, and these messages can sent on to other tools, and gathered together in a warehouse for later analysis. Segment can also bring in information about your users from external systems, such as helpdesks or CRM systems, and collate that information to help you analyze your data, build audiences of users, and personalize your users’ experiences.

Once you (or your organizations’ developers) have your Segment Sources set up and sending data, you can log in to the Segment App and set up Destinations, which are how Segment sends that data to other tools (like Google Analytics, Mixpanel, and many others).

Environments and Labels

Depending on your organization’s configuration and access settings, you might be able to see one or multiple Environments (for example, “Production”, “Testing”, “Development”), or one or multiple 
Labels
, which control access to different parts of your organization’s Segment system.  If you see several environments, contact your Segment administrator for more details so you can make sure you make your changes in the right place.

Data inside Segment

Data enters the Segment systems from Sources, but once data is in the system, your organization may have different tools configured to control and change it. This could change what data is available to you, or any destinations you set up.

For example, Protocols makes sure that data coming into Segment follows specific formats and patterns, and might block and discard malformed or unwanted data. The Privacy tool can be configured to remove Personally Identifiable Information (PII) from the data. And several different methods are available to 
filter data
 so that it doesn’t send certain types of events, or reach specific destinations or warehouses.

Set up a Destination

Depending on the access level you have in your organization’s Segment workspace, you might be able to create new Destinations, or you might only be able to edit existing ones.

To add a new Destination, you’ll usually need some information (such as a token or API key) from the destination tool to start. You’ll enter that into the Segment App so we can connect to and send data to that tool. You’ll also need to know which Source you’ll be sending data from.

To set up a destination:

Tip
: Segment usually is able to translate data into a format that the destination expects, however some destinations (such as Adobe Analytics) may require manual mapping steps to configure properly. If you see additional fields for mapping configuration, read the documentation for that destination to learn more.

Troubleshooting

If you’re setting up a destination to use 
cloud-mode data
 (data that’s sent through Segment, rather than directly from a user’s device), you can use the 
Event Tester
 and 
Event Delivery
 tools to check that data is arriving, and being correctly delivered to the destination.

Have suggestions for things to add to this guide? 
Drop us a line
!

This page was last modified: 14 Jul 2021

Need support?

Questions? Problems? Need more info? Contact Segment Support for assistance!

Help improve these docs!

Was this page helpful?

Thanks for your feedback!

Can we improve this doc? 
Send us feedback!

Get started with Segment

On this page

Was this page helpful?

Thanks for your feedback!

Can we improve
 this doc? 
Send us feedback!

Product

For Developers

Company

Support

© 2025 Segment.io, Inc.

=== Content from https://segment.com/docs/unify/data-graph// ===

Data Graph
        

Unify requires a Business tier account and is included with Engage.


See the 
available plans
, or 
contact Support
.

On this page

The Data Graph acts as a semantic layer that allows businesses to define relationships between various entity datasets in the warehouse — such as accounts, subscriptions, households, and products — with the Segment Profile. It makes these relational datasets easily accessible to business teams for targeted and personalized customer engagements.

Prerequisites

To use the Data Graph, you’ll need the following:

Step 1: Set up Data Graph permissions in your data warehouse

Data Graph, Reverse ETL, and Profiles Sync require different warehouse permissions.

To get started with the Data Graph, set up the required permissions in your warehouse. Segment supports the following:

To track the data sent to Segment on previous syncs, Segment uses 
Reverse ETL
 infrastructure to store diffs in tables within a dedicated schema called 
_segment_reverse_etl
 in your data warehouse. You can choose which database or project in your warehouse this data lives in.

Step 2: Connect your warehouse to the Data Graph

To connect your warehouse to the Data Graph:

Step 3: Build your Data Graph

The Data Graph is a semantic layer that represents a subset of relevant business data that marketers and business stakeholders can use for audience targeting and personalization in downstream tools. Use the configuration language spec and the following features to build your Data Graph:

Key steps to build your Data Graph

Defining Relationships

Similar to the concept of 
cardinality in data modeling
, the Data Graph supports 3 types of relationships:

Data Graph Example



data_graph
 
{

    
version
 
=
  
"
v1.0.0
"

  
    
# Define entities

    
entity
 
"
account-entity
"
 
{

      
name
 
=
 
"
account
"

      
table_ref
 
=
 
"
PRODUCTION.CUST.ACCOUNT
"

      
primary_key
 
=
 
"
ID
"

    
}

  
    
entity
 
"
product-entity
"
 
{

      
name
 
=
 
"
product
"

      
table_ref
 
=
 
"
PRODUCTION.PROD.PRODUCT_SKUS
"

      
primary_key
 
=
 
"
SKU
"

    
}

  
    
entity
 
"
cart-entity
"
 
{

      
name
 
=
 
"
cart
"

      
table_ref
 
=
 
"
PRODUCTION.CUST.CART
"

      
primary_key
 
=
 
"
ID
"

      
enrichment_enabled
 
=
 
true

    
}


    
entity
 
"
household-entity
"
 
{

      
name
 
=
 
"
household
"

      
table_ref
 
=
 
"
PRODUCTION.CUST.HOUSEHOLD
"

      
primary_key
 
=
 
"
HOUSEHOLD_ID
"

    
}


    
entity
 
"
subscription-entity
"
 
{

      
name
 
=
 
"
subscription
"

      
table_ref
 
=
 
"
PRODUCTION.CUST.SUBSCRIPTION
"

      
primary_key
 
=
 
"
SUB_ID
"

    
}

  
    
# Define the profile entity, which corresponds to Segment Profiles tables synced via Profiles Sync

    
# Recommend setting up Profiles Sync materialized views to optimize warehouse compute costs

    
profile
 
{

      
profile_folder
 
=
 
"
PRODUCTION.SEGMENT
"

      
type
 
=
 
"
segment:materialized
"

  
      
# First branch - relate accounts table to the profile

      
# This is a unique type of relationship between an entity and the profile block

      
relationship
 
"
user-accounts
"
 
{

        
name
 
=
 
"
Premium Accounts
"

        
related_entity
 
=
 
"
account-entity
"

        
# Join the profile entity with an identifier (e.g. email) on the related entity table

        
# Option to replace with the trait block below to join with a profile trait on the entity table instead

        
external_id
 
{

          
type
 
=
 
"
email
"

          
join_key
 
=
 
"
EMAIL_ID
"

        
}

  
        
# Define 1:many relationship between accounts and carts

        
# e.g. an account can be associated with many carts

        
relationship
 
"
user-carts
"
 
{

          
name
 
=
 
"
Shopping Carts
"

          
related_entity
 
=
 
"
cart-entity
"

          
join_on
 
=
 
"
account-entity.ID = cart-entity.ACCOUNT_ID
"

    
          
# Define many:many relationship between carts and products

          
# e.g. there can be multiple carts, and each cart can be associated with multiple products

          
relationship
 
"
products
"
 
{
 
            
name
 
=
 
"
Purchased Products
"

            
related_entity
 
=
 
"
product-entity
"

            
junction_table
 
{

              
primary_key
 
=
 
"
ID
"

              
table_ref
 
=
 
"
PRODUCTION.CUSTOMER.CART_PRODUCT
"

              
left_join_on
 
=
 
"
cart-entity.ID = CART_ID
"

              
right_join_on
 
=
 
"
PRODUCT_ID = product-entity.SKU
"

            
}
      
          
}

        
}

      
}


      
# Second branch - relate households table to the profile by joining with an external ID block

      
relationship
 
"
user-households
"
 
{

        
name
 
=
 
"
Households
"

        
related_entity
 
=
 
"
household-entity
"

        
external_id
 
{

          
type
 
=
 
"
email
"

          
join_key
 
=
 
"
EMAIL_ID
"

        
}

  
        
# Define 1:many relationship between households and subscriptions

        
# e.g. a household can be associated with multiple subscriptions

        
relationship
 
"
user-subscriptions
"
 
{

          
name
 
=
 
"
Subscriptions
"

          
related_entity
 
=
 
"
subscription-entity
"

          
join_on
 
=
 
"
household-entity.SUB_ID = subscription-entity.HOUSEHOLD_ID
"

    
}


}




3a: Define entities

The first step in creating a Data Graph is to define your entities. An entity corresponds to a table in the warehouse.

Example:

data_graph
 
{

    
entity
 
"
account-entity
"
 
{

      
name
 
=
 
"
account
"

      
table_ref
 
=
 
"
PRODUCTION.CUST.ACCOUNT
"

      
primary_key
 
=
 
"
ID
"

    
}

    
    
entity
 
"
cart-entity
"
 
{

      
name
 
=
 
"
cart
"

      
table_ref
 
=
 
"
PRODUCTION.CUST.CART
"

      
primary_key
 
=
 
"
ID
"

      
enrichment_enabled
 
=
 
true

    
}


}



3b: Define the profile

Segments recommends that you select materialized views under the Profiles 
Selective Sync settings
 to optimize warehouse compute costs.

Next, define the profile. This is a special class of entity that represents Segment Profiles, which corresponds to the Profiles Sync tables and models. For Linked Audiences, this allows marketers to filter on profile traits, event history, etc. There can only be one profile for a Data Graph.

Example:



data_graph
 
{

    
# Define entities

    
...

  
    
# Define the profile entity, which corresponds to Segment Profiles tables synced via Profiles Sync

    
# Recommend setting up Profiles Sync materialized views to optimize warehouse compute costs

    
profile
 
{

      
profile_folder
 
=
 
"
PRODUCTION.SEGMENT
"

      
type
 
=
 
"
segment:materialized
"

    
}


}




3c: Define relationships

Now define your relationships between your entities. Similar to the concept of 
cardinality in data modeling
, the Data Graph supports 3 types of relationships below. All relationship types require you to define the relationship slug, name, and related entity. Each type of relationship has unique join on conditions.

This is the first level of relationships and a unique type of relationship between the Segment profile entity and a related entity.

To define a profile-to-entity relationship, reference your entity table and depending on your table columns, choose to join on one of the following:

Option 1 (Most common) - Join on an external ID:
 Use the 
external_id
 block to join the profile entity with an entity table using external IDs from your 
Unify ID resolution
 settings. Typically these identifiers are  
user_id
, 
email
, or 
phone
 depending on the column in the entity table that you want to join with.

Option 2 - Join on a profile trait:
 Use the 
trait
 block to join the profile entity with an entity table using 
Profile Traits
.

Example:

data_graph
 
{
 
    
entity
 
"
account-entity
"
 
{

      
name
 
=
 
"
account
"

      
table_ref
 
=
 
"
PRODUCTION.CUST.ACCOUNT
"

      
primary_key
 
=
 
"
ID
"

    
}

  
    
# Define additional entities...


    
# Note: Relationships are nested

    
profile
 
{

      
profile_folder
 
=
 
"
PRODUCTION.SEGMENT
"

      
type
 
=
 
"
segment:materialized
"

  
      
# Relate accounts table to the profile 

      
relationship
 
"
user-accounts
"
 
{

        
name
 
=
 
"
Premium Accounts
"

        
related_entity
 
=
 
"
account-entity
"

  
        
# Option 1: Join the profile entity with an identifier (e.g. email) on the related entity table

        
external_id
 
{

          
type
 
=
 
"
email
"

          
join_key
 
=
 
"
EMAIL_ID
"

        
}

  
        
# Option 2: Join the profile entity with a profile trait on the related entity table

        
trait
 
{

          
name
 
=
 
"
cust_id
"

          
join_key
 
=
 
"
ID
"

        
}

      
}

    
}


}



For 1:many relationships, define the join on between the two entity tables using the spec below.

Example:

data_graph
 
{
 
    
entity
 
"
cart-entity
"
 
{

      
name
 
=
 
"
cart
"

      
table_ref
 
=
 
"
PRODUCTION.CUST.CART
"

      
primary_key
 
=
 
"
ID
"

    
}

  
   
# Define additional entities...


    
# Note: Relationships are nested

    
profile
 
{

      
profile_folder
 
=
 
"
PRODUCTION.SEGMENT
"

      
type
 
=
 
"
segment:materialized
"

                   
      
relationship
 
"
user-accounts
"
 
{

        
...

  
        
# Define 1:many relationship between accounts and carts

        
relationship
 
"
user-carts
"
 
{

          
name
 
=
 
"
Shopping Carts
"

          
related_entity
 
=
 
"
carts-entity
"

          
join_on
 
=
 
"
account-entity.ID = cart-entity.ACCOUNT_ID
"

        
}

      
}

    
}


}



For many:many relationships, define the join on between the two entity tables with the 
junction_table
.

Attributes from a junction table are not referenceable via the Linked Audience builder. If a marketer would like to filter upon a column on the junction table, you must define the junction as an entity and define a relationship.

Junction table spec

Example:



data_graph
 
{
 
    
# Define entities


    
# Note: Relationships are nested

    
profile
 
{

      
# Define profile

         
      
relationship
 
"
user-accounts
"
 
{

        
...

  
        
relationship
 
"
user-carts
"
 
{

          
...

  
          
# Define many:many relationship between carts and products

          
relationship
 
"
products
"
 
{

            
name
 
=
 
"
Purchased Products
"

            
related_entity
 
=
 
"
product-entity
"

            
junction_table
 
{

              
table_ref
 
=
 
"
PRODUCTION.CUSTOMER.CART_PRODUCT
"

              
primary_key
 
=
 
"
ID
"

              
left_join_on
 
=
 
"
cart-entity.ID = CART_ID
"

              
right_join_on
 
=
 
"
PRODUCT_ID = product-entity.SKU
"

            
}

          
}

        
}

      
}

  
}


}

         


Step 4: Validate your Data Graph

You can validate your Data Graph using the preview, then click Save. After you’ve set up your Data Graph, your partner teams can start leveraging these datasets with with 
Linked Events
 and 
Linked Audiences
.

Edit and manage your Data Graph

To edit your Data Graph:

View Data Graph data consumers

A data consumer refers to a Segment feature like Linked Events and Linked Audiences that are referencing datasets, such as entities and/or relationships, from the Data Graph. You can view a list of data consumers in two places:

Understand changes that may cause breaking and potential breaking changes

Upon editing and saving changes to your Data Graph, a modal will pop up to warn of breaking and/or potential breaking changes to your data consumers. You must acknowledge and click 
Confirm and save
 in order to proceed.

Detect warehouse breaking changes

Segment has a service that regularly scans and monitors the Data Graph for changes that occur in your warehouse that may break components of the Data Graph, like when the table being referenced by the Data Graph gets deleted from your warehouse or when the primary key column no longer exists. An alert banner will be displayed on the Data Graph landing page. The banner will be removed once the issues are resolved in your warehouse and/or the Data Graph. You will also have the option to trigger a manual sync of your warehouse schema.

Receive alerts for warehouse breaking changes

Configure alerts for breaking changes to receive notifications over Slack, email, or in-app notification whenever Segment detects a breaking change in your warehouse.

To configure alerts for breaking changes:

This page was last modified: 26 Feb 2025

Need support?

Questions? Problems? Need more info? Contact Segment Support for assistance!

Help improve these docs!

Was this page helpful?

Thanks for your feedback!

Can we improve this doc? 
Send us feedback!

Get started with Segment

On this page

Was this page helpful?

Thanks for your feedback!

Can we improve
 this doc? 
Send us feedback!

Product

For Developers

Company

Support

© 2025 Segment.io, Inc.

=== Content from https://segment.com/docs/connections/sources/visual-tagger/ ===

Visual Tagger
        

On this page

Visual Tagger entering maintenance mode

Visual Tagger is entering maintenance mode on 
April 5th, 2021
.

You can continue to use Visual Tagger with sources on which it’s already enabled. However, the feature will no longer be available to new Segment customers and existing customers will not be able to add new Visual Tagger sources.

Segment is committed to enabling customers to collect and deliver high quality customer data to the tools they need to run their businesses. As a CSS-based event tracking method, Visual Tagger has limitations that can prevent detailed data from being consistently collected. For code-based collection best practices, see the 
Segment Spec
.

Going forward, support for the feature will include:

The following best practices can make your website more compatible with the feature and eliminate common issues:

Visual Tagger is a tool that enables you to collect data about what your customers are doing on your websites without having to write any code. Specifically, it helps you implement 
track
 events by pointing and clicking on parts of your website.

With Visual Tagger, you can:

The Visual Tagger has two main views: the 
Visual Tagger Home
 and the 
Event Editor
, which shows your website in an iframe.

info “Analytics.js snippet required for the Visual Tagger” 
The website you’re tagging must include the Segment analytics.js snippet before you can use the Visual Tagger.

Setting up Visual Tagger

Before you begin

Get the following things set up before you use Visual Tagger:

Note
: Your website must use HTTPS.

Enable Visual Tagger

Once you have all the prerequisites set up, you’re ready to get started:

As you click each event, you’re prompted to select the part of the website that should trigger the event, and describe the properties that event should have.

You are now ready to tag events on your website!

Tip
: If you ever need to get back to this screen, you can navigate to the JavaScript website source again, click the 
Visual tagger
 tab, then click 
Add Event
.

Step 1: Choose the event type and select an element

To create events, start in the 
Event Editor
 on the 
Build
 screen. This page shows an iframe with your website.



You can either choose one of the Recommended Events, or you can click 
Add Event
 and create a new one. Recommended Events work the same way, except Segment defines the Event Name, and each event comes with a set of default properties.

When you click 
Add Event
, three types of events are available:

Button or Link Click
. Select any 
button
 and 
link
 elements on the page to create events that fire when a user clicks that button or link. Depending on the construction of your website, some elements that look like buttons might not actually be CSS “button” elements - use the “Any Element Click” event instead for these elements.

Form Submit
. Select this option to highlight any 
form
 elements on the page so you can select them. When you choose Form Submit, Segment adds all form fields to your event automatically as 
properties
.

Any Element Click
. Selecting this option will allow you to click on any element on your page. This will allow you to create an event for whenever a user on your website clicks on that element.



Once you choose an event type, mouse over your website in the iframe to highlight elements that you can select to create an event, and click one to start the process.

If the element on the page has siblings, you can tag them all at the same time, or tag just the one specific element you clicked.

When you choose a Recommended Event, it works much in the same way, but the Event Name is pre-set and each event comes with a set of default properties.

The events you create are not saved until you publish them.

Step 2: Add details to the event

When you click on an element on your website, a window appears where you can enter the details for the event.

Event Name
. Edit the event name to be simple yet descriptive.
Segment recommends that you use an “Object Action” format (for example, 
Blog Post Clicked
, and use Title Case (capitalize the first letter of each word ) when naming events.

Once you’re satisfied with the event name and properties, click 
Add
.

Tip
! You can create more than one event before you move on to the next step.



More information on properties

When you use Visual Tagger, you can create 
static properties
 by entering the value that the property should have. When a property is static its value is hardcoded, and is always the same regardless of any actions from the user.

You can also create 
dynamic properties
 by selecting a piece of text from the website that you want as the value for that property. When a property is dynamic, its value is different depending on what the user clicked.

For example, if you run an e-commerce web shop and want to create an event that fires every time a user clicks on a product on your search results page, you would create a 
Product Clicked
 event.

To specify where the event was fired from, you would add a property called 
location
 to the event. You would hardcode the value of that property as “Search Results Page”. This is an example of a static property.

To show which product the user clicked, you can add a property called 
product_name
 to the event. You can use the Visual Tagger “select from page” feature to select the text on the iframe-version of your website that gives the product name. The value of the 
product_name
 property would then be different, depending on what users click.

More information on forms

When you tag a form you can track both that the user 
submitted
 the form, and also update traits about the user.

This is useful if the form collects information about the user (such as Name, Email, Country). (This wouldn’t be useful on a Search or Add To Cart form.)

This is an illustration of how traits and events might appear for a user in a Destination or Engage.



If you enable the Identify Users feature, you can specify which traits to update in the same way as properties for the event.



More information on URLs

By default, events created using the Visual Tagger only fire on the same URL as the one where you tagged the event. This prevents unexpected events if the same CSS selector exists on other pages of your site.

If you have similar pages with different URLs such as 
/products/1
 and 
/products/2
 and you want to same event to fire on both, you can change the 
URL Page Targeting
 to match 
/products
 instead of 
/products/1
 or any page on the entire website.

Step 3: Test the event(s)

Once you finish filling out the event details, click 
Continue
 to go to the Test and Publish screen. On this screen you can test your event to confirm that it works as expected before you publish it.

Test your events by clicking around on your website in the iframe, and doing things on the iframe-version of your site that should trigger the event.

If something doesn’t look right, click 
Back
 to return to the Build screen and edit your tags.

If you’re having trouble validating your events, see 
Troubleshooting Tips
.

Step 4: Publish the event(s)

When you’ve finished setting up and testing your events, click 
Publish
.

Events can take up to 10 minutes appear on your website. Once they are live, events begin sending to Segment when people visit your website and interact with the elements that you created events for. The data from these events appears in the Website Source’s Debugger.

Step 5: Test the event to confirm that it works

After you publish your event and wait for ~10 minutes, do a last test to make sure your tags are working expected and that you see your data landing in your Segment Debugger.

In one window, open up your website where you created the event. In another window (side by side), open the 
Segment Debugger
 for your JavaScript Source. The Debugger is a Segment tool that shows a live stream of the data coming from that Source.

Good to know
: The Debugger automatically pauses after ~1 minute of idle time. Refresh the page if you leave and come back to it, and don’t see new data.

On your website, pretend that you are an end-user and try to trigger the event that you created in Visual Tagger. Watch the Debugger in the other window to confirm that the event fired when you took that action.

All events created using Visual Tagger automatically get a context property that says 
"visual_tagger" : true
 so that you can distinguish between events that you implemented using Visual Tagger and events that you implemented using code.



Managing and editing Visual Tagger events

Once you publish your events, they appear on the 
Visual Tagger Home
 view in the 
All Published Events
 table. From here, you can create click 
Add Event
 to create new events, and edit or delete existing ones.



Troubleshooting your events

If your events are not working as expected, try the following steps to troubleshoot the issue.

Ensure that you toggle Visual Tagger integration on. You find it in the Settings for your Source.

Ensure that your website has analytics.js installed and running. You can confirm this by visiting your website and watching the Segment Debugger. A 
page
 call should flow into your Debugger whenever someone visits a page on your website if you have implemented analytics.js. Note that if you have an ad blocker enabled when you visit your website, data will not get fired into Segment when you interact with your website.

Confirm that the CSS selector has not changed in any way since you created the event in the Visual Tagger (for example, a button could change locations or be removed entirely). Because Visual Tagger relies on the CSS selector to tie events to user actions, if the CSS selector changes, the event stops sending. If this happens, edit the event in Visual Tagger and update the CSS selector. Segment does not have a way to alert you when events you created using Visual Tagger begin to fail.

Because Visual Tagger relies on stable CSS selectors to fire events, it is not compatible with websites that have CSS selectors that are dynamically generated on every page load.

If your website has any components in iframes (for example, if you embed Typeforms into your site), Visual Tagger cannot create events for those components. Segment recommends that you install analytics.js on the iframed-in site and use Visual Tagger directly on that site.

If you use the same name for multiple events (whether in code or using Visual Tagger), duplicate events are 
not
 created downstream. Instead, Segment merges those events into one event.

If your events still don’t work as expected, contact 
Segment Customer Support
 for help.

FAQs

Are there situations where Visual Tagger will not work?

Yes! Visual Tagger relies on CSS selectors, which involves “class names” such as “hero” or “footer”. Some web technologies automatically generate these names and change them on a regular basis, which makes the Visual Tagger events stop firing.

Squarespace uses this technique, and you can examine the HTML of your site to see if the “class names” are randomized letters/numbers.

Visual Tagger also does not support embedded elements, such as a YouTube video player or a Hubspot form.

What do I do if my website does not behave correctly inside the VT iframe?

When you load your website in the Visual tagger iframe, you might see unexpected or incorrect behavior. This is because browsers load websites differently inside an 
iframe
 than in a regular browser window.
For example, Google Chrome blocks certain types of cookies when a page is loaded inside an iframe, and this can cause problems with authentication or other functions.

Click the 
Open in Popup
 button (above the top right corner of the iframed website) if you experience unexpected behavior when you load your website in the VT iframe, including issues with the login or authentication, or errors with form submissions. This opens the website in a new browser window (outside of an iframe) which is connected to the Visual Tagger.

Tip!
: You might want to change the width of both the Visual tagger window and your website window so you can view them side by side for easier tagging.

How can I make my website ideal for Visual Tagger?

The most stable way for Visual tagger to identify elements is if each one has a unique ID that persists even if the page is reorganized.

When should I use Visual Tagger instead of a coded instrumentation?

Visual Tagger is a great way to get started with tracking, but over time you might need to augment with coded instrumentation.

These are ideal use cases for Visual Tagger:

Understanding how users are engaging with your public website. Beyond page tracking, you can learn which CTA’s are most popular, collect information from forms, learn when users engage with interactive content like a carousel.

Similarly for campaign landing pages, that needs to go live with short turnaround, and require tracking forms, CTA, and other interactive content

Understanding your ecommerce funnel, how users browse and filter products, add them to cart before checking out and completing an order

Learn more about how users use your product or service, after logging in. Track key semantic events in your product such as Project Created (project management app), Account Upgraded, Listing Favorited (apartment rental site) etc to learn about adoption, engagement, and retention

These are examples of when to augment with coded instrumentation:

If your website or application changes frequently, you will need to keep track of each change and update Visual Tagger events accordingly. In that situation, it can be beneficial for the engineering team to have the event tracking in code and update at the same time as changing how a feature work.

Will using Visual Tagger impact my site or app’s performance?

The Visual Tagger integration has negligible impact to your site’s performance, because it installs a single event handler that makes an asynchronous call to Segment when a tag’s event is invoked.

However, adding a large number of tags to your site could potentially impact your site’s performance. To guard against this we limit the number of tags you can add using Visual Tagger to a maximum of 50.

Does Visual Tagger work with dynamically generated elements or Single Page Applications?

Yes. You can track dynamically generated elements like modals (for example) using the Visual Tagger.

Why does my form submit event not work?

Visual Tagger binds its event listeners to the 
document
 object. Forms using 
stopPropagation
 and 
stopImmediatePropagation
 methods prevent the event from bubbling up, causing Visual Tagger to not execute the handlers for emitting track/identify calls. Remove any 
stopPropagation
 and 
stopImmediatePropagation
 method calls from your form handler to allow Visual Tagger to process the event.

Does Visual Tagger have a data layer so that I can make use of data that’s not rendered on the page?

Currently, only information that is visually present on the page is available for use in the Visual Tagger.

How can I tell which events were created using Visual Tagger versus which were implemented using code?

Events that were added using the Visual Tagger (as opposed to in code) have a 
context
 property in the event payload that says 
"visual_tagger": true
. Events 
not
 implemented using the Visual Tagger do not have this property.

The old version of Visual Tagger didn’t have support for MFA or SSO. What about the new version?

Because the new version of Visual Tagger is available in the Segment app, Workspaces that have MFA or SSO enabled are able to access it.

This page was last modified: 17 Jan 2025

Need support?

Questions? Problems? Need more info? Contact Segment Support for assistance!

Help improve these docs!

Was this page helpful?

Thanks for your feedback!

Can we improve this doc? 
Send us feedback!

Get started with Segment

On this page

Was this page helpful?

Thanks for your feedback!

Can we improve
 this doc? 
Send us feedback!

Product

For Developers

Company

Support

© 2025 Segment.io, Inc.

=== Content from https://segment.com/docs/connections/reverse-etl/reverse-etl-source-setup-guides/azure-setup/ ===

Azure Reverse ETL Setup
        

On this page

Set up Azure as your Reverse ETL source.

At a high level, when you set up Azure dedicated SQL pools for Reverse ETL, the configured user needs read permissions for any resources (databases, schemas, tables) the query needs to access. Segment keeps track of changes to your query results with a managed schema (
__SEGMENT_REVERSE_ETL
), which requires the configured user to allow write permissions for that schema.

Required permissions

Make sure the user you use to connect to Segment has permissions to use that warehouse. You can follow the process below to set up a new user with sufficient permissions for Segment’s use.

To create a login in your master database, run:

  CREATE LOGIN <login name of your choice> WITH PASSWORD = 'Str0ng_password'; -- password of your choice


Execute the commands below in the database where your data resides.

To create a user for Segment, run:

  CREATE USER <user name of your choice> FOR LOGIN <login name of your choice>;


To grant access to the user to read data from all schemas in the database, run:

  EXEC sp_addrolemember 'db_datareader', '<user name of your choice>';


To grant Segment access to read from certain schemas, run:

  CREATE ROLE <role name of your choice>;
  GRANT SELECT ON SCHEMA::[schema_name] TO <role name of your choice>;
  EXEC sp_addrolemember '<role name of your choice>', '<user name of your choice>';


To grant Segment access to create a schema to keep track of the running syncs, run:

  GRANT CREATE SCHEMA TO <user name of your choice>;


If you want to create the schema yourself and then give Segment access to it, run:

  CREATE SCHEMA  __segment_reverse_etl;
  GRANT CONTROL ON SCHEMA::__segment_reverse_etl TO <user name of your choice>;
  GRANT CREATE TABLE ON DATABASE::[database_name] TO <user name of your choice>;


Set up guide

To set up Azure as your Reverse ETL source:

After you’ve successfully added your Azure source, 
add a model
 and follow the rest of the steps in the Reverse ETL setup guide.

This page was last modified: 10 Jun 2024

Need support?

Questions? Problems? Need more info? Contact Segment Support for assistance!

Help improve these docs!

Was this page helpful?

Thanks for your feedback!

Can we improve this doc? 
Send us feedback!

Get started with Segment

On this page

Was this page helpful?

Thanks for your feedback!

Can we improve
 this doc? 
Send us feedback!

Product

For Developers

Company

Support

© 2025 Segment.io, Inc.

=== Content from https://segment.com/docs/protocols/tracking-plan/create/ ===

The Protocols Tracking Plan
        

On this page

A Tracking Plan is a data spec outlining the events and properties you intend to collect across your Segment Sources. Crafting a comprehensive Tracking Plan takes time and effort across a range of teams within your organization, and a deep understanding of your business objectives. Once created, the Tracking Plan becomes a highly valuable resource for both the engineers instrumenting Segment and all consumers of the data flowing through Segment. You can 
learn more about data quality best practices
 in the Protocols docs.

When building a Tracking Plan, it’s best to start with the key metrics that drive value for your business. Key metrics may include new user signups, top line revenue, product use and more. With key metrics defined, it becomes much easier to define which user actions help track or improve those key metrics. Each user action maps to a distinct event, or Track call, that you will track in Segment. The Tracking Plan can also validate Identify, Page and Group calls.

The Segment Tracking Plan feature allows you to validate your expected events against the live events that are delivered to Segment. Violations generate when an event doesn’t match the spec’d event in the Tracking Plan.

Segment can infer event data types, but is unable to do so if several data types are sent for a specific category.

Tracking Plans are stored in workspaces and can be connected to one or more Sources.

Segment Consent Preference Updated Event

After setting up a consent category, users of Consent Management see a Segment Consent Preference Updated Event added to all existing Tracking Plans.

Create a Tracking Plan

To create a new Tracking Plan:

Consent Management users see the Segment Consent Preference Updated event on new Tracking Plans

If you are a Consent Management user and have created at least one consent category, Segment automatically adds the 
Segment Consent Preference Updated event
 to all new Tracking Plans.

Copy a Tracking Plan

To create a copy of an existing Tracking Plan:

Download a Tracking Plan

To download a Tracking Plan:

The columns in the Tracking Plan CSV file corresponds to the Tracking Plan UI options. For example:

Once you’ve downloaded a Tracking Plan, you can 
upload it
 as a template for a new Tracking Plan or use it to make changes to an existing Tracking Plan.

Upload a Tracking Plan

You can create a Tracking Plan or make changes to an existing Tracking Plan by uploading a CSV that contains the rules and events you’d like to track. Segment provides a Tracking Plan template file that you can download during the import process, or you can 
download an existing Tracking Plan
 to use as your template.

Tracking Plan CSV requirements

Tracking Plan CSV files uploaded to Segment must be smaller than 15 mb and contain one header row and one or more rows of data. Tracking Plans CSVs must also have fewer than 100,000 rows and 2,000 rules.

Create a new Tracking Plan

To create a new Tracking Plan by uploading a CSV file:

After uploading your CSV file, you are redirected to the Upload & Download History page while the upload is in progress. If the CSV upload fails, you’ll be able to either view the error directly in the Reports column on the Upload & Download History page or download the 
error_report.csv
 file that corresponds to the Tracking Plan you uploaded.

Tracking Plans created by an uploaded file are reflected in the 
Audit Trail
 and 
Tracking Plan changelog
. If you are a Consent Management user and have created at least one consent category, Segment automatically adds the 
Segment Consent Preference Updated event
 to all new Tracking Plans.

Update an existing Tracking Plan

Tracking Plans with imported libraries cannot be changed using the Upload a Tracking Plan method

If you have a Tracking Plan with imported libraries, you must make changes to your Tracking Plan in the Segment app.

To update a Tracking Plan by uploading a CSV file:

After uploading your CSV file, you are redirected to the Upload & Download History page while the upload is in progress. If the CSV upload fails, you’ll be able to either view the error directly in the Reports column on the Upload & Download History page or download the 
error_report.csv
 file that corresponds to the Tracking Plan you uploaded.

Any changes made to a Tracking Plan using an uploaded file are reflected in the 
Audit Trail
 and 
Tracking Plan changelog
.

Delete a Tracking Plan

Deleting a Tracking Plan requires Workspace Owner or Tracking Plan Admin permissions

You must have Workspace Owner or Tracking Plan Admin roles to delete a Tracking Plan. For more information about roles in Segment, see the 
Roles documentation
.

To delete a Tracking Plan:

Edit a Tracking Plan

The Tracking Plan editor is organized as a spreadsheet to help you  add new events and properties, and edit the relevant fields for each. Like a spreadsheet, you can navigate across cells in a single event with your arrow keys and press enter to edit a cell.

The Status, Data Type, and Permitted Values columns appear as you add a Track call property.

Add a new Track call

To add a new Track call:

Add a Track call property

To add a Track call property:

Add a Track call object or array property

Segment supports object and array data types in the Tracking Plan editor. These complex data structures have limited use cases and should be used sparingly as some destinations aren’t able to ingest the data structures. To add an object or array:

When creating array properties in your Tracking Plan, add the 
items
 nested property, denoted by the name of the array property with a 
.$
 suffix, to ensure that the nested property is marked as planned in the Source Schema.

Add Identify or Group traits

You can define which traits you expect to see passed in Identify or Group calls like how you would add Track calls to the Tracking Plan. Navigate to the 
Identify
 or 
Group
 tab in your Tracking Plan and click the 
(+)
 button to add a new trait.

It’s best to keep traits optional because Identify and Group are often called and pass only 
new or changed
 traits, because Segment’s client-side libraries (analytics.js, Swift, Kotlin) cache traits in local storage. See the 
Identify Best Practices
 to learn more.

Remove a source from your Tracking Plan

Removing a source from a Tracking Plan requires Workspace Owner or Tracking Plan Admin permissions

You must have Workspace Owner or Tracking Plan Admin roles to remove a source from a Tracking Plan. For more information about roles in Segment, see the 
Roles documentation
.

To remove a source from your Tracking Plan:

Add a label

You can apply 
key:value
 labels to each event to help organize your Tracking Plan. These labels are helpful when multiple teams are managing a single Tracking Plan, or if you want to specify a priority, platform, product, or similar meta-data for each event. You can filter by label from the Tracking Plan, Schema, Data Validation and Violations Summary views.

For consistency purposes, it’s best that you create a standard way of labeling events and share it with all parts of your organization that will use Segment.



Note:
 Tracking Plan Labels are only available for Track and Page events.

Filter events in the Tracking Plan

You can filter the Tracking Plan events by keyword or by label. The applied filter generates a permanent link so you can share specific events with teammates. Label filters also persist after you leave the Tracking Plan.

Edit underlying JSON Schema

Protocols Tracking Plans use 
JSON Schemas
 to validate Segment event payloads. To support a broader range of validation use-cases, Segment lets you to edit your underlying JSON schema.

Editing a JSON schema requires technical expertise. The 
JSON schema documentation
 and 
JSON schema validator
 are helpful resources you can use.

You can edit the JSON schema for each Track event listed in the Tracking Plan, and a common JSON schema definition that applies across all events.

Each Track event in the Tracking Plan has a separate JSON schema definition to validate the properties in that event. To edit, click on the overflow menu next to each event row in the Tracking Plan.

Advanced edits to the JSON schema are not visible in the Tracking Plan and make it harder for other users to understand the validation logic. Be sure to communicate to any other Protocols users that you are making changes in the validation logic.

The Tracking Plan also uses a common JSON schema definition that applies to the entire payload of every event sent from sources connected to the Tracking Plan.

The common JSON schema definition is unique for each Tracking Plan. An example use of this feature is to validate that all Track, Identify and Page events sent to Segment include a 
context.device.advertisingId
 property. This validation ensures that every Segment call has a userId, anonymousId, and context object with a nested 
"device": { "advertisingId": "e23sfsdf"}
 object.

{

    
"$schema"
:
 
"http://json-schema.org/draft-07/schema#"
,

    
"type"
:
"object"
,

    
"properties"
:
 
{

      
"context"
:
 
{

        
"type"
:
 
"object"
,

        
"properties"
:
 
{

          
"device"
:
 
{

            
"type"
:
 
"object"
,

            
"properties"
:
 
{

              
"advertisingId"
:
 
{
 
"type"
:
 
"string"
 
}

            
},

            
"required"
:
 
[
"advertisingId"
]

          
}

        
},

        
"required"
:
 
[
"device"
]

      
},

      
"anonymousId"
:
 
{

        
"type"
:
 
"string"

      
},

      
"userId"
:
 
{

        
"type"
:
 
"string"

      
},

      
"properties"
:{

        
"type"
:
 
"object"
,

        
"properties"
:
 
{

          
"primary_business_unit"
:
 
{

            
"type"
:
 
"string"

          
}

        
},

        
"required"
:
 
[
"primary_business_unit"
]

      
},

      
"traits"
:{

        
"type"
:
 
"object"
,

        
"properties"
:
 
{

          
"first_name"
:
 
{

            
"type"
:
 
"string"

          
},

          
"last_name"
:
 
{

            
"type"
:
 
"string"

          
},

          
"email"
:
 
{

            
"type"
:
 
"string"

          
}

        
},

        
"required"
:
 
[
"email"
]

      
}

  
},

  
"required"
:
 
[
"context"
,
"anonymousId"
,
"userId"
,
"traits"
]


}



To edit the common JSON schema:

To edit the common JSON schema using the Public API, you’ll need to add your new JSON schema under the 
"global"
: object.

Negative lookahead regexes (
?!
)
 aren’t supported. This means you can’t use regex to prevent matches with a specific following character or expression. But, you can use 
not
 in the regex of your JSON schema to generate violations when a property key or value doesn’t match the provided regex pattern.

Specifying data type

Property or trait data type should adhere to the 
data types defined by JSON schema
. Data type names must be lower-cased as specified in JSON schema. Date/time properties should be represented as a 
string
 type with 
format
 keyword
 (for example: “format”: “date-time”).

Blocking data

JSON schema violation event blocking is 
only
 supported in cloud-mode Destinations. See 
the Customize your schema controls docs
 for more information on blocking data.

Extend the Tracking Plan

Some customers prefer to manage the Tracking Plan with outside tools and resources. See the 
APIs and extensions
 section to learn more.

Tracking Plan Event Versioning

Segment offers Tracking Plan Event Versioning if you use Protocols to manage mobile sources, or to help you centrally manage a Tracking Plan for multiple teams. With Event Versioning, you can create multiple versions of an event definition, and validate events using a version key included in the Track event payload.

This can be helpful for mobile developers who might have several released versions of their app sending data at the same time. For example, a new mobile app release might add a new required property to an event like 
Order Completed
. In this scenario, if you updated the Tracking Plan, all  
Order Completed
 events from your old mobile app versions would be invalid, because some customers won’t have updated to the latest version yet. Instead, with event versioning, you can allow validation of both the old and new versions of an event at the same time. When you’re ready to deprecate those old event versions, you can delete the version in your Tracking Plan.

For example, say you want to add 
subtotal
 as a required property to your 
Order Completed
 event. You would start by adding the required property to the event in the Tracking Plan as shown in the example below.



Create a new event version

With event versioning, you can now create multiple versions of the event definition as shown in the example below. To create a new event version, click into the overflow menu for an event and select 
Add Event Version
.



Dynamically validate Track events against an event version

To ensure the Track events you send to a Segment source validate against the correct event version, you need to instrument your events to include a 
context.protocols.event_version
 key and version value. The version value must pass as an integer, and should match the number shown in the Tracking Plan version tab. In the example below, the version number would be 
2
.



Next, add the event version number to the context object. For 
analytics.js
 Track calls, you would instrument the event as in the example below. Note how the JSON objects for 
context
, 
protocols
, and 
event_version
 are nested.

analytics
.
track
(
'
Order Completed
'
,
 
{

  
subtotal
:
 
23
,

  
products
:
 
[{

    
product_name
:
 
'
Air Balloon
'
,

    
product_id
:
 
'
32rd9jfs
'

  
}],

  
order_id
:
 
'
2df90eiwc9wjec
'
,

  
revenue
:
 
33


},
 
{

  
context
:
 
{

    
protocols
:
 
{

      
event_version
:
 
2

    
}

  
}


});



Note:
 Protocols validate events against the oldest event version in the Tracking Plan for event payloads that are 1) missing the context.protocols.event_version key, or 2) contain an invalid/undefined event version (ex: event_version:3.2).

This page was last modified: 16 May 2024

Need support?

Questions? Problems? Need more info? Contact Segment Support for assistance!

Help improve these docs!

Was this page helpful?

Thanks for your feedback!

Can we improve this doc? 
Send us feedback!

Get started with Segment

On this page

Was this page helpful?

Thanks for your feedback!

Can we improve
 this doc? 
Send us feedback!

Product

For Developers

Company

Support

© 2025 Segment.io, Inc.

=== Content from https://segment.com/docs/engage/journeys/journey-context/ ===

Journey Context
        

Engage Foundations requires a Business tier account and includes Unify.


See the 
available plans
, or 
contact Support
.

On this page

Event-Triggered Journeys
 redefine how you orchestrate and personalize customer experiences.

This page explains Journey context, which can help you dynamically adapt each journey to individual user interactions, creating highly relevant, real-time workflows.

Public Beta

Event-Triggered Journeys is in public beta, and Segment is actively working on this feature. Some functionality may change before it becomes generally available. Event-Triggered Journeys is not currently HIPAA eligible.

Overview

Unlike traditional audience-based journeys, which rely solely on user progress through predefined steps, event-triggered journeys capture and store the details of user-triggered events. This shift allows you to access the data that caused users to reach a specific step and use it to make more precise decisions throughout the journey.

With journey context, you can:

For example:

What is Journey context?

Journey context is a flexible data structure that captures key details about the events and conditions that shape a customer’s journey. Journey context provides a point-in-time snapshot of event properties, making accurate and reliable data available throughout the journey.

Journey context stores event property information tied to specific user actions, like 
Appointment ID
 or 
Order ID
.

Journey context doesn’t store:

However, the up-to-date values of profile traits and audience membership can be added in a payload sent to a destination.

This focused approach ensures journey decisions are always based on static, reliable data points.

Examples of stored context

Event properties are the foundation of Journey context. Examples of event properties include:

Segment captures each event’s properties as a point-in-time snapshot when the event occurs, ensuring that the data remains consistent for use in personalization.

Using Journey context in Event-Triggered Journeys

Journey context provides the framework for capturing and referencing data about events and conditions within a journey. It allows Event-Triggered Journeys to dynamically respond to user behavior by making event-specific data available for decisions and actions at each step.

This is useful for scenarios like:

By incorporating event-specific data at each step, journey context helps workflows remain relevant and adaptable to user actions.

Journey steps that use context

Journey context gets referenced and updated at various steps in an event-triggered journey. Each step plays a specific role in adapting the journey to user behavior or conditions.

This step checks whether a user performs a specific event within a given time window. If the event occurs, Segment adds its details to journey context for use in later steps.

For example, a journey may wait to see if a 
checkout_completed
 event occurs within two hours of a user starting checkout. If the event happens, its properties are added to context and the workflow can proceed; otherwise, it may take an alternate path. The data captured includes event properties (like 
Order ID
).

If a Hold Until branch is set to send profiles back to the beginning of the step when the event is performed, those events are also captured in context. Because they may or may not be performed during a journey, they will show as available in future steps but will not be guaranteed for every user’s progression through the journey.

The send to destination step allows journey context data to be included in payloads sent to external tools, like messaging platforms or analytics systems.

For example, a payload sent to a messaging platform might include 
Order ID
 and 
Cart Contents
 to personalize the message. Users can select which parts of journey context to include in the payload.

Context structure

The structure of journey context organizes event-specific data gets and makes it accessible throughout the journey workflow. By standardizing how data is stored, Segment makes it easier to reference, use, and send this information at different stages of a journey.

Journey context is organized as a collection of key-value pairs, where each key represents a data point or category, and its value holds the associated data.

For example, when a user triggers an event like 
Appointment Scheduled
, Segment stores its properties (like 
Appointment ID
, 
Appointment Start Time
) as key-value pairs. You can then reference these values in later journey steps or include them in external payloads.

The following example shows how journey context might look during a workflow. In this case, the user scheduled an appointment, and the workflow added related event data to the context:

{

  
"journey_context"
:
 
{

    
"appointment_scheduled"
:
 
{

      
"appointment_id"
:
 
12345
,

      
"start_time"
:
 
"2024-12-06T10:00:00Z"
,

     
"end_time"
:
 
"2024-12-06T11:00:00Z"
,

      
"provider_name"
:
 
"Dr. Smith"

    
},

    
"appointment_rescheduled"
:
 
{

      
"appointment_id"
:
 
12345
,

      
"start_time"
:
 
"2024-12-07T10:00:00Z"
,

      
"end_time"
:
 
"2024-12-07T11:00:00Z"
,

      
"provider_name"
:
 
"Dr. Jameson"

    
}

  
}


}



This payload contains:

Journey context and Event-Triggered Journeys

Journey context underpins the flexibility and precision of Event-Triggered Journeys. By capturing key details about events and decisions as they happen, journey context lets workflows respond dynamically to user actions and conditions.

Whether you’re orchestrating real-time abandonment recovery or personalizing messages with event-specific data, journey context provides the tools to make your workflows more relevant and effective.

To learn more about how Event-Triggered Journeys work, visit the 
Event-Triggered Journeys documentation
.

This page was last modified: 19 Dec 2024

Need support?

Questions? Problems? Need more info? Contact Segment Support for assistance!

Help improve these docs!

Was this page helpful?

Thanks for your feedback!

Can we improve this doc? 
Send us feedback!

Get started with Segment

On this page

Was this page helpful?

Thanks for your feedback!

Can we improve
 this doc? 
Send us feedback!

Product

For Developers

Company

Support

© 2025 Segment.io, Inc.

=== Content from https://segment.com/docs/getting-started/03-planning-full-install/ ===

Planning a Full Installation
        

On this page

Now that you’ve seen Segment in action, step back and think through what a full implementation of Segment for your organization would look like. Figuring out what events to track in Segment can feel overwhelming. You should expect this planning process to have the following steps:

Be prepared to invest time deciding with stakeholders how to track your data, and planning how you’ll analyze it. The time you spend here will save you lots of time in the future, as following Segment’s best practices allows you to easily change your tracking later.

Define Business Objectives

Tracking is about learning and taking action. Think about what you want to know about your product or customers. Think about what assumptions need to be tested and what theories need to be proven true or false. Think about the unknowns. Here are some helpful questions to get started:

While it may seem obvious, we highly recommend documenting your high-level business objectives. More specifically, ask yourself: what are the measurable business outcomes you want to achieve this year? Do you want to acquire new customers? Generate more new sign-ups, drive more incremental revenue among your current customer base?

The best way to answer this question is to interview stakeholders in your organization who will consume the data.

With your business goals documented, the next step is to map user actions to those business goals. For example, if one of your goals is to activate new signups, you want to think about which activities are related to a signup. Ask yourself, what actions do people take 
before
 signing up? Do specific actions predict a user signing up?

As an example, you might end up with a list like this:

While this list represents a tiny fraction of the user actions you 
could
 track, it gives a list focused on your top business objectives. This helps break up the huge project of data collection into smaller chunks.

Decide what to collect

With your business objectives documented and mapped to user actions, it’s time to build standards that you can use when deciding what to track. With your stakeholders, make a list of the actual events (page or screen views, and user actions) that you want to track. Think about all of the ways your users can interact with your site or app

When you’re first starting out, we recommend that you limit your tracking plan to a few core events, but add lots of properties to provide context about them. We generally see more success with the “less is more” philosophy of tracking data, but you might also decide to take a more liberal “track more and analyze later” approach. Like everything, each alternative has pros and cons that are important to consider especially as it relates to your company’s needs.

Shortcut: Check if a Business Spec meets your needs

Segment maintains several “Business Specs”, which are recommendations based on your type of business that give recommendations on what to track, what additional traits or properties to collect, and how to format them. The two most common are the B2B (business-to-business) Spec, Ecommerce Spec, and Mobile and Video specs.

If these specs meet your business needs, you’re in luck. These specs are built into Segment tracking plan templates, so you don’t need to start from a blank slate.

If your organization sells a product or services to other businesses, you might have different analytics and marketing needs than most companies. You need to understand your customer behaviors both at the user-level, and also at the company or team-level.  You can read more about 
how Segment thinks about B2B tracking
, and 
read more about the B2B Spec
.

If your organization sells products online, the E-commerce Spec covers the customer’s journey as they browse your store, click on promotions, view products, add those products to a cart, and complete a purchase. It also provides recommendations about off-page interactions, including interactions with email promotions, coupons, and other systems. You can read more about 
why companies need an Ecommerce Spec
, read more about 
Ecommerce tracking plans
, and dive directly into our 
Ecommerce Spec
.

The native Mobile Spec is a common blueprint for the mobile user lifecycle. The Spec outlines the most important events for mobile apps to track, and automatically collects many of these events when you use the 
Segment Android and iOS SDKs
. Read more about the 
benefits of the native mobile spec
, or read through 
the Native Mobile Spec
 directly.

Segment’s video spec helps you understand how customers engage with your video and ad content, including playback events, types of media displayed, and performance metrics. You can 
read more about our Video Spec
.

Create naming conventions

Regardless of approach, here are some important best practices to keep in mind:

Pick a casing convention:
 We recommend 
Title Case
 for event names and 
snake_case
 for property names. Make sure you pick a casing standard and enforce it across your events and properties.

Pick an event name structure:
 As you may have noticed from our 
specs
, we’re big fans of the Object (
Blog Post
) + Action (
Read
) framework for event names. Pick a convention and stick to it.

Don’t create event names dynamically:
 Avoid creating events that pull a dynamic value into the event name (for example, 
User Signed Up (11-01-2019)
). If and when you send these to a warehouse for analysis, you end up with huge numbers of tables and schema bloat.

Don’t create events to track properties:
 Avoid adding values to event names when they could be a property. Instead, add values as a property. For example, rather than having an event called “Read Blog Post - Best Tracking Plans Ever”, create a “Blog Post Read” event and with a property like 
"blog_post_title":"Best Tracking Plans Ever"
.

Don’t create property keys dynamically:
 Avoid creating property names like 
"feature_1":"true"
,
"feature_2":"false"
 as these are ambiguous and very difficult to analyze



Got all that? Great. You’re now ready to develop a Tracking Plan.

Develop a tracking plan

A 
tracking plan
 clarifies what events to track, where those events live in the code base, and why you’re tracking those events (from a business perspective). 
A good tracking plan represents the single source of truth about what data you collect, and why.

Your tracking plan is probably maintained in a spreadsheet (unless you use Segment’s tracking-plan tool, 
Protocols
), and serves as a project management tool to get your organization in agreement about what data to use to make decisions. A tracking plan helps build a shared understanding of the data among marketers, product managers, engineers, analysts, and any other data users.

Plan your Identify and Group calls

The Identify call updates all records of the user with a set of traits, and so is extremely important for building your understanding of your users. But how do you choose which traits to include?
The example below shows an Identify call using 
analytics.js
) for Segment:

analytics
.
identify
({

  
name
:
 
'
Jane Kim
'
,

  
email
:
 
'
janekim@example.com
'
,

  
login
:
 
'
janekay
'
,

  
type
:
 
'
user
'
,

  
created
:
 
'
2016-11-07T16:40:52.238Z
'
,


});



The traits represent dimensions in your data that you can group or pivot on. For example, in the above, you can easily create cohorts of all types that are 
users
 or accounts created within a time window of your choosing.

When you plan your deployment, think about what information you can collect as traits that would be useful to you when grouping users together, and plan how you will collect that information.

The Group call is similar to the Identify call, but it adds traits associated with a parent account to the user’s profile. If your organization is a B2B company, you should also plan the group traits to collect, and how you’ll use them once they’re applied to a user account.

Plan your Track events

We recommend starting with fewer events that are directly tied to one of your 
business objectives
, to help avoid becoming overwhelmed by endless number of possible actions to track. As you get more comfortable, you can add more events to your tracking plan that can answer more specialized questions.

At Segment, we started out tracking these events:

Then we added some peripheral events to to better understand how we’re performing, for the following reasons:

For an Ecommerce company, however, the main events might be something like:

Tip
: As mentioned 
above
, Segment has a set of “reserved” event names specifically for ecommerce, called the 
Ecommerce Spec
. Check it out to see which events Segment covers and how they are used in downstream destinations.

An online community, on the other hand, has an entirely different set of actions that indicate engagement, as listed below. For example, a community might want to track actions like:

With these actions tracked, the community can develop metrics around engagement, and understand how users move towards their ultimate conversion events. You can read more in 
this article from the online community GrowthHackers
 about the events they track and why.

Define your Track event properties

Each Track call can accept an optional dictionary of properties, which can contain any key-value pair. These properties act as dimensions that allow destination tools to group, filter, and analyze the events. They give you additional detail on broader events.

Events should be generic and high-level, but properties should be specific and detailed. For example, at Segment, 
Business Tier Workspace Created
 is a horrible event name. Instead, we used 
Workspace Created
 with a 
property
 of 
account_tier
 and value of 
business
 :

analytics
.
track
(
'
Workspace Created
'
,
 
{

  
account_tier
:
 
'
business
'


})



Similar to the traits in the Identify call, the properties provide a column that you can pivot against or filter on in your analytics tools or allow you to create a cohort of users in email tools.

Don’t create dynamically generated property names in the properties dictionary. Each 
key
 creates a new column in your downstream tools, and dynamically generated keys clutter your tools with fragmented data that makes it difficult and confusing to use later.

Here is Segment’s 
Lead Captured
 Track call:

analytics
.
track
(
userId
,
 
'
Lead Captured
'
,
 
{

  
email
:
 
'
email
'
,

  
location
:
 
'
header navbar
'

  
url
:
 
'
https://segment.com/
'


});



The high-level event is 
Lead Captured
, and all of the details appear in the properties dictionary. Because of this, we can easily see in our downstream tools how many leads were captured, and from which parts of the site.

If you want to learn more about how properties are used by downstream tools, check out 
The Anatomy of a Track Call
.

Plan for destination tools

Once you’ve completed your tracking plan, there’s one more step you might want to do before you move on to actually implementing Segment. The 
Segment destination catalog
 contains hundreds of tools, many of which you’ll be familiar with already.

If your organization has an established set of analytics tools, look for those tools in the catalog and bookmark their documentation pages. These docs pages contain important information about how Segment transforms data for the destination tool, and they also contain useful details about troubleshooting, set-up, and implementation considerations.

Once you have an initial list of the destination tools your organization uses, you can also check 
which Segment methods those tools accept
. This helps you at implementation time to ensure that the calls you use can be consumed by the tools they’re intended for.

Additionally, you should check 
which connection modes each tool supports
, so you know ahead of time which destinations may need to be bundled.

Tip
: If you know you’re looking for a tool for a specific purpose, but haven’t chosen one yet, you can also check the 
Connection Modes by category page
 to see which tools might be compatible with the least implementation changes.

Walk through a disposable, demo implementation.

Take your plans, and make them real.

This page was last modified: 30 Mar 2023

Need support?

Questions? Problems? Need more info? Contact Segment Support for assistance!

Help improve these docs!

Was this page helpful?

Thanks for your feedback!

Can we improve this doc? 
Send us feedback!

Get started with Segment

On this page

Was this page helpful?

Thanks for your feedback!

Can we improve
 this doc? 
Send us feedback!

Product

For Developers

Company

Support

© 2025 Segment.io, Inc.

=== Content from https://segment.com/docs/connections/functions/functions-copilot-nutrition-facts/ ===

Functions Copilot Nutrition Facts Label
        

Twilio’s 
AI Nutrition Facts
 provide an overview of the AI feature you’re using, so you can better understand how the AI is working with your data. Function Copilot’s AI qualities are outlined in the following Nutrition Facts label. For more information, including the glossary regarding the AI Nutrition Facts label, refer to the 
AI Nutrition Facts
 page.





AI Nutrition Facts






Functions Copilot

Description






 Functions Copilot is an AI-powered coding assistant designed to streamline the development of custom integrations, and enrich and transform Segment Functions.

Privacy Ladder Level 


1



Feature is Optional


Yes

Model Type 


Generative

Base Model 


OpenAI - GPT-4

Trust Ingredients

Base Model Trained with Customer Data


No







Customer Data Shared with Model Vendor


No







Training Data Anonymized   


N/A

Data Deletion


Yes

Human in the Loop 


Yes

Data Retention  


N/A

Input/Output Consistency


Yes

Other Resources






Learn more at: 
https://twilioalpha.com/
 

This page was last modified: 28 Jun 2024

Need support?

Questions? Problems? Need more info? Contact Segment Support for assistance!

Help improve these docs!

Was this page helpful?

Thanks for your feedback!

Can we improve this doc? 
Send us feedback!

Get started with Segment

Was this page helpful?

Thanks for your feedback!

Can we improve
 this doc? 
Send us feedback!

Product

For Developers

Company

Support

© 2025 Segment.io, Inc.

=== Content from https://segment.com/docs/connections/functions/copilot/ ===

Functions Copilot
        

On this page

Functions Copilot helps you generate JavaScript code for functions using natural language prompts. For more information about the language model used to generate JavaScript code, see the 
Functions Copilot Nutrition Facts Label
.

Functions Copilot benefits

Functions Copilot improves efficiency and productivity by streamlining the process of creating and managing custom functions.

Functions Copilot can help you:

Example prompts

This table lists example prompts you can use with Functions Copilot:

Best practices and limitations

Follow this guidance when you use Functions Copilot:

Limitations

Keep the following limitations in mind as you work with Functions Copilot:

This page was last modified: 17 Nov 2024

Need support?

Questions? Problems? Need more info? Contact Segment Support for assistance!

Help improve these docs!

Was this page helpful?

Thanks for your feedback!

Can we improve this doc? 
Send us feedback!

Get started with Segment

On this page

Was this page helpful?

Thanks for your feedback!

Can we improve
 this doc? 
Send us feedback!

Product

For Developers

Company

Support

© 2025 Segment.io, Inc.

=== Content from https://segment.com/docs/unify/profiles-sync/profiles-sync-setup/databricks-profiles-sync/ ===

Databricks for Profiles Sync
        

Unify requires a Business tier account and is included with Engage.


See the 
available plans
, or 
contact Support
.

On this page

With Databricks for Profiles Sync, you can use 
Profiles Sync
 to sync Segment profiles into your Databricks Lakehouse.

Getting started

Before getting started with Databricks Profiles Sync, note the following prerequisites for setup.

Warehouse size and performance

A SQL warehouse is required for compute. Segment recommends a warehouse with the the following characteristics:

To improve the query performance of the Delta Lake, Segment recommends creating compact jobs per table using OPTIMIZE following 
Databricks recommendations
.

Segment recommends manually starting your SQL warehouse before setting up your Databricks destination. If the SQL warehouse isn’t running, Segment attempts to start the SQL warehouse to validate the connection and may experience a timeout when you hit the 
Test Connection
 button during setup.

Set up Databricks for Profiles Sync

Connect your Databricks warehouse

Use the five steps below to connect to your Databricks warehouse.

To configure your warehouse, you’ll need read and write permissions.

Step 1: Name your schema

Pick a name to help you identify this space in the warehouse, or use the default name provided. You can’t change this name once the warehouse is connected.

Step 2: Enter the Databricks compute resources URL

You’ll use the Databricks workspace URL, along with Segment, to access your workspace API.

Check your browser’s address bar when inside the workspace. The workspace URL should resemble: 
https://<workspace-deployment-name>.cloud.databricks.com
. Remove any characters after this portion and note the URL for later use.

Step 3: Enter a Unity catalog name

This catalog is the target catalog where Segment lands your schemas and tables.

Step 4: Add the SQL warehouse details from your Databricks warehouse

Next, add SQL warehouse details about your compute resource.

Step 5: Add the service principal client ID and client secret

Segment uses the service principal to access your Databricks workspace and associated APIs.

Service principal client ID
: Follow the 
Databricks guide for adding a service principal to your account
. This name can be anything, but Segment recommends something that identifies the purpose (for example, “Segment Profiles Sync”).  Segment doesn’t require 
Account admin
 or 
Marketplace admin
 roles.

The service principal needs the following setup:

Client secret
: Follow the 
Databricks instructions to generate an OAuth secret
.

Once you’ve configured your warehouse, test the connection and click 
Next
.

Set up selective sync

With selective sync, you can choose exactly which tables you want synced to the Databricks warehouse. Segment syncs materialized view tables as well by default.

Select tables to sync, then click 
Next
. Segment creates the warehouse and connects databricks to your Profiles Sync space.

You can view sync status, and the tables you’re syncing from the Profiles Sync overview page.

Learn more about 
using selective sync
 with Profiles Sync.

This page was last modified: 03 Jun 2024

Need support?

Questions? Problems? Need more info? Contact Segment Support for assistance!

Help improve these docs!

Was this page helpful?

Thanks for your feedback!

Can we improve this doc? 
Send us feedback!

Get started with Segment

On this page

Was this page helpful?

Thanks for your feedback!

Can we improve
 this doc? 
Send us feedback!

Product

For Developers

Company

Support

© 2025 Segment.io, Inc.

=== Content from https://segment.com/docs/connections/sources/catalog/ ===

Sources Catalog
        

Below is a list of the available sources on the Segment platform.


        Website
      


        Mobile
      

Beta

Beta


        Server
      


        A/B Testing
      

Beta

Beta


        Advertising
      


        Analytics
      

Beta

Beta

Beta

Beta


        Attribution
      

Beta


        CRM
      

Beta

Beta

Beta


        Custom
      


        Customer Success
      


        Email Marketing
      

Beta

Beta

Beta

Beta

Beta


        Enrichment
      

Beta

Beta

Beta

Beta

Beta


        Feature Flagging
      

Beta


        Helpdesk
      


        Learning Management System
      

Beta


        Livechat
      

Beta

Beta


        Marketing Automation
      

Beta

Beta

Beta

Beta

Beta

Beta

Beta


        Ott
      


        Payments
      


        Performance Monitoring
      

Beta


        Personalization
      

Beta

Beta

Beta

Beta

Beta

Beta

Beta

Beta

Beta

Beta

Beta


        Raw Data
      

Beta

Beta

Beta

Beta


        Referrals
      


        SMS & Push Notifications
      

Beta


        Surveys
      

Beta


        Virtual Assistant
      

On this page

Was this page helpful?

Thanks for your feedback!

Can we improve
 this doc? 
Send us feedback!

Product

For Developers

Company

Support

© 2025 Segment.io, Inc.

=== Content from https://segment.com/docs/api/config-api/ ===

Config API Overview
        

On this page

The Segment Public API is available

Segment’s 
Public API
 is available for Team and Business tier customers to use. You can use the Public API and Config APIs in parallel, but moving forward any API updates will come to the Public API exclusively. 

Please contact your account team or 
friends@segment.com
 with any questions.

The Config API enables you to programmatically manage Segment workspaces, sources, destinations and more. With the API you can:

The Config API is a set of REST services under segmentapis.com:

To see all the API methods and models see the 
Segment Config API Reference
.

At this time there are no language-specific clients. However the 
API Reference
 also contains example code snippets for cURL, Go, Node, Python and more.

Quick Start

You can interact with the API from the command line. First install the 
curl
 tool.

$ 
brew 
install 
curl


Access Tokens

You can use the Config API with an access token to programmatically access Segment resources that the token can access. Access tokens are created by workspace owners using the Access Management page, and can only access resources that the token has permission to.

These are currently only suitable for first party, trusted applications, such as your personal local scripts and server side programs. Partners should not prompt Segment users for their username and password and save an access token as a way to delegate access. See the 
Authentication
 doc for more information.

When you create an access token, you’ll give it a description, a workspace, and determine whether it has workspace owner or member access.

Secret Token

You can not retrieve the plain-text 
token
 later, so you should save it in a secret manager. If you lose the 
token
 you can generate a new one.

info
As of February 1, 2024, new Config API tokens cannot be created in the app as Segment moves toward exclusive support for the 
Public API
. 
Migrate your implementation to the Public API
 to access the latest features and available endpoints. To create a new Config API token, reach out to friends@segment.com for support.

API Requests

Now that you have an access token, you can use this token to access the rest of the Config API by setting it in the 
Authorization
 header of your requests, for example:

$ ACCESS_TOKEN
=
qiTgISif4zprgBb_5j4hXfp3qhDbxrntWwwOaHgAMr8.gg9ok4Bk7sWlP67rFyXeH3ABBsXyWqNuoXbXZPv1y2g


$ 
curl 
\

  
-X
 GET 
\

  
-H
 
"Authorization: Bearer 
$ACCESS_TOKEN
"
 
\

  https://platform.segmentapis.com/v1beta/workspaces


Example response:

{

  
"workspaces"
:
 
[

    
{

      
"name"
:
 
"workspaces/myworkspace"
,

      
"display_name"
:
 
"My Space"
,

      
"id"
:
 
"e5bdb0902b"
,

      
"create_time"
:
 
"2018-08-08T13:24:02.651Z"

    
}

  
],

  
"next_page_token"
:
 
""


}



Reference

For an overview of the API’s common design patterns and important information about versioning and compatibility, see the 
API Design
 document.

To see all the API methods and models see the 
Segment Config API Reference
.

This page was last modified: 01 Feb 2024

Need support?

Questions? Problems? Need more info? Contact Segment Support for assistance!

Help improve these docs!

Was this page helpful?

Thanks for your feedback!

Can we improve this doc? 
Send us feedback!

Get started with Segment

On this page

Was this page helpful?

Thanks for your feedback!

Can we improve
 this doc? 
Send us feedback!

Product

For Developers

Company

Support

© 2025 Segment.io, Inc.

=== Content from https://segment.com/docs/guides/usage-and-billing/discounts-for-startups-npos/ ===

Discounts or Coupons
        

On this page

Segment currently offers coupons on an ongoing basis for:

Early-stage startups

Segment offers a 
Startup Program
 to enable early startups to track data correctly and easily test the marketing and analytics tools necessary to grow their business. Participating startups receive $25,000 in annual credit toward our monthly 
Team plan
 for as long as they meet our eligibility requirements (up to 2 years).

Learn more about the Segment Startup Program and eligibility requirements here.

Participants and Alumni of Accelerator Partners

We currently partner with various accelerator programs around the globe to offer promotions for both current and alumni participants. Contact your accelerator administrator to see if they partner with Segment and for more information on how to redeem the coupon.

If your accelerator does not participate in our program, have them 
apply here
 or 
contact us
 to us with the details of your program and the best point of contact for consideration.

Non-profits

We offer non-profit customers a $120 per month discount on our monthly 
Team plan
, which typically covers 10,000 MTUs per month. 
Contact us
 to our support team with proof of your non-profit status for more details.

Special Promotions

We occasionally offer special promotions. Customers will be notified directly if they are eligible for a special promotion.

Coupon FAQ

How do coupons work?

Coupons are applied to your monthly (or annual) bill, which reduces the corresponding charge to your credit card. Coupons can either take the format of a percent-off or a dollar value-off your bill. If your coupon is a percentage-off your bill, the dollar value of the coupon may change as your bill may fluctuate month-to-month.

How do I redeem a coupon?

Eligible startups can 
apply directly
 for the Segment Startup Program. Other coupons can be redeemed by reaching out to Segment’s 
support
 team, who will apply the promotion to your account.

Where can I view which coupons are applied to my account?

The Startup Program credits are reflected in the Workspace usage and billing page. Other coupons applied to your workspace are not currently reflected in the Segment application. If you are curious about a promotion you are currently on, or if you workspace has a coupon applied, 
contact the Segment support team
.

Do I have to be a “new” customer to receive a coupon?

The Segment Startup Program is only for customers that have not previously received any other coupon. Both the non-profit and accelerator promotion can be redeemed regardless if you’re a new customer or have been with us for years. A user/workspace can only receive any coupon once.

What happens when my coupon expires?

When your promotion expires, your bill returns to the normal, non-discounted rate. Certain promotions may include a follow-up discount immediately after the promotion expires.

This page was last modified: 14 Jul 2021

Need support?

Questions? Problems? Need more info? Contact Segment Support for assistance!

Help improve these docs!

Was this page helpful?

Thanks for your feedback!

Can we improve this doc? 
Send us feedback!

Get started with Segment

On this page

Was this page helpful?

Thanks for your feedback!

Can we improve
 this doc? 
Send us feedback!

Product

For Developers

Company

Support

© 2025 Segment.io, Inc.

=== Content from https://segment.com/docs/connections/storage/warehouses/warehouse-syncs/ ===

Warehouse Syncs
        

On this page

Instead of constantly streaming data to the warehouse destination, Segment loads data to the warehouse in bulk at regular intervals. Before the data loads, Segment inserts and updates events and objects, and automatically adjusts the schema to make sure the data in the warehouse is inline with the data in Segment.

When Segment loads data into your warehouse, each sync goes through two steps:

Warehouses sync with all data coming from your source. However, Business plan members can manage the data that is sent to their warehouses using 
Selective Sync
.

Sync Frequency

Your plan determines how frequently data is synced to your warehouse.

*If you’re a Business plan member and would like to adjust your sync frequency, you can do so using the Selective Sync feature. To enable Selective Sync, please go to 
Warehouse
 > 
Settings
 > 
Sync Schedule
.

Why can't I sync more than 24 times per day?

Segment does not set syncs to happen more than once per hour (24 times per day). The warehouse product is not designed for real-time data, so more frequent syncs would not necessarily be helpful.

Sync History

You can use the Sync History page to see the status and history of data updates in your warehouse. The Sync History page is available for every source connected to each warehouse. This page helps you answer questions like, “Has the data from a specific source been updated recently?” “Did a sync completely fail, or only partially fail?” and “Why wasn’t this sync successful?”

The Sync History includes the following information:

If a sync run shows a partial success or failure, the next sync attempts to sync any data that was not successfully synced in the prior run.

View the Sync History

To view the Sync History:

Warehouse Selective Sync

Warehouse Selective Sync allows you to manage the data that you send to your warehouses. You can use this feature to stop syncing specific events (also known as collections) or properties that aren’t relevant, and may slow down your warehouse syncs.

This feature is only available to Business Tier customers. 
You must be a Workspace Owner to change Selective Sync settings.

With Selective Sync, you can customize which collections and properties from a source are sent to each warehouse. This helps you control the data that is sent to each warehouse, allowing you to sync different sets of data from the same source to different warehouses.

This feature only affects 
warehouses
, and doesn’t prevent data from going to any other 
destinations
.

When you disable a source, collection or property, Segment no longer syncs data from that source. Segment won’t delete any historical data from your warehouse. When you re-enable a source, Segment syncs all events since the last sync. This doesn’t apply when a collection or property is re-enabled. Only new data generated after re-enabling a collection or property will sync to your warehouse.

For each warehouse only the first 5,000 collections per source and 5,000 properties per collection are visible in the Selective Sync user interface. 
Learn more about the limits
.

Disabling the 
received_at
 column will cause your syncs to fail, as all tables use 
received_at
 as the sort key.

When to use Selective Sync

By default, all sources and their collections and properties are sent, and no data is prevented from reaching warehouses.

When you disable sources, collections, or properties using Selective Sync, Segment stops sending new data for these sources, collections, or properties to your warehouse. It doesn’t delete any existing data in the warehouse.

If you choose to re-enable a source to begin syncing again, Segment loads all data that arrived since the last sync into the warehouse, but doesn’t backfill data that was omitted while these were disabled. When a collection or property is re-enabled, data only syncs going forward. It will not be loaded from the last sync.

Enable Selective Sync

To use Selective Sync:

Change sync settings to a single warehouse from multiple sources

To change the sync settings to a single warehouse from multiple sources, follow the same steps as 
above
.

This may be valuable if you’re looking to make changes in bulk, such as when setting up a new warehouse.

Change sync settings on a specific Warehouse to Source connection

To manage data from one specific source to an individual warehouse:

This may be valuable when you’re making smaller changes, for example, disabling all properties from one unnecessary collection.

All changes made through Selective Sync only impact an individual warehouse. They don’t impact multiple warehouses at once. To make changes to multiple warehouses, you need to enable/disable data for each individual warehouse.

Selective Sync User Interface Limits

Regardless of schema size, for each warehouse only the first 5,000 collections per source and 5,000 properties per collection can be managed using the Selective Sync user interface. After you hit any of these limits, all future data is still tracked and sent to your warehouse. New collections created after hitting this limit is not displayed in the Selective Sync table.

You will see a warning in the Selective Sync user interface when the warehouse schema has reached 80% of the limit for collections and/or properties. An error message will appear when you’ve reached the limit.

Contact 
Support
 to edit Selective Sync settings for any collections and/or properties which exceed the limit.

Only Workspace Owners can change Selective Sync settings.

This page was last modified: 07 Feb 2025

Need support?

Questions? Problems? Need more info? Contact Segment Support for assistance!

Help improve these docs!

Was this page helpful?

Thanks for your feedback!

Can we improve this doc? 
Send us feedback!

Get started with Segment

On this page

Was this page helpful?

Thanks for your feedback!

Can we improve
 this doc? 
Send us feedback!

Product

For Developers

Company

Support

© 2025 Segment.io, Inc.

=== Content from https://segment.com/docs/unify/traits/predictions/using-predictions/ ===

Using Predictions
        

Unify Plus requires a business tier account and is included with Engage


See the 
available plans
, or 
contact Support
.

On this page

Working with Predictions in Segment

Predictions are stored as 
computed traits
 in user profiles, with scores represented as percentage cohorts. For example, a score of 
0.8
 indicates the user is in the 80th percentile, or the top 20% of the cohort.

After selecting a cohort, use Predictions with the following Segment features:

Prediction tab

You can access generated Predictions in the 
Prediction
 tab of your Trait. The Prediction tab gives you actionable insight into your prediction.



The 
Explore your prediction
 section of the Prediction tab visualizes prediction data and lets you create Audiences to target. An interactive chart displays a percentile cohort score that indicates the likelihood of users in each group to convert on your chosen goal. You can choose the top 20%, bottom 80%, or create custom ranges for specific use cases.

You can then create an Audience from the group you’ve selected, letting you send efficient, targeted marketing campaigns within Journeys. You can also send your prediction data to downstream destinations.

Model monitoring

Predictions rank your customers by their likelihood to perform a specific conversion event, from most to least likely.

For each custom prediction, Segment monitors the percentile cohort where customers were ranked when they performed the predicted conversion event. After around 7 days, Segment creates a graph data visualization, allowing you to evaluate the prediction’s accuracy based on real workspace data.



For example, suppose you’re predicting the likelihood of customers completing an 
order_completed
 event. The graph shows that:

This pattern shows that the prediction was extremely accurate in identifying customers most likely to convert. Ideally, most graphs will show a similar trend, where the highest-ranked cohorts have the most conversion activity.

However, this pattern can change depending on how you use Predictions. For example, if you run a marketing campaign targeting the bottom 10% cohort, you might see an increase in conversions for that group instead.

Like any AI or machine learning tool, Predictions may not always be perfect. Start small, test your predictions, and refine your approach as needed. Model monitoring makes it easier to measure and improve the accuracy of your predictions.

The Predictions tab’s 
Understand your prediction
 section provides insights into the performance of the underlying predictive model. This information helps you understand the data points that contribute to the prediction results.



The Understand your prediction dashboard displays the following model metrics:

The 
Understand your prediction
 tab isn’t available for the Predicted LTV computed trait because it relies solely on 
Order Completed
 events for its calculation. Other predictive traits use multiple event types, which enables this feature.

Predictions use cases

Predictions offer more value in some situations than others. This sections covers common scenarios where predictions have high impact, as well as others where alternative approaches may be more appropriate.

Marketing opportunities

Data science use cases

When to use a prediction

Predictions are most effective in the following situations:

When other approaches work better

Predictions may not be as beneficial in the following situations:

FAQs

Segment uses a binary classification model that uses decision trees.

Once Segment creates your prediction, you can check the model statistics page, where Segments shows you how the model was created. Segment also maintains automated systems that monitor model performance and will alert you if your model is not predictive.

Trait creation depends on the amount of data, but Segment expects predictions to be completed in around 24 hours. For larger customers, however, this could take 48 hours. Predictions shows a status of 
In Progress
 while computing; Segment updates this status when customers are scored.

These data science statistics measure the effectiveness of Segment’s predictions when tested against historical data. For more information, refer to 
ROC Curve and AUC
, 
The Lift Curve in Machine Learning
, and 
Intuition behind log-loss score
.

The Prediction Quality Score factors AUC, log loss, and lift quality to determine whether Segment recommends using the prediction. A model can have a score of Poor, Fair, Good, or Excellent.

The created trait value represents the user’s percentile cohort. This value will refresh when we re score the customers based on your refresh cadence. If you see 
0.85
 on a user’s profile, this means the user is in the 85th percentile, or the top 15% for the prediction.

Segment rebuilds the machine learning model every 30 days.

By default, Segment refreshes scores every 7 days. However, you can request that trait values update daily. Reach out to your CSM to determine your eligibility.

Predictive Traits can’t be updated, but Predictive Audiences can. To modify a Predictive Trait, you’ll need to recreate it.

You get five predictions as part of Engage Foundations or Unify Plus. To purchase more predictions, reach out to your CSM.

Predictive Audiences contribute to the Engage limit of 100 audiences. Whether you create the audience manually or with predictive modeling, the audience counts towards the 100-audience limit.

Yes.

Yes. Keep the following in mind when you work with Predictions:

The Predictions Builder doesn’t display nested properties.

Segment calculates the average by adding the probabilities for all users and dividing by the total number of users. If a user’s score in 
Likelier to convert than average
 is below 1, they are less likely to convert compared to the average user.

This page was last modified: 06 Feb 2025

Need support?

Questions? Problems? Need more info? Contact Segment Support for assistance!

Help improve these docs!

Was this page helpful?

Thanks for your feedback!

Can we improve this doc? 
Send us feedback!

Get started with Segment

On this page

Was this page helpful?

Thanks for your feedback!

Can we improve
 this doc? 
Send us feedback!

Product

For Developers

Company

Support

© 2025 Segment.io, Inc.

=== Content from https://segment.com/docs/privacy/user-deletion-and-suppression/ ===

User Deletion and Suppression
        

On this page

Segment offers you the ability to delete and suppress data about your end-users when they are identifiable by a 
userId
 to support your compliance with privacy regulations like the GDPR and CCPA. For example, if your end-user invokes the Right to Object or Right to be Forgotten, you can block ongoing data collection about that user and delete all historical data about them from Segment’s systems, any of your connected warehouses or S3 buckets, and some supported downstream partners.

Business Plan Customers

If you use this feature to delete data, you can not Replay the deleted data. For standard Replay requests, you must wait for any pending deletions to complete, and you cannot submit new deletion requests for the period of time that Segment replays data for you.

Regulations

All deletion and suppression actions in Segment are asynchronous and categorized as Regulations, or requests to Segment to control your data flow. You can issue Regulations from:

With Regulations, you can issue a single request to delete and suppress data about a user by 
userId
. Segment scopes Regulations to all sources in your workspace.

Data sent to device-mode destinations cannot be suppressed

Destinations set up in device mode are sent directly to destinations and bypass the point in the pipeline where Segment suppresses events.

The following regulation types are available:

All regulations are rate limited to 110,000 users within a 30-day period

To send more than 110,000 
SUPPRESS_ONLY
, 
UNSUPRESS
, 
DELETE_INTERNAL
 and/or 
SUPPRESS_WITH_DELETE_INTERNAL
 Regulations over a 30 day period, 
contact Segment Support
.

Deletion Support

When you create a 
SUPPRESS_WITH_DELETE
 and 
SUPPRESS_WITH_DELETE_INTERNAL
 regulation, Segment begins to suppress new data ingestion for that user, and begins to permanently delete previously ingested data associated with this user from your workspace. This includes scanning and removing all messages related to that 
userId
 from all data stores that don’t automatically expire data within 30 days.

Segment deletes messages with this 
userId
 from the following warehouses and storage destinations:

Warehouse deletions occur using a DML run against your cluster or instance. Segment deletes from S3 by “recopying” clean versions of any files in your bucket that included data about that 
userId
.

The deletion requests tab shows a log of all regulations and their status.

In the Segment App (Settings > End User Privacy > Deletion Requests), you can click a 
userId
 to view its status in Segment internal systems and in the connected destinations.

The deletion request can have one of the following statuses:

When checking the status of deletion requests using Segment’s API, the deletion will report an overall status of all of the deletion processes. As a result, Segment returns a 
FAILED
 status because of a failure on an unsupported destination, even if the deletion from the Segment Internal Systems and supported destinations were completed successfully.

Segment has a 30-day SLA for completing deletion requests in Segment’s internal stores for deletion requests of fewer than 110,000 users made over 30 days. Your requests will be rate limited if you submit more than 110,000 deletion requests within 30 days.

This 30-day SLA is limited to only Segment's internal stores

Segment cannot guarantee that deletions in your Amazon S3 instance, your connected data warehouse, or other third-party destinations will be completed during that 30-day period.

Segment forwards your deletion requests to a 
growing list of supported partners
, but you should confirm that each partner fulfills the request. You will also need to contact any unsupported destinations separately to manage user data deletion.

Users that you UNSUPPRESS after issuing a deletion request may have remaining data

If you 
UNSUPPRESS
 a user after issuing a deletion request for that user, Segment’s deletion functionality does not clean up data sent after removing the user from the suppression list.

The Right to be Forgotten and Suppression Support

When your customers exercise their Right to be Forgotten, sometimes known as Right to Erasure, they expect you to stop collecting new data and delete all previously collected data from your systems: including from Segment and other downstream tools.

Segment offers suppression tools to help you manage the challenge of users opting-out across different channels and platforms. Segment encourages and expects that you design your systems and applications so you don’t collect or forward data to Segment until you have unambiguous, specific, informed consent or have established another lawful legal basis to do so.

Suppression is not a substitute for gathering affirmative, unambiguous consent about data collection and its uses.

Suppression support

SUPPRESS
 regulations
 add a user to your suppression list by the 
userId
. Segment blocks suppressed users across all sources, and messages you send to Segment with a suppressed 
userId
 are blocked at the API. These messages do not appear in the debugger, are not saved in archives and systems, and are not sent to any downstream server-side destinations.

To 
remove a user from the suppression list
, create an 
UNSUPPRESS
 regulation.

The Suppressed Users tab in Segment App (Settings > End User Privacy) allows you to create new Suppression requests and also shows a list of 
userId
s that are 
actively
 being suppressed.

To create a suppression regulation and add a 
userId
 to this list, click 
Suppress New User
, and enter the 
userId
 in the field that appears. Then click 
Request Suppression
.

Segment creates a 
SUPPRESS
 regulation, and adds the 
userId
 to your suppression list, mostly processed within 24 hours. In some cases, the suppression request can take up to 30 days to process, depending on the number of requests that are in the queue for your workspace. Once you’ve created the request, Segment blocks data about these users across all sources.

SUPPRESS_WITH_DELETE requests

The Suppressed Users tab only includes 
SUPPRESS_ONLY
 regulations. If you created a User Deletion request using the UI, you will need to check the 
Deletion Requests
 tab, as those are 
SUPPRESS_WITH_DELETE
 regulation types.

To remove a user from the suppression list, click the ellipses (
…
) icon on the 
userId
 row, and click 
Remove
.

This creates an 
UNSUPPRESS
 regulation and removes the 
userId
 from your suppression list. Segment processes most 
UNSUPPRESS
 regulations within 24 hours.

Data retention

Segment stores a copy of all event data received in Segment’s secure event archives on S3. By default, all workspaces store data for an unlimited period of time, but you can modify the lifecycle policies for the data stored internally. Segment uses this data for 
data replays
 and for troubleshooting purposes.

Segment recommends keeping your data for at least 30 days to enable 
replays
 of your data.

To change your data retention settings, open Segment and navigate to 
Privacy > Settings > Data Retention
.

Workspace Default Archive Retention Period

Select the default retention period for the workspace in this setting. This value applies to all sources in the workspace, unless overridden in the 
Source-Level Archive Retention Periods
 setting.

7 day Retention Periods will be deprecated on March 6, 2025

After March 6, you will no longer be able to set your workspace’s retention period to 7 days. All workspaces with 7 day retention periods will be updated to have 14 day retention periods.

You can select from the following Archive Retention time periods:

Source-Level Archive Retention Periods

Source-Level Archive Retention Periods will be deprecated on April 15, 2025

After April 15, you will no longer be able to override your workspace’s default retention period on a source-by-source basis.

Override the workspace default retention period on a per-source level.

You can select from the following Archive Retention time periods:

This page was last modified: 18 Feb 2025

Need support?

Questions? Problems? Need more info? Contact Segment Support for assistance!

Help improve these docs!

Was this page helpful?

Thanks for your feedback!

Can we improve this doc? 
Send us feedback!

Get started with Segment

On this page

Was this page helpful?

Thanks for your feedback!

Can we improve
 this doc? 
Send us feedback!

Product

For Developers

Company

Support

© 2025 Segment.io, Inc.

=== Content from https://segment.com/docs/api/public-api/ ===

Public API
        

The Public API is available to customers on Team or Business plans.


See the 
available plans
, or 
contact Support
.

On this page

The Segment Public API helps you manage your Segment workspaces and its resources. You can use the API to perform CRUD (create, read, update, delete) operations at no extra charge. This includes working with resources such as Sources, Destinations, Warehouses, Tracking Plans, and the Segment Destinations and Sources Catalogs. The Public API is available to Team and Business Tier customers.

All CRUD endpoints in the API follow REST conventions and use standard HTTP methods. Different URL endpoints represent different resources in a workspace.

Research and test the Public API's available endpoints.

If your application is built in Javascript / Typescript, Go, Java, or Swift, check out 
Segment’s Public API SDKs
.

Config API vs Public API

The Public API includes the following benefits over the Config API:

Create a Public API token

Only Workspace Owners can create a Public API token

Only users with the Workspace Owner role can create a Public API token. For more information about roles, see Segment’s 
Roles
 documentation.

To create a Public API token in your Segment workspace:

To begin sending requests to the Public API, make sure to include the Public API Token into your HTTP requests with the 
Authorization
 Header and configured with 
Bearer Token
 and the value of the newly generated Public API token.

API Token Security

To enhance API token security, Segment partners with GitHub to prevent fraudulent use of exposed API tokens found in public git repositories. This helps to prevent malicious actors from using exposed tokens to perform unauthorized actions in your Segment workspace.

Within seconds, GitHub scans each commit in public repositories for Public API tokens, and sends detected tokens to Segment. Valid tokens are automatically revoked and workspace owners are notified.

Learn more about 
GitHub’s secret scanning program
.

FAQs

In most cases, identifying and revoking an exposed token takes seconds. Segment recommends you check the 
audit trail
 to ensure no unauthorized actions were taken with the token.

Developers can accidentally commit tokens to public repositories, exposing them to the public. This can happen when developers use a token in a local development environment and forget to remove it before committing their code.

By automatically revoking the exposed token, Segment helps keep your workspace secure and prevents potential abuse of the token.

This feature is automatically enabled for all workspaces on Team or Business tier plans.

If you see a CORS error, this means you’re attempting to make a request to the Public API on the front-end. The Public API is used for server-side only. To get rid of the error, move all Public API requests to a server.

Only 
users that have a 
Workspace Owner
 role
 can create Public API Tokens.

Troubleshooting

When you don’t have a source to forward violations or blocked events to, then exclude the fields 
forwardingViolationsTo
 or 
forwardingBlockedEventsTo
 entirely from the request and the setting will be disabled.

PATCH
  endpoint : 
https://api.segmentapis.com/sources/{sourceId}/settings

{
    "group": {
      "allowTraitsOnViolations": false,
      "allowUnplannedTraits": false,
      "commonEventOnViolations": "ALLOW"
    },
    "identify": {
      "allowTraitsOnViolations": true,
      "allowUnplannedTraits": true,
      "commonEventOnViolations": "Block"
    },
    "track": {
      "allowEventOnViolations": false,
      "allowPropertiesOnViolations": false,
      "allowUnplannedEventProperties": false,
      "allowUnplannedEvents": false,
      "commonEventOnViolations": "OMIT_PROPERTIES"
    }
  }


What is the difference between a destination’s Instance ID and Meta ID?

The destination’s Instance ID is specific to a single destination within your workspace. The destination’s Meta ID, which is returned by the delivery metrics endpoint, identifies which integration you’ve set up. For example, if you had a 
dev
 Mixpanel (Actions) destination and a 
prod
 Mixpanel (Actions) destination, they would have the same Meta ID but two different Instance IDs.

This page was last modified: 24 Jun 2024

Need support?

Questions? Problems? Need more info? Contact Segment Support for assistance!

Help improve these docs!

Was this page helpful?

Thanks for your feedback!

Can we improve this doc? 
Send us feedback!

Get started with Segment

On this page

Was this page helpful?

Thanks for your feedback!

Can we improve
 this doc? 
Send us feedback!

Product

For Developers

Company

Support

© 2025 Segment.io, Inc.

=== Content from https://segment.com/docs/guides/how-to-guides/measure-advertising-funnel/ ===

Measuring Your Advertising Funnel
        

On this page

It’s surprisingly hard to answer questions about the ROI of your ad campaigns. What does a click actually result in? How much should I pay for it? We built our Sources for Facebook Ads and Google Adwords to help you understand the true performance and cost of your campaigns.

In this article, we dig into the nuances of data collection and potential gotchas around measuring clicks, pageviews, and ultimately, conversions.

Measuring Campaign Performance

Today, most marketing teams think about their paid acquisition funnel as three major steps…



This makes sense when looking at overall campaign performance, but hides several crucial funnel steps that can make the difference between increasing a campaign’s spend and shutting it off due to poor results.

Because page optimization and ad blockers can impact measurement of your funnel, it’s important to look at the four additional steps happening between the ad click and conversions.



Let’s go through each true funnel step in a little more detail.

Impressions & Clicks:
 When a user views an ad, the ad platform increments the count of impressions for that ad. When an ad is clicked, the ad platform logs a click. This is all handled by the ad platform’s servers. Facebook and Google work hard to filter invalid and fraudulent traffic, whether that’s a mistaken click, a bot, or a competitor looking to drain your advertising budget. Any bad traffic is removed from both your reporting and your monthly bill.

Page Request Initiated:
 After an ad is clicked, a user’s browser attempts to load your landing page. This request is the first contact your application has with the user, and the server responds with the content to render the landing page.

First JavaScript Loaded:
 The user’s browser starts to download the landing page content, which includes the HTML, JavaScript, and CSS. The browser parses and renders this content, loading the JavaScript sequentially as it parses the page. By default, analytics.js uses the 
async
 
tag
, which means that the browser won’t block the page and will load analytics.js once everything else is ready. Analytics.js wants to get out of the way where possible so you can create the best experience for your customers.

Page Fully Rendered:
 The page is fully rendered once all the html, css and scripts have been loaded on the page. This time can vary a lot based on the speed of the internet connection (how fast all the assets download) and the device itself (how fast the local computer can run all of the scripts).

Third-Party Scripts Loaded:
 Finally, third-party scripts are asynchronously loaded onto the page. The speed at which these scripts are loaded depends on a variety of factors, like the page size, network speed, and the size and number of the third-party scripts. Once these scripts are loaded, analytics.js triggers a 
page
 call to our API.

Conversion Event:
 From there, a user might fill out a form, signup, or buy your product!

How does this impact my ad reporting?

There are three less-obvious contributors to fall-off across the paid acquisition funnel: slow loads, ad blockers, and bounces.

For the sake of illustration, this means that if you have 100 ad clicks, you will be able to count most but not all corresponding page views because some visitors may bounce (exit or hit the back button) before analytics.js is executed. Similarly, you may miss some attributable conversions due to slow load times (your page calls can’t fire in time) and ad blockers (which often block analytics not just ads).



Here’s how it works.

Slow Loads

Slow loads can impact your attribution modeling, making campaigns appear to have worse performance than reality. In the general case, when a user hits your landing page, your tracking code loads and triggers a pageview event that you can use to attribute that user to a campaign.

But if third-party scripts take on the order of seconds to load (for example, on 1x or 3G networks), users may click off the page before your tracking code executes. In this case, the pageview never gets recorded and your ability to attribute that click to a conversion is lost.

This is generally not an issue for most companies because they are focused more on people who spend a good deal of time on their pages. However, it is a potential source of opaqueness, particularly for users with slow or bad network connection.

Bounces

Bounces can occur at any stage of the funnel between an ad being clicked and third-party scripts loading on the page.

Some bounces are not tracked because the user doesn’t even last the few seconds to request your HTML, render it, and execute tracking. If they quickly hit back or close the browser window, your ad platform will report clicks that don’t show up in your analytics tracking.

Ad blockers

It is likely the case that some percentage of your users are using ad blockers. It’s estimated that 
22% of mobile smartphones worldwide
 and 
16% of US web traffic
 use ad blockers. Segment customers have reported ad blockers for as little as a few percentage points of their visitors, to upwards of 70% of traffic for companies with very tech-forward audiences.

But just because a user is using ad blockers doesn’t mean that they aren’t seeing and clicking on ads. Facebook recently announced that they would be 
suppressing ad blockers
, and Adblock Plus, the most popular ad blocking and anti-tracking software, categorizes Google Search ads as 
acceptable ads
.

That said, many ad blockers do block analytics tools like Google Analytics, Mixpanel and Segment. This means that there exists some percentage of your conversions that actually came through your paid acquisition channels, but are unattributable due to ad blockers.

What if I need more precise tracking?

Segment offers two ways of joining your user clickstream data to your paid acquisition channels: standard client-side tracking or advanced server-side page calls.



Both options come with their own tradeoffs that are important to consider for your use case.

Client-side Tracking (Standard)

Analytics.js is loaded with the async tag by default, which means that the library and all it’s destinations are loaded near the end of the page rendering. The benefit is that analytics.js doesn’t slow down page loads, but it does mean that tracking is not executed immediately on page load.

When you use standard client-side tracking, you’ll lose pageview data for visitors who bounce or click off the page before analytics.js executes, and for visitors with ad blockers enabled.

Server-side Page Calls (Advanced)

If you want to capture adblock, bounce, and slow load traffic, we recommend adding an additional 
page()
 call to the server-side. This allows you avoid the browser altogether and see the total number of requests emanating from your paid acquisition channels. You’ll get visibility on an extra step in that funnel.

The general approach is to use an arbitrary 
anonymousId
 (e.g. a UUID) in the server-side 
page()
 call and then also set the 
anonymousId
 as the 
ajs_anonymous_id
 cookie in the browser. You can read more about how to implement that here. This approach is tricky to implement, so we recommend that this is undertaken only for use cases in which bounce and/or adblock data is critical.

Estimating the Impact of Moving Server-side

If you want to get a quick estimate for the number of additional clicks you’d track using server-side tracking, you can use “redirect tracking” with a URL shortener to estimate the number of clicks coming from Google Adwords or Facebook Ads. This will give you an estimate for the number of times an ad is clicked (minus some bounce in the few hundred milliseconds of the redirect), which will closely match server-side 
page()
 tracking should you choose to implement it.



Here’s how it works…

Use a URL shortener like bit.ly to link to a landing page, with a custom parameter like 
?ttg=2
 .

Add the shortened link to your ad.

Measure total clicks from the bit.ly stats page.


In your warehouse, count the number of pages with that unique url parameter from step 1 (make sure you’re looking at the same timeframe).

select
 
received_at
,
 
url


from
 
<
site
>
.
pages


where
 
url
 
like
 
'%/warehouses%'


and
 
search
 
like
 
'%ttg=2'


order
 
by
 
received_at



We hope this overview helps explain the technical nuances of measuring what happens when a customer finds you using an ad! If you have any other questions, feel free to share them in the Segment Community for discussion.

This page was last modified: 21 Apr 2023

Need support?

Questions? Problems? Need more info? Contact Segment Support for assistance!

Help improve these docs!

Was this page helpful?

Thanks for your feedback!

Can we improve this doc? 
Send us feedback!

Get started with Segment

On this page

Was this page helpful?

Thanks for your feedback!

Can we improve
 this doc? 
Send us feedback!

Product

For Developers

Company

Support

© 2025 Segment.io, Inc.

=== Content from https://segment.com/docs/privacy/consent-management/consent-faq/ ===

Frequently Asked Questions
        

Consent Management is available to customers on Business tier plans.


See the 
available plans
, or 
contact Support
.

On this page

Is Segment’s Consent Manager part of Consent Management?

No. Segment’s deprecated 
open-source Consent Manager
, which 
captures
 end user cookie consent, is not part of Segment’s Consent Management product, which focuses only on 
enforcing
 end user consent. Enforcing end user consent means sharing your end users’ data with only the destinations they consented to share data with and blocking the flow of their data to all other destinations.

Segment recommends moving from the deprecated, open-source Consent Manager to one that meets your legal compliance requirements.

What destinations support consent enforcement?

All event streams destinations, with the exception of AWS S3 and Engage destinations, support consent enforcement.

Can I share current end user consent preferences with my destinations?

You can use the 
Destination Actions framework
 to share the current status of your end-users’ consent with your Actions destinations.

For more information, see the 
Sharing consent with Actions destinations
 documentation.

Can I use a Consent Management Platform (CMP) other than OneTrust to collect consent from my end users?

Yes, you can use any commercially available CMP or custom solution to collect consent from your end users. If you use a CMP other than OneTrust, you must generate your own wrapper or other mechanism to add the following objects to the events collected from your sources:

Segment provides guidance about creating your own wrapper in the 
@segment/analytics-consent-tools
 GitHub repository.

This page was last modified: 02 May 2024

Need support?

Questions? Problems? Need more info? Contact Segment Support for assistance!

Help improve these docs!

Was this page helpful?

Thanks for your feedback!

Can we improve this doc? 
Send us feedback!

Get started with Segment

On this page

Was this page helpful?

Thanks for your feedback!

Can we improve
 this doc? 
Send us feedback!

Product

For Developers

Company

Support

© 2025 Segment.io, Inc.

=== Content from https://segment.com/docs/connections/functions/environment/ ===

The Functions Editing Environment
        

On this page

Segment Functions create reusable code that can be run in your Segment workspace either as sources to format incoming events, or as destinations, to handle specific event types.

When you create a function, write code for it, and save it, the function appears in the Catalog in your workspace 
only
. You can then deploy that function in your workspace just as you would a conventional source or destination.

Access to Functions is controlled by specific 
access management roles
. You may need additional access to create and deploy functions.

Creating functions

Only 
Functions admins
 can create or edit functions.

Select the type of function you want to build, and click 
Build
.

When you click 
Build
, a code editor appears. Different template code is available depending on which type of function you created.

After you click 
Create Function
, the function appears on the 
Functions catalog page
 in your workspace.

Editing a function

If you are a 
Workspace Owner
 or 
Functions Admin
, you can manage your function from the 
Functions catalog page
.

If you’re editing an existing function, you can 
Save
 changes without changing the behavior of existing instances of the function.

You can also choose to 
Save & Deploy
 to push changes to all, or specific functions in your workspace that are already deployed. You might need additional permissions to deploy these changes.

Testing a function

You have the option to test your functions code with either a sample event or by loading a default event that you can customize yourself.

Once the payload you want to test is ready, click 
Run
.

If you create settings in your function, then you need to fill in the setting values before clicking 
Run
.

Deploying source functions

You must be a 
Workspace Owner
 or 
Source Admin
 to connect an instance of your function in your workspace.

Deploying destination functions

If you’re editing an existing function, you can 
Save
 changes without changing the behavior of your deployed function. You can also choose to 
Save & Deploy
 to push changes to all, or specific functions in your workspace that are already deployed.

When you deploy your destination function in your workspace, you fill out the settings on the destination configuration page, similar to how you would configure a normal destination.

Functions Versioning

With Functions Versioning, you can access a complete change history for each source or destination function. View version history and creation details, then use a unified or split display to compare code and restore previous versions of a function.

View and compare version history

To view the version history of a function:

Select previous versions to compare code using a 
unified
 or 
split
 view. With the split view, Segment displays the latest version on the left and the version you’ve selected on the right.

Unified and split compare screens are read-only. While you can copy code, you can’t make changes directly from these screens.

In the Version History panel, Segment displays 
LATEST
 and 
DEPLOYED
 labels that represent a function version state. You’ll see the 
LATEST
 version at the top.

Segment labels a version as the 
LATEST
 when:

The 
DEPLOYED
 version is the function version that’s currently deployed.

Restore a previous version

To restore a previous function version:

Use Versioning with Segment’s Public API

You can use Functions Versioning with Segment’s 
Public API
 to retrieve version history records and source code, as well as to restore previous versions.

Here are some Public API use case examples:

Get Version history
: Use the 
/versions
 endpoint to retrieve a list of version records and metadata of a certain page size. You can also use this endpoint to get version source code for a given version ID.

Restore a previous version
: Use the 
/restore
 endpoint to restore a previous function version. This creates a new version with the same source as the version you are restoring.

Create or update versions
: Create or update a function to add a version record and save the source code.

Deploy a function
: Use the Public API to deploy a function. After you deploy, Segment marks the function version as 
DEPLOYED
. Learn more about function version states in the 
Latest and deployed versions
 section.

View Segment’s 
Public API
 docs for more information on how to use Functions Versioning with the Public API.

Functions permissions

Functions have specific roles which can be used for 
access management
 in your Segment workspace.

Access to functions is controlled by two permissions 
roles
:

You also need additional 
Source Admin
 permissions to enable source functions, connect destination functions to a source, or to deploy changes to existing functions.

️Settings and secrets

Settings allow you to pass configurable variables to your function, which is the best way to pass sensitive information such as security tokens. For example, you might use 
settings
 as placeholders to use information such as an API endpoint and API key. This way, you can use the same code with different settings for different purposes. When you deploy a function in your workspace, you are prompted to fill out these settings to configure the function.

First, add a setting in 
Settings
 tab in the code editor:



Click 
Add Setting
 to add your new setting.



You can configure the details about this setting, which change how it’s displayed to anyone using your function:

As you change the values, a preview to the right updates to show how your setting will look and work.

Click 
Add Setting
 to save the new setting.

Once you save a setting, it appears in the 
Settings
 tab for the function. You can edit or delete settings from this tab.



Runtime and dependencies

On March 26, 2024, Segment is upgrading the Functions runtime environment to Node.js v18, which is the current long-term support (LTS) release.

This upgrade keeps your runtime current with industry standards. Based on the 
AWS Lambda
 and 
Node.js
 support schedule, Node.js v16 is no longer in 
Maintenance LTS
. Production applications should only use releases of Node.js that are in 
Active LTS
 or 
Maintenance LTS
.

All new functions will use Node.js v18 starting March 26, 2024.

For existing functions, this change automatically occurs as you update and deploy an existing function. Segment recommends that you check your function post-deployment to ensure everything’s working. Your function may face issues due to the change in sytax between different Node.js versions and dependency compatibility.

Limited time opt-out option 


If you need more time to prepare, you can opt out of the update before March 19, 2024. 
 Note that if you opt out: 

- The existing functions will continue working on Node.js v16. 

- You won’t be able to create new functions after July 15, 2024. 

- You won’t be able to update existing functions after August 15, 2024. 

- You won’t receive future bug fixes, enhancements, and dependency updates to the functions runtime. 


Contact Segment
 to opt-out or with any questions. 

Node.js 18 

Segment strongly recommends updating to Node.js v18 to benefit from future runtime updates, the latest security, and performance improvements.

Functions do not currently support importing dependencies, but you can 
contact Segment Support
 to request that one be added.

The following dependencies are installed in the function environment by default.

zlib v1.0.5
 exposed as 
zlib.zlib

 
uuidv5
 is exposed as an object. Use 
uuidv5.uuidv5
 to access its functions. For example:

  
async
 
function
 
onRequest
(
request
,
 
settings
)
 
{

       
uuidv5
 
=
 
uuidv5
.
uuidv5
;

       
console
.
log
(
typeof
 
uuidv5
);


        
//Generate a UUID in the default URL namespace

        
var
 
urlUUID
 
=
 
uuidv5
(
'
url
'
,
 
'
http://google/com/page
'
);

        
console
.
log
(
urlUUID
);


        
//Default DNS namespace

        
var
 
dnsUUID
 
=
 
uuidv5
(
'
dns
'
,
 
'
google.com
'
);

        
console
.
log
(
dnsUUID
);

    
}



zlib
’s asynchronous methods 
inflate
 and 
deflate
 must be used with 
async
 or 
await
. For example:

zlib
 
=
 
zlib
.
zlib
;
  
// Required to access zlib objects and associated functions


async
 
function
 
onRequest
(
request
,
 
settings
)
 
{

  
const
 
body
 
=
 
request
.
json
();


  
const
 
input
 
=
 
'
something
'
;


  
// Calling inflateSync method

  
var
 
deflated
 
=
 
zlib
.
deflateSync
(
input
);


  
console
.
log
(
deflated
.
toString
(
'
base64
'
));


  
// Calling inflateSync method

  
var
 
inflated
 
=
 
zlib
.
inflateSync
(
new
 
Buffer
.
from
(
deflated
)).
toString
();


  
console
.
log
(
inflated
);


  
console
.
log
(
'
Done
'
);

  
}



The following Node.js modules are available:

Other built-in Node.js modules
 aren’t available.

For more information on using the 
aws-sdk
 module, see how to 
set up functions for calling AWS APIs
.

Caching

Basic cache storage is available through the 
cache
 object, which has the following methods defined:

Some important notes about the cache:

const
 
ttl
 
=
 
5
 
*
 
60
 
*
 
1000
 
// 5 minutes


const
 
val
 
=
 
await
 
cache
.
load
(
"
mycachekey
"
,
 
ttl
,
 
async 
()
 
=>
 
{

    
const
 
res
 
=
 
await
 
fetch
(
"
http://echo.jsontest.com/key/value/one/two
"
)

    
const
 
data
 
=
 
await
 
res
.
json
()

    
return
 
data


})



This page was last modified: 17 Jan 2025

Need support?

Questions? Problems? Need more info? Contact Segment Support for assistance!

Help improve these docs!

Was this page helpful?

Thanks for your feedback!

Can we improve this doc? 
Send us feedback!

Get started with Segment

On this page

Was this page helpful?

Thanks for your feedback!

Can we improve
 this doc? 
Send us feedback!

Product

For Developers

Company

Support

© 2025 Segment.io, Inc.

=== Content from https://segment.com/docs/privacy/hipaa-eligible-segment/ ===

HIPAA Eligible Segment
        

HIPAA eligible workspaces require a Business Assosciate Addendum.


See the 
available plans
, or 
contact Support
.

On this page

Segment is a HIPAA eligible platform, and meets the data privacy and security requirements of healthcare customers and their stakeholders. For more information about Segment becoming HIPAA eligible, see the 
announcement blog post
.

Business Associate Addendum

Twilio BAA are available to customers on a Business Tier plan.

Before you begin, check that the Segment products and services you’ll use for your HIPAA workflows are on the list of Twilio’s 
HIPAA Eligible Products and Services
. After you’ve verified availability, contact your Account Expert to 
request a demo
.

Verify your Workspace

Ensure your Workspace is eligible for HIPAA before you configure and send any personal health information (PHI).

With the BAA signed and Workspace confirmed as eligible, you can start building. For more information about starting a HIPAA compliant implementation, see Twilio’s 
Architecting for HIPAA on Twilio
, which outlines the shared responsibilities and requirements for building and maintaining HIPAA-compliant workflows in Segment.

HIPAA Auditing

Segment maintains audit logs of every read and update action a user performs in the Segment app that may involve PHI/PII.

Data captured in the HIPAA audit logs includes:

These logs can be provided upon request. For specific requests, please reach out to 
friends@segment.com
.

Data encryption

Segment encrypts the data in select fields 
marked as yellow in the Privacy Portal
 before sending them to event stream, cloud mode destinations, further supporting HIPAA compliance in your destinations. Segment encrypts data using a RSAES OAEP SHA 256 algorithm.

Data encryption does not support “fuzzy matching”. You can encrypt 
Default PII matchers
, 
Custom PII matchers
, and any 
Synonyms
 you’ve created for keys.

Data encryption only supports event-stream, cloud-mode destinations

Only data fields in 
context
, 
traits
, and 
property
 objects can be encrypted.

After Segment encrypts the data, the encrypted data value is always a 
string
. Any downstream validation that looks for 
integer
 or 
boolean
 data types will fail for encrypted values.

Configure data encryption for a new destination

To configure data encryption while setting up a new destination:

Private Key is not recoverable

Segment does not save the private key created during the data encryption setup flow, and cannot retrieve the key after you finish setting up your destination. You can generate a new key using the instructions in the 
Configure new key pairs
 section. Any data encrypted prior to generating a new key pair cannot be decrypted with the new key.

Configure data encryption for an existing destination

To configure data encryption for an existing destination:

Private Key is not recoverable

Segment does not save the private key created during the data encryption setup flow, and cannot retrieve the key after you finish setting up your destination. You can generate a new key using the instructions in the 
Configure new key pairs
 section. Any data encrypted prior to generating a new key pair cannot be decrypted with the new key.

Configure new key pairs

If you lose access to your private key, you can generate a new key pair in your destination’s Data Encryption tab. Any data previously encrypted using the previous key pair is unaffected, but cannot be decrypted using the new key.

To generate a new key pair:

Edit encrypted fields

After enabling encryption for a destination, you can add or remove encrypted data fields in your destination’s 
Data Encryption
 tab. All changes made to fields are forward-looking. You may experience some latency between making the changes and having the changes take effect.

To make changes to your selected fields:

Remove encryption

Disabling the data encryption setting removes encryption on all previously configured data.

To remove encryption from incoming data:

Disabling the data encryption setting does not decrypt existing data, but does prevent any future data from being encrypted.

User session timeouts

Segment automatically logs out all users with access to HIPAA eligible workspaces after 15 minutes of inactivity.

This page was last modified: 25 Jan 2024

Need support?

Questions? Problems? Need more info? Contact Segment Support for assistance!

Help improve these docs!

Was this page helpful?

Thanks for your feedback!

Can we improve this doc? 
Send us feedback!

Get started with Segment

On this page

Was this page helpful?

Thanks for your feedback!

Can we improve
 this doc? 
Send us feedback!

Product

For Developers

Company

Support

© 2025 Segment.io, Inc.

=== Content from https://segment.com/docs/connections/alerting/ ===

Connections Alerting
        

On this page

Connections Alerting allows Segment users to receive in-app, email, and Slack notifications related to the performance and throughput of an event-streaming connection.

To access Connections Alerting, select an event-streaming connection (like a web library source or cloud mode destination) and click the 
Alerts
 tab.

On the Alerts tab, you can create alerts and view all active alerts for this connection. You can only edit or delete the alerts that you create.

Source volume alerts

You can create an alert that notifies you when the volume of events received by your source in the last 24 hours changes beyond a percentage you set. For example, if you set a change percentage of 4% and your source received 100 events over the first 24 hours, Segment would notify you the following day if your source ingested fewer than 96 or more than 104 events.

To receive a source volume alert in a Slack channel, you must first create a Slack webhook. For more information about Slack webhooks, see the 
Sending messages using incoming webhooks
 documentation.



To create a source volume alert:

To make changes to a source volume alert, select the icon in the Actions column for the alert and click 
Edit
.

To delete a source volume alert, select the icon in the Actions column for the alert and click 
Delete
.

Deleting alerts created by other users requires Workspace Owner permissions

All users can delete source volume alerts that they created, but only those with Workspace Owner permissions can delete alerts created by other users.

Successful delivery rate alerts

You can create an alert that notifies you when the volume of events successfully received by your destination in the last 24 hours falls below a percentage you set. For example, if you set a percentage of 99%, Segment notifies you if your destination had a successful delivery rate of 98% or below.

To receive a successful delivery rate alert in a Slack channel, you must first create a Slack webhook. For more information about Slack webhooks, see the 
Sending messages using incoming webhooks
 documentation.

To create a successful delivery rate alert:

To make changes to a successful delivery rate alert, select the icon in the Actions column for the alert and click 
Edit
.

To delete a successful delivery rate alert, select the icon in the Actions column for the alert and click 
Delete
.

Deleting alerts created by other users requires Workspace Owner permissions

All users can delete successful delivery alerts that they created, but only those with Workspace Owner permissions can delete alerts created by other users.

Segment generates delivery alerts for failed deliveries and successful deliveries, which are the last two stages of the delivery pipeline. As a result, alerts are based on Segment’s attempts to send qualified events to your destination, excluding those filtered out by business rules (like protocols, destination filters, or mappings).

This page was last modified: 02 Jul 2024

Need support?

Questions? Problems? Need more info? Contact Segment Support for assistance!

Help improve these docs!

Was this page helpful?

Thanks for your feedback!

Can we improve this doc? 
Send us feedback!

Get started with Segment

On this page

Was this page helpful?

Thanks for your feedback!

Can we improve
 this doc? 
Send us feedback!

Product

For Developers

Company

Support

© 2025 Segment.io, Inc.

=== Content from https://segment.com/docs/guides/how-to-guides/ ===

How to Guides Index
        

On this page

Segment’s How-to Guides provide an in-depth walk through and examples of the many things you can do to implement, automate, engage with, and begin analyzing your data. We’ve also got a series of 
Quickstart Guides
 for each of our Source libraries.

Implementation

Engagement and Automation

Analytics

Quickstart Guides

This page was last modified: 25 Apr 2022

Need support?

Questions? Problems? Need more info? Contact Segment Support for assistance!

Help improve these docs!

Was this page helpful?

Thanks for your feedback!

Can we improve this doc? 
Send us feedback!

Get started with Segment

On this page

Was this page helpful?

Thanks for your feedback!

Can we improve
 this doc? 
Send us feedback!

Product

For Developers

Company

Support

© 2025 Segment.io, Inc.

=== Content from https://segment.com/docs/engage/journeys/build-journey/ ===

Build a Journey
        

Engage Foundations requires a Business tier account and includes Unify.


See the 
available plans
, or 
contact Support
.

On this page

Before you begin

Verify that you’ve connected at least one source to your Engage space, with events streaming in.

For more information, see 
Setting up your Sources
.

Adding the entry condition

Segment recommends that your entry condition’s time window be shorter than or equal to any 
exit settings
 you have. This prevents users from repeating your journey an excessive amount of times.

Using historical data for the entry step

If you select the 
Use historical data
 option, Segment queries all historical data to generate a list of users who enter the Journey upon publication. If you don’t select 
Use historical data
, only users who satisfy the entry condition 
after
 you publish enter the Journey.

Your 
Use historical data
 selection won’t impact subsequent Journey steps.  Only future events and existing trait memberships trigger post-entry Journey steps.

Available step types

Once you’ve created an entry condition, you can begin adding steps to your Journey.

Journeys offers the following steps:

For more details on each available Journey step, view the 
Journey step types documentation
.

Publishing a Journey

Once you’ve added steps, you’re ready to publish the Journey.

To publish and activate a Journey, click 
Publish Journey
 from the Journey Overview. You can also click 
Publish Journey
 in the bottom-right corner of the Journey Builder.

Some Journey features can only be edited before publication. For more information, see the difference between Draft and Published Journeys below.

Your Journey is now live. Next, you’ll learn about making changes to a published Journey.

Working with a published Journey

You may find that you need to make changes to a published Journey, like adding new steps or pausing entry to the Journey. This section explains how to pause, resume, and clone a Journey so that you can modify it as needed.

Pausing and resuming a Journey

Pausing a published Journey prevents new users from joining your Journey. Users already in the Journey, however, will continue their progress.

Follow these steps to pause a Journey:

Compute Limits

Because pausing only affects new Journey members, paused Journeys still count towards compute credit limits.

Resuming a Journey

You can resume new user entries to a paused Journey at any time.

After you resume a Journey, users who meet the Journey’s entry conditions will join the Journey. New users will not enter the Journey, however, if they met its entry conditions while it was paused.

Follow these steps to resume entry to a paused Journey:

Cloning a Journey

You can duplicate a Journey by cloning it.

Follow these steps to clone a Journey:

Segment then creates a draft of your Journey.

You can also clone a Journey from a Journey’s Overview by clicking the 
…
 icon.

Archive a Journey

Use the Journey archive setting when you want to end a Journey but preserve its data.

No new users enter archived Journeys, and progress stops for any users already in the Journey. Archived Journeys no longer 
send data to Destinations
.

Steps in archived Journeys don’t count towards your compute credits.

Journey exits and re-entry

Journey exits

You can apply exit settings to both single entry and re-entry Journeys. Users who exit a Journey leave all Journey steps and Destinations.

Configure exit settings during initial Journey setup by enabling exit settings and entering the number of days that should pass before users exit the Journey. Journeys exits users once this time passes.

If you don’t apply exit settings to a Journey, users will remain in the Journey indefinitely.

Journey re-entry

The Journeys re-entry setting allows users to repeat Journeys they’ve already exited. Common use cases for Journeys re-entry include the following:

Exit and re-entry times

To let users re-enter a Journey they’ve exited, you’ll need to enable two Journeys settings:

Journeys exits users based off of the exit time you configure. Users can re-enter the Journey once they meet the Journey’s entry condition again and your defined re-entry time has passed. You can configure re-entry time by hour, day, or week. Re-entry time begins once a user exits the Journey.

Suppose, for example, you enable re-entry for an abandoned cart campaign. You set exit to seven days and re-entry to 30 days. A user who abandons their cart will progress through the journey and exit no later than seven days after entering. Once 30 days after exit have passed, the user will immediately re-enter the journey if the user still satisfies the journey’s entry condition.

Ad-based exit settings

Exit settings you configure for the 
Show an ad step
 don’t impact other Journey steps. Users can exit an ad step but remain in the Journey.

Setting up re-entry

To enable Journey re-entry for a new Journey, follow these steps:

Drafting a Journey

When you’ve finished creating your Journey, click 
Save as Draft
 in the bottom-right corner.

About published Journeys

Keep the following in mind when working with a published Journey:

This page was last modified: 04 Dec 2024

Need support?

Questions? Problems? Need more info? Contact Segment Support for assistance!

Help improve these docs!

Was this page helpful?

Thanks for your feedback!

Can we improve this doc? 
Send us feedback!

Get started with Segment

On this page

Was this page helpful?

Thanks for your feedback!

Can we improve
 this doc? 
Send us feedback!

Product

For Developers

Company

Support

© 2025 Segment.io, Inc.

=== Content from https://segment.com/docs/connections/spec/group/ ===

Spec: Group
        

On this page

The Group API call is how you associate an individual user with a group, such as a company, organization, account, project, or team.

The Group call enables you to identify what account or organization your users are part of. There are two IDs that are relevant in a Group call: the 
userId
, which belongs and refers to the user, and the 
groupId
, which belongs and refers to the specific group. A user can be in more than one group which would mean different 
groupId
s, but the user will only have one 
userId
 that is associated to each of the different groups. Keep in mind that not all platforms support multiple groups for a single user.

Check out our high-level overview of these APIs in Segment University. (Must be logged in to access.)

In addition to the 
groupId
, which is how you’d identify the specific group or company, the group method receives traits that are specific to the group, like industry or number of employees for example, that belong to that specific account. Like the traits of an identify call, you can update these when you call the same trait with a different value.

When using the Group call, it’s helpful if you have accounts with multiple users.

Segment doesn't have an ungroup call

If you’re using a device-mode destination that has a method for ungrouping users, you can invoke it directly on the client side 
using Segment’s ready() method
.

For cloud-mode destinations, you can 
create a Destination Function
 to ungroup users.

Here’s the payload of a typical Group call, with most 
common fields
 removed:

{

  
"type"
:
 
"group"
,

  
"groupId"
:
 
"0e8c78ea9d97a7b8185e8632"
,

  
"traits"
:
 
{

    
"name"
:
 
"Initech"
,

    
"industry"
:
 
"Technology"
,

    
"employees"
:
 
329
,

    
"plan"
:
 
"enterprise"
,

    
"total billed"
:
 
830

  
}


}



And here’s the corresponding JavaScript event that would generate the above payload:

analytics
.
group
(
"
0e8c78ea9d97a7b8185e8632
"
,
 
{

  
name
:
 
"
Initech
"
,

  
industry
:
 
"
Technology
"
,

  
employees
:
 
329
,

  
plan
:
 
"
enterprise
"
,

  
"
total billed
"
:
 
830


});



Based on the library you use, the syntax in the examples might be different. You can find library-specific documentation on the 
Sources Overview
 page.

Beyond the common fields, the Group call takes the following fields:

Example

Here’s a complete example of a Group call:

{

  
"
anonymousId
"
:
 
"
507f191e810c19729de860ea
"
,

  
"
channel
"
:
 
"
browser
"
,

  
"
context
"
:
 
{

    
"
ip
"
:
 
"
8.8.8.8
"
,

    
"
userAgent
"
:
 
"
Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_5) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/40.0.2214.115 Safari/537.36
"

  
},

  
"
integrations
"
:
 
{

    
"
All
"
:
 
true
,

    
"
Mixpanel
"
:
 
false
,

    
"
Salesforce
"
:
 
false

  
},

  
"
messageId
"
:
 
"
022bb90c-bbac-11e4-8dfc-aa07a5b093db
"
,

  
"
receivedAt
"
:
 
"
2015-02-23T22:28:55.387Z
"
,

  
"
sentAt
"
:
 
"
2015-02-23T22:28:55.111Z
"
,

  
"
timestamp
"
:
 
"
2015-02-23T22:28:55.111Z
"
,

  
"
traits
"
:
 
{

    
"
name
"
:
 
"
Initech
"
,

    
"
industry
"
:
 
"
Technology
"
,

    
"
employees
"
:
 
329
,

    
"
plan
"
:
 
"
enterprise
"
,

    
"
total billed
"
:
 
830

  
},

  
"
type
"
:
 
"
group
"
,

  
"
userId
"
:
 
"
97980cfea0067
"
,

  
"
groupId
"
:
 
"
0e8c78ea9d97a7b8185e8632
"
,

  
"
version
"
:
 
"
1.1
"


}



Create your own Group call

Use the following interactive code pen to see what your Group calls would look like with user-provided information:

 Sample Group call


Sample output goes here!


Identities

The User ID is a unique identifier for the user performing the actions. Check out the 
User ID docs
 for more detail.

The Anonymous ID can be any pseudo-unique identifier, for cases where you don’t know who the user is, but you still want to tie them to an event. Check out the 
Anonymous ID docs
 for more detail.

Note: In our browser and mobile libraries a User ID is automatically added
 from the state stored by a previous 
identify
 call, so you do not need to add it yourself. They will also automatically handle Anonymous IDs under the covers.

Group ID

A Group ID is the unique identifier which you recognize a group by in your own database. For example, if you’re using MongoDB it might look something like 
507f191e810c19729de860ea
.

Traits

Traits are pieces of information you know about a group that are passed along with the Group call, like 
employees
 or 
website
.

Segment has reserved some traits that have semantic meanings for groups, and handles them in special ways. You should 
only use reserved traits for their intended meaning
.

The following are the reserved traits Segment has standardized:

Note:
 You might be used to some destinations recognizing special properties differently. For example, Mixpanel has a special 
track_charges
 method for accepting revenue. Luckily, you don’t have to worry about those inconsistencies. Just pass along 
revenue
.  
Segment handles all of the destination-specific conversions for you automatically.
 Same goes for the rest of the reserved properties.

If you pass these values, 
on null
 will throw a 
NullPointerException
.
You may continue to set values inside the trait.  If you do so, this would work the same as the rules do with NoSQL data. If you had set a value previously for a user and on the next request you sent the same value of that property as 
on null
, it will be replaced by 
null
, but if you do not send that property, the original value is persisted.

Traits are case-insensitive
, so in JavaScript you can match the rest of your camel-case code by sending 
createdAt
, and in Ruby you can match your snake-case code by sending 
created_at
. That way the API never seems alien to your code base.

This page was last modified: 23 Jul 2024

Need support?

Questions? Problems? Need more info? Contact Segment Support for assistance!

Help improve these docs!

Was this page helpful?

Thanks for your feedback!

Can we improve this doc? 
Send us feedback!

Get started with Segment

On this page

Was this page helpful?

Thanks for your feedback!

Can we improve
 this doc? 
Send us feedback!

Product

For Developers

Company

Support

© 2025 Segment.io, Inc.

=== Content from https://segment.com/docs/connections/destinations/catalog/ ===

Destination Catalog
        

Want a simpler list?

Check out the 
list of 
all
 destinations
.




      A/B Testing
      

Beta

Beta

Beta

Beta

Beta

Beta

Beta


      Advertising
      

Beta

Beta

Beta

Beta

Beta

Beta

Beta

Beta

Beta

Beta

Beta

Beta


      Analytics
      

Beta

Beta

Beta

Beta

Beta

Beta

Beta

Beta

Beta

Beta

Beta

Beta

Beta

Beta

Beta

Beta

Beta

Beta

Beta

Beta

Beta

Beta

Beta

Beta

Beta

Beta

Beta

Beta

Beta

Beta


      Attribution
      

Beta

Beta

Beta


      CRM
      

Beta

Beta

Beta

Beta

Beta

Beta

Beta

Beta

Beta


      Customer Success
      

Beta

Beta

Beta

Beta

Beta

Beta

Beta


      Deep Linking
      


      Email
      


      Email Marketing
      

Beta


      Enrichment
      

Beta

Beta

Beta

Beta

Beta


      Feature Flagging
      

Beta

Beta


      Heatmaps & Recordings
      

Beta


      Livechat
      

Beta

Beta


      Marketing Automation
      

Beta

Beta

Beta

Beta

Beta

Beta

Beta

Beta

Beta

Beta

Beta

Beta

Beta


      Performance Monitoring
      

Beta


      Personalization
      

Beta

Beta

Beta

Beta

Beta

Beta

Beta

Beta

Beta

Beta

Beta

Beta

Beta

Beta

Beta


      Raw Data
      

Beta

Beta

Beta

Beta

Beta

Beta


      Referrals
      


      SMS & Push Notifications
      

Beta


      Security & Fraud
      

Beta


      Surveys
      

Beta

Beta

Beta

Beta

Beta

Beta

Beta

Beta

Beta


      Tag Managers
      


      Video
      

Beta

On this page

Was this page helpful?

Thanks for your feedback!

Can we improve
 this doc? 
Send us feedback!

Product

For Developers

Company

Support

© 2025 Segment.io, Inc.

=== Content from https://segment.com/docs/connections/storage/data-lakes/ ===

Segment Data Lakes Overview
        

Data Lakes is available for the listed account plans only.


See the 
available plans
, or 
contact Support
.

On this page

Segment Data Lakes will enter Limited Access in October 2024

After Segment Data Lakes enters Limited Access, new customers will no longer be able to create Segment Data Lake instances. Existing Segment customers with Data Lakes instances will continue to receive data and can create Data Lakes Destinations.

Segment recommends considering alternative solutions, like 
AWS S3
 or 
Databricks
.

A 
data lake
 is a centralized cloud storage location that holds structured and unstructured data.

Data lakes typically have four layers:



Segment Data Lakes sends Segment data to a cloud data store, either AWS S3 or Azure Data Lake Storage Gen2 (ADLS), in a format optimized to reduce processing for data analytics and data science workloads. Segment data is great for building machine learning models for personalization and recommendations, and for other large scale advanced analytics. Data Lakes reduces the amount of processing required to get real value out of your data.

Segment Data Lakes deletion policies

Segment Data Lakes (AWS) and Segment Data Lakes (Azure) do not support Segment’s 
user deletion and suppression
 capabilities, as you retain your data in systems that you manage.

To learn more about Segment Data Lakes, check out the Segment blog post 
Introducing Segment Data Lakes
.

How Data Lakes work

Segment supports Data Lakes hosted on two cloud providers: Amazon Web Services (AWS) and Microsoft Azure. Each cloud provider has a similar system for managing data, but offer different query engines, post-processing systems, and analytics options.

How Segment Data Lakes (AWS) works

Data Lakes store Segment data in S3 in a read-optimized encoding format (Parquet) which makes the data more accessible and actionable. To help you zero-in on the right data, Data Lakes also creates logical data partitions and event tables, and integrates metadata with existing schema management tools, such as the AWS Glue Data Catalog. The resulting data set is optimized for use with systems like Spark, Athena, EMR, or machine learning vendors like DataBricks or DataRobot.



Segment sends data to S3 by orchestrating the processing in an EMR (Elastic MapReduce) cluster within your AWS account using an assumed role. Customers using Data Lakes own and pay AWS directly for these AWS services.



How Segment Data Lakes (Azure) works

Data Lakes store Segment data in ADLS in a read-optimized encoding format (Parquet) which makes the data more accessible and actionable. To help you zero-in on the right data, Data Lakes also creates logical data partitions and event tables, and integrates metadata with existing schema management tools, like the Hive Metastore. The resulting data set is optimized for use with systems like Power BI and Azure HDInsight or machine learning vendors like Azure Databricks or Azure Synapse Analytics.



Set up Segment Data Lakes (Azure)

For detailed Segment Data Lakes (Azure) setup instructions, see the 
Data Lakes setup page
.

Set up Segment Data Lakes (AWS)

When setting up your data lake using the 
Data Lakes catalog page
, be sure to consider the EMR and AWS IAM components listed below.

Data Lakes uses an EMR cluster to run jobs that load events from all sources into Data Lakes. The 
AWS resources portion of the set up instructions
 sets up an EMR cluster using the 
m5.xlarge
 node type. Data Lakes keeps the cluster always running, however the cluster auto-scales to ensure it’s not always running at full capacity. Check the Terraform module documentation for the 
EMR specifications
.

Data Lakes uses an IAM role to grant Segment secure access to your AWS account. The required inputs are:

Set up Segment Data Lakes (Azure)

To connect Segment Data Lakes (Azure), you must set up the following components in your Azure environment:

For more information about configuring Segment Data Lakes (Azure), see the 
Data Lakes setup page
.

Data Lakes schema

Segment Data Lakes applies a standard schema to make the raw data easier and faster to query. Partitions are applied to the S3 data for granular access to subsets of the data, schema components such as data types are inferred, and a map of the underlying data structure is stored in a Glue Database.


Segment Data Lakes (AWS) schema

Segment partitions the data in S3 by the Segment source, event type, then the day and hour an event was received by Segment, to ensure that the data is actionable and accessible.

The file path looks like:

s3://<top-level-Segment-bucket>/data/<source-id>/segment_type=<event type>/day=<YYYY-MM-DD>/hr=<HH>

Here are a few examples of what events look like:

s3:YOUR_BUCKET/segment-data/data/SOURCE_ID/segment_type=identify/day=2020-05-11/hr=11/


s3:YOUR_BUCKET/segment-data/data/SOURCE_ID/segment_type=identify/day=2020-05-11/hr=12/


s3:YOUR_BUCKET/segment-data/data/SOURCE_ID/segment_type=identify/day=2020-05-11/hr=13/

s3:YOUR_BUCKET/segment-data/data/SOURCE_ID/segment_type=page_viewed/day=2020-05-11/hr=11/


s3:YOUR_BUCKET/segment-data/data/SOURCE_ID/segment_type=page_viewed/day=2020-05-11/hr=12/


s3:YOUR_BUCKET/segment-data/data/SOURCE_ID/segment_type=page_viewed/day=2020-05-11/hr=13/

By default, the date partition structure is 
day=<YYYY-MM-DD>/hr=<HH>
 to give you granular access to the S3 data. You can change the partition structure during the 
set up process
, where you can choose from the following options:

Data Lakes stores the inferred schema and associated metadata of the S3 data in AWS Glue Data Catalog. This metadata includes the location of the S3 file, data converted into Parquet format, column names inferred from the Segment event, nested properties and traits which are now flattened, and the inferred data type.




New columns are appended to the end of the table in the Glue Data Catalog as they are detected.

The schema inferred by Segment is stored in a Glue database within Glue Data Catalog. Segment stores the schema for each source in its own Glue database to organize the data so it is easier to query. To make it easier to find, Segment writes the schema to a Glue database named using the source slug by default. The database name can be modified from the Data Lakes settings.

The recommended IAM role permissions grant Segment access to create the Glue databases on your behalf. If you do not grant Segment these permissions, you must manually create the Glue databases for Segment to write to.

Segment Data Lakes (Azure) schema

Segment Data Lakes (Azure) applies a consistent schema to make raw data accessible for queries. A transformer automatically calculates the desired schema and uploads a schema JSON file for each event type to your Azure Data Lake Storage (ADLS) in the 
/staging/
 directory.

Segment partitions the data in ALDS by the Segment source, event type, then the day and hour an event was received by Segment, to ensure that the data is actionable and accessible.

The file path looks like this:

<storage-account-name>/<container-name>/staging/<source-id>/

Data types

Data Lakes infers the data type for an event it receives. Groups of events are polled every hour to infer the data type for that each event.

The data types supported in Segment Data Lakes are:

Schema evolution

Once Data Lakes sets a data type for a column, all subsequent data will attempt to be cast into that data type. If incoming data does not match the data type, Data Lakes tries to cast the column to the target data type.

Size mismatch

If the data type in Glue is wider than the data type for a column in an on-going sync (for example, a decimal vs integer, or string vs integer), then the column is cast to the wider type in the Glue table. If the column is narrower (for example, integer in the table versus decimal in the data), the data might be dropped if it cannot be cast at all, or in the case of numbers, some data might lose precision. The original data in Segment remains in its original format, so you can fix the types and 
replay
 to ensure no data is lost. Learn more about type casting by reading the 
W3School’s Java Type Casting
 page.

Data mismatch

If Data Lakes sees a bad data type, for example text in place of a number or an incorrectly formatted date, it attempts a best effort conversion to cast the field to the target data type. Fields that cannot be cast may be dropped. You can also correct the data type in the schema to the desired type and Replay to ensure no data is lost. 
Contact Segment Support
 if you find a data type needs to be corrected.

Data Lake deduplication

In addition to Segment’s 
99% guarantee of no duplicates
 for data within a 24 hour look-back window, Data Lakes have another layer of deduplication to ensure clean data in your Data Lake. Segment removes duplicate events at the time your Data Lake ingests data.  Data Lakes deduplicate any data synced within the last seven days, based on the 
messageId
 field.

Using a Data Lake with a Data Warehouse

The Data Lakes and Warehouses products are compatible using a mapping, but do not maintain exact parity with each other. This mapping helps you to identify and manage the differences between the two storage solutions, so you can easily understand how the data in each is related. You can 
read more about the differences between Data Lakes and Warehouses
.

When you use Data Lakes, you can either use Data Lakes as your 
only
 source of data and query all of your data directly from S3 or ADLS or you can use Data Lakes in addition to a data warehouse.

FAQ

Data Lakes supports data from all event sources, including website libraries, mobile, server and event cloud sources. Data Lakes doesn’t support loading 
object cloud source data
, as well as the users and accounts tables from event cloud sources.

Are user deletions and suppression supported?

Segment doesn’t support User deletions in Data Lakes, but supports 
user suppression
.

How does Data Lakes handle schema evolution?

As the data schema evolves, both Segment Data Lakes (AWS) and Segment Data Lakes (Azure) can detect new columns and add them to Glue Data Catalog or Azure Data Lake Storage (ADLS). However, Segment can’t update existing data types. To update Segment-created data types, please reach out to 
AWS Support
 or 
Azure Support
.

How does Data Lakes work with Protocols?

Data Lakes has no direct integration with 
Protocols
.

Any changes to events at the source level made with Protocols also change the data for all downstream destinations, including Data Lakes.

Mutated events
 - If Protocols mutates an event due to a rule set in the Tracking Plan, then that mutation appears in Segment’s internal archives and reflects in your data lake. For example, if you use Protocols to mutate the event 
product_id
 to be 
productID
, then the event appears in both Data Lakes and Warehouses as 
productID
.

Blocked events
 - If a Protocols Tracking Plan blocks an event, the event isn’t forwarded to any downstream Segment destinations, including Data Lakes. However events which are only marked with a violation 
are
 passed to Data Lakes.

Data types and labels available in Protocols aren’t supported by Data Lakes.

How frequently does my Data Lake sync?

Data Lakes offers 12 syncs in a 24 hour period and doesn’t offer a custom sync schedule or selective sync.

What is the cost to use AWS Glue?

You can find details on Amazon’s 
pricing for Glue
 page. For reference, Data Lakes creates 1 table per event type in your source, and adds 1 partition per hour to the event table.

What is the cost to use Microsoft Azure?

You can find details on Microsoft’s 
pricing for Azure
 page. For reference, Data Lakes creates 1 table per event type in your source, and adds 1 partition per hour to the event table.

What limits does AWS Glue have?

AWS Glue has limits across various factors, such as number of databases per account, tables per account, and so on. See the 
full list of Glue limits
 for more information.

The most common limits to keep in mind are:

Segment stops creating new tables for the events after you exceed this limit. However you can contact your AWS account representative to increase these limits.

You should also read the 
additional considerations in Amazon’s documentation
 when using AWS Glue Data Catalog.

What analytics tools are available to use with Segment Data Lakes (Azure)?

Segment Data Lakes (Azure) supports the following analytics tools:

This page was last modified: 02 Aug 2024

Need support?

Questions? Problems? Need more info? Contact Segment Support for assistance!

Help improve these docs!

Was this page helpful?

Thanks for your feedback!

Can we improve this doc? 
Send us feedback!

Get started with Segment

On this page

Was this page helpful?

Thanks for your feedback!

Can we improve
 this doc? 
Send us feedback!

Product

For Developers

Company

Support

© 2025 Segment.io, Inc.

=== Content from https://segment.com/docs/engage/journeys/ ===

Journeys Overview
        

Engage Foundations requires a Business tier account and includes Unify.


See the 
available plans
, or 
contact Support
.

On this page

Journeys, a feature of Engage, provides a way for marketers to personalize experiences through planning how and when to engage customers with the right campaigns and messages.

Journeys enable you to define steps in a user’s journey based on event behavior and traits. You can build Journeys from your tracking events, traits, computed traits, or audiences. At each step of a journey, you can send your list of users to any Engage-compatible destination.

Get started

Start with the visual builder to define entrance criteria, build out conditional branching logic, then focus messaging to drive conversion. Repeat purchase campaigns, trial conversions, and onboarding flows are great examples to get started from. For more information, see 
Build a Journey
.

Send data to your destinations

Connect destinations to your Journey to send events or user lists when users reach the corresponding step in the Journey. For more information, see 
Send Journeys data to a Destination
.

Best practices and FAQ

For information about best practices for getting started with Journeys, and to view frequently asked questions about Journeys, see 
Best Practices and FAQ
.

Journeys use cases

See 
Examples Journeys Use Cases
 for examples of ways you can use Journeys in your marketing workflow.

Journeys glossary

For a list of key terms related to Journeys, see 
Journeys Key Terms
.

Journeys Product Limits

For information about Product Limits related to Journeys, see 
Product Limits - Journeys
.

This page was last modified: 27 Sep 2022

Need support?

Questions? Problems? Need more info? Contact Segment Support for assistance!

Help improve these docs!

Was this page helpful?

Thanks for your feedback!

Can we improve this doc? 
Send us feedback!

Get started with Segment

On this page

Was this page helpful?

Thanks for your feedback!

Can we improve
 this doc? 
Send us feedback!

Product

For Developers

Company

Support

© 2025 Segment.io, Inc.

=== Content from https://segment.com/docs/engage/campaigns/mobile-push/push-campaigns/ ===

Mobile Push Campaigns
        

Engage Premier requires a Business tier account and includes Engage Foundations and Unify.


See the 
available plans
, or 
contact Support
.

On this page

Engage Premier entered an End of Sale (EOS) period effective  June 10, 2024. Existing Segment customers will continue to have access and support to Engage Premier until an end-of-life (EOL) date is announced. We recommend exploring the following pages in preparation of a migration or future MCM needs:

Twilio Marketing Campaigns

Preferred ISV Partners:

Airship Blog
 


Bloomreach Blog
 


Braze Blog
 


Insider Blog
 


Klaviyo Blog
 


Twilio Engage Foundations Documentation
 

With Twilio Engage, you can send campaigns to users who have opted in to receive your marketing materials. On this page, you’ll learn how to create and send a mobile push campaign.

Some knowledge of the Journeys product will benefit you as you read through this guide. If you’re new to Journeys, the 
Journeys documentation
 will bring you up to speed.

How Engage campaigns work

Twilio Engage uses Journeys to send campaigns.  With Journeys, you add conditions and steps that trigger actions like sending an email, an SMS, or a mobile push.

You’ll build and then send your campaign in three stages:

Create a journey

Because Engage campaigns exist within Journeys, begin by creating a journey:

Add a Journey condition

With your Journey created, you’ll now create a 
condition
 that will trigger your campaign:

With your entry condition added, you’re now ready to create your mobile push campaign.

Create, test, and publish your mobile push campaign

Follow these steps to create a mobile push campaign:

Your mobile push campaign is now live. Users who trigger the mobile push step’s parent Journey condition will receive your campaign.

Test your mobile push template

Push tokens

Push tokens are unique identifiers Segment associates with each profile. For mobile push, you’ll need to configure identity resolution settings for the push tokens 
ios.push_token
 and 
android.push_token
. Using the Profile explorer, you can find a profile’s push tokens by opening a profile and then selecting the Identities tab. You can only send mobile pushes to profiles with push tokens enabled.

Follow these steps to test your mobile push:

Segment verifies that the profile you’re sending a test to has a push token, then sends the test. If the test mobile push doesn’t work as expected, confirm that the profile you’re sending to has a push token.

This page was last modified: 15 Jul 2024

Need support?

Questions? Problems? Need more info? Contact Segment Support for assistance!

Help improve these docs!

Was this page helpful?

Thanks for your feedback!

Can we improve this doc? 
Send us feedback!

Get started with Segment

On this page

Was this page helpful?

Thanks for your feedback!

Can we improve
 this doc? 
Send us feedback!

Product

For Developers

Company

Support

© 2025 Segment.io, Inc.

